num,label,code_len,con_num,keyword_num,keyword_fix,username,permission,ds
58588,0,746,5,0,0,lcs-crr,0,"title:BinaryFocalCrossentropy loss not working description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.9.1### Custom CodeNo### OS Platform and DistributionWindows 10### Mobile device_No response_### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version11.2### GPU model and memoryNVIDIA Quadro T2000### Current Behaviour?```shellI'm not able to declare BinaryFocalCrossentropy loss function when apply_class_balancing or alpha arguments are provided.```### Standalone code to reproduce the issue```shellimport tensorflow as tfloss = tf.keras.losses.BinaryFocalCrossentropy(    apply_class_balancing=False,    alpha=0.25,    gamma=2.0,    from_logits=False,    label_smoothing=0.0)```### Relevant log output```shellTraceback (most recent call last):  File ""C:\Users\User\AppData\Roaming\Python\Python39\site-packages\IPython\core\interactiveshell.py"", line 3251, in run_code    exec(code_obj, self.user_global_ns, self.user_ns)  File ""<ipython-input-8-235cf9a4168d>"", line 1, in <module>    loss = tf.keras.losses.BinaryFocalCrossentropy(TypeError: __init__() got an unexpected keyword argument 'apply_class_balancing'```</details>
"
58499,1,10463,10,0,0,swapnilsayansaha,0,"title:Pruning Example broken in TensorFlow 2.9.2 description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.9.2### Custom CodeNo### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version3.8.10### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version11.6### GPU model and memoryNvidia Rtx 3090, 24 GB### Current Behaviour?```shellThe pruning example (https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras) is completely broken in TF 2.9.2 (tf-model-optimization 0.7.3). The error is shown in the log output. The error is reproducible using the exact same code provided in the pruning with keras guide.N.B. For another model on the CIFAR10 dataset, I get an inaccessible tensor error. The error there happens in the first conv layer. I thought I did something wrong so tried the exact example code but looks like something is wrong inside Tensorflow itself.```### Standalone code to reproduce the issue```shellhttps://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras```### Relevant log output```shellUnknownError                              Traceback (most recent call last)Cell In [5], line 8      1 logdir = tempfile.mkdtemp()      3 callbacks = [      4   tfmot.sparsity.keras.UpdatePruningStep(),      5   tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),      6 ]----> 8 model_for_pruning.fit(train_images, train_labels,      9                   batch_size=batch_size, epochs=epochs, validation_split=validation_split,     10                   callbacks=callbacks)File ~/swapnil_debug_2/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67, in filter_traceback.<locals>.error_handler(*args, **kwargs)     65 except Exception as e:  # pylint: disable=broad-except     66   filtered_tb = _process_traceback_frames(e.__traceback__)---> 67   raise e.with_traceback(filtered_tb) from None     68 finally:     69   del filtered_tbFile ~/swapnil_debug_2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)     52 try:     53   ctx.ensure_initialized()---> 54   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,     55                                       inputs, attrs, num_outputs)     56 except core._NotOkStatusException as e:     57   if name is not None:UnknownError: Graph execution error:Detected at node 'sequential/prune_low_magnitude_conv2d/FloorMod' defined at (most recent call last):    File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main      return _run_code(code, main_globals, None,    File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code      exec(code, run_globals)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/ipykernel_launcher.py"", line 17, in <module>      app.launch_new_instance()    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/traitlets/config/application.py"", line 982, in launch_instance      app.start()    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/ipykernel/kernelapp.py"", line 712, in start      self.io_loop.start()    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/tornado/platform/asyncio.py"", line 215, in start      self.asyncio_loop.run_forever()    File ""/usr/lib/python3.8/asyncio/base_events.py"", line 570, in run_forever      self._run_once()    File ""/usr/lib/python3.8/asyncio/base_events.py"", line 1859, in _run_once      handle._run()    File ""/usr/lib/python3.8/asyncio/events.py"", line 81, in _run      self._context.run(self._callback, *self._args)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/ipykernel/kernelbase.py"", line 510, in dispatch_queue      await self.process_one()    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/ipykernel/kernelbase.py"", line 499, in process_one      await dispatch(*args)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/ipykernel/kernelbase.py"", line 406, in dispatch_shell      await result    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/ipykernel/kernelbase.py"", line 730, in execute_request      reply_content = await reply_content    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/ipykernel/ipkernel.py"", line 383, in do_execute      res = shell.run_cell(    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/ipykernel/zmqshell.py"", line 528, in run_cell      return super().run_cell(*args, **kwargs)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/IPython/core/interactiveshell.py"", line 2940, in run_cell      result = self._run_cell(    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/IPython/core/interactiveshell.py"", line 2995, in _run_cell      return runner(coro)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner      coro.send(None)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/IPython/core/interactiveshell.py"", line 3194, in run_cell_async      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/IPython/core/interactiveshell.py"", line 3373, in run_ast_nodes      if await self.run_code(code, result, async_=asy):    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/IPython/core/interactiveshell.py"", line 3433, in run_code      exec(code_obj, self.user_global_ns, self.user_ns)    File ""/tmp/ipykernel_2426111/471826281.py"", line 8, in <module>      model_for_pruning.fit(train_images, train_labels,    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 64, in error_handler      return fn(*args, **kwargs)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/keras/engine/training.py"", line 1409, in fit      tmp_logs = self.train_function(iterator)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/keras/engine/training.py"", line 1051, in train_function      return step_function(self, iterator)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/keras/engine/training.py"", line 1040, in step_function      outputs = model.distribute_strategy.run(run_step, args=(data,))    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/keras/engine/training.py"", line 1030, in run_step      outputs = model.train_step(data)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/keras/engine/training.py"", line 889, in train_step      y_pred = self(x, training=True)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 64, in error_handler      return fn(*args, **kwargs)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/keras/engine/training.py"", line 490, in __call__      return super().__call__(*args, **kwargs)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 64, in error_handler      return fn(*args, **kwargs)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/keras/engine/base_layer.py"", line 1014, in __call__      outputs = call_fn(inputs, *args, **kwargs)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 92, in error_handler      return fn(*args, **kwargs)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/keras/engine/sequential.py"", line 374, in call      return super(Sequential, self).call(inputs, training=training, mask=mask)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/keras/engine/functional.py"", line 458, in call      return self._run_internal_graph(    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/keras/engine/functional.py"", line 596, in _run_internal_graph      outputs = node.layer(*args, **kwargs)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 64, in error_handler      return fn(*args, **kwargs)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/keras/engine/base_layer.py"", line 1014, in __call__      outputs = call_fn(inputs, *args, **kwargs)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 92, in error_handler      return fn(*args, **kwargs)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py"", line 280, in call      update_mask = utils.smart_cond(training, add_update, no_op)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/keras/utils.py"", line 50, in smart_cond      if isinstance(pred, variables.Variable):    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/keras/utils.py"", line 54, in smart_cond      pred, true_fn=true_fn, false_fn=false_fn, name=name)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py"", line 268, in add_update      with tf.control_dependencies(    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_impl.py"", line 310, in conditional_mask_update      return tf.distribute.get_replica_context().merge_call(    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_impl.py"", line 307, in mask_update_distributed      return tf.cond(maybe_update_masks(), update_distributed, no_update)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_impl.py"", line 260, in maybe_update_masks      if self._sparsity_m_by_n:    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_impl.py"", line 264, in maybe_update_masks      return self._pruning_schedule(self._step_fn())[0]    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_schedule.py"", line 246, in __call__      sparsity)    File ""/home/nesl/swapnil_debug_2/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_schedule.py"", line 61, in _should_prune_in_step      is_pruning_turn = tf.math.equal(Node: 'sequential/prune_low_magnitude_conv2d/FloorMod'JIT compilation failed.	 [[{{node sequential/prune_low_magnitude_conv2d/FloorMod}}]] [Op:__inference_train_function_34086]```</details>
"
58485,1,1295,18,0,0,kennysong,0,"title:The tutorial Keras SavedModels from TF 2.7 do not load in TF 2.8, 2.9, 2.10 description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow VersionTF 2.7.0 to 2.10.0### Custom CodeNo### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?I ran the official Keras tutorial ""[Structured data classification from scratch](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/)"" in TF 2.7, and saved the model as a SavedModel.This SavedModel successfully loads in TF 2.7, but cannot be loaded TF 2.8, 2.9, or 2.10. Error message:```File ~/.../regressor.py:337, in load_model(filepath, library)--> 337             model = tf.keras.models.load_model(extract_dir)File ~/.../env/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67, in filter_traceback.<locals>.error_handler(*args, **kwargs)     65 except Exception as e:  # pylint: disable=broad-except     66   filtered_tb = _process_traceback_frames(e.__traceback__)---> 67   raise e.with_traceback(filtered_tb) from None     68 finally:     69   del filtered_tbFile ~/.../env/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:56, in _SatisfiesTypeConstraint(dtype, attr_def, param_name)     54 allowed_values = "", "".join(dtypes.as_dtype(x).name for x in allowed_list)     55 if dtype not in allowed_list:---> 56   raise TypeError(     57       f""Value passed to parameter '{param_name}' has DataType ""     58       f""{dtypes.as_dtype(dtype).name} not in list of allowed values: ""     59       f""{allowed_values}"")TypeError: Exception encountered when calling layer ""string_lookup"" (type StringLookup).Value passed to parameter 'weights' has DataType string not in list of allowed values: int32, int64, float32, float64Call arguments received:  闂?inputs=tf.Tensor(shape=(None, 1), dtype=string)```Based my my tests, here's a compatibility table for saving/loading this model across TF versions:|                    | Load with TF 2.6 | Load with TF 2.7 | Load with TF 2.8 | Load with TF 2.9 | Load with TF 2.10 ||--------------------|------------------|------------------|------------------|------------------|-------------------|| Saved with TF 2.6  | 闂?               | 闂?               | 闂?               | 闂?               | 闂?                || Saved with TF 2.7  | 闂?               | 闂?               | 闂?               | 闂?               | 闂?                || Saved with TF 2.8  | 闂?               | 闂?               | 闂?               | 闂?               | 闂?                || Saved with TF 2.9  | 闂?               | 闂?               | 闂?               | 闂?               | 闂?                || Saved with TF 2.10 | 闂?               | 闂?               | 闂?               | 闂?               | 闂?                |### Standalone code to reproduce the issueColab to train and save the Keras tutorial model: https://colab.research.google.com/drive/1c-UDWPW0OQjbgItijs_4UB4sm-dQ7MdS?usp=sharingColab to load the Keras tutorial SavedModel: https://colab.research.google.com/drive/1KM9nUBdsqGFW5zXUuU7b2ZtcKA1BiUXz?usp=sharing### Relevant log output_No response_</details>
"
58479,1,1868,0,0,0,fuzzyswan,0,"title:`tf.math.pow()` is inaccurate with some small integer values presented by tf.float32 on GPU description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.9.2### Custom CodeYes### OS Platform and DistributionColab### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version11.2.152, 8.1.1### GPU model and memory_No response_### Current Behaviour?```shelltf.math.pow() loses accuracy when computing squares of small integer values represented by tf.float32 on GPU, such as 5.0^2.0,  7.0^2.0 and 13.0^2.0. What make this strange are:- This is not happening on CPU- The integer values are very small- This is not happening on tf.float64```### Standalone code to reproduce the issue```shellints = tf.convert_to_tensor(np.arange(20, dtype=np.int32), dtype=tf.int32)floats = tf.cast(ints, dtype=tf.float32)doubles = tf.cast(ints, dtype=tf.float64)with tf.device(""/cpu:0""):  print(""cpu ints^2"", tf.math.pow(ints, 2))  print(""cpu float32^2"", tf.math.pow(floats, 2.))  print(""cpu float64^2"", tf.math.pow(doubles, 2.))with tf.device(""/gpu:0""):  print(""gpu ints^2"", tf.math.pow(ints, 2))  print(""INACCURATE gpu float32^2"", tf.math.pow(floats, 2.))  print(""gpu float64^2"", tf.math.pow(doubles, 2.))```### Relevant log output```shellcpu ints^2 tf.Tensor([  0   1   4   9  16  25  36  49  64  81 100 121 144 169 196 225 256 289 324 361], shape=(20,), dtype=int32)cpu float32^2 tf.Tensor([  0.   1.   4.   9.  16.  25.  36.  49.  64.  81. 100. 121. 144. 169. 196. 225. 256. 289. 324. 361.], shape=(20,), dtype=float32)cpu float64^2 tf.Tensor([  0.   1.   4.   9.  16.  25.  36.  49.  64.  81. 100. 121. 144. 169. 196. 225. 256. 289. 324. 361.], shape=(20,), dtype=float64)gpu ints^2 tf.Tensor([  0   1   4   9  16  25  36  49  64  81 100 121 144 169 196 225 256 289 324 361], shape=(20,), dtype=int32)INACCURATE gpu float32^2 tf.Tensor([  0.         1.         4.         9.        16.        24.999998  36.        48.999996  64.        81.        99.99999  121. 144.       169.       196.       225.       256.       288.99997 324.       361.      ], shape=(20,), dtype=float32)gpu float64^2 tf.Tensor([  0.   1.   4.   9.  16.  25.  36.  49.  64.  81. 100. 121. 144. 169. 196. 225. 256. 289. 324. 361.], shape=(20,), dtype=float64)```</details>
"
58472,0,3115,269,0,0,elfringham,0,"title://tensorflow/python/data/experimental/kernel_tests/service/cross_trainer_cache_test fails on high CPU count description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiongit HEAD### Custom CodeNo### OS Platform and DistributionUbuntu 20.04### Mobile devicen/a### Python version3.8.10### Bazel version5.3.0### GCC/Compiler version10.2.1### CUDA/cuDNN versionn/a### GPU model and memoryn/a### Current Behaviour?```shellOn a machine with a high CPU core count, //tensorflow/python/data/experimental/kernel_tests/service:cross_trainer_cache_test will fail due to prefetching more data than the test allows for.```### Standalone code to reproduce the issue```shellbazel test --test_timeout=500,900,-1,-1 --flaky_test_attempts=1 --test_output=all --cache_test_results=no --config=nonccl --config=mkl_aarch64_threadpool --copt=-mtune=generic --copt=-march=armv8-a --copt=-O3 --test_env=TF_ENABLE_ONEDNN_OPTS=1 --verbose_failures --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64 --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64 --build_tests_only --test_lang_filters=cc,py -- //tensorflow/python/data/experimental/kernel_tests/service:cross_trainer_cache_test```### Relevant log output```shell======================================================================FAIL: testConcurrentReaders_test_mode_graph_tfapiversion_2 (__main__.CrossTrainerCacheTest)CrossTrainerCacheTest.testConcurrentReaders_test_mode_graph_tfapiversion_2testConcurrentReaders_test_mode_graph_tfapiversion_2(mode='graph', tf_api_version=2)----------------------------------------------------------------------Traceback (most recent call last):  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/cross_trainer_cache_test.runfiles/absl_py/absl/testing/parameterized.py"", line 314, in bound_param_test    return test_method(self, **testcase_params)  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/cross_trainer_cache_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 360, in decorated    execute_test_method()  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/cross_trainer_cache_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 343, in execute_test_method    test_method(**kwargs_to_pass)  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/cross_trainer_cache_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/cross_trainer_cache_test.py"", line 136, in testConcurrentReaders    self.assertEqual(self.evaluate(iterators[j]()), i)AssertionError: 26 != 0----------------------------------------------------------------------Ran 9 tests in 4.935sFAILED (failures=1)2022-11-07 18:30:46.995018: I tensorflow/core/data/service/server_lib.cc:91] Shut down WorkerServer server running at port 463332022-11-07 18:30:46.995434: I tensorflow/core/data/service/server_lib.cc:91] Shut down DispatchServer server running at port 46249================================================================================```</details>
"
58463,1,33092,7,0,1,OverLordGoldDragon,0,"title:`tf.constant(1)` maxes VRAM description: ### Issue TypeBug### Sourcebinary### Tensorflow Version2.10.0### Custom CodeYes### OS Platform and DistributionWindows 10### Mobile device_No response_### Python version3.10.4### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versioncudatoolkit 11.3.1### GPU model and memoryGTX 1070, 8GB### Current Behaviour?7.5GB VRAM used until kernel is restarted. Removing `import torch` fixes this.`torch` installed with `conda`, `tensorflow-gpu` with `pip`.  <b>conda list</b>```# Name                    Version                   Build  Channelabsl-py                   1.0.0              pyhd8ed1ab_0    conda-forgeaiohttp                   3.8.1           py310he2412df_1    conda-forgeaiosignal                 1.2.0              pyhd8ed1ab_0    conda-forgealabaster                 0.7.12                     py_0    anacondaaom                       3.3.0                h0e60522_1    conda-forgeappdirs                   1.4.4              pyh9f0ad1d_0    conda-forgeargh                      0.26.2          pyh9f0ad1d_1002    conda-forgeargon2-cffi               21.3.0             pyhd8ed1ab_0    conda-forgeargon2-cffi-bindings      21.2.0          py310he2412df_2    conda-forgeargunparse                0.1.2                    pypi_0    pypiarrow                     1.2.2              pyhd8ed1ab_0    conda-forgeastroid                   2.11.2          py310h5588dad_1    conda-forgeasttokens                 2.0.5              pyhd8ed1ab_0    conda-forgeastunparse                1.6.3              pyhd8ed1ab_0    conda-forgeasync-timeout             4.0.2              pyhd8ed1ab_0    conda-forgeatomicwrites              1.4.0              pyh9f0ad1d_0    conda-forgeattrs                     21.4.0             pyhd8ed1ab_0    conda-forgeaudioread                 2.1.9           py310h5588dad_3    conda-forgeautopep8                  1.6.0              pyhd8ed1ab_1    conda-forgebabel                     2.9.1              pyh44b312d_0    conda-forgebackcall                  0.2.0              pyh9f0ad1d_0    conda-forgebackports                 1.1                pyhd3eb1b0_0    anacondabackports.functools_lru_cache 1.6.4              pyhd8ed1ab_0    conda-forgebcrypt                    3.2.0           py310he2412df_3    conda-forgebinaryornot               0.4.4                      py_1    conda-forgeblack                     22.10.0         py310h5588dad_0    conda-forgeblas                      1.0                         mkl    anacondableach                    4.1.0              pyhd8ed1ab_0    conda-forgeblinker                   1.4                        py_1    conda-forgeblosc                     1.21.0               h0e60522_0    conda-forgebrotli                    1.0.9                h8ffe710_7    conda-forgebrotli-bin                1.0.9                h8ffe710_7    conda-forgebrotlipy                  0.7.0           py310he2412df_1004    conda-forgebzip2                     1.0.8                h8ffe710_4    conda-forgec-blosc2                  2.0.4                h09319c2_1    conda-forgeca-certificates           2022.9.24            h5b45459_0    conda-forgecached-property           1.5.2                hd8ed1ab_1    conda-forgecached_property           1.5.2              pyha770c72_1    conda-forgecachetools                5.0.0              pyhd8ed1ab_0    conda-forgecairo                     1.16.0            h15b3021_1010    conda-forgecertifi                   2022.9.24          pyhd8ed1ab_0    conda-forgecffi                      1.15.0          py310hcbf9ad4_0    conda-forgecfitsio                   4.1.0                h5a969a9_0    conda-forgechardet                   4.0.0           py310h5588dad_3    conda-forgecharls                    2.3.4                h39d44d4_0    conda-forgecharset-normalizer        2.0.11             pyhd8ed1ab_0    conda-forgeclick                     8.1.3           py310h5588dad_0    conda-forgecloudpickle               2.0.0              pyhd8ed1ab_0    conda-forgecolorama                  0.4.4              pyh9f0ad1d_0    conda-forgecommonmark                0.9.1                      py_0    anacondaconda                     4.12.0          py310h5588dad_0    conda-forgeconda-package-handling    1.8.1           py310h4f637d6_1    conda-forgeconfigparser              5.2.0              pyhd8ed1ab_0    conda-forgecontourpy                 1.0.5           py310h232114e_0    conda-forgecookiecutter              1.7.3              pyh6c4a22f_1    conda-forgecoverage                  6.5.0                    pypi_0    pypicryptography              36.0.2          py310ha857299_1    conda-forgecudatoolkit               11.3.1               h59b6b97_2    anacondacycler                    0.11.0             pyhd8ed1ab_0    conda-forgecython                    0.29.32         py310h00ffb61_1    conda-forgecytoolz                   0.11.2          py310he2412df_2    conda-forgedask-core                 2022.4.0           pyhd8ed1ab_0    conda-forgedataclasses               0.8                pyhc8e2a94_3    conda-forgedebugpy                   1.5.1           py310h8a704f9_1    conda-forgedecorator                 4.4.2                      py_0    anacondadefusedxml                0.7.1              pyhd8ed1ab_0    conda-forgedeprecated                1.2.13             pyh6c4a22f_0    conda-forgediff-match-patch          20200713           pyh9f0ad1d_0    conda-forgedill                      0.3.4              pyhd8ed1ab_0    conda-forgedoce                      0.2.dev0                  dev_0    <develop>docker-pycreds            0.4.0                      py_0    anacondadocutils                  0.17.1          py310h5588dad_1    conda-forgedummm                     0.1.2                    pypi_0    pypidumspin                   0.1.2                    pypi_0    pypientrypoints               0.4                pyhd8ed1ab_0    conda-forgeexceptiongroup            1.0.0              pyhd8ed1ab_0    conda-forgeexecuting                 0.8.2              pyhd8ed1ab_0    conda-forgeexpat                     2.5.0                h1537add_0    conda-forgeffmpeg                    4.3.1                ha925a31_0    conda-forgefire                      0.4.0              pyh44b312d_0    conda-forgeflake8                    4.0.1              pyhd8ed1ab_2    conda-forgeflatbuffers               2.0                      pypi_0    pypiflit-core                 3.7.1              pyhd8ed1ab_0    conda-forgefont-ttf-dejavu-sans-mono 2.37                 hab24e00_0    conda-forgefont-ttf-inconsolata      3.000                h77eed37_0    conda-forgefont-ttf-source-code-pro  2.038                h77eed37_0    conda-forgefont-ttf-ubuntu           0.83                 hab24e00_0    conda-forgefontconfig                2.14.0               hce3cb01_0    conda-forgefonts-conda-ecosystem     1                             0    conda-forgefonts-conda-forge         1                             0    conda-forgefonttools                 4.29.1          py310he2412df_0    conda-forgefreetype                  2.12.1               h546665d_0    conda-forgefribidi                   1.0.10               h8d14728_0    conda-forgefrozenlist                1.3.0           py310he2412df_1    conda-forgefsspec                    2022.1.0           pyhd8ed1ab_0    conda-forgefuture                    0.18.2          py310h5588dad_5    conda-forgegast                      0.4.0                    pypi_0    pypigetopt-win32              0.1                  h8ffe710_0    conda-forgegettext                   0.19.8.1          ha2e2712_1008    conda-forgegiflib                    5.2.1                h8d14728_2    conda-forgegin-config                0.5.0                    pypi_0    pypigitdb                     4.0.7              pyhd3eb1b0_0    anacondagitpython                 3.1.27             pyhd8ed1ab_0    conda-forgegoogle-auth               2.6.0              pyh6c4a22f_1    conda-forgegoogle-auth-oauthlib      0.4.6              pyhd8ed1ab_0    conda-forgegoogle-pasta              0.2.0              pyh8c360ce_0    conda-forgegprof2dot                 2022.7.29                pypi_0    pypigraphite2                 1.3.13                     1000    conda-forgegraphviz                  4.0.0                had6c3a3_0    conda-forgegrpcio                    1.43.0          py310ha05212b_0    conda-forgegts                       0.7.6                h7c369d9_2    conda-forgeh5py                      3.6.0           nompi_py310h00cbb18_100    conda-forgeharfbuzz                  4.2.0                hc1763ed_0    conda-forgehdf5                      1.12.1          nompi_h2a0e4a3_104    conda-forgeicu                       69.1                 h0e60522_0    conda-forgeidna                      3.3                pyhd8ed1ab_0    conda-forgeimagecodecs               2022.2.22       py310h7653d06_3    conda-forgeimageio                   2.15.0             pyhcf75d05_0    conda-forgeimageio-ffmpeg            0.4.5              pyhd8ed1ab_0    conda-forgeimagesize                 1.3.0              pyhd8ed1ab_0    conda-forgeimportlib-metadata        4.11.3          py310h5588dad_1    conda-forgeimportlib-resources       3.3.1              pyhd8ed1ab_0    conda-forgeimportlib_metadata        4.11.3               hd8ed1ab_1    conda-forgeimportlib_resources       5.6.0              pyhd8ed1ab_0    conda-forgeinflection                0.5.1              pyh9f0ad1d_0    conda-forgeiniconfig                 1.1.1              pyh9f0ad1d_0    conda-forgeintel-openmp              2022.0.0          h57928b3_3663    conda-forgeintervaltree              3.1.0                      py_0    anacondaipykernel                 6.12.1          py310hbbfc1a7_0    conda-forgeipython                   7.32.0          py310h5588dad_0    conda-forgeipython_genutils          0.2.0                      py_1    conda-forgeipywidgets                7.6.5              pyhd8ed1ab_0    conda-forgeisort                     5.10.1             pyhd8ed1ab_0    conda-forgejams                      0.3.4                    pypi_0    pypijbig                      2.1               h8d14728_2003    conda-forgejedi                      0.18.1          py310h5588dad_1    conda-forgejellyfish                 0.9.0           py310he2412df_1    conda-forgejinja2                    3.0.3              pyhd8ed1ab_0    conda-forgejinja2-time               0.2.0              pyhd8ed1ab_3    conda-forgejoblib                    1.1.0              pyhd8ed1ab_0    conda-forgejpeg                      9e                   h8ffe710_0    conda-forgejsonschema                4.4.0              pyhd8ed1ab_0    conda-forgejupyter                   1.0.0           py310h5588dad_7    conda-forgejupyter_client            7.2.2              pyhd8ed1ab_1    conda-forgejupyter_console           6.4.3              pyhd8ed1ab_0    conda-forgejupyter_core              4.9.2           py310h5588dad_0    conda-forgejupyterlab_pygments       0.2.0              pyhd8ed1ab_0    conda-forgejupyterlab_widgets        1.1.0              pyhd8ed1ab_0    conda-forgejxrlib                    1.1                  hfa6e2cd_2    conda-forgekaleido-core              0.2.1                h8ffe710_0    conda-forgekeras                     2.10.0                   pypi_0    pypikeras-preprocessing       1.1.2                    pypi_0    pypikeyring                   23.4.0          py310h5588dad_2    conda-forgekiwisolver                1.3.2           py310h476a331_1    conda-forgekrb5                      1.19.3               h1176d77_0    conda-forgelazy-object-proxy         1.7.1           py310he2412df_1    conda-forgelcms2                     2.12                 h2a16943_0    conda-forgelerc                      3.0                  h0e60522_0    conda-forgelibaec                    1.0.6                h39d44d4_0    conda-forgelibarchive                3.5.2                hb45042f_1    conda-forgelibavif                   0.10.0               h8ffe710_1    conda-forgelibblas                   3.9.0              12_win64_mkl    conda-forgelibbrotlicommon           1.0.9                h8ffe710_7    conda-forgelibbrotlidec              1.0.9                h8ffe710_7    conda-forgelibbrotlienc              1.0.9                h8ffe710_7    conda-forgelibcblas                  3.9.0              12_win64_mkl    conda-forgelibclang                  13.0.0                   pypi_0    pypilibcurl                   7.82.0               h789b8ee_0    conda-forgelibdeflate                1.10                 h8ffe710_0    conda-forgelibffi                    3.4.2                h8ffe710_5    conda-forgelibflac                   1.3.4                h0e60522_0    conda-forgelibgd                     2.3.3                h217ff3b_2    conda-forgelibglib                   2.72.1               h3be07f2_0    conda-forgelibiconv                  1.16                 he774522_0    conda-forgeliblapack                 3.9.0              12_win64_mkl    conda-forgelibmamba                  0.22.1               h81a967f_1    conda-forgelibmambapy                0.22.1          py310hd80b381_1    conda-forgelibogg                    1.3.5                h2bbff1b_1    anacondalibopus                   1.3.1                h8ffe710_1    conda-forgelibpng                    1.6.37               ha81a0f5_2    conda-forgelibprotobuf               3.19.4               h7755175_0    conda-forgelibrosa                   0.9.0              pyhd8ed1ab_0    conda-forgelibsndfile                1.0.31               h0e60522_1    conda-forgelibsodium                 1.0.18               h62dcd97_1    conda-forgelibsolv                   0.7.20               h23ce68f_0    anacondalibspatialindex           1.9.3                h39d44d4_4    conda-forgelibssh2                   1.10.0               h680486a_2    conda-forgelibtiff                   4.3.0                hc4061b1_3    conda-forgelibuv                     1.40.0               he774522_0    anacondalibvorbis                 1.3.7                ha925a31_0    conda-forgelibwebp                   1.2.2                h57928b3_0    conda-forgelibwebp-base              1.2.2                h8ffe710_1    conda-forgelibxcb                    1.13              hcd874cb_1004    conda-forgelibxml2                   2.9.12               hf5bbc77_2    conda-forgelibzlib                   1.2.13               hcfcfb64_4    conda-forgelibzopfli                 1.0.3                ha925a31_0    anacondallvmlite                  0.38.0          py310h2c03ce5_1    conda-forgelocket                    0.2.1           py310haa95532_2    anacondalz4-c                     1.9.3                h8ffe710_1    conda-forgelzo                       2.10              hfa6e2cd_1000    conda-forgem2r2                      0.3.3              pyhd8ed1ab_0    conda-forgem2w64-gcc-libgfortran     5.3.0                         6    conda-forgem2w64-gcc-libs            5.3.0                         7    conda-forgem2w64-gcc-libs-core       5.3.0                         7    conda-forgem2w64-gmp                 6.1.0                         2    conda-forgem2w64-libwinpthread-git   5.0.0.4634.697f757               2    conda-forgemamba                     0.22.1          py310h9376f3e_1    conda-forgemarkdown                  3.3.6              pyhd8ed1ab_0    conda-forgemarkupsafe                2.0.1           py310he2412df_1    conda-forgemathjax                   2.7.7                h57928b3_3    conda-forgematplotlib                3.6.1           py310h5588dad_0    conda-forgematplotlib-base           3.6.1           py310h51140c5_0    conda-forgematplotlib-inline         0.1.3              pyhd8ed1ab_0    conda-forgemccabe                    0.6.1                      py_1    conda-forgemenuinst                  1.4.18          py310h5588dad_1    conda-forgemido                      1.2.10             pyhd8ed1ab_0    conda-forgemir-eval                  0.7                      pypi_0    pypimirdata                   0.3.6                    pypi_0    pypimistune                   0.8.4           py310he2412df_1005    conda-forgemkl                       2021.4.0           h0e2418a_729    conda-forgemkl-service               2.4.0           py310hcf6e17e_0    conda-forgemkl_fft                   1.3.1           py310ha0764ea_0    anacondamkl_random                1.2.2           py310hc02be91_0    anacondamoviepy                   1.0.1                      py_0    conda-forgemsys2-conda-epoch         20160418                      1    conda-forgemultidict                 6.0.2           py310he2412df_1    conda-forgemunkres                   1.1.4              pyh9f0ad1d_0    conda-forgemypy_extensions           0.4.3           py310h5588dad_5    conda-forgenbclient                  0.5.10             pyhd8ed1ab_1    conda-forgenbconvert                 6.4.1           py310h5588dad_0    conda-forgenbformat                  5.1.3              pyhd8ed1ab_0    conda-forgenbsphinx                  0.8.9                    pypi_0    pypinest-asyncio              1.5.4              pyhd8ed1ab_0    conda-forgenetworkx                  2.6.3              pyhd8ed1ab_1    conda-forgennaudio                   0.3.1                    pypi_0    pypinotebook                  6.4.8              pyha770c72_0    conda-forgenumba                     0.55.0          py310h4ed8f06_0    anacondanumexpr                   2.8.1           py310hb57aa6b_1    anacondanumpy                     1.21.5                   pypi_0    pypinumpydoc                  1.2.1              pyhd8ed1ab_2    conda-forgeoauthlib                  3.2.0              pyhd8ed1ab_0    conda-forgeopencv-python             4.6.0.66                 pypi_0    pypiopenjpeg                  2.4.0                hb211442_1    conda-forgeopenssl                   1.1.1s               hcfcfb64_0    conda-forgeopt-einsum                3.3.0                    pypi_0    pypipackaging                 21.3               pyhd8ed1ab_0    conda-forgepandas                    1.4.0           py310hf5e1058_0    conda-forgepandoc                    2.17.1.1             h57928b3_0    conda-forgepandocfilters             1.5.0              pyhd8ed1ab_0    conda-forgepango                     1.50.7               h66df5b2_0    conda-forgeparamiko                  2.10.3             pyhd8ed1ab_0    conda-forgeparso                     0.8.3              pyhd8ed1ab_0    conda-forgepartd                     1.2.0              pyhd8ed1ab_0    conda-forgepathspec                  0.9.0              pyhd8ed1ab_0    conda-forgepathtools                 0.1.2                      py_1    anacondapatsy                     0.5.2              pyhd8ed1ab_0    conda-forgepcre                      8.45                 h0e60522_0    conda-forgepexpect                   4.8.0              pyh9f0ad1d_2    conda-forgepickleshare               0.7.5                   py_1003    conda-forgepillow                    9.0.1           py310h767b3fd_2    conda-forgepip                       22.1.2             pyhd8ed1ab_0    conda-forgepixman                    0.40.0               h8ffe710_0    conda-forgeplatformdirs              2.4.1              pyhd8ed1ab_1    conda-forgeplotly                    5.10.0                     py_0    plotlypluggy                    1.0.0           py310h5588dad_3    conda-forgepooch                     1.6.0              pyhd8ed1ab_0    conda-forgepoyo                      0.5.0                      py_0    conda-forgepretty-midi               0.2.9                    pypi_0    pypiproglog                   0.1.9                      py_0    conda-forgeprometheus_client         0.14.1             pyhd8ed1ab_0    conda-forgepromise                   2.3             py310h5588dad_5    conda-forgeprompt-toolkit            3.0.27             pyha770c72_0    conda-forgeprompt_toolkit            3.0.27               hd8ed1ab_0    conda-forgeprotobuf                  3.19.4          py310h8a704f9_0    conda-forgepsutil                    5.9.0           py310he2412df_1    conda-forgepthread-stubs             0.4               hcd874cb_1001    conda-forgeptyprocess                0.7.0              pyhd3deb0d_0    conda-forgepy                        1.11.0             pyh6c4a22f_0    conda-forgepyasn1                    0.4.8                      py_0    anacondapyasn1-modules            0.2.8                      py_0    anacondapybind11-abi              4                    hd8ed1ab_3    conda-forgepycodestyle               2.8.0              pyhd8ed1ab_0    conda-forgepycosat                   0.6.3           py310he2412df_1010    conda-forgepycparser                 2.21               pyhd8ed1ab_0    conda-forgepydeprecate               0.3.1              pyhd8ed1ab_0    conda-forgepydocstyle                6.1.1              pyhd8ed1ab_0    conda-forgepyflakes                  2.4.0              pyhd8ed1ab_0    conda-forgepygments                  2.11.2             pyhd8ed1ab_0    conda-forgepyjwt                     2.3.0              pyhd8ed1ab_1    conda-forgepylint                    2.13.5             pyhd8ed1ab_0    conda-forgepyls-spyder               0.4.0              pyhd8ed1ab_0    conda-forgepynacl                    1.5.0           py310h4f637d6_1    conda-forgepyopenssl                 22.0.0             pyhd8ed1ab_0    conda-forgepyparsing                 3.0.7              pyhd8ed1ab_0    conda-forgepyqt                      5.12.3          py310h5588dad_8    conda-forgepyqt-impl                 5.12.3          py310h8a704f9_8    conda-forgepyqt5-sip                 4.19.18         py310h8a704f9_8    conda-forgepyqtchart                 5.12            py310h8a704f9_8    conda-forgepyqtwebengine             5.12.1          py310h8a704f9_8    conda-forgepyrsistent                0.18.1          py310he2412df_1    conda-forgepysimplegui               4.60.3                   pypi_0    pypipysocks                   1.7.1           py310h5588dad_5    conda-forgepysoundfile               0.10.3.post1       pyhd3deb0d_0    conda-forgepytest                    7.2.0              pyhd8ed1ab_2    conda-forgepytest-cov                4.0.0                    pypi_0    pypipytest-profiling          1.7.0                    pypi_0    pypipython                    3.10.4          h9a09f29_0_cpython    conda-forgepython-dateutil           2.8.2              pyhd8ed1ab_0    conda-forgepython-kaleido            0.2.1              pyhd8ed1ab_0    conda-forgepython-lsp-black          1.2.1              pyhd8ed1ab_0    conda-forgepython-lsp-jsonrpc        1.0.0              pyhd8ed1ab_0    conda-forgepython-lsp-server         1.4.1              pyhd8ed1ab_1    conda-forgepython-slugify            6.1.1              pyhd8ed1ab_0    conda-forgepython_abi                3.10                    2_cp310    conda-forgepythonmagick              0.9.19                   pypi_0    pypipytorch                   1.11.0          py3.10_cuda11.3_cudnn8_0    pytorchpytorch-lightning         1.7.7              pyhd8ed1ab_0    conda-forgepytorch-mutex             1.0                        cuda    pytorchpytz                      2021.3             pyhd8ed1ab_0    conda-forgepyu2f                     0.1.5              pyhd8ed1ab_0    conda-forgepywavelets                1.2.0           py310h2873277_1    conda-forgepywin32                   303             py310he2412df_0    conda-forgepywin32-ctypes            0.2.0           py310h5588dad_1005    conda-forgepywinpty                  2.0.5           py310h00ffb61_1    conda-forgepyyaml                    6.0             py310he2412df_4    conda-forgepyzmq                     22.3.0          py310h73ada01_2    conda-forgeqdarkstyle                3.0.2              pyhd8ed1ab_0    conda-forgeqstylizer                 0.2.1              pyhd8ed1ab_0    conda-forgeqt                        5.12.9               h556501e_6    conda-forgeqtawesome                 1.1.1              pyhd8ed1ab_0    conda-forgeqtconsole                 5.3.2              pyhd8ed1ab_0    conda-forgeqtconsole-base            5.3.2              pyha770c72_0    conda-forgeqtpy                      2.2.1              pyhd8ed1ab_0    conda-forgereproc                    14.2.4               hd77b12b_1    anacondareproc-cpp                14.2.4               hd77b12b_1    anacondarequests                  2.27.1             pyhd8ed1ab_0    conda-forgerequests-oauthlib         1.3.1              pyhd8ed1ab_0    conda-forgeresampy                   0.2.2                      py_0    conda-forgerich                      12.2.0             pyhd8ed1ab_0    conda-forgerope                      1.0.0              pyhd8ed1ab_0    conda-forgersa                       4.8                pyhd8ed1ab_0    conda-forgertree                     1.0.0           py310h1cbd46b_1    conda-forgeruamel_yaml               0.15.100        py310h2bbff1b_0    anacondascikit-image              0.19.1          py310hf5e1058_0    conda-forgescikit-learn              1.0.2           py310h4dafddf_0    conda-forgescipy                     1.8.0           py310h33db832_1    conda-forgeseaborn                   0.11.2               hd8ed1ab_0    conda-forgeseaborn-base              0.11.2             pyhd8ed1ab_0    conda-forgesemver                    2.13.0             pyh9f0ad1d_0    conda-forgesend2trash                1.8.0              pyhd8ed1ab_0    conda-forgesentry-sdk                1.5.6              pyhd8ed1ab_0    conda-forgesetproctitle              1.2.2           py310he2412df_1    conda-forgesetuptools                65.5.0             pyhd8ed1ab_0    conda-forgeshortuuid                 1.0.8           py310h5588dad_0    conda-forgesix                       1.16.0             pyh6c4a22f_0    conda-forgesmart-open                5.2.1                    pypi_0    pypismmap                     4.0.0              pyhd3eb1b0_0    anacondasnappy                    1.1.8                ha925a31_3    conda-forgesnowballstemmer           2.2.0              pyhd8ed1ab_0    conda-forgesortedcontainers          2.4.0              pyhd8ed1ab_0    conda-forgesphinx                    4.5.0              pyh6c4a22f_0    conda-forgesphinx-gallery            0.12.0.dev0              pypi_0    pypisphinx-rtd-theme          1.0.0                    pypi_0    pypisphinxcontrib-applehelp   1.0.2                      py_0    anacondasphinxcontrib-devhelp     1.0.2                      py_0    anacondasphinxcontrib-htmlhelp    2.0.0              pyhd8ed1ab_0    conda-forgesphinxcontrib-jsmath      1.0.1                      py_0    anacondasphinxcontrib-qthelp      1.0.3                      py_0    anacondasphinxcontrib-serializinghtml 1.1.5              pyhd8ed1ab_2    conda-forgespyder                    5.3.1           py310h5588dad_0    conda-forgespyder-kernels            2.3.2           py310h5588dad_0    conda-forgesqlite                    3.38.2               h2bbff1b_0    anacondastatsmodels               0.13.2          py310h2873277_0    conda-forgetables                    3.7.0                    pypi_0    pypitbb                       2021.5.0             h2d74725_1    conda-forgetenacity                  8.0.1           py310haa95532_1tensorboard               2.10.1             pyhd8ed1ab_0    conda-forgetensorboard-data-server   0.6.1                    pypi_0    pypitensorboard-plugin-wit    1.8.1              pyhd8ed1ab_0    conda-forgetensorflow                2.10.0                   pypi_0    pypitensorflow-estimator      2.10.0                   pypi_0    pypitensorflow-gpu            2.10.0                   pypi_0    pypitensorflow-io-gcs-filesystem 0.24.0                   pypi_0    pypitermcolor                 1.1.0                      py_2    conda-forgeterminado                 0.13.1          py310h5588dad_0    conda-forgetestpath                  0.5.0              pyhd8ed1ab_0    conda-forgetexext                    0.6.7                    pypi_0    pypitext-unidecode            1.3                        py_0    anacondatextdistance              4.2.2              pyhd8ed1ab_0    conda-forgetf-estimator-nightly      2.8.0.dev2021122109          pypi_0    pypithreadpoolctl             3.1.0              pyh8a188c0_0    conda-forgethree-merge               0.1.1              pyh9f0ad1d_0    conda-forgetifffile                  2022.2.9           pyhd8ed1ab_0    conda-forgetinycss2                  1.1.1              pyhd8ed1ab_0    conda-forgetk                        8.6.12               h8ffe710_0    conda-forgetoml                      0.10.2             pyhd8ed1ab_0    conda-forgetomli                     1.2.2              pyhd8ed1ab_0    conda-forgetoolz                     0.11.2             pyhd8ed1ab_0    conda-forgetorchaudio                0.11.0              py310_cu113    pytorchtorchinfo                 1.6.5              pyhd8ed1ab_0    conda-forgetorchmetrics              0.9.2              pyhd8ed1ab_0    conda-forgetorchvision               0.12.0              py310_cu113    pytorchtornado                   6.1             py310he2412df_3    conda-forgetqdm                      4.62.3             pyhd8ed1ab_0    conda-forgetraitlets                 5.1.1              pyhd8ed1ab_0    conda-forgetyped-ast                 1.5.2           py310he2412df_0    conda-forgetyping-extensions         4.0.1                hd8ed1ab_0    conda-forgetyping_extensions         4.0.1              pyha770c72_0    conda-forgetzdata                    2022a                h191b570_0    conda-forgeucrt                      10.0.20348.0         h57928b3_0    conda-forgeujson                     5.2.0           py310h8a704f9_1    conda-forgeunicodedata2              14.0.0          py310he2412df_1    conda-forgeunidecode                 1.3.4              pyhd8ed1ab_0    conda-forgeurllib3                   1.26.8             pyhd8ed1ab_1    conda-forgevc                        14.2                 hc4473a8_6    conda-forgeversion-query             1.1.0                    pypi_0    pypivs2015_runtime            14.29.30139          h890b9b1_8    conda-forgewandb                     0.13.4             pyhd8ed1ab_0    conda-forgewatchdog                  2.1.7           py310h5588dad_1    conda-forgewavespin                  0.1.2                    pypi_0    pypiwcwidth                   0.2.5              pyh9f0ad1d_2    conda-forgewebencodings              0.5.1                      py_1    conda-forgewerkzeug                  2.0.3              pyhd8ed1ab_1    conda-forgewheel                     0.37.1             pyhd8ed1ab_0    conda-forgewidgetsnbextension        3.5.2           py310h5588dad_1    conda-forgewin_inet_pton             1.1.0           py310h5588dad_4    conda-forgewinpty                    0.4.3                         4    anacondawrapt                     1.13.3          py310he2412df_1    conda-forgexorg-kbproto              1.0.7             hcd874cb_1002    conda-forgexorg-libice               1.0.10               hcd874cb_0    conda-forgexorg-libsm                1.2.3             hcd874cb_1000    conda-forgexorg-libx11               1.7.2                hcd874cb_0    conda-forgexorg-libxau               1.0.9                hcd874cb_0    conda-forgexorg-libxdmcp             1.1.3                hcd874cb_0    conda-forgexorg-libxext              1.3.4                hcd874cb_1    conda-forgexorg-libxpm               3.5.13               hcd874cb_0    conda-forgexorg-libxt                1.2.1                hcd874cb_2    conda-forgexorg-xextproto            7.3.0             hcd874cb_1002    conda-forgexorg-xproto               7.0.31            hcd874cb_1007    conda-forgexz                        5.2.5                h62dcd97_1    conda-forgeyaml                      0.2.5                h8ffe710_2    conda-forgeyaml-cpp                  0.6.3                ha925a31_4    conda-forgeyapf                      0.32.0             pyhd8ed1ab_0    conda-forgeyarl                      1.7.2           py310he2412df_2    conda-forgeyaspin                    2.1.0              pyhd8ed1ab_0    conda-forgezeromq                    4.3.4                h0e60522_1    conda-forgezfp                       0.5.5                h0e60522_8    conda-forgezipp                      3.7.0              pyhd8ed1ab_1    conda-forgezlib                      1.2.13               hcfcfb64_4    conda-forgezstd                      1.5.2                h6255e5f_0    conda-forge```</details>### Standalone code to reproduce the issue```shellimport torchimport ten"
58387,0,2617,269,0,0,elfringham,0,"title:Two unit test failures on high CPU core count machines description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiongit HEAD### Custom CodeNo### OS Platform and DistributionUbuntu 20.04### Mobile devicen/a### Python version3.8.10### Bazel version5.3.0### GCC/Compiler version10.2.1### CUDA/cuDNN versionn/a### GPU model and memoryn/a### Current Behaviour?```shell//tensorflow/python/data/experimental/kernel_tests/service:worker_tags_test and //tensorflow/python/data/experimental/kernel_tests/service:local_workers_test timeout which is down to the elements in the dataset being prefetched, one per CPU core. This can result in unexpected exceptions due to End of sequence causing the test to fail when the CPU core count is more than approximately 200.```### Standalone code to reproduce the issue```shellbazel test --test_timeout=30,50,-1,-1 --flaky_test_attempts=1 --test_output=all --cache_test_results=no --config=nonccl --config=mkl_aarch64_threadpool --copt=-mtune=generic --copt=-march=armv8-a --copt=-O3 --test_env=TF_ENABLE_ONEDNN_OPTS=1 --verbose_failures --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64 --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64 --build_tests_only -- //tensorflow/python/data/experimental/kernel_tests/service:worker_tags_test //tensorflow/python/data/experimental/kernel_tests/service:local_workers_test```### Relevant log output```shell======================================================================ERROR: testMultipleConsumers_test_mode_graph_tfapiversion_2 (__main__.LocalWorkersTest)LocalWorkersTest.testMultipleConsumers_test_mode_graph_tfapiversion_2testMultipleConsumers_test_mode_graph_tfapiversion_2(mode='graph', tf_api_version=2)----------------------------------------------------------------------Traceback (most recent call last):  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1378, in _do_call    return fn(*args)  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1361, in _run_fn    return self._call_tf_sessionrun(options, feed_dict, fetch_list,  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,tensorflow.python.framework.errors_impl.OutOfRangeError: End of sequence	 [[{{node IteratorGetNext_6}}]]```</details>
"
58378,1,1049,0,0,0,RenuPatelGoogle,0,"title:Video classification notebook is showing error in output when replicated using TF 2.9 description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow VersionTF 2.9### Custom CodeNo### OS Platform and DistributionGoogle Colab, Linux### Mobile device_No response_### Python version3.7### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellI am getting an error in the last section of code in the Video classification notebook when I tried replicating the code with TF 2.9 using Google Colab. The code is not showing any error if I use TF 2.8 and TF 2.10 to execute the code.https://www.tensoreflow.org/tutorials/load_data/video#next_steps```### Standalone code to reproduce the issue```shellnet = tf.keras.applications.EfficientNetB0(include_top = False)net.trainable = Falsemodel = tf.keras.Sequential([    tf.keras.layers.TimeDistributed(net),    tf.keras.layers.Dense(10),    tf.keras.layers.GlobalAveragePooling3D()])model.compile(optimizer = 'adam',              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),              metrics=['accuracy'])model.fit(train_ds,           epochs = 10,          validation_data = val_ds,          callbacks = tf.keras.callbacks.EarlyStopping(patience = 2, monitor = 'val_loss'))```### Relevant log output```shellPlease find this [gist](https://colab.research.google.com/gist/RenuPatelGoogle/f56e7e03cfc57f5293b6e6716d9ecbd3/video.ipynb) for your reference.```</details>
"
58352,0,569,0,0,0,creakseek,0,"title:RaggedTensorToVariant abort description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.10### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shell`tf.raw_ops.RaggedTensorToVariant` crash with abort with `batched_input=True`.```### Standalone code to reproduce the issue```shellimport tensorflow as tfinput_splits = [[0, 2, 3, 5, 6], [0, 1, 2], [0, 1, 2, 3, 4, 5]]input_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]output_tensor = tf.raw_ops.RaggedTensorToVariant(rt_nested_splits=input_splits, rt_dense_values=input_values, batched_input=True)```### Relevant log output```shellF tensorflow/core/framework/tensor_shape.cc:186] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Expected shape dimensions to be non-negative, got -1abort (core dumped)```</details>
"
58351,0,821,0,0,0,tianxiangsong,0,"title:tf.raw_ops.DepthwiseConv2dNative test error based on tensorflow 2.8.2 description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.8.2### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 18.04### Mobile deviceLinux Ubuntu 18.04### Python version3.8.2### Bazel version5.0.0### GCC/Compiler version9.6.0### CUDA/cuDNN version11.4### GPU model and memory3080ti 11g### Current Behaviour?```shelltensorflow op tf.raw_ops.DepthwiseConv2dNative when dilation is not 1, result is error.It is not suit description file. (https://www.tensorflow.org/api_docs/python/tf/raw_ops/DepthwiseConv2dNative?hl=zh-cn)```### Standalone code to reproduce the issue```shellExample 1:'''import numpy as npimport tensorflow as tfx = tf.random.normal(shape=[8,2,5,1], dtype=tf.float32)kernel = tf.random.normal(shape=[1,3,1,1], dtype=tf.float32)print(""============kernel: "", kernel)with tf.device(""CPU""):    c = tf.raw_ops.DepthwiseConv2dNative(input = x, filter = kernel,                                         strides=[1, 1, 1, 1], padding='VALID',                                         data_format='NHWC',                                         dilations=[1, 2, 3, 1])print(c.shape)'''The shape of output(c) is error, it is default value.```### Relevant log output_No response_</details>
"
58344,0,477,0,0,1,dradenvandewind,0,"title:how to force a session to run on a virtual gpu description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.5 GPU### Custom CodeYes### OS Platform and DistributionUbuntu 18.04.6 LTS### Mobile device_No response_### Python versionPython 3.6.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version nvidia/cuda:11.2.2-cudnn8-devel-ubuntu18.04### GPU model and memoryQuadro P620 computeCapability: 6.1 coreClock: 1.4425GHz coreCount: 4 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 89.53GiB/s### Current Behaviour?```shellHello,I need help for fix this.Initially, i updated a code in version v1 to use it in v2.5.0to do this, I need to limit the vram consumption of my process but i don't found how to run  my session on virtualgpuFor this :physical_devices = tf.config.list_physical_devices('GPU')        print("""")        print(""###### physical_devices"",physical_devices)        tf.config.experimental.set_virtual_device_configuration(physical_devices[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=512),tf.config.experimental.VirtualDeviceConfiguration(memory_limit=512)])        logical_gpus = tf.config.experimental.list_logical_devices('GPU')        print(len(physical_devices), ""Physical GPUs,"", len(logical_gpus), ""Logical GPUs"", logical_gpus)        gpus = tf.config.experimental.get_virtual_device_configuration(physical_devices[0])        print(len(gpus),""###virtual devices"",gpus)2022-10-27 17:16:29.592054: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.2022-10-27 17:16:29.592069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 512 MB memory) -> physical GPU (device: 0, name: Quadro P620, pci bus id: 0000:01:00.0, compute capability: 6.1)1 Physical GPUs, 2 Logical GPUs [LogicalDevice(name='/device:GPU:0', device_type='GPU'), LogicalDevice(name='/device:GPU:1', device_type='GPU')]2 ###virtual devices [LogicalDeviceConfiguration(memory_limit=512, experimental_priority=None), LogicalDeviceConfiguration(memory_limit=512, experimental_priority=None)]I manage to load my model on a virtual gpu gst_python           | 27.10 17:16:30.752 | MainThread | Model (/home/workingsrc/frozen_inference_graph.pb) placed on /job:localhost/replica:0/task:0/device:GPU:1with tf.device(f""/job:localhost/replica:0/task:0/device:GPU:1""):        graph = tf.Graph()        with graph.as_default():            tf.import_graph_def(graph_def, name=name)            return graphbut when i start sessionwith tf.device(f""/job:localhost/replica:0/task:0/device:GPU:1""):   self._session = tf.compat.v1.Session( graph=graph, config=tf_config)i can see in my logs this :2022-10-27 17:16:30.753864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2263 MB memory) -> physical GPU (device: 0, name: Quadro P620, pci bus id: 0000:01:00.0, compute capability: 6.1)devices = self._session.list_devices()for d in devices:           print(d.name)/job:localhost/replica:0/task:0/device:CPU:0/job:localhost/replica:0/task:0/device:GPU:02022-10-27 17:16:30.752807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2022-10-27 17:16:30.753020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: pciBusID: 0000:01:00.0 name: Quadro P620 computeCapability: 6.1coreClock: 1.4425GHz coreCount: 4 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 89.53GiB/s2022-10-27 17:16:30.753069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2022-10-27 17:16:30.753258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2022-10-27 17:16:30.753418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 02022-10-27 17:16:30.753441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:2022-10-27 17:16:30.753446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 2022-10-27 17:16:30.753449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N 2022-10-27 17:16:30.753506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2022-10-27 17:16:30.753696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2022-10-27 17:16:30.753864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2263 MB memory) -> physical GPU (device: 0, name: Quadro P620, pci bus id: 0000:01:00.0, compute capability: 6.1)My session is dispatched to the physical device,consuming too much memory/job:localhost/replica:0/task:0/device:GPU:0 with 2263 MB memory) -> physical GPUHow to force the session on my virtual device for consume 512Mb./job:localhost/replica:0/task:0/device:GPU:1```### Standalone code to reproduce the issue```shellHow to force the session on my virtual device```### Relevant log output```shellHow to force the session on my virtual device```</details>
"
58281,0,3049,177,0,1,maxhgerlach,0,"title:Undefined symbol `xla::HloComputation::CollectUnreachableRoots() const` with tf-nightly pip package description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf-nightly-cpu==2.12.0.dev20221019 and newer### Custom CodeYes### OS Platform and DistributionUbuntu 20.04### Mobile device_No response_### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?Starting from version `2.12.0.dev20221019` Horovod cannot be built correctly with `tf-nightly`. `tf-nightly-cpu==2.12.0.dev20221018` and earlier are fine.Upon import of `horovod.tensorflow` an undefined symbol error is encountered: ```tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/horovod/tensorflow/mpi_lib.cpython-38-x86_64-linux-gnu.so: undefined symbol: _ZNK3xla14HloComputation23CollectUnreachableRootsEv```In demangled form that is `xla::HloComputation::CollectUnreachableRoots() const`. Has the implementation been moved to a different dynamic library recently? Horovod currently links to `libtensorflow_framework.so.2` and to `_pywrap_tensorflow_internal.so`.### Standalone code to reproduce the issue```shell$ pip install tf-nightly-cpuCollecting tf-nightly-cpu  Downloading tf_nightly_cpu-2.12.0.dev20221024-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225.0 MB)     闂傚倸鍊搁崐鐑芥倿閿旂晫绠惧┑鐘叉搐閻掑灚銇勯幒宥堝厡妞ゎ剙鍚嬮妵鍕棘濞嗙偓鈻堥梺鍝勭焿缁绘繈宕洪埀顒併亜閹烘垵顏柡鍛絻椤法鎹勯悮鏉戝闂佸憡锚閻°劎鎹㈠☉娆愮秶闁告挆鍐ㄧ厒闁诲孩顔栭崰鏍偉閻撳寒鍤曟い鎰╁焺濞尖晠鏌ら崫銉︽毄闁告ɑ鎹囧缁樼瑹閸パ傜凹缂備礁顑嗛悧鐘汇€佸▎鎰窞闁归偊鍘鹃崢鎼佹⒑闁偛鑻晶鎾煛娴ｇ懓濮堢€垫澘瀚换婵嬪礋椤愩垻顓哄┑鐘垫暩閸嬬娀骞撻鍡楃筏闁芥ê顦介崵鏇㈡煕椤愶絾绀€闁哄鐒﹂幈銊ノ熼崹顔惧帿闂佺粯鎸搁崯鏉戭潖濞差亶鏁冮柨婵嗘祩閻忓崬顪冮妶鍡樼叆妞わ妇鏁诲濠氭晸閻樿尙鍔﹀銈嗗笒鐎氼參寮查幓鎺濈唵閻犺櫣灏ㄩ崝鐔兼煕濡櫣鎽犵紒缁樼⊕濞煎繘宕滆閸╁矂鎮楀▓鍨灈闁绘牜鍘ч锝夘敃閵忊晜鍍甸梺鐓庢憸閺佹悂宕㈤幘缁樷拻濞撴艾娲ゆ禍婵堢磼鐎ｎ偆澧垫い銏＄懄瀵板嫰骞囬鐘插箰闂備線鈧偛鑻晶鎾煛娴ｇ懓濮堢€垫澘瀚换婵嬪礋椤愩垻顓哄┑鐘垫暩閸嬬娀骞撻鍡楃筏闁芥ê顦介崵鏇㈡煕椤愶絾绀€闁哄鐒﹂幈銊ノ熼崹顔惧帿闂佺粯鎸搁崯鏉戭潖濞差亶鏁冮柨婵嗘祩閻忓崬顪冮妶鍡樼叆妞わ妇鏁诲濠氭晸閻樿尙鍔﹀銈嗗笒鐎氼參寮查幓鎺濈唵閻犺櫣灏ㄩ崝鐔兼煕濡櫣鎽犵紒缁樼⊕濞煎繘宕滆閸╁矂鎮楀▓鍨灈闁绘牜鍘ч锝夘敃閵忊晜鍍甸梺鐓庢憸閺佹悂宕㈤幘缁樷拻濞撴艾娲ゆ禍婵堢磼鐎ｎ偆澧垫い銏＄懄瀵板嫰骞囬鐘插箰闂備線鈧偛鑻晶鎾煛娴ｇ懓濮堢€垫澘瀚换婵嬪礋椤愩垻顓哄┑鐘垫暩閸嬬娀骞撻鍡楃筏闁芥ê顦介崵鏇㈡煕椤愶絾绀€闁哄鐒﹂幈銊ノ熼崹顔惧帿闂佺粯鎸搁崯鏉戭潖濞差亶鏁冮柨婵嗘祩閻忓崬顪冮妶鍡樼叆妞わ妇鏁诲濠氭晸閻樿尙鍔﹀銈嗗笒鐎氼參寮查幓鎺濈唵閻犺櫣灏ㄩ崝鐔兼煕濡櫣鎽犵紒缁樼⊕濞煎繘宕滆閸╁矂鎮楀▓鍨灈闁绘牜鍘ч锝夘敃閵忊晜鍍甸梺鐓庢憸閺佹悂宕㈤幘缁樷拻濞撴艾娲ゆ禍婵堢磼鐎ｎ偆澧垫い銏＄懄瀵板嫰骞囬鐘插箰闂備線鈧偛鑻晶鎾煛娴ｇ懓濮堢€垫澘瀚换婵嬪礋椤愩垻顓哄┑鐘垫暩閸嬬娀骞撻鍡楃筏闁芥ê顦介崵鏇㈡煕椤愶絾绀€闁哄鐒﹂幈銊ノ熼崹顔惧帿闂佺粯鎸搁崯鏉戭潖濞差亶鏁冮柨婵嗘祩閻忓崬顪冮妶鍡樼叆妞わ妇鏁诲濠氭晸閻樿尙鍔﹀銈嗗笒鐎氼參寮查幓鎺濈唵閻犺櫣灏ㄩ崝鐔兼煕濡櫣鎽犵紒缁樼⊕濞煎繘宕滆閸╁矂鎮楀▓鍨灈闁绘牜鍘ч锝夘敃閵忊晜鍍甸梺鐓庢憸閺佹悂宕㈤幘缁樷拻濞撴艾娲ゆ禍婵堢磼鐎ｎ偆澧垫い銏＄懄瀵板嫰骞囬鐘插箰闂備線鈧偛鑻晶鎾煛娴ｇ懓濮堢€垫澘瀚换婵嬪礋椤愩垻顓哄┑鐘垫暩閸嬬娀骞撻鍡楃筏闁芥ê顦介崵鏇㈡煕椤愶絾绀€闁?225.0/225.0 MB 1.2 MB/s eta 0:00:00# ...$ pip install -v horovod# ...  Tensorflow_LIBRARIES := -L.../horovod-tf-nightly-venv/lib/python3.9/site-packages/tensorflow -l:libtensorflow_framework.so.2 .../horovod-tf-nightly-venv/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so  -- Found Tensorflow: -L.../horovod-tf-nightly-venv/lib/python3.9/site-packages/tensorflow -l:libtensorflow_framework.so.2 .../horovod-tf-nightly-venv/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so (found suitable version ""2.12.0-dev20221024"", minimum required is ""1.15.0"")# ...$ python -c 'import horovod.tensorflow'```Error message quoted below under ""relevant log output"".### Relevant log output```shell2022-10-24 16:12:49.274846: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.Traceback (most recent call last):  File ""<string>"", line 1, in <module>  File "".../horovod-tf-nightly-venv/lib/python3.9/site-packages/horovod/tensorflow/__init__.py"", line 27, in <module>    from horovod.tensorflow import elastic  File "".../horovod-tf-nightly-venv/lib/python3.9/site-packages/horovod/tensorflow/elastic.py"", line 24, in <module>    from horovod.tensorflow.functions import broadcast_object, broadcast_object_fn, broadcast_variables  File "".../horovod-tf-nightly-venv/lib/python3.9/site-packages/horovod/tensorflow/functions.py"", line 24, in <module>    from horovod.tensorflow.mpi_ops import allgather, broadcast, broadcast_  File "".../horovod-tf-nightly-venv/lib/python3.9/site-packages/horovod/tensorflow/mpi_ops.py"", line 53, in <module>    raise e  File "".../horovod-tf-nightly-venv/lib/python3.9/site-packages/horovod/tensorflow/mpi_ops.py"", line 50, in <module>    MPI_LIB = _load_library('mpi_lib' + get_ext_suffix())  File "".../horovod-tf-nightly-venv/lib/python3.9/site-packages/horovod/tensorflow/mpi_ops.py"", line 45, in _load_library    library = load_library.load_op_library(filename)  File "".../horovod-tf-nightly-venv/lib/python3.9/site-packages/tensorflow/python/framework/load_library.py"", line 54, in load_op_library    lib_handle = py_tf.TF_LoadLibrary(library_filename)tensorflow.python.framework.errors_impl.NotFoundError: .../horovod-tf-nightly-venv/lib/python3.9/site-packages/horovod/tensorflow/mpi_lib.cpython-39-x86_64-linux-gnu.so: undefined symbol: _ZNK3xla14HloComputation23CollectUnreachableRootsEv```</details>
"
58277,1,2271,18,0,0,jgrou,0,"title:Tensorflow:AutoGraph could not transform function description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.10.0### Custom CodeNo### OS Platform and DistributionWindows 10 Enterprise### Mobile device_No response_### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellA function to find the value in one tensor closest to another tensor returns a warning. It also returns the output as expected.```### Standalone code to reproduce the issue```shellimport tensorflow as tf# Sampling function - randomly sample space pairs def space_sampler(nSim):    S = tf.random.uniform(shape=[nSim,1], minval=0, maxval=2.0)    V = tf.random.uniform(shape=[nSim,1], minval=-1.0, maxval=1.0)    return S, V   # Find interior value closest to boundary value@tf.function(experimental_relax_shapes=True)def BoundaryPenalty(S_int, V_int, S_bound, V_bound):    interior = tf.concat([S_int, V_int],1)    bound = tf.concat([S_bound, V_bound],1)    y_n = tf.ones([1,tf.shape(bound)[1]])    for i in range(bound.shape[0]):        tf.autograph.experimental.set_loop_options(shape_invariants=[(y_n, tf.TensorShape([None]))])        tf.autograph.experimental.set_loop_options(shape_invariants=[(interior, tf.TensorShape([None]))])        index = tf.argmin(tf.reduce_sum(tf.math.squared_difference(interior, bound[i]),1))        y_n = tf.concat([y_n,interior[index,None]],0)        interior = tf.concat([interior[:index], interior[index+1:]],0)        y_n = y_n[1:]    return y_n    # Call the functionsS_interior, V_interior = space_sampler(12)S_boundary, V_boundary = space_sampler(6)BoundaryPenalty(S_interior, V_interior, S_boundary, V_boundary)```### Relevant log output```shellWARNING:tensorflow:AutoGraph could not transform <function BoundaryPenalty at 0x0000022BDB6BA280> and will run it as-is.Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.Cause: ""set_loop_options"" must be the first statement in the loop blockTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convertWARNING: AutoGraph could not transform <function BoundaryPenalty at 0x0000022BDB6BA280> and will run it as-is.Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.Cause: ""set_loop_options"" must be the first statement in the loop blockTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert```</details>
"
58272,1,1472,83,0,0,shijy16,0,"title:A check fail can be triggered in MaxPoolGradWithArgmax description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.9 and 2.12.0-dev20221018### Custom CodeNo### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version3.8### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versionCUDA 11.5### GPU model and memory_No response_### Current Behaviour?```shellA crash due to check fail can be triggered in MaxPoolGradWithArgmax.```### Standalone code to reproduce the issue```shellimport osos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'import tensorflow as tfimport numpy as npprint(tf.__version__)for _ in range(20):    try:        ksize_0 = 1        ksize_1 = 2        ksize_2 = 4        ksize_3 = 1        ksize = [ksize_0, ksize_1, ksize_2, ksize_3, ]        strides_0 = 1        strides_1 = 2        strides_2 = 1        strides_3 = 16        strides = [strides_0, strides_1, strides_2, strides_3, ]        padding = ""SAME""        include_batch_in_index = False        input = tf.saturate_cast(tf.random.uniform([16, 16, 1, 1], minval=0, maxval=64, dtype=tf.int64), dtype=tf.uint64)        grad = tf.saturate_cast(tf.random.uniform([16, 8, 1, 1], minval=0, maxval=64, dtype=tf.int64), dtype=tf.uint64)        argmax = tf.saturate_cast(tf.random.uniform([16, 8, 1, 1], minval=0, maxval=64, dtype=tf.int64), dtype=tf.int64)        res = tf.raw_ops.MaxPoolGradWithArgmax(            ksize=ksize,            strides=strides,            padding=padding,            include_batch_in_index=include_batch_in_index,            input=input,            grad=grad,            argmax=argmax,        )    except:        pass```### Relevant log output```shellF tensorflow/core/kernels/maxpooling_op.cc:1065] Check failed: grad_out_index >= output_start && grad_out_index < output_end Invalid output gradient index: 263, 0, 256Aborted (core dumped)```</details>
"
58258,1,4660,108,0,0,jmp75,0,"title:Failing to fit an lstm model.fit(x=g,) with generator `g`, custom loss function and `run_eagerly` is False description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versionv2.10.0-rc3-6-g359c3cdfc5f 2.10.0### Custom CodeYes### OS Platform and DistributionLinux xxxxxx 5.10.0-19-amd64   SMP Debian 5.10.149-1 (2022-10-17) x86_64 GNU/Linux### Mobile device_No response_### Python versionpython  3.9.13   h9a8a25e_0_cpython    conda-forge### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versioncudatoolkit  11.2.2  hbe64b41_10  conda-forge,  cudnn  8.1.0.77  h90431f1_0  conda-forge### GPU model and memoryQuadro RTX 4000, 8GB### Current Behaviour?I was trying to rerun some earlier work, that used to work on TF 2.6.0 I think (not sure). In a nutshell, extracted for a repro gist thereafter: ```pythondef create_and_fit(x, run_eagerly):    model = new_model()    model.compile(optimizer=Adam(learning_rate=1e-4), loss=mean_absolute_error_na, metrics=[some_metric], run_eagerly=run_eagerly)    return model.fit(        x,        steps_per_epoch=steps_per_epoch,        epochs=epochs,        validation_data=None    )```if x is a `<generator object train_generator_batch>` and `run_eagerly` is `False`, it seems that invalid ground truth observations `y_true` is passed to the custom loss function, which reports:```textEpoch 1/5y_true = Tensor(""mean_absolute_error_na/remove_squeezable_dimensions/Squeeze:0"", shape=(None, None), dtype=float32)y_pred = Tensor(""model_1/dense_1/BiasAdd:0"", shape=(16, 1), dtype=float32)etc etc stack trace...    ValueError: y_true and y_pred do not have compatible shapes: (None, None) and (16, 1). Respective types are <class 'tensorflow.python.framework.ops.Tensor'> and <class 'tensorflow.python.framework.ops.Tensor'>```Model fitting works however if `run_eagerly` is True, or if x is passed a dataset created with `tf.data.Dataset.from_generator`.Note that I run the jupyter python kernel launched via `optirun` to access the GPU, in case this has relevance. The bug anyway occurs also if forcing the execution on the CPU.### Standalone code to reproduce the issueA self-contained notebook (python file obtained with jupytext) is available at [this gist](https://gist.github.com/jmp75/d55e5f73e337012ba610c72d20a4398f)### Relevant log output```shellEpoch 1/5y_true = Tensor(""mean_absolute_error_na/remove_squeezable_dimensions/Squeeze:0"", shape=(None, None), dtype=float32)y_pred = Tensor(""model_1/dense_1/BiasAdd:0"", shape=(16, 1), dtype=float32)---------------------------------------------------------------------------ValueError                                Traceback (most recent call last)Cell In [40], line 1----> 1 create_and_fit(train_gen, run_eagerly = False)Cell In [37], line 4, in create_and_fit(x, run_eagerly)      2 model = new_model()      3 model.compile(optimizer=Adam(learning_rate=1e-4), loss=mean_absolute_error_na, metrics=[some_metric], run_eagerly=run_eagerly)----> 4 return model.fit(      5     x,      6     steps_per_epoch=steps_per_epoch,      7     epochs=epochs,      8     validation_data=None      9 )File ~/miniconda/envs/my_env_name/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)     67     filtered_tb = _process_traceback_frames(e.__traceback__)     68     # To get the full stack trace, call:     69     # `tf.debugging.disable_traceback_filtering()`---> 70     raise e.with_traceback(filtered_tb) from None     71 finally:     72     del filtered_tbFile /tmp/__autograph_generated_file7kl24yk4.py:15, in outer_factory.<locals>.inner_factory.<locals>.tf__train_function(iterator)     13 try:     14     do_return = True---> 15     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)     16 except:     17     do_return = FalseFile /tmp/__autograph_generated_filexp4m9ubh.py:11, in outer_factory.<locals>.inner_factory.<locals>.tf__mean_absolute_error_na(y_true, y_pred)      9 do_return = False     10 retval_ = ag__.UndefinedReturnValue()---> 11 (y_true_m, y_pred_m) = ag__.converted_call(ag__.ld(remove_items_missing_observations), (ag__.ld(y_true), ag__.ld(y_pred)), None, fscope)     12 try:     13     do_return = TrueFile /tmp/__autograph_generated_filei0twifpa.py:24, in outer_factory.<locals>.inner_factory.<locals>.tf__remove_items_missing_observations(y_true, y_pred)     22 def else_body():     23     pass---> 24 ag__.if_stmt(ag__.ld(y_true).shape != ag__.ld(y_pred).shape, if_body, else_body, get_state, set_state, (), 0)     25 try:     26     do_return = TrueFile /tmp/__autograph_generated_filei0twifpa.py:20, in outer_factory.<locals>.inner_factory.<locals>.tf__remove_items_missing_observations.<locals>.if_body()     18 ag__.ld(print)(f'y_true = {ag__.ld(y_true)}')     19 ag__.ld(print)(f'y_pred = {ag__.ld(y_pred)}')---> 20 raise ag__.converted_call(ag__.ld(ValueError), (f'y_true and y_pred do not have compatible shapes: {ag__.ld(y_true).shape} and {ag__.ld(y_pred).shape}. Respective types are {ag__.converted_call(ag__.ld(type), (ag__.ld(y_true),), None, fscope)} and {ag__.converted_call(ag__.ld(type), (ag__.ld(y_pred),), None, fscope)}',), None, fscope)ValueError: in user code:    File ""/home/xxxyyy/miniconda/envs/my_env_name/lib/python3.9/site-packages/keras/engine/training.py"", line 1160, in train_function  *        return step_function(self, iterator)    File ""/tmp/ipykernel_36702/144245855.py"", line 13, in mean_absolute_error_na  *        y_true_m, y_pred_m = remove_items_missing_observations(y_true, y_pred)    File ""/tmp/ipykernel_36702/144245855.py"", line 8, in remove_items_missing_observations  *        raise ValueError(f""y_true and y_pred do not have compatible shapes: {y_true.shape} and {y_pred.shape}. Respective types are {type(y_true)} and {type(y_pred)}"")    ValueError: y_true and y_pred do not have compatible shapes: (None, None) and (16, 1). Respective types are <class 'tensorflow.python.framework.ops.Tensor'> and <class 'tensorflow.python.framework.ops.Tensor'>```</details>
"
58211,1,1248,148,0,0,sleepingcat4,0,"title:Input has undefined rank. Received: input_shape=<unknown> description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.9.2### Custom CodeYes### OS Platform and DistributionGoogle Colab### Mobile device_No response_### Python version3.7.15### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellA bug happened! I have created a derived version of AlexNet, I think BatchNormalization layers might be the cuprit behind the error. As without BatchNormalization layer it works perfectly fine. I don't think it's relevant but I used some custom activation functions. with combination of ""relu"". Though those functions weren't that different from ""relu"" so, I don't expect any error. It'll be a great help, if someone can help me to solve this error :)Relevant pointers and hints will also be appreciated!```### Standalone code to reproduce the issue```shell# input_shape=(256, 256, 3)model.build(input_shape=(None, 220, 220, 3))model.summary()```### Relevant log output```shellValueError                                Traceback (most recent call last)<ipython-input-16-a5158c5140ec> in <module>      1 # input_shape=(256, 256, 3)----> 2 model.build(input_shape=(None, 256, 256, 3))      3 model.summary()3 frames/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py in validate_axis(axis, input_shape)    241   if not rank:    242     raise ValueError(--> 243         f'Input has undefined rank. Received: input_shape={input_shape}')    244     245   # Convert axis to list and resolve negativesValueError: Input has undefined rank. Received: input_shape=<unknown>```</details>
"
58184,1,5398,185,0,0,maifeeulasad,0,"title:Trying to save a model compiled with tfa.MultiOptimizer | gives error saying `TypeError: ('Not JSON Serializable:', ...` description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.6.4### Custom CodeYes### OS Platform and DistributionKaggle kernel### Mobile device_No response_### Python version3.7.12### Bazel version_No response_### GCC/Compiler versionGCC 9.4.0### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellclass ModelSaverCallback(Callback):    def on_epoch_end(self, epoch, logs=None):        print('name: ' + self.model._name)        self.model.save('epoch-' + str(epoch + 1) + '-' + self.model._name + '.h5',overwrite=True,    include_optimizer=True,)        callbacks = [ModelSaverCallback()]model_w_multioptimizer = tf.keras.Sequential([    tf.keras.layers.Dense(12, activation = 'relu', input_shape=(len(xs[0]),)),    tf.keras.layers.Dense(1, activation = 'tanh')])model_w_multioptimizer._name = ""model_w_multioptimizer""optimizers = [    SGD(learning_rate=3e-4),    Adam(learning_rate=3e-4),]optimizers_and_layers = [(optimizers[0], model_w_multioptimizer.layers[:1]), (optimizers[1], model_w_multioptimizer.layers[1:])]optimizer = MultiOptimizer(optimizers_and_layers)model_w_multioptimizer.compile(                optimizer = optimizer,                      loss = tf.keras.losses.BinaryCrossentropy(),                metrics=['accuracy']    )    model_w_multioptimizer.fit(xs,ys,        epochs = 50,        batch_size=32,      callbacks=callbacks,        validation_data=(valid_xs, valid_ys))```It simply can't save the model when there are multiple optimizers.### Standalone code to reproduce the issue```shellKaggle kernel: https://www.kaggle.com/code/maifeeulasad/tfa-multioptimizer-model-save?scriptVersionId=108636090```### Relevant log output```shell---------------------------------------------------------------------------TypeError                                 Traceback (most recent call last)/tmp/ipykernel_20/1141714396.py in <module>     24         batch_size=32,     25       callbacks=callbacks,---> 26         validation_data=(valid_xs, valid_ys))/opt/conda/lib/python3.7/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)   1228           epoch_logs.update(val_logs)   1229 -> 1230         callbacks.on_epoch_end(epoch, epoch_logs)   1231         training_logs = epoch_logs   1232         if self.stop_training:/opt/conda/lib/python3.7/site-packages/keras/callbacks.py in on_epoch_end(self, epoch, logs)    411     logs = self._process_logs(logs)    412     for callback in self.callbacks:--> 413       callback.on_epoch_end(epoch, logs)    414     415   def on_train_batch_begin(self, batch, logs=None):/tmp/ipykernel_20/3193528328.py in on_epoch_end(self, epoch, logs)      3         print('name: ' + self.model._name)      4         self.model.save('epoch-' + str(epoch + 1) + '-' + self.model._name + '.h5',overwrite=True,----> 5     include_optimizer=True,)      6       7 callbacks = [ModelSaverCallback()]/opt/conda/lib/python3.7/site-packages/keras/engine/training.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)   2144     # pylint: enable=line-too-long   2145     save.save_model(self, filepath, overwrite, include_optimizer, save_format,-> 2146                     signatures, options, save_traces)   2147    2148   def save_weights(self,/opt/conda/lib/python3.7/site-packages/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)    144           'or using `save_weights`.')    145     hdf5_format.save_model_to_hdf5(--> 146         model, filepath, overwrite, include_optimizer)    147   else:    148     with generic_utils.SharedObjectSavingScope():/opt/conda/lib/python3.7/site-packages/keras/saving/hdf5_format.py in save_model_to_hdf5(model, filepath, overwrite, include_optimizer)    112       if isinstance(v, (dict, list, tuple)):    113         f.attrs[k] = json.dumps(--> 114             v, default=json_utils.get_json_type).encode('utf8')    115       else:    116         f.attrs[k] = v/opt/conda/lib/python3.7/json/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)    236         check_circular=check_circular, allow_nan=allow_nan, indent=indent,    237         separators=separators, default=default, sort_keys=sort_keys,--> 238         **kw).encode(obj)    239     240 /opt/conda/lib/python3.7/json/encoder.py in encode(self, o)    197         # exceptions aren't as detailed.  The list call should be roughly    198         # equivalent to the PySequence_Fast that ''.join() would do.--> 199         chunks = self.iterencode(o, _one_shot=True)    200         if not isinstance(chunks, (list, tuple)):    201             chunks = list(chunks)/opt/conda/lib/python3.7/json/encoder.py in iterencode(self, o, _one_shot)    255                 self.key_separator, self.item_separator, self.sort_keys,    256                 self.skipkeys, _one_shot)--> 257         return _iterencode(o, 0)    258     259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,/opt/conda/lib/python3.7/site-packages/keras/saving/saved_model/json_utils.py in get_json_type(obj)    140     return obj.value    141 --> 142   raise TypeError('Not JSON Serializable:', obj)TypeError: ('Not JSON Serializable:', <tf.Tensor 'gradient_tape/model_w_multioptimizer/dense_2/MatMul:0' shape=(3, 12) dtype=float32>)```</details>
"
58177,0,1234,83,0,0,shijy16,0,"title:Crash due to check-fail in CropAndResize description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.9 and 2.12.0-dev20221018### Custom CodeNo### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version3.8### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versionCUDA 11.5### GPU model and memory_No response_### Current Behaviour?```shellA check-fail can be triggered in CropAndResize.```### Standalone code to reproduce the issue```shellimport osos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'import tensorflow as tfimport numpy as npprint(tf.__version__)for _ in range(20):    try:        with tf.device(""GPU:0""):            method = ""bilinear""            extrapolation_value = -90.71406992626788            image = tf.saturate_cast(tf.random.uniform([1, 4, 1, 8], minval=0, maxval=64, dtype=tf.int64), dtype=tf.int64)            boxes = tf.random.uniform([4, 0], dtype=tf.float32)            box_ind = tf.saturate_cast(tf.random.uniform([8, 0, 10, 4], minval=0, maxval=64, dtype=tf.int64), dtype=tf.int32)            crop_size = tf.saturate_cast(tf.random.uniform([2], minval=0, maxval=64, dtype=tf.int64), dtype=tf.int32)            res = tf.raw_ops.CropAndResize(                method=method,                extrapolation_value=extrapolation_value,                image=image,                boxes=boxes,                box_ind=box_ind,                crop_size=crop_size,            )    except:        pass```### Relevant log output```shellF tensorflow/core/framework/tensor_shape.cc:45] Check failed: NDIMS == dims() (1 vs. 4)Asking for tensor of 1 dimensions from a tensor of 4 dimensions```</details>
"
58164,0,375,144,0,0,learning-to-play,1,"title:Significant increase in the size of macOS wheel description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf-nightly 2.11### Custom CodeNo### OS Platform and DistributionmacOS### Mobile device_No response_### Python version3.7, 3.8, 3.9, 3.10### Bazel version5.3.0### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shell[PR 55941](https://github.com/tensorflow/tensorflow/pull/55941) refactored and deduplicated TensorFlow C++ dependencies from _pywrap_tensorflow_internal.so into libtensorflow_cc.so. This change increased the size of macOS pip package from 240 MB to 350 MB.```### Standalone code to reproduce the issue```shellhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/rel/macos/cpu_py310_pip.sh```### Relevant log output_No response_</details>
"
58143,1,850,112,0,0,catqaq,0,"title:Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/reshape.cc:66 num_input_elements != num_output_elements (1024 != 1633876594) description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf2.3/2.8### Custom CodeYes### OS Platform and DistributionUbuntu 20.04.4 LTS### Mobile deviceandroid### Python version3.6/3.7### Bazel version4.2.1### GCC/Compiler versiongcc version 9.4.0 (Ubuntu 9.4.0-1ubuntu1~20.04.1) ### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellNative crash.```### Standalone code to reproduce the issue```shellThe code cannot be shared due to company privacy regulations and I would like to know the possible reasons for this type of issue.```### Relevant log output```shellFor tflite2.3:Abort message: 'Scudo ERROR: race on chunk header at address 0x007b45478370/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+100)...FORTIFY: pthread_mutex_lock called on a destroyed mutex (0x7dc6aeab18)...Abort message: 'Scudo ERROR: race on chunk header at address 0x007b4546caf0Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/reshape.cc:66 num_input_elements != num_output_elements (1024 != 1633876594)For tflite2.8:Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/reshape.cc:85 num_input_elements != num_output_elements (1024 != 1407367860)...```</details>
"
58135,1,1474,11,0,0,JustASquid,0,"title:XLA compiler error: left_branch_shape.rank() == right_branch_shape.rank() description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.10.0### Custom CodeYes### OS Platform and DistributionWindows 10### Mobile device_No response_### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versioncudatoolkit=11.2 cudnn=8.1.0### GPU model and memoryGTX 1080 TI### Current Behaviour?XLA compiler error for this model when `jit_compile=True`It's very hard to determine the cause of the error, as the model works fine when running normally.### Standalone code to reproduce the issue```shellimport tensorflow as tfimport numpy as npclass Chooser(tf.keras.layers.Layer):    @tf.function    def call(self, options_input, choices_input_logits):        choices = tf.nn.softmax(choices_input_logits, axis=2)        result = tf.linalg.matmul(choices, options_input)        return resultdef get_model():    options_input = tf.keras.layers.Input(shape=(10,3), name=""options"")    choices_input = tf.keras.layers.Input(shape=(5,10), name=""choices"")    net = Chooser()(options_input, choices_input)    net = tf.keras.layers.Flatten()(net)    net = tf.keras.layers.Dense(1)(net)    return tf.keras.Model([options_input, choices_input], net)model = get_model()model.compile(    optimizer=tf.keras.optimizers.Adam(0.001),    loss=tf.keras.losses.MeanAbsoluteError(),    jit_compile=True)def batch_gen():    while True:        o = np.random.uniform(low=-1.0, high=1.0, size=(10, 3))        c = np.random.uniform(low=0.0, high=1.0, size=(5, 10))        y = 1        yield {""options"": o, ""choices"": c}, ydataset = tf.data.Dataset.from_generator(batch_gen, output_types=({""options"": tf.float32, ""choices"": tf.float32}, tf.float32))dataset = dataset.batch(32)model.fit(dataset, steps_per_epoch=100, epochs=1)```### Relevant log output```shelltensorflow/compiler/xla/client/lib/dynamic_shaped_ops.cc:92] Check failed: left_branch_shape.rank() == right_branch_shape.rank() (1 vs. 2)left rank of (1) vs. right rank of (2)```</details>
"
58133,0,574,0,0,0,fuzzyswan,0,"title:`tf.range` have accumulate floating point error on CPU description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.10.0### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?`tf.range` have accumulate greater and greater floating point error on CPU. In the example below, `range(- 10, 10, 0.01)` gives `[... 9.980267 9.990267]` when the correct result should be `[... 9.98 9.99]`, and this deviation is too much in my opinion.### Standalone code to reproduce the issue```shellimport tensorflow as tfwith tf.device('cpu'):    tensor1 = tf.range(- 10, 10, 0.01) # [-10., -9.99 ... 9.980267 9.990267]    print(tensor1)with tf.device('gpu'):    tensor2 = tf.range(- 10, 10, 0.01) # [-10., -9.99 ... 9.98 9.99]    print(tensor2)assert np.allclose(tensor1, tensor2) # AssertionError```### Relevant log output```shelltf.Tensor([-10.        -9.99      -9.98     ...   9.970266   9.980267   9.990267], shape=(2000,), dtype=float32)tf.Tensor([-10.        -9.99      -9.98     ...   9.969999   9.98       9.99    ], shape=(2000,), dtype=float32)AssertionError```</details>
"
58131,1,685,0,0,0,creakseek,0,"title:`tf.scan` has inconsistent result on CPU/GPU description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.10### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shell`tf.scan` has inconsistent result on CPU/GPU:- on CPU: [[10 20 30] [ 0  0  0]]- on GPU: [[10 20 30] [ 5332261958806667264 -8722786653543858176 -4459375070678089728]]```### Standalone code to reproduce the issue```shellimport tensorflow as tfwith tf.device('cpu'):    input_data = np.array([[10, 20, 30], [11, 12, 13]])    result = tf.scan((lambda a, x: (a ** (x ** 2))), input_data)    o = result.numpy()    print(o)with tf.device('gpu'):    input_data = np.array([[10, 20, 30], [11, 12, 13]])    result = tf.scan((lambda a, x: (a ** (x ** 2))), input_data)    o = result.numpy()    print(o)```### Relevant log output```shell[[10 20 30] [ 0  0  0]][[10 20 30] [ 5332261958806667264 -8722786653543858176 -4459375070678089728]]```</details>
"
58107,0,1580,83,0,0,shijy16,0,"title:Floating Point Exception in AvgPool3DGrad description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.9 and 2.11.0-dev20221013### Custom CodeNo### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version3.8### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versionCUDA 11.5### GPU model and memory_No response_### Current Behaviour?```shellA Floating Point Exception can be triggerred in AvgPool3DGrad.```### Standalone code to reproduce the issue```shellimport osos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'import tensorflow as tfimport numpy as npprint(tf.__version__)for _ in range(20):    try:        with tf.device(""GPU:0""):            ksize_0 = 1            ksize_1 = 1            ksize_2 = 32            ksize_3 = 36            ksize_4 = 128            ksize = [ksize_0, ksize_1, ksize_2, ksize_3, ksize_4, ]            strides_0 = 1            strides_1 = 1            strides_2 = 128            strides_3 = 128            strides_4 = 128            strides = [strides_0, strides_1, strides_2, strides_3, strides_4, ]            padding = ""SAME""            data_format = ""NCDHW""            orig_input_shape_0 = 39            orig_input_shape_1 = 104            orig_input_shape_2 = 57            orig_input_shape_3 = 0            orig_input_shape_4 = 61            orig_input_shape = [orig_input_shape_0, orig_input_shape_1, orig_input_shape_2, orig_input_shape_3, orig_input_shape_4, ]            grad = tf.random.uniform([0, 4, 1, 1, 1], dtype=tf.float32)            res = tf.raw_ops.AvgPool3DGrad(                ksize=ksize,                strides=strides,                padding=padding,                data_format=data_format,                orig_input_shape=orig_input_shape,                grad=grad,            )    except:        pass```### Relevant log output```shelltensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8101Floating point exception (core dumped)```</details>
"
58074,1,0,0,0,0,WangFengtu1996,0,"title:Cross-build failed tensorflow lite demo minimal description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.10### Custom CodeNo### OS Platform and DistributionUbuntu 18.04### Mobile devicearm32### Python version_No response_### Bazel version_No response_### GCC/Compiler versionclang 10### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shell* I follow the document[https://github.com/tensorflow/tensorflow/blob/v2.10.0/tensorflow/lite/examples/minimal/README.md] to build the minimal demomkdir build_minimalcd build_minimalcmake  -DCMAKE_TOOLCHAIN_FILE=/home/yons/data/ohos-sdk/linux/native/build/cmake/ohos.toolchain.cmake ../tensorflow/lite/examples/minimal/cmake --build . -j 20```* I got error ```Building CXX object CMakeFiles/minimal.dir/minimal.cc.objclang++: warning: argument unused during compilation: '--gcc-toolchain=/home/yons/data/ohos-sdk/linux/native/llvm' [-Wunused-command-line-argument]In file included from /home/yons/data/tensorflow/tensorflow/lite/examples/minimal/minimal.cc:17:In file included from /home/yons/data/tensorflow/tensorflow/lite/kernels/register.h:18:In file included from /home/yons/data/tensorflow/tensorflow/lite/model.h:21:/home/yons/data/tensorflow/tensorflow/lite/interpreter_builder.h:26:10: fatal error: 'flatbuffers/flatbuffers.h' file not found#include ""flatbuffers/flatbuffers.h""  // from @flatbuffers         ^~~~~~~~~~~~~~~~~~~~~~~~~~~1 error generated.CMakeFiles/minimal.dir/build.make:62: recipe for target 'CMakeFiles/minimal.dir/minimal.cc.obj' failedmake[2]: *** [CMakeFiles/minimal.dir/minimal.cc.obj] Error 1CMakeFiles/Makefile2:1295: recipe for target 'CMakeFiles/minimal.dir/all' failedmake[1]: *** [CMakeFiles/minimal.dir/all] Error 2Makefile:129: recipe for target 'all' failedmake: *** [all] Error 2```* Please guide me to configure the include directory!* thanks for your time!```### Standalone code to reproduce the issue```shellhttps://github.com/tensorflow/tensorflow/tree/v2.10.0/tensorflow/lite/examples/minimal```* I change the `CMakeLists.txt````cmake_minimum_required(VERSION 3.16)project(minimal C CXX)set(TENSORFLOW_SOURCE_DIR """" CACHE PATH  ""Directory that contains the TensorFlow project"")if(NOT TENSORFLOW_SOURCE_DIR)  get_filename_component(TENSORFLOW_SOURCE_DIR    ""${CMAKE_CURRENT_LIST_DIR}/../../../../""    ABSOLUTE  )endif()add_subdirectory(  ""${TENSORFLOW_SOURCE_DIR}/tensorflow/lite""  ""${CMAKE_CURRENT_BINARY_DIR}/tensorflow-lite""  EXCLUDE_FROM_ALL)message(TARCE ""TuT "" ${TENSORFLOW_SOURCE_DIR})include_directories(${TENSORFLOW_SOURCE_DIR}/)#include_directories(${CMAKE_CURRENT_LIST_DIR}/flatbuffers/include)set(CMAKE_CXX_STANDARD 17)add_executable(minimal  minimal.cc)add_library(tf-lite STATIC IMPORTED )set_target_properties(    tf-lite    PROPERTIES IMPORTED_LOCATION    /home/yons/data/tensorflow/build_lite/libtensorflow-lite.a    )target_link_libraries(minimal  tf-lite)```### Relevant log output_No response_</details>
"
58053,0,1338,24,0,0,huan1372,0,"title:tf.saturate_cast errors for tf.complex type tensor description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.10.0### Custom CodeNo### OS Platform and DistributionUbuntu 20.04.5 LTS### Mobile device_No response_### Python version 3.8.10### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellFrom code below, the tf.saturate_cast need to check dtype's minhttps://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/ops/math_ops.py#L713-L743However, from code below, min is not support for complex64 or complex128https://github.com/tensorflow/tensorboard/blob/master/tensorboard/compat/tensorflow_stub/dtypes.py#L188-L194The documentation for tf.saturate_cast said it support any tensor or dtype desired. Therefore, either code or documentation need to be changedhttps://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/ops/math_ops.py#L716-L729```### Standalone code to reproduce the issue```shellCode like: import tensorflow as tfarg_0_tensor = tf.complex(tf.random.uniform([2, 1], dtype=tf.float32),tf.random.uniform([2, 1], dtype=tf.float32))arg_0_tensor = tf.saturate_cast(arg_0_tensor,dtype=tf.float32)```### Relevant log output```shellTraceback (most recent call last):  File ""<stdin>"", line 1, in <module>  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler    raise e.with_traceback(filtered_tb) from None  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py"", line 110, in min    raise TypeError(f""Cannot find minimum value of {self} with ""TypeError: Cannot find minimum value of <dtype: 'complex64'> with type <dtype: 'complex64'>.```</details>
"
58034,1,4813,1,0,0,vivekex,0,"title:XLA JIT compile failing on TF 2.9.2, A100 GPUs (on GCP VMs) description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.9.2### Custom CodeYes### OS Platform and DistributionDebian GNU/Linux 10 (buster)### Mobile device_No response_### Python version3.7.12### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version11.3/8### GPU model and memoryA100, 80GB### Current Behaviour?```shellTill a few weeks ago, I was able to successfully use XLA compilation when training with GPUs on GCP VMs.Specifically, I used `compile_jit=True` when doing a model.compile() for a TF Keras model. Having trained my models, I didn't run any TF training jobs for a while. Recently (a week ago) I tried running the same models, but the code now complains that it cannot find libcudnn.so.8. The same code works without `compile_jit=True`, but that does require a slightly smaller batch size to fit in memory.The log with `compile_jit=True` says ""Could not load library libcudnn_cnn_train.so.8"", but that library is most definitely present, since the same code does work without using XLA JIT compilation.```### Standalone code to reproduce the issue```shellUnable to provide a small case, haven't been able to reproduce on small models.```### Relevant log output```shellCould not load library libcudnn_cnn_train.so.8. Error: /opt/conda/bin/../lib/./libcudnn_ops_train.so.8: symbol _Z22cudnnGenericOpTensorNdILi3EE13cudnnStatus_tP12cudnnContext16cudnnGenericOp_t21cudnnNanPropagation_tPKdPKvPK17cudnnTensorStructS8_S8_SB_S8_S8_SB_Pv version libcudnn_ops_infer.so.8 not defined in file libcudnn_ops_infer.so.8 with link time referenceCould not load library libcudnn_cnn_train.so.8. Error: /opt/conda/bin/../lib/./libcudnn_ops_train.so.8: symbol _Z22cudnnGenericOpTensorNdILi3EE13cudnnStatus_tP12cudnnContext16cudnnGenericOp_t21cudnnNanPropagation_tPKdPKvPK17cudnnTensorStructS8_S8_SB_S8_S8_SB_Pv version libcudnn_ops_infer.so.8 not defined in file libcudnn_ops_infer.so.8 with link time referenceCould not load library libcudnn_cnn_train.so.8. Error: /opt/conda/bin/../lib/./libcudnn_ops_train.so.8: symbol _Z22cudnnGenericOpTensorNdILi3EE13cudnnStatus_tP12cudnnContext16cudnnGenericOp_t21cudnnNanPropagation_tPKdPKvPK17cudnnTensorStructS8_S8_SB_S8_S8_SB_Pv version libcudnn_ops_infer.so.8 not defined in file libcudnn_ops_infer.so.8 with link time referenceCould not load library libcudnn_cnn_train.so.8. Error: /opt/conda/bin/../lib/./libcudnn_ops_train.so.8: symbol _Z22cudnnGenericOpTensorNdILi3EE13cudnnStatus_tP12cudnnContext16cudnnGenericOp_t21cudnnNanPropagation_tPKdPKvPK17cudnnTensorStructS8_S8_SB_S8_S8_SB_Pv version libcudnn_ops_infer.so.8 not defined in file libcudnn_ops_infer.so.8 with link time referenceCould not load library libcudnn_cnn_train.so.8. Error: /opt/conda/bin/../lib/./libcudnn_ops_train.so.8: symbol _Z22cudnnGenericOpTensorNdILi3EE13cudnnStatus_tP12cudnnContext16cudnnGenericOp_t21cudnnNanPropagation_tPKdPKvPK17cudnnTensorStructS8_S8_SB_S8_S8_SB_Pv version libcudnn_ops_infer.so.8 not defined in file libcudnn_ops_infer.so.8 with link time referencePlease make sure libcudnn_cnn_train.so.8 is in your library path!Please make sure libcudnn_cnn_train.so.8 is in your library path!Please make sure libcudnn_cnn_train.so.8 is in your library path!Please make sure libcudnn_cnn_train.so.8 is in your library path!Please make sure libcudnn_cnn_train.so.8 is in your library path!Please make sure libcudnn_cnn_train.so.8 is in your library path!Please make sure libcudnn_cnn_train.so.8 is in your library path!  (0) UNKNOWN:  Failed to determine best cudnn convolution algorithm for:%cudnn-conv-bw-filter = (f32[1,1,768,1664]{1,0,2,3}, u8[0]{0}) custom-call(f32[2,768,90,180]{3,2,1,0} %add.19138, f32[2,1664,90,180]{3,2,1,0} %multiply.19168), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""gradient_tape/model/decode/Conv2D_1/Conv2DBackpropFilter"" source_file=""/opt/conda/lib/python3.7/site-packages/keras/optimizers/optimizer_experimental/optimizer.py"" source_line=176}, backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}""Original error: INTERNAL: All algorithms tried for %cudnn-conv-bw-filter = (f32[1,1,768,1664]{1,0,2,3}, u8[0]{0}) custom-call(f32[2,768,90,180]{3,2,1,0} %add.19138, f32[2,1664,90,180]{3,2,1,0} %multiply.19168), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""gradient_tape/model/decode/Conv2D_1/Conv2DBackpropFilter"" source_file=""/opt/conda/lib/python3.7/site-packages/keras/optimizers/optimizer_experimental/optimizer.py"" source_line=176}, backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}"" failed. Falling back to default algorithm.  Per-algorithm errors:To ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.         [[{{node while/replica_3/StatefulPartitionedCall}}]]         [[while/loop_body_control/_3358/_36]]```</details>
"
58030,1,521,0,0,0,MThalberg,0,"title:tf.keras.metrics.CategoricalAccuracy crash with xla on cuda when dtype is set to complex description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.10### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shelltf.keras.metrics.CategoricalAccuracy crash with xla on cuda when dtype is set to copmlex```### Standalone code to reproduce the issue```shellimport tensorflow as tfCategoricalAccuracy_class = tf.keras.metrics.CategoricalAccuracy(dtype=tf.complex64)@tf.function(jit_compile=True)def f(x0, x1, x2):    return CategoricalAccuracy_class(x0, x1, x2)x0 = [[0,0,1],[0,1,0]]x1 = [[0.1, 0.1, 0.8], [0.05, 0, 0.95]]x2 = [[0.5], [0.2]]f(x0, x1, x2)```### Relevant log output```shellF tensorflow/compiler/xla/literal_util.cc:200] No min value for given type.  abort (core dumped)```</details>
"
58007,0,317,186,0,0,wenscarl,0,"title:transpose_op_test.py fails with rank 1 complex input on GPU description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf2.10### Custom CodeYes### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellTests fails for cases(Same was observed for complex64).def testComplex128(self):  self._testBoth(np.array(1 + 2j).astype(np.complex128))  self._testBoth((1 + 2j) * np.arange(0, 21).astype(np.complex128))```It's unclear if it's necessary to conjugate a rank-1 complex input with conjugate=True. Even if passing input to output is the default case, the imag part became 1.999023 instead of 2.0 which could also fail `assertAllEqual`.```### Standalone code to reproduce the issue```shellpython tensorflow/python/kernel_tests/math_ops/transpose_op_test.py```### Relevant log output```shellnot equal lhs = array(1.-2.j)not equal rhs = array(1.+1.99902344j)Mismatched elements: 1 / 1 (100%)Max absolute difference: 3.99902344Max relative difference: 1.78911649 x: array(1.-2.j) y: array(1.+1.999023j)```</details>
"
57999,1,397,0,0,0,MThalberg,0,"title:`tf.gather` outputs wrong result with jit_compile description:  ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.10, tf-nightly### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 20.04### Current Behaviour?According to documentation, `tf.gather` should return `0` when `indices` is out of bound on GPU. However, in the code below, the jit compiled function returns `0.2` instead of `0`.### Standalone code to reproduce the issue```shellimport tensorflow as tfv = tf.constant([[0.1], [0.2]])i = tf.constant([[3]])def f(p):    return tf.gather(v, indices=i, validate_indices=None, axis=None, batch_dims=0)print(f(v)) # [[[0. ]]]f_jitted = tf.function(jit_compile=True)(f)print(f_jitted(v)) # [[[0.2]]]```### Relevant log output```tf.Tensor([[[0.]]], shape=(1, 1, 1), dtype=float32)tf.Tensor([[[0.2]]], shape=(1, 1, 1), dtype=float32)```
"
57981,1,1381,83,0,0,shijy16,0,"title:Another Check-fail in Conv2DBackpropFilter description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.10 and 2.11.0-dev20221005### Custom CodeNo### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version3.8### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versionCUDA 11.5### GPU model and memory_No response_### Current Behaviour?```shellIn current implementation of Conv2DBackpropFilter, arguments' shapes are not checked carefully. As a result, a check-fail can be triggered, which can lead to crash and DoS.The bug is similar to #57980, but has different inputs and outputs. So they might have different causes.```### Standalone code to reproduce the issue```shellimport osos.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'import tensorflow as tfprint(tf.__version__)with tf.device(""GPU:0""):    input = tf.random.uniform([1, 1, 1, 1, 1], dtype=tf.bfloat16)    filter_sizes = tf.saturate_cast(tf.random.uniform([1], minval=-128, maxval=129, dtype=tf.int64), dtype=tf.int32)    out_backprop = tf.random.uniform([1, 1, 1, 1], dtype=tf.bfloat16)    strides = [1, 1, 1]    use_cudnn_on_gpu = True    padding = ""SAME""    explicit_paddings = []    data_format = ""NHWC""    dilations = [1, 1, 1, 1]    res = tf.raw_ops.Conv2DBackpropFilter(        input=input,        filter_sizes=filter_sizes,        out_backprop=out_backprop,        strides=strides,        use_cudnn_on_gpu=use_cudnn_on_gpu,        padding=padding,        explicit_paddings=explicit_paddings,        data_format=data_format,        dilations=dilations,    )```### Relevant log output```shell2022-10-05 16:56:46.002944: F ./tensorflow/core/util/tensor_format.h:427] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, CAborted (core dumped)```</details>
"
57945,1,3147,1,0,0,Urkchar,0,"title:Error loading fashion_mnist dataset description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.10### Custom CodeYes### OS Platform and DistributionMicrosoft Windows 10 Home ersion 10.0.19043 Build 19043### Mobile device_No response_### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versionCUDA-11.2.2 cuDNN-8.1.0.77### GPU model and memoryGeForce GTX 1070### Current Behaviour?```shellI expected to be able to load the `fashion_mnist` data. This might be an issue with converting `:` to `%3A` in the urls on `fashion_mnist.py`.Here is the code from `fashion_mnist.py` that I think is relevant.    dirname = os.path.join(""datasets"", ""fashion-mnist"")    base = ""https://storage.googleapis.com/tensorflow/tf-keras-datasets/""    files = [        ""train-labels-idx1-ubyte.gz"",        ""train-images-idx3-ubyte.gz"",        ""t10k-labels-idx1-ubyte.gz"",        ""t10k-images-idx3-ubyte.gz"",    ]    paths = []    for fname in files:        paths.append(get_file(fname, origin=base + fname, cache_subdir=dirname))As you can see, `base` and `fname` are getting combined using string concatenation instead of something like urljoin. Relevant stackoverflow question: https://stackoverflow.com/questions/27115803/urllib-error-urlerror-urlopen-error-unknown-url-type-https```### Standalone code to reproduce the issue```shellimport tensorflow as tffashion_mnist = tf.keras.datasets.fashion_mnist(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()```### Relevant log output```shellDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gzTraceback (most recent call last):  File ""C:\Users\urkch\miniconda3\envs\tf\lib\site-packages\keras\utils\data_utils.py"", line 296, in get_file    urlretrieve(origin, fpath, DLProgbar())  File ""C:\Users\urkch\miniconda3\envs\tf\lib\site-packages\keras\utils\data_utils.py"", line 84, in urlretrieve    response = urlopen(url, data)  File ""C:\Users\urkch\miniconda3\envs\tf\lib\urllib\request.py"", line 214, in urlopen    return opener.open(url, data, timeout)  File ""C:\Users\urkch\miniconda3\envs\tf\lib\urllib\request.py"", line 517, in open    response = self._open(req, data)  File ""C:\Users\urkch\miniconda3\envs\tf\lib\urllib\request.py"", line 539, in _open    return self._call_chain(self.handle_open, 'unknown',  File ""C:\Users\urkch\miniconda3\envs\tf\lib\urllib\request.py"", line 494, in _call_chain    result = func(*args)  File ""C:\Users\urkch\miniconda3\envs\tf\lib\urllib\request.py"", line 1417, in unknown_open    raise URLError('unknown url type: %s' % type)urllib.error.URLError: <urlopen error unknown url type: https>During handling of the above exception, another exception occurred:Traceback (most recent call last):  File ""c:\Users\urkch\AppData\Local\Programs\Python\Python_Projects\tensorflow\Basic classification Classify images of clothing\classify.py"", line 12, in <module>    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()  File ""C:\Users\urkch\miniconda3\envs\tf\lib\site-packages\keras\datasets\fashion_mnist.py"", line 93, in load_data    paths.append(get_file(fname, origin=base + fname, cache_subdir=dirname))  File ""C:\Users\urkch\miniconda3\envs\tf\lib\site-packages\keras\utils\data_utils.py"", line 300, in get_file    raise Exception(error_msg.format(origin, e.errno, e.reason))Exception: URL fetch failure on https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz: None -- unknown url type: https```</details>
"
57934,1,0,0,0,0,DorianRudolph,0,"title:Class GpuDelegateFactory missing in Tensorflow Lite Java description:It looks like the `GpuDelegateFactory` is missing in the latest version of tflite on Android.When ipmorting with `org.tensorflow.lite.gpu.GpuDelegateFactory`, compilation fails with `Unresolved reference: GpuDelegateFactory`.I have imported the following packages:```    implementation 'org.tensorflow:tensorflow-lite:2.10.0'    implementation 'org.tensorflow:tensorflow-lite-api:2.10.0'    implementation 'org.tensorflow:tensorflow-lite-gpu:2.10.0'    implementation 'org.tensorflow:tensorflow-lite-gpu-delegate-plugin:0.4.2'    implementation 'org.tensorflow:tensorflow-lite-support:0.4.2'```But none of these seem to contain `GpuDelegateFactory`.Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.10.0### Custom CodeNo### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellIt looks like the `GpuDelegateFactory` is missing in the latest version of tflite on Android.When ipmorting with `org.tensorflow.lite.gpu.GpuDelegateFactory`, compilation fails with `Unresolved reference: GpuDelegateFactory`.I have imported the following packages:    implementation 'org.tensorflow:tensorflow-lite:2.10.0'    implementation 'org.tensorflow:tensorflow-lite-api:2.10.0'    implementation 'org.tensorflow:tensorflow-lite-gpu:2.10.0'    implementation 'org.tensorflow:tensorflow-lite-gpu-delegate-plugin:0.4.2'    implementation 'org.tensorflow:tensorflow-lite-support:0.4.2'```But none of these seem to contain `GpuDelegateFactory`.```### Standalone code to reproduce the issue```shellimport org.tensorflow.lite.gpu.GpuDelegateFactory```### Relevant log output_No response_</details>
"
57924,1,1102,17,0,1,denizgedik,0,"title:random.set_seed() does not work on tensorflow-macos description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.10### Custom CodeNo### OS Platform and DistributionMacOS 13.0 Ventura Beta 7### Mobile device_No response_### Python version3.9.13### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memoryM1 Metal### Current Behaviour?```shellusing tf.random.set_seed works perfectly on google colab environment, yet using tensorflow-macos version, it does not seem to work at all. It's not giving me an error either. Everytime I run my code, I get different results.Code I gave as an example uses numpy but even using data imported from pandas as DataFrames do not work. Which work on google colab.My miniconda installation and python installation is all fresh. I was using python version 3.10 but the apple website does not mention that, so uninstalled miniconda and reinstalled it using an older version, effectively downgrading python, but that did not fix the problem either.```### Standalone code to reproduce the issue```shellimport tensorflow as tfimport numpy as npX = np.array([-7.0, -4.0, -1.0, 2.0, 5.0, 8.0, 11.0, 14.0])y = np.array([3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0])tf.random.set_seed(42)model = tf.keras.Sequential([  tf.keras.layers.Dense(1)])model.compile(loss=tf.keras.losses.mae,              optimizer=tf.keras.optimizers.SGD(),              metrics=[""mae""])model.fit(tf.expand_dims(X, axis=-1), y, epochs=5)```### Relevant log output**First Time:**Epoch 1/52022-09-30 18:25:08.200023: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.1/1 [==============================] - 1s 583ms/step - loss: 15.3376 - mae: 15.3376Epoch 2/51/1 [==============================] - 0s 18ms/step - loss: 15.0563 - mae: 15.0563Epoch 3/51/1 [==============================] - 0s 21ms/step - loss: 14.8426 - mae: 14.8426Epoch 4/51/1 [==============================] - 0s 17ms/step - loss: 14.7101 - mae: 14.7101Epoch 5/51/1 [==============================] - 0s 38ms/step - loss: 14.5776 - mae: 14.5776<keras.callbacks.History at 0x2a0960730>**Second Time**Epoch 1/51/1 [==============================] - 0s 222ms/step - loss: 11.3347 - mae: 11.3347Epoch 2/51/1 [==============================] - 0s 10ms/step - loss: 11.2022 - mae: 11.2022Epoch 3/51/1 [==============================] - 0s 15ms/step - loss: 11.0697 - mae: 11.0697Epoch 4/51/1 [==============================] - 0s 16ms/step - loss: 10.9372 - mae: 10.9372Epoch 5/51/1 [==============================] - 0s 19ms/step - loss: 10.8047 - mae: 10.80472022-09-30 18:28:03.323935: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.<keras.callbacks.History at 0x2a09f6c40>
"
57902,1,1728,0,0,0,VictoriaGriffith,0,"title:`tf.compat.v1.nn.embedding_lookup` computes wrong forward gradient when indices are out of bound description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.9.1### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?`tf.compat.v1.nn.embedding_lookup` computes wrong forward gradient when indices are out of bound. In the example code below, `ids=[4]` is out of bound, so the output is all zero. The jacobian therefore is all zeros. However, the forward gradient is wrongly computed as`1.0` w.r.t. to `params[1,0]`.### Standalone code to reproduce the issue```shellimport tensorflow as tfparams = tf.constant([[0.4], [0.6]], dtype=tf.float32)ids = tf.constant([4], dtype=tf.int32)def f(params):    partition_strategy = ""mod""    validate_indices = False    max_norm = None    return tf.compat.v1.nn.embedding_lookup(params, ids,     partition_strategy=partition_strategy,     validate_indices=validate_indices,     max_norm=max_norm, )with tf.GradientTape(persistent=True) as tape:    tape.watch(params)    out = f(params)print(""input params: \n"", params)print(""outputs: "", out)jac = tape.jacobian(out, [params])print(""jacobian: "", jac)tangents = tf.constant([[0.], [1.]])with tf.autodiff.ForwardAccumulator(params,tangents) as acc:  out = f(params)print(""forward outputs: "", out)jvp = acc.jvp(params)print(""forward gradient: "", jvp)```### Relevant log output```shellWARNING:tensorflow:Converting IndexedSlices(indices=Tensor(""gradient_tape/Reshape_2:0"", shape=(1,), dtype=int32), values=Tensor(""gradient_tape/Reshape_1:0"", shape=(1, 1), dtype=float32), dense_shape=Tensor(""gradient_tape/Cast:0"", shape=(2,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)) to a dense representation may make it slow. Alternatively, output the indices and values of the IndexedSlices separately, and handle the vectorized outputs directly.input params:  tf.Tensor([[0.42328522] [0.47058594]], shape=(2, 1), dtype=float32)outputs:  tf.Tensor([[0.]], shape=(1, 1), dtype=float32)jacobian:  [<tf.Tensor: shape=(1, 1, 2, 1), dtype=float32, numpy=array([[[[0.],         [0.]]]], dtype=float32)>]forward outputs:  tf.Tensor([[0.]], shape=(1, 1), dtype=float32)forward gradient:  tf.Tensor([[0.] [1.]], shape=(2, 1), dtype=float32)```</details>
"
57896,0,1162,17,0,1,DNXie,0,"title:tf.histogram_fixed_width crash with segmentation fault description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.11.0-dev20220916### Custom CodeNo### OS Platform and DistributionUbuntu 18.04.4 LTS (x86_64)### Mobile device_No response_### Python version3.7.6### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versionN/A### GPU model and memory_No response_### Current Behaviour?```shell`tf.histogram_fixed_width` crash with segmentation fault```### Standalone code to reproduce the issue```shellimport numpy as npimport tensorflow as tftf.histogram_fixed_width(values=np.array([3e+38,100], dtype=np.float32), value_range=np.array([-1e+38,  3e+38]), nbins=1)```### Relevant log output```shell2022-09-28 19:23:13.760398: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory2022-09-28 19:23:13.760444: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)2022-09-28 19:23:13.760518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist2022-09-28 19:23:13.761047: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.Segmentation fault (core dumped)```</details>
"
57893,1,10044,1,0,0,JGuillaumin,0,"title:`Nan` errors multiple-gpu training with `MirroredStrategy` - RTX A5000 PCIe4.0 description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.10### Custom CodeNo### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version3.8.10### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versionCUDA11.0 & cuDNN8.1.0### GPU model and memoryRTX A5000 24GB### Current Behaviour?```shellI moved 8 Nvidia RTX A5000 from an old server (ASRockRack 3U8G-C612 : 2 Intel Xeon E5-2640 v4 with 8 PCIe3.0 lanes. Details here https://www.asrockrack.com/general/productdetail.asp?Model=3U8G-C612#Specifications) to a more recent server populated with 10 Nvidia RTX A5000 (including the 8 ones from the former server + 2 new RTX A5000) (Supermicro SYS-420GP-TNR : 2 Intel Xeon Gold 5317 with 12 PCIe4.0 lanes. Details here https://www.supermicro.com/en/products/system/gpu/4u/sys-420gp-tnr). I kept the same OS version (Ubuntu 20.04, with kernel=5.13.0-52-generic) and same driver version 515.57.All the 10 RTX A5000 are visible with `nvidia-smi`, but when I start to execute my code I have `nan` in losses and model's outputs after few iterations (less than 10, sometimes at the second iteration). It happens only when I use multiple GPUs. I wrote code that reproduces the errors.I used docker image provided by Tensorflow : `tensorflow/tensorflow:2.10.0-gpu-jupyter` (SHA `[tensorflow/tensorflow@sha256:a72deb34d32e26cf4253608b0e86ebb4e5079633380c279418afb5a131c499d6]`). Choice of `cross_device_ops=tf.distribute.ReductionToOneDevice(reduce_to_device=""/cpu:0"")` ? * If I use `tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.NcclAllReduce())`, the call to `distributed_train_step()` is hanging on the new server .. Need to `docker stop` the container. * Whatever the `cross_device_ops` the script is working properly on the old server. What you can see in the outputs : * Everything is OK for the first replicate, no `nan` in his inputs, `per_samples_losses`* But for the second replicate (if 2 GPUs) we have `nan` values in `images` used in `train_step()` and `nan` What is strange ? * `images` from the distributed dataset does not contain `nan` values whiles `images` collected after the train steps contains `nan`* And these `nan` values are only present on the second replica (or more if more than 2 GPUs) never on the 1st replica. After searching on past issues and StackOverflow, as first actions:* I disabled 'Intel VMX' within BIOS options. Same errors after. * Re-install drivers (even to 515.76). Same errors after. I roll-back to 515.57Also, I added old Nvidia TitanX (Pascal) into the older server to test the code. And before, when the 8 RTX A5000 were in the old server, everything was working well with my experiments. Feel free to ask me any questions or more details. Best regards```### Standalone code to reproduce the issue```shellimport tensorflow as tfimport numpy as npimport osfrom pprint import pprintdef main():    print(f""tf.version.GIT_VERSION : {tf.version.GIT_VERSION}"")    print(f""tf.version.VERSION : {tf.version.VERSION}"")    print(""tf.config.list_physical_devices() : "")    pprint(tf.config.list_physical_devices())    fashion_mnist = tf.keras.datasets.fashion_mnist    (train_images, train_labels), (_, _) = fashion_mnist.load_data()    # create 4D np.ndarray [N,H,W] -> [N,H,W,1]    train_images = train_images[..., np.newaxis]    # scale pixels to [0, 1]    train_images = train_images / np.float32(255)    # get the tf.distribute.Strategy    # strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.NcclAllReduce())  # tested    strategy = tf.distribute.MirroredStrategy(        cross_device_ops=tf.distribute.ReductionToOneDevice(reduce_to_device=""/cpu:0"")    )    print(f""Strategy : {strategy}"")    print(f""Number of devices: {strategy.num_replicas_in_sync}"")    # some training arguments    buffer_size = len(train_images)    batch_size_per_replica = 64    global_batch_size = batch_size_per_replica * strategy.num_replicas_in_sync    # create tf.data.Dataset    train_dataset = (        tf.data.Dataset.from_tensor_slices((train_images, train_labels))        .shuffle(buffer_size)        .batch(global_batch_size)    )    # extend to a distributed one version    train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)    with strategy.scope():        # Set reduction to `NONE` so you can do the reduction afterwards and divide by        # global batch size.        loss_object = tf.keras.losses.SparseCategoricalCrossentropy(            from_logits=True, reduction=tf.keras.losses.Reduction.NONE        )        def compute_loss(_labels, _predictions):            _per_example_loss = loss_object(_labels, _predictions)            _averaged_loss = tf.nn.compute_average_loss(                _per_example_loss, global_batch_size=global_batch_size            )            return _averaged_loss, _per_example_loss    with strategy.scope():        model = create_model()        optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)    def train_step(inputs):        _images, _labels = inputs        with tf.GradientTape() as tape:            _predictions = model(_images, training=True)            _averaged_loss, _per_example_loss = compute_loss(_labels, _predictions)        gradients = tape.gradient(_averaged_loss, model.trainable_variables)        optimizer.apply_gradients(zip(gradients, model.trainable_variables))        # return : averaged loss on all example, per_example_loss ([batch_size,] Tensor) and input images        return _averaged_loss, _per_example_loss, _images    @tf.function    def distributed_train_step(dataset_inputs):        _per_replica_averaged_losses, _per_replica_per_example_loss, _per_replica_images = strategy.run(            train_step, args=(dataset_inputs,)        )        return (            strategy.reduce(tf.distribute.ReduceOp.SUM, _per_replica_averaged_losses, axis=None),            _per_replica_per_example_loss,            _per_replica_images,        )    train_dist_iter = iter(train_dist_dataset)    iteration = 0    for _ in range(10):        # get next (images, labels) from the dataset        images, labels = next(train_dist_iter)        # do we have nan in these images ?        print(f""iteration={iteration}"")        if strategy.num_replicas_in_sync > 1:            print(                ""\t nan in per_replica_images from dataset : "",                {replica: np.isnan(images.numpy()).any() for replica, images in enumerate(images.values)},            )        else:            # if single replica (CPU or mono-GPU) : specific print statements            print(                ""\t nan in per_replica_images from dataset : "",                {0: np.isnan(images.numpy()).any()},            )        averaged_loss, per_replica_per_sample_losses, per_replica_images = distributed_train_step(            (images, labels)        )        print(f""\t averaged_loss = {averaged_loss}"")        if strategy.num_replicas_in_sync > 1:            # do we have nan in images returned by train_step() and distributed_train_step() ?            print(                ""\t nan in per_replica_images              : "",                {                    replica: np.isnan(images.numpy()).any()                    for replica, images in enumerate(per_replica_images.values)                },            )            # do we have nan in losses per samples per replica ?            print(                ""\t nan in per_replica_per_sample_losses   : "",                {                    replica: np.isnan(per_sample_losses.numpy()).any()                    for replica, per_sample_losses in enumerate(per_replica_per_sample_losses.values)                },            )            print(""\n"")        else:            # if single replica (CPU or mono-GPU) : specific print statements.            # do we have nan in images returned by train_step() and distributed_train_step() ?            print(                ""\t nan in per_replica_images              : "",                {0: np.isnan(per_replica_images.numpy()).any()},            )            # do we have nan in losses per samples per replica ?            print(                ""\t nan in per_replica_per_sample_losses   : "",                {0: np.isnan(per_replica_per_sample_losses.numpy()).any()},            )            print(""\n"")        iteration += 1def create_model():    model = tf.keras.Sequential(        [            tf.keras.layers.Conv2D(32, 3, activation=""relu""),            tf.keras.layers.MaxPooling2D(),            tf.keras.layers.Conv2D(64, 3, activation=""relu""),            tf.keras.layers.MaxPooling2D(),            tf.keras.layers.Flatten(),            tf.keras.layers.Dense(64, activation=""relu""),            tf.keras.layers.Dense(10),        ]    )    return modelif __name__ == ""__main__"":    main()```### Relevant log output```shellOn new server with 2 GPUs : nan in `images` used in the `train_step()` after 1 iterationCUDA_VISIBLE_DEVICES=""0,1"" python nan_error_tf.py[...logs...]iteration=0	 nan in per_replica_images from dataset :  {0: False, 1: False}	 averaged_loss = 2.320328712463379	 nan in per_replica_images              :  {0: False, 1: False}	 nan in per_replica_per_sample_losses   :  {0: False, 1: False}iteration=1	 nan in per_replica_images from dataset :  {0: False, 1: False}	 averaged_loss = nan	 nan in per_replica_images              :  {0: False, 1: True}	 nan in per_replica_per_sample_losses   :  {0: False, 1: False}iteration=2	 nan in per_replica_images from dataset :  {0: False, 1: False}	 averaged_loss = nan	 nan in per_replica_images              :  {0: False, 1: True}	 nan in per_replica_per_sample_losses   :  {0: True, 1: False}iteration=3	 nan in per_replica_images from dataset :  {0: False, 1: False}	 averaged_loss = nan	 nan in per_replica_images              :  {0: False, 1: True}	 nan in per_replica_per_sample_losses   :  {0: True, 1: False}On new server with 1 GPU : No `nan`, no issue. ```shellCUDA_VISIBLE_DEVICES=""0"" python nan_error_tf.py[... logs ...]iteration=0	 nan in per_replica_images from dataset :  {0: False}	 averaged_loss = 2.3019003868103027	 nan in per_replica_images              :  {0: False}	 nan in per_replica_per_sample_losses   :  {0: False}iteration=1	 nan in per_replica_images from dataset :  {0: False}	 averaged_loss = 2.2981200218200684	 nan in per_replica_images              :  {0: False}	 nan in per_replica_per_sample_losses   :  {0: False}iteration=2	 nan in per_replica_images from dataset :  {0: False}	 averaged_loss = 2.0697689056396484	 nan in per_replica_images              :  {0: False}	 nan in per_replica_per_sample_losses   :  {0: False}iteration=3	 nan in per_replica_images from dataset :  {0: False}	 averaged_loss = 1.8743340969085693	 nan in per_replica_images              :  {0: False}	 nan in per_replica_per_sample_losses   :  {0: False}```If I use the same script, with 1 or 2 (or more) GPUs on the old server, everything is working well.```</details>
"
57840,1,5700,31,0,0,farmaker47,0,"title:Build smaller AAR files locally (tensorflow-lite.aar, tensorflow-lite-select-tf-ops.aar) fail. description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.8.2### Custom CodeYes### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version5.3.1### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellTrying to follow the tutorial at https://www.tensorflow.org/lite/android/lite_build#build_and_install to build smaller .aar files using a .tflite model with TensorFlow ops the procedure fails with unknowk error. This happens when it tries to load the native TensorFlow runtime.```### Standalone code to reproduce the issue```shellGiving you the link to download the colab notebook and see all the output logs.https://colab.research.google.com/drive/1D_mdWel9Pk4zVbW1Lxk9yT8P5MZdjeGg?usp=sharing```### Relevant log output```shell...........................    Compiling tensorflow/core/common_runtime/session_state.cc; 7s local[15,079 / 16,006] 24 actions running    //tensorflow/core:portable_tensorflow_lib_lite; 24s local    Compiling tensorflow/core/common_runtime/partitioning_utils.cc; 12s local    Compiling tensorflow/core/common_runtime/ring_gatherer.cc; 10s local    Compiling tensorflow/core/common_runtime/ring_reducer.cc; 10s local    Compiling tensorflow/core/common_runtime/collective_util.cc; 9s local    //tensorflow/core:portable_tensorflow_lib_lite; 8s local    Compiling tensorflow/core/common_runtime/session_state.cc; 7s localERROR: /tensorflow_src/tensorflow/BUILD:1419:19: Executing genrule //tensorflow:tf_python_api_gen_v2 failed: (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument skipped)2022-09-26 04:46:44.851959: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.Traceback (most recent call last):  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>    from tensorflow.python._pywrap_tensorflow_internal import *ImportError: /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow10checkpoint26OpenTableTensorSliceReaderERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEPPNS0_17TensorSliceReader5TableEDuring handling of the above exception, another exception occurred:Traceback (most recent call last):  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 22, in <module>    from tensorflow.python.tools.api.generator import doc_srcs  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 36, in <module>    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 78, in <module>    f'{traceback.format_exc()}'ImportError: Traceback (most recent call last):  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>    from tensorflow.python._pywrap_tensorflow_internal import *ImportError: /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow10checkpoint26OpenTableTensorSliceReaderERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEPPNS0_17TensorSliceReader5TableEFailed to load the native TensorFlow runtime.See https://www.tensorflow.org/install/errors for some common causes and solutions.If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.[15,081 / 16,006] 23 actions running    //tensorflow/core:portable_tensorflow_lib_lite; 24s local    Compiling tensorflow/core/common_runtime/ring_gatherer.cc; 10s local    Compiling tensorflow/core/common_runtime/ring_reducer.cc; 10s local    Compiling tensorflow/core/common_runtime/collective_util.cc; 9s local    //tensorflow/core:portable_tensorflow_lib_lite; 8s local    Compiling tensorflow/core/common_runtime/session_state.cc; 7s local    Compiling tensorflow/core/common_runtime/lower_case_op.cc; 7s localTarget //tmp:tensorflow-lite-select-tf-ops failed to buildUse --verbose_failures to see the command lines of failed build steps.ERROR: /tensorflow_src/tensorflow/python/tools/BUILD:281:10 Middleman _middlemen/tensorflow_Spython_Stools_Sprint_Uselective_Uregistration_Uheader-runfiles failed: (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument skipped)INFO: Elapsed time: 4769.260s, Critical Path: 564.99sINFO: 13392 processes: 135 internal, 13257 local.FAILED: Build did NOT complete successfully```</details>
"
57820,1,1969,0,0,0,ShiqiaoZhou,0,"title:RoBERTa example from tfhub produces error ""During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string"" description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.8.2### Custom CodeYes### OS Platform and Distribution_No response_### Mobile device_No response_### Python version3.7.14 ### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellA bug happened!I would like to use the roberta-base model from tfhub. I am trying to run the example below, although I get an error when I try to feed sentences to model as input. I get the following error Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string. I am using python 3.7, tensorflow 2.8```### Standalone code to reproduce the issue```shell# define a text embedding modeltext_input = tf.keras.layers.Input(shape=(), dtype=tf.string)preprocessor = hub.KerasLayer(""https://tfhub.dev/jeongukjae/roberta_en_cased_preprocess/1"")encoder_inputs = preprocessor(text_input)encoder = hub.KerasLayer(""https://tfhub.dev/jeongukjae/roberta_en_cased_L-12_H-768_A-12/1"", trainable=True)encoder_outputs = encoder(encoder_inputs)pooled_output = encoder_outputs[""pooled_output""]      # [batch_size, 768].sequence_output = encoder_outputs[""sequence_output""]  # [batch_size, seq_length, 768].model = tf.keras.Model(text_input, pooled_output)# You can embed your sentences as followssentences = tf.constant([""(your text here)""])print(model(sentences))```### Relevant log output```shellInvalidArgumentError: Graph execution error:2 root error(s) found.  (0) INVALID_ARGUMENT:  During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string	 [[{{node map/TensorArrayUnstack/TensorListFromTensor}}]]	 [[model/preprocessing/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/bpe_sentencepiece_tokenizer/StatefulPartitionedCall/RaggedFromRowSplits_1/RowPartitionFromRowSplits/assert_non_negative/assert_less_equal/Assert/AssertGuard/else/_18720/RaggedFromRowSplits_1/RowPartitionFromRowSplits/assert_non_negative/assert_less_equal/Assert/AssertGuard/Assert/data_0/_135]]  (1) INVALID_ARGUMENT:  During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string	 [[{{node map/TensorArrayUnstack/TensorListFromTensor}}]]0 successful operations.0 derived errors ignored. [Op:__inference_train_function_534484]```</details>
"
57806,0,4529,1,0,0,supercharleszhu,0,"title:Asset file vanishes after loading & saving the model twice description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.4### Custom CodeNo### OS Platform and DistributionLinux### Mobile device_No response_### Python version3.8### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?When I created the tf.Module with an asset file inside the TextFileInitializer, and save & load the module twice, the asset file will vanish after the 2nd save, and the loading will fail, complaining the absolute path of that file cannot be found. ```import shutilimport tensorflow as tfclass PrimaryModule(tf.Module):    def __init__(self, name=None):        super(PrimaryModule, self).__init__(name=name)        initializer = tf.lookup.TextFileInitializer(            ""chzhu_vocab.txt"",            key_dtype=tf.int64,            key_index=0,            value_dtype=tf.int64,            value_index=1,            delimiter="" "",        )        self.table = tf.lookup.StaticVocabularyTable(initializer, 1)    @tf.function    def __call__(self, inputs):        return self.table.lookup(inputs)model = PrimaryModule()print(model(tf.constant([509323409], dtype=tf.int64)))tf.saved_model.save(model, ""asset_model"")imported_new = tf.saved_model.load(""asset_model"")print(imported_new(tf.constant([509323409], dtype=tf.int64)))tf.saved_model.save(imported_new, ""wrapped_assets_model"")# Error when loading the model for the second time, it will be trying to find the original path# instead of path in the asset folder inside modelshutil.rmtree(""asset_model"")shutil.rm(""chzhu_vocab.txt"")tf.saved_model.load(""wrapped_assets_model"")```Error: see full stack trace below ```FileNotFoundError:  chzhu_vocab.txt; No such file or directory```When changing `initializer` to be `self.initializer` in the object constructor, it will work as normal```import tensorflow as tfclass PrimaryModule(tf.Module):    def __init__(self, name=None):        super(PrimaryModule, self).__init__(name=name)        self.initializer = tf.lookup.TextFileInitializer(            ""chzhu_vocab.txt"",            key_dtype=tf.int64,            key_index=0,            value_dtype=tf.int64,            value_index=1,            delimiter="" "",        )        self.table = tf.lookup.StaticVocabularyTable(self.initializer, 1)    @tf.function    def __call__(self, inputs):        return self.table.lookup(inputs)# ..same saving logics```I did a deep dive into the model saving logic in TF 2.4, and found the difference here:When saving the model for the first time, the model saver will save the object dependencies into the SavedObjectGraph:object_graph_def in the meta_graph_def:```object_graph_def {  nodes {    children {      node_id: 1      local_name: ""table""    }    children {      node_id: 2      local_name: ""signatures""    }    children {      node_id: 5      local_name: ""__call__""    }    user_object {      identifier: ""_generic_user_object""      version {        producer: 1        min_consumer: 1      }    }  }  nodes {    children {      node_id: 3      local_name: ""_initializer""    }    children {      node_id: 6      local_name: ""_create_resource""    }    children {      node_id: 7      local_name: ""_initialize""    }    children {      node_id: 8      local_name: ""_destroy_resource""    }    resource {    }  }闂傚倸鍊搁崐鐑芥嚄閸洖纾婚柕濞炬櫅绾惧潡鏌涘┑鍡楊仱闁哄鐗楅妵鍕煛娴ｈ鍤傾s we can see above, `_initializer` is not the attribute of the top-level object and is the child of `VocabularyTable`, which is a resource object. The dependency will look like object -> table -> initializer -> Asset.During the first time loading* The Loader will try to recreate all the objects in the SavedObjectGraph, * The resource object will be recreated as _RestoredResource, which inherits the base.Trackable (not AutoTrackable). * When calling `_setattr_` in the _add_object_graph_edges(), it will not update _checkpoint_dependencies (unlike normal object inheriting AutoTrackable, which override the _setattr_ function to update the attribute dependencies.) Therefore, the table -> initializer -> Asset dependency is not entirely recovered.During the second time saving, it will * find out the Asset objects in the object dependencies via Breadth First Traversal, and add that into asset_info. * in the _fill_meta_graph_def(), we will create asset_initializer_ops for all Assets in asset_info.* in the _fill_meta_graph_def(), the asset_initializer and its downstream variables will be written into init_ops, and the main graph will be updated to initialize the variable from the corresponding asset_initializer.However, since in the previous loading stage, the _checkpoint_dependencies will be empty for the 闂傚倸鍊搁崐鐑芥嚄閸洖纾婚柕濞炬櫅绾惧潡鏌熺紒妯哄潑婵℃彃鐗撻弻锝咁潩椤撗勭le闂?attribute, and we cannot access the Asset object via Breadth First Traversal of the object graph. Thus we are getting empty asset_info, and 2, and 3 above will not happen.### Standalone code to reproduce the issue```shellimport shutilimport tensorflow as tfclass PrimaryModule(tf.Module):    def __init__(self, name=None):        super(PrimaryModule, self).__init__(name=name)        initializer = tf.lookup.TextFileInitializer(            ""chzhu_vocab.txt"",            key_dtype=tf.int64,            key_index=0,            value_dtype=tf.int64,            value_index=1,            delimiter="" "",        )        self.table = tf.lookup.StaticVocabularyTable(initializer, 1)    @tf.function    def __call__(self, inputs):        return self.table.lookup(inputs)model = PrimaryModule()print(model(tf.constant([509323409], dtype=tf.int64)))tf.saved_model.save(model, ""asset_model"")imported_new = tf.saved_model.load(""asset_model"")print(imported_new(tf.constant([509323409], dtype=tf.int64)))tf.saved_model.save(imported_new, ""wrapped_assets_model"")# Error when loading the model for the second timeshutil.rmtree(""asset_model"")tf.saved_model.load(""wrapped_assets_model"")```### Relevant log output```shellTraceback (most recent call last):  File ""/home/chzhu/tf2-trainer/tf2-trainer-component-training/test/asset_testing.py"", line 32, in <module>    tf.saved_model.load(""custom_model/wrapped_assets_model"")  File ""/home/chzhu/tf2-trainer/build/tf2-trainer-component-training/environments/development-venv/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py"", line 859, in load    return load_internal(export_dir, tags, options)[""root""]  File ""/home/chzhu/tf2-trainer/build/tf2-trainer-component-training/environments/development-venv/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py"", line 893, in load_internal    str(err) + ""\n If trying to load on a different device from the ""FileNotFoundError:  custom_model/asset_model/assets/chzhu_vocab.txt; No such file or directory         [[{{node StatefulPartitionedCall/text_file_init/InitializeTableFromTextFileV2}}]] [Op:__inference_restored_function_body_375]```</details>
"
57786,1,268,0,0,0,VictoriaGriffith,0,"title:Wrong NaN gradient for `tf.acos` description: ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.10### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?`tf.math.acos` is an element-wise operation, thus the gradient should be`0` when the input and output elements have different indices. However, in the example below, the gradient of `tf.acos(x)[0]` w.r.t. `x[2]` is computed as `nan`, which is wrong. It should be `0`.### Standalone code to reproduce the issue```shellimport tensorflow as tfx = tf.constant([-0.5, 0.5, 2.0])with tf.GradientTape(persistent=True) as tape:    tape.watch(x)    y = tf.acos(x)[0:1]print(tape.gradient(y, x))```### Relevant log output```shelltf.Tensor([-1.1547005 -0.               nan], shape=(3,), dtype=float32)```
"
57779,0,325,50,0,0,jxy,0,"title:tf.cast python float to tf.float64 or tf.complex128 leads to loss of precision description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versionv2.10.0-rc3-6-g359c3cdfc5f 2.10.0### Custom CodeNo### OS Platform and Distributiontested on Linux and macOS ### Mobile device_No response_### Python versiontested with 3.9 and 3.10### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shell>>> tf.cast(0.2, tf.float64)<tf.Tensor: shape=(), dtype=float64, numpy=0.20000000298023224>>>> tf.cast(0.2, tf.complex128)<tf.Tensor: shape=(), dtype=complex128, numpy=(0.20000000298023224+0j)>```### Standalone code to reproduce the issue```shellimport tensorflow as tftf.print(tf.cast(0.2, tf.float64))tf.print(tf.math.real(tf.cast(0.2, tf.complex128)))```### Relevant log output_No response_</details>
"
57775,0,1571,14,0,0,psunn,0,"title:TESTs Failed: kernels:unsorted_segment_{prod, max, min, sum}_test description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versionlatest master branch### Custom CodeNo### OS Platform and DistributionUbuntu 9.4.0-1ubuntu1~18.04### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_</details>### Current Behaviour?```shellTFLite kernel TESTs failing://tensorflow/lite/kernels:unsorted_segment_prod_test//tensorflow/lite/kernels:unsorted_segment_max_test//tensorflow/lite/kernels:unsorted_segment_min_test//tensorflow/lite/kernels:unsorted_segment_sum_test```### Standalone code to reproduce the issue```shellbazel test //tensorflow/lite/kernels:unsorted_segment_min_test --test_output=all```### Relevant log output```shell(unsorted_segment_min_test as example, similar log for other TESTs)...ERROR: /media/WORK/mirror-tensorflow/tensorflow/lite/kernels/BUILD:2850:8: Compiling tensorflow/lite/kernels/unsorted_segment_min_test.cc failed: (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 197 arguments skipped)tensorflow/lite/kernels/unsorted_segment_min_test.cc:136:67: error: expected constructor, destructor, or type conversion before ';' token  136 | GTEST_ALLOW_UNINSTANTIATED_PARAMETERIZED_TEST(UnsortedSegmentTest);      |                                                                   ^cc1plus: warning: unrecognized command line option '-Wno-array-parameter'cc1plus: warning: unrecognized command line option '-Wno-unknown-warning'Target //tensorflow/lite/kernels:unsorted_segment_min_test failed to buildUse --verbose_failures to see the command lines of failed build steps.INFO: Elapsed time: 7.023s, Critical Path: 6.42sINFO: 12 processes: 12 internal.FAILED: Build did NOT complete successfully```
"
57755,1,0,0,0,0,DNXie,0,"title:tf.image.combined_non_max_suppression crash with segmentation fault description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.11.0-dev20220916### Custom CodeNo### OS Platform and DistributionUbuntu 18.04.4 LTS (x86_64)### Mobile device_No response_### Python version3.7.6### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versionN/A### GPU model and memory_No response_### Current Behaviour?```shelltf.image.combined_non_max_suppression crash with segmentation fault when `max_total_size` is given a large value```### Standalone code to reproduce the issue```shellimport numpy as npimport tensorflow as tftf.image.combined_non_max_suppression(max_output_size_per_class=10, max_total_size=1916847725, scores=np.ones((2,2,1)), boxes=np.ones((2,2,1,4)))```### Relevant log output```shell2022-09-19 20:56:42.880536: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory2022-09-19 20:56:42.880586: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)2022-09-19 20:56:42.880661: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist2022-09-19 20:56:42.881322: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2022-09-19 20:56:42.924370: W tensorflow/core/kernels/image/non_max_suppression_op.cc:995] Detected a large value for `max_total_size`. This may cause OOM error. (max_total_size: 1916847725)Segmentation fault (core dumped)```</details>
"
57754,0,445,17,0,0,DNXie,0,"title:tf.image.crop_and_resize crash (abort) when given num_boxes=0 description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.11.0-dev20220916### Custom CodeNo### OS Platform and DistributionUbuntu 18.04.4 LTS (x86_64)### Mobile device_No response_### Python version3.7.6### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versionN/A### GPU model and memory_No response_### Current Behaviour?```shelltf.image.crop_and_resize crash (abort) when given num_boxes=0```### Standalone code to reproduce the issue```shellimport numpy as npimport tensorflow as tftf.image.crop_and_resize(crop_size=[1,1], box_indices=np.ones((0,1)), boxes=np.ones((0,4)), image=np.ones((2,2,2,2)))```### Relevant log output```shell2022-09-19 20:55:05.906144: F tensorflow/core/framework/tensor_shape.cc:45] Check failed: NDIMS == dims() (1 vs. 2)Asking for tensor of 1 dimensions from a tensor of 2 dimensionsAborted (core dumped)```</details>
"
57753,1,606,0,0,0,creakseek,0,"title:`UnimplementedError` when calling Conv2D in TF 2.10 description:  ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.10, tf-nightly### Custom CodeNo### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?The example below is from the [Con2D documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D?hl=en). It fails to execute the basic Conv2D in TensorFlow 2.10 and tf-nightly (on CUDA). This can be reproduced in Colab.This issue does not happen in TensorFlow 2.9 and 2.8.### Standalone code to reproduce the issue```shell# In Colab:# !pip install tensorflow==2.10input_shape = (4, 28, 28, 3)x = tf.random.normal(input_shape)y = tf.keras.layers.Conv2D(2, 3, activation='relu', input_shape=input_shape[1:])(x)print(y.shape)```### Relevant log output```shellUnimplementedError: Exception encountered when calling layer ""conv2d_2"" ""                 f""(type Conv2D).{{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:GPU:0}} DNN library is not found. [Op:Conv2D]Call arguments received by layer ""conv2d_2"" ""                 f""(type Conv2D):  闂?inputs=tf.Tensor(shape=(4, 28, 28, 3), dtype=float32)```
"
57752,1,1200,0,0,0,creakseek,0,"title:session crashes (CPU->GPU Memcpy failed) when running BatchNormalization description:  ### Issue TypeBug### Sourcebinary### Tensorflow Versiontfnightly### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?When I run the code snippet below (`relu` and `BatchNormalization`) for the first time, I encounter `InternalError`. Then I try to run it again and session crashes.### Standalone code to reproduce the issue1. Run for a single time: throws InternalError```shellimport tensorflow as tfimport numpy as npprint(tf.__version__)input_data = np.random.rand(1, 3, 3, 1).astype(np.float32)output_data = tf.keras.activations.relu(    tf.keras.layers.BatchNormalization(axis=-1)(input_data))```Log output:```shell2.11.0-dev20220919InternalError: Exception encountered when calling layer 'batch_normalization' (type BatchNormalization).{{function_node __wrapped__FusedBatchNormV3_device_/job:localhost/replica:0/task:0/device:GPU:0}} cuDNN launch failure : input shape ([1,3,3,1]) [Op:FusedBatchNormV3]Call arguments received by layer 'batch_normalization' (type BatchNormalization):  闂?inputs=tf.Tensor(shape=(1, 3, 3, 1), dtype=float32)  闂?training=None```2. Run for a second time: crash```import tensorflow as tfimport numpy as nptry:  input_data = np.random.rand(1, 3, 3, 1).astype(np.float32)  output_data = tf.keras.activations.relu(    tf.keras.layers.BatchNormalization(axis=-1)(input_data))except:  passinput_data = np.random.rand(1, 3, 3, 1).astype(np.float32)output_data = tf.keras.activations.relu(    tf.keras.layers.BatchNormalization(axis=-1)(input_data)) # crash here```Relevant logs:```F tensorflow/core/common_runtime/gpu/gpu_util.cc:386] CPU->GPU Memcpy failed```
"
57749,0,3161,1,0,0,wsmigaj,0,"title:Integer divide-by-0 during fused convolution with oneDNN on CPUs supporting AVX512 instructions description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.9.1, 2.10.0### Custom CodeNo### OS Platform and DistributionWindows 10### Mobile device_No response_### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memoryNo GPU### Current Behaviour?With oneDNN optimisations enabled and a CPU supporting AVX512 instructions, an _Integer divide-by-zero_ exception occurs during execution of fused convolutions with certain input sizes.To reproduce:1. Use a machine equipped with a CPU supporting AVX512 instructions (e.g. Intel i7-1165G7).2. Enable oneDNN optimisations by setting the `TF_ENABLE_ONEDNN_OPTS` environment variable to `1` and disable GPU usage by setting the `CUDA_VISIBLE_DEVICES` environment variable to `-1`.3. Run the script below.4. The `python.exe` process will crash with an _Integer divide-by-zero_ exception.The problem can be worked around by setting the `DNNL_MAX_CPU_ISA` variable to e.g. `AVX2` in order to prevent oneDNN from using AVX512 instructions.The issue occurs in TensorFlow 2.9.1 and 2.10.0 installed using `pip`. To debug it, I've built 2.9.1 TensorFlow from source with (some) debugging symbols and obtained the following stack trace:```[0x0]   _pywrap_tensorflow_internal!dnnl::impl::cpu::x64::brgemm_convolution_utils::brg_blocking_t::est_eff_1x1 + 0x38d   [0x1]   _pywrap_tensorflow_internal!dnnl::impl::cpu::x64::brgemm_convolution_utils::brg_blocking_t::calc_blocks_1x1 + 0x9fc   [0x2]   _pywrap_tensorflow_internal!dnnl::impl::cpu::x64::brgemm_convolution_utils::init_1x1_conf + 0x434   [0x3]   _pywrap_tensorflow_internal!dnnl::impl::cpu::x64::brgemm_1x1_convolution_fwd_t<71>::pd_t::init + 0x2f4   [0x4]   _pywrap_tensorflow_internal!dnnl::impl::primitive_desc_t::create<dnnl::impl::cpu::x64::brgemm_1x1_convolution_fwd_t<71>::pd_t> + 0x17c   [0x5]   _pywrap_tensorflow_internal!dnnl_primitive_desc_iterator::operator++ + 0x2c1   [0x6]   _pywrap_tensorflow_internal!dnnl_primitive_desc_iterator_create + 0x1de   [0x7]   _pywrap_tensorflow_internal!dnnl::primitive_desc::primitive_desc + 0x73   [0x8]   _pywrap_tensorflow_internal!tensorflow::MklConvFwdPrimitive<float,float,float,float>::Setup + 0x6d7   [0x9]   _pywrap_tensorflow_internal!tensorflow::MklConvFwdPrimitive<float,float,float,float>::MklConvFwdPrimitive<float,float,float,float> + 0x1a8   [0xa]   _pywrap_tensorflow_internal!tensorflow::MklConvFwdPrimitiveFactory<float,float,float,float>::Get + 0x14c   [0xb]   _pywrap_tensorflow_internal!tensorflow::MklConvOp<Eigen::ThreadPoolDevice,float,float,float,float,float,int,1,0,0,1>::Compute + 0x1ba0   [0xc]   _pywrap_tensorflow_internal!tensorflow::ThreadPoolDevice::Compute + 0x4a   [0xd]   _pywrap_tensorflow_internal!tensorflow::`anonymous namespace'::ExecutorState<tensorflow::SimplePropagatorState>::ProcessSync + 0x13f   [0xe]   _pywrap_tensorflow_internal!tensorflow::`anonymous namespace'::ExecutorState<tensorflow::SimplePropagatorState>::Process + 0xf58   [0xf]   _pywrap_tensorflow_internal!std::_Func_class<void>::operator() + 0xf   [0x10]   _pywrap_tensorflow_internal!tensorflow::thread::EigenEnvironment::ExecuteTask + 0x13   [0x11]   _pywrap_tensorflow_internal!Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop + 0x4b6   [0x12]   _pywrap_tensorflow_internal!std::_Func_class<void>::operator() + 0xf   [0x13]   _pywrap_tensorflow_internal!tensorflow::thread::EigenEnvironment::CreateThread::__l2::<lambda_fe7aa395b13fe170862dcdb4d85eb030>::operator() + 0x38   [0x14]   _pywrap_tensorflow_internal!std::invoke + 0x38   [0x15]   _pywrap_tensorflow_internal!std::_Invoker_ret<void,1>::_Call + 0x38   [0x16]   _pywrap_tensorflow_internal!std::_Func_impl_no_alloc<<lambda_fe7aa395b13fe170862dcdb4d85eb030>,void>::_Do_call + 0x41   [0x17]   _pywrap_tensorflow_internal!std::thread::_Invoke<std::tuple<std::function<void __cdecl(void)> >,0> + 0x18   [0x18]   ucrtbase!thread_start<unsigned int (__cdecl*)(void *),1> + 0x42   ```### Standalone code to reproduce the issue```pythonimport tensorflow as tfconv2d = tf.keras.layers.Conv2D(filters=8, kernel_size=(1, 1), padding='same')relu = tf.keras.layers.ReLU()@tf.functiondef fused_conv_bias_relu(x):    y = conv2d(x)    y = relu(y)    return yx_shape = [1, 2048, 2048, 8]inputs = tf.random.normal(x_shape)outputs = fused_conv_bias_relu(inputs)print(""Success!"")```### Relevant log output_No response_</details>
"
57739,1,0,0,1,0,Chaztikov,0,"title:2022-09-17 23:49:02.600018: I tensorflow/stream_executor/gpu/asm_compiler.cc:323] ptxas warning : Registers are spilled to local memory in function 'fusion_24', 8 bytes spill stores, 16 bytes spill loads ptxas warning : Registers are spilled to local memory in function '__internal_trig_reduction_slowpathd', 4 bytes spill stores, 4 bytes spill loads description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.9.2### Custom CodeYes### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellTrying to run deepxde with tensorflow (TF2) backend.  I think this is related tohttps://github.com/tensorflow/tensorflow/issues/33375This question partly relates to the answer provided by thereIn that issue, the following answer is given> Hi @kleyersoma. The workaround for this particular problem on unix-based machines is to link your cuda bin to your working directory. Go to the directory, where you launch your python code and create the link:> `ln -s /full/path/to/your/cuda/installation/bin .`> This sovles the problem. The point is that TF first tries to load the ptxas from ./bin directory, then from /usr/local/cuda/bin. Unfortunately, it completely ignores the environment variables (which I consider to be a bug).It is not clear to me what ""the directory, where you launch your python code"" refers toI am running Ubuntu 22.04, and which python3gives/usr/bin/python3Is this the directory you're referring to?If so, in my case I would doln -s /usr/local/cuda/bin /usr/bin/python3Is that correct? Thanks.```### Standalone code to reproduce the issue```shellhttps://colab.research.google.com/drive/1rYD_GMLAWJ6uTx76RfYvLWXB2nf9NP7Q?usp=sharing```### Relevant log output```shellU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2022-09-17 23:48:43.139452: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.2022-09-17 23:48:43.139501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10126 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1mexclusions  []Compiling model...'compile' took 0.000393 sWarning: epochs is deprecated and will be removed in a future version. Use iterations instead.Training model...2022-09-17 23:48:46.762887: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x560ea9633610 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:2022-09-17 23:48:46.762922: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.12022-09-17 23:48:46.842763: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.2022-09-17 23:48:51.944301: I tensorflow/stream_executor/gpu/asm_compiler.cc:323] ptxas warning : Registers are spilled to local memory in function 'input_fusion_reduce_4', 33468 bytes spill stores, 38624 bytes spill loadsptxas warning : Registers are spilled to local memory in function '__internal_accurate_pow', 132 bytes spill stores, 132 bytes spill loadsptxas warning : Registers are spilled to local memory in function '__internal_trig_reduction_slowpathd', 56 bytes spill stores, 48 bytes spill loads2022-09-17 23:48:51.953246: I tensorflow/compiler/jit/xla_compilation_cache.cc:478] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.2022-09-17 23:48:57.307073: I tensorflow/stream_executor/gpu/asm_compiler.cc:323] ptxas warning : Registers are spilled to local memory in function 'input_fusion_reduce_4', 33468 bytes spill stores, 38624 bytes spill loadsptxas warning : Registers are spilled to local memory in function '__internal_accurate_pow', 132 bytes spill stores, 132 bytes spill loadsptxas warning : Registers are spilled to local memory in function '__internal_trig_reduction_slowpathd', 56 bytes spill stores, 48 bytes spill loadsStep      Train loss                                                                                    Test loss                                                                                     Test metric0         [1.98e+03, 3.67e+02, 3.39e-02, 2.04e-01, 3.69e-02, 2.30e-01, 9.99e-02, 2.29e-01, 7.95e-02]    [1.98e+03, 3.67e+02, 3.39e-02, 2.04e-01, 3.69e-02, 2.30e-01, 9.99e-02, 2.29e-01, 7.95e-02]    []  2022-09-17 23:49:02.600018: I tensorflow/stream_executor/gpu/asm_compiler.cc:323] ptxas warning : Registers are spilled to local memory in function 'fusion_24', 8 bytes spill stores, 16 bytes spill loadsptxas warning : Registers are spilled to local memory in function '__internal_trig_reduction_slowpathd', 4 bytes spill stores, 4 bytes spill loads```</details>
"
57738,1,4110,147,0,0,aliencaocao,0,"title:Crashes with no error message but exit code -1073740791 (0xC0000409) on CUDA 11.7 description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.9.2, tf 2.10 both affected (did not test previous versions)### Custom CodeYes### OS Platform and DistributionWindows 10 19043### Mobile device_No response_### Python version3.9.13### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version11.7/8401### GPU model and memory_No response_### Current Behaviour?```shellAny python script that attempts to train a tensorflow model immediately crashes during model.fit call.```### Standalone code to reproduce the issue```shellimport tensorflow as tffrom tensorflow.keras.models import Model, load_modelfrom tensorflow.keras.layers import *(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()x_train, x_test = x_train / 255.0, x_test / 255.0  # normalize to between 0-1# model layersxIn = Input((28, 28, 1))x = Conv2D(32, (3, 3), activation='relu')(xIn)x = Dropout(0.4)(x)x = Conv2D(64, (3, 3), activation='relu')(x)x = Dropout(0.4)(x)x = Conv2D(128, (3, 3), activation='relu')(x)x = Dropout(0.4)(x)x = Flatten()(x)x = Dense(128, activation='swish')(x)x = Dropout(0.5)(x)xOut = Dense(10)(x)model = Model(inputs=xIn, outputs=xOut)model.compile(    optimizer=tf.keras.optimizers.Adam(1e-3),    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),    metrics=['accuracy'])callbacks = [    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True),    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=5, verbose=1)]model.summary()model.fit(x_train, y_train, epochs=100, batch_size=16, validation_data=(x_test, y_test), callbacks=callbacks)```### Relevant log output```shell""C:\Program Files\Python39\python.exe"" C:\Users\alien\Documents\PyCharm-Projects\IRS-ML\test.py 2022-09-18 12:12:09.933811: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2022-09-18 12:12:10.296700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9436 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:2b:00.0, compute capability: 8.6Model: ""model""_________________________________________________________________ Layer (type)                Output Shape              Param #   ================================================================= input_1 (InputLayer)        [(None, 28, 28, 1)]       0                                                                           conv2d (Conv2D)             (None, 26, 26, 32)        320                                                                         dropout (Dropout)           (None, 26, 26, 32)        0                                                                           conv2d_1 (Conv2D)           (None, 24, 24, 64)        18496                                                                       dropout_1 (Dropout)         (None, 24, 24, 64)        0                                                                           conv2d_2 (Conv2D)           (None, 22, 22, 128)       73856                                                                       dropout_2 (Dropout)         (None, 22, 22, 128)       0                                                                           flatten (Flatten)           (None, 61952)             0                                                                           dense (Dense)               (None, 128)               7929984                                                                     dropout_3 (Dropout)         (None, 128)               0                                                                           dense_1 (Dense)             (None, 10)                1290                                                                       =================================================================Total params: 8,023,946Trainable params: 8,023,946Non-trainable params: 0_________________________________________________________________Epoch 1/1002022-09-18 12:12:11.925564: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401Process finished with exit code -1073740791 (0xC0000409)```![image](https://user-images.githubusercontent.com/20109683/190885518-bbaca094-59d3-43cc-8580-1e5a0636314f.png)can see GPU memory being briefly taken up before the crash and GPU usage spikes to 100 for a second too.</details>
"
57735,1,213,10,0,1,crbean,0,"title:Vulnerability CVE-2022-37434 description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.6.5### Custom CodeNo### OS Platform and DistributionEulerOS 2.5\EulerOS 2.10### Mobile device-### Python version3.9### Bazel version-### GCC/Compiler version-### CUDA/cuDNN version-### GPU model and memory-### Current Behaviour?```shellHi there, it seems that zlib 1.2.12 has a vulnerability CVE-2022-37434. Please fix it or confirm tf 2.6.5 is not affected by it.Here is the detail:https://nvd.nist.gov/vuln/detail/CVE-2022-37434```### Standalone code to reproduce the issue```shell-```### Relevant log output_No response_</details>
"
57734,1,477,10,0,1,crbean,0,"title:Vulnerability CVE-2022-32207闂傚倸鍊搁崐椋庢濮橆剦鐒界憸宥堢亱闂佸搫鍟崐濠氬础濮樿京纾兼い鏍电稻閻?2022-32205闂傚倸鍊搁崐椋庢濮橆剦鐒界憸宥堢亱闂佸搫鍟崐濠氬础濮樿京纾兼い鏍电稻閻?2022-32206闂傚倸鍊搁崐椋庢濮橆剦鐒界憸宥堢亱闂佸搫鍟崐濠氬础濮樿京纾兼い鏍电稻閻?2022-32208闂傚倸鍊搁崐椋庢濮橆剦鐒界憸宥堢亱闂佸搫鍟崐濠氬础濮樿京纾兼い鏍电稻閻?2022-35252  description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.6.5### Custom CodeNo### OS Platform and DistributionEulerOS 2.5\EulerOS 2.10### Mobile device-### Python version3.9### Bazel version-### GCC/Compiler version-### CUDA/cuDNN version-### GPU model and memory-### Current Behaviour?```shellHi there, it seems that curl 7.83.1 has a vulnerability CVE-2022-32207 CVE-2022-32205 CVE-2022-32206 CVE-2022-32208 CVE-2022-35252. Please fix it or confirm tf 2.6.5 is not affected by it.Here is the detail:https://nvd.nist.gov/vuln/detail/CVE-2022-32207https://nvd.nist.gov/vuln/detail/CVE-2022-32205https://nvd.nist.gov/vuln/detail/CVE-2022-32206https://nvd.nist.gov/vuln/detail/CVE-2022-32208https://vulners.com/redhatcve/RH:CVE-2022-35252```### Standalone code to reproduce the issue```shell-```### Relevant log output```shell-```</details>
"
57728,0,301,17,0,0,DNXie,0,"title:tf.random.poisson crash(abort) description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.11.0-dev20220914### Custom CodeNo### OS Platform and DistributionUbuntu 18.04.4 LTS (x86_64)### Mobile device_No response_### Python version3.7.6### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versionN/A### GPU model and memory_No response_### Current Behaviour?```shelltf.random.poisson crash(abort)```### Standalone code to reproduce the issue```shellimport numpy as npimport tensorflow as tf tf.random.poisson(lam=np.ones((10,10,11,2)), shape=[27, 187, 229])```### Relevant log output```shell2022-09-16 19:45:10.220556: F tensorflow/core/util/work_sharder.cc:34] Check failed: total >= 0 (0 vs. -1751281096)Aborted (core dumped)```</details>
"
57722,1,5179,37,0,0,princyiakov,0,"title:Tensorflow Integration Issue with Cloudpickle for AdamW optimizer description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.8### Custom CodeNo### OS Platform and DistributionLinux### Mobile device_No response_### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellHi team,Tensorflow model with AdamWeightDecay optimizer does not load with cloudpickle. Here is the error when I load the pickled model: 闂傚倸鍊搁崐鐑芥嚄閸洖纾婚柕濞炬櫅绾惧潡鏌＄仦璇插姕闁绘挶鍨介弻鐔烘崉閾忕懓鐐妘eError: Unknown optimizer: AdamWeightDecay. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.""The problem is only encountered with AdamWeightDecay as there are no issues when adam optimizer is used instead of AdamWeightDecay for the same code. Is there a way we can load the model with AdamW using cloudpickle?```### Standalone code to reproduce the issue```shell!pip install -q -U ""tensorflow-text==2.8.*""!pip install cloudpickle!pip install -q tf-models-official==2.7.0import osimport shutilimport cloudpickleimport tensorflow as tfimport tensorflow_hub as hubimport tensorflow_text as textfrom official.nlp import optimization  # to create AdamW optimizerurl = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,                                  untar=True, cache_dir='.',                                  cache_subdir='')dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')train_dir = os.path.join(dataset_dir, 'train')AUTOTUNE = tf.data.AUTOTUNEbatch_size = 32seed = 42raw_train_ds = tf.keras.utils.text_dataset_from_directory(    'aclImdb/train',    batch_size=batch_size,    validation_split=0.2,    subset='training',    seed=seed)class_names = raw_train_ds.class_namestrain_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)val_ds = tf.keras.utils.text_dataset_from_directory(    'aclImdb/train',    batch_size=batch_size,    validation_split=0.2,    subset='validation',    seed=seed)val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)test_ds = tf.keras.utils.text_dataset_from_directory(    'aclImdb/test',    batch_size=batch_size)test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)bert_model = hub.KerasLayer(tfhub_handle_encoder)def build_classifier_model():  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')  encoder_inputs = preprocessing_layer(text_input)  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')  outputs = encoder(encoder_inputs)  net = outputs['pooled_output']  net = tf.keras.layers.Dropout(0.1)(net)  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)  return tf.keras.Model(text_input, net)classifier_model = build_classifier_model()loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)metrics = tf.metrics.BinaryAccuracy()epochs = 1steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()num_train_steps = steps_per_epoch * epochsnum_warmup_steps = int(0.1*num_train_steps)init_lr = 3e-5optimizer = optimization.create_optimizer(init_lr=init_lr,                                          num_train_steps=num_train_steps,                                          num_warmup_steps=num_warmup_steps,                                          optimizer_type='adamw')classifier_model.compile(optimizer=optimizer,                         loss=loss,                         metrics=metrics)classifier_model.fit(x=train_ds,                               validation_data=val_ds,                               epochs=epochs)pickle = cloudpickle.dumps(classifier_model)unpickle = cloudpickle.loads(pickle)```### Relevant log output```shellWARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 124). These functions will not be directly callable after loading.---------------------------------------------------------------------------ValueError                                Traceback (most recent call last)<ipython-input-16-58cfd5217ca1> in <module>      1 import cloudpickle      2 pickle = cloudpickle.dumps(classifier_model)----> 3 unpickle = cloudpickle.loads(pickle)2 frames/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py in class_and_config_for_serialized_keras_object(config, module_objects, custom_objects, printable_module_name)    561   if cls is None:    562     raise ValueError(--> 563         f'Unknown {printable_module_name}: {class_name}. Please ensure this '    564         'object is passed to the `custom_objects` argument. See '    565         'https://www.tensorflow.org/guide/keras/save_and_serialize'ValueError: Unknown optimizer: AdamWeightDecay. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.```</details>
"
57716,0,1668,17,0,0,DNXie,0,"title:tf.math.unsorted_segment_min (max/sum/prod) crash (abort) description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.11.0-dev20220914### Custom CodeNo### OS Platform and DistributionUbuntu 18.04.4 LTS (x86_64)### Mobile device_No response_### Python version3.7.6### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versionN/A### GPU model and memory_No response_### Current Behaviour?`tf.math.unsorted_segment_min`, `tf.math.unsorted_segment_max`, `tf.math.unsorted_segment_sum`, `tf.math.unsorted_segment_prod` crash with abortion.Also reproduced in the [gist](https://colab.research.google.com/drive/1BM8HWcrSTH6qyPwujFl5QjhB_BlT8Nat?usp=sharing)### Standalone code to reproduce the issue```shellimport numpy as npimport tensorflow as tftf.math.unsorted_segment_min(data=np.ones((3)),segment_ids=898042203, num_segments=8327099846119777499)import numpy as npimport tensorflow as tftf.math.unsorted_segment_max(data=np.ones((3)),segment_ids=898042203, num_segments=8327099846119777499)import numpy as npimport tensorflow as tftf.math.unsorted_segment_sum(data=np.ones((3)),segment_ids=898042203, num_segments=8327099846119777499)import numpy as npimport tensorflow as tftf.math.unsorted_segment_prod(data=np.ones((3)),segment_ids=898042203, num_segments=8327099846119777499)```### Relevant log output```shell2022-09-15 21:11:52.268123: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory2022-09-15 21:11:52.268149: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)2022-09-15 21:11:52.268205: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist2022-09-15 21:11:52.268549: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2022-09-15 21:11:52.305843: F tensorflow/core/framework/tensor_shape.cc:404] Check failed: 0 <= new_num_elements (0 vs. -1)Aborted (core dumped)```</details>
"
57715,0,386,17,0,0,DNXie,0,"title:tf.nn.conv2d_transpose crash with abort whtn `output_shape` has negative value description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.11.0-dev20220914### Custom CodeNo### OS Platform and DistributionUbuntu 18.04.4 LTS (x86_64)### Mobile device_No response_### Python version3.7.6### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versionN/A### GPU model and memory_No response_### Current Behaviour?`tf.nn.conv2d_transpose` crash with abort whtn `output_shape` has negative valueAlso reproduced in this [gist](https://colab.research.google.com/drive/1Yx5pvWM7HoVpwAmCfzU5DIi1Y9IRsHrP?usp=sharing)### Standalone code to reproduce the issue```shellimport numpy as npimport tensorflow as tftf.nn.conv2d_transpose(input=np.ones((1,1,1,1)), filters=np.ones((1,1,1,1)), output_shape=[2,-2], strides=[1])```### Relevant log output```shell2022-09-15 20:35:37.763729: F tensorflow/core/framework/tensor_shape.cc:186] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Expected shape dimensions to be non-negative, got -2Aborted (core dumped)```</details>
"
57709,0,408,17,0,0,DNXie,0,"title:tf.keras.backend.conv2d_transpose crash abort in the nightly version description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.11.0-dev20220914### Custom CodeNo### OS Platform and DistributionUbuntu 18.04.4 LTS (x86_64)### Mobile device_No response_### Python version3.7.6### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versionN/A### GPU model and memory_No response_### Current Behaviour?`tf.keras.backend.conv2d_transpose` crash with abortion.Also reproduced in this [gist](https://colab.research.google.com/drive/15IXylJP_NgNuE1HBPw4P5QrMgbHQCFVd?usp=sharing)### Standalone code to reproduce the issue```import numpy as npimport tensorflow as tftf.keras.backend.conv2d_transpose(x=np.ones((2,1,1,1)), kernel=1, output_shape=np.array([-3697127975084027074, 1],dtype=np.int32))```### Relevant log outputOutput:```2022-09-15 17:30:45.974542: F tensorflow/core/framework/tensor_shape.cc:186] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Expected shape dimensions to be non-negative, got -470478018Aborted (core dumped)```
"
57702,0,607,0,0,0,MThalberg,0,"title:`tf.broadcast_to` crashes with JIT description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.9.1### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version11.4### GPU model and memory_No response_### Current Behaviour?```shellSession crashes when running the (compiled) `tf.broadcast_to` with an invalid shape, and can potentially trigger a denial of service attack. This bug only happens with using `@tf.function(jit_compile=True) `.```### Standalone code to reproduce the issue```shellimport tensorflow as tfinput_tensor = tf.zeros([1, 1], dtype=tf.int32)@tf.function(jit_compile=True) def f(input):    shape = [-1, 1]    return tf.broadcast_to(input, shape, ) f(input_tensor)```### Relevant log output```shellF tensorflow/core/framework/tensor_shape.cc:186] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Expected shape dimensions to be non-negative, got -1abort```</details>
"
57696,1,5493,1,0,0,Yahor10,0,"title:Silent update in google play for android mobile  release crash tensorflowlite library description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.8.0### Custom CodeYes### OS Platform and DistributionAndroid arm OS 8-11### Mobile deviceAndroid devices### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellA bug happened!After publish new update with new tensorflowlites ml files new crashes began.It happening of tensorflow generated file that can not correct unpack new data.Attach loggs below```### Standalone code to reproduce the issue```shellits not possible in debug mode or do it with manual tests```### Relevant log output```shell//JAVA LOGandroid.content.res.AssetManager.nativeOpenAssetFd (AssetManager.java)android.content.res.AssetManager.openFd (AssetManager.java:950)org.tensorflow.lite.support.common.FileUtil.loadMappedFile (FileUtil.java:159)org.tensorflow.lite.support.model.Model.createModel (Model.java:172)Model.<init> (Model.java:23)ModelnewInstance (Model.java:29)java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1167)java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:641)java.lang.Thread.run (Thread.java:923) // IT CAN BE SEPARATE PROBLEMS..... // JNI LOG==/split_config.armeabi_v7a.apk!libtensorflowlite_jni.solibtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run)  #00  pc 0x00000000000d3bd5  /apex/com.android.art/lib/libart.so (art_quick_invoke_stub_internal)  #00  pc 0x00000000004eb915  /apex/com.android.art/lib/libart.so (art_quick_invoke_static_stub)  #00  pc 0x000000000012bf3f  /apex/com.android.art/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*))  #00  pc 0x000000000023f6d7  /apex/com.android.art/lib/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*))  #00  pc 0x0000000000237397  /apex/com.android.art/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*))  #00  pc 0x00000000004df7a7  /apex/com.android.art/lib/libart.so (MterpInvokeStatic)  #00  pc 0x00000000000ce794  /apex/com.android.art/lib/libart.so (mterp_op_invoke_static)  #00  pc 0x0000000000538eb4  /data/app/~~fnfQMxtYKukpS_kOKZ4rqA==/-Y6IEuVzbvGcozyoeNelTZw==/oat/arm/base.vdex (org.tensorflow.lite.NativeInterpreterWrapper.run)  #00  pc 0x00000000004e2e93  /apex/com.android.art/lib/libart.so (MterpInvokeVirtualQuick)  #00  pc 0x00000000000d2394  /apex/com.android.art/lib/libart.so (mterp_op_invoke_virtual_quick)  #00  pc 0x0000000000538894  /data/app/~~fnfQMxtYKukpS_kOKZ4rqA==/-Y6IEuVzbvGcozyoeNelTZw==/oat/arm/base.vdex (org.tensorflow.lite.InterpreterImpl.run)  #00  pc 0x00000000004e2e93  /apex/com.android.art/lib/libart.so (MterpInvokeVirtualQuick)  #00  pc 0x00000000000d2394  /apex/com.android.art/lib/libart.so (mterp_op_invoke_virtual_quick)  #00  pc 0x000000000026ec90  /data/app/~~fnfQMxtYKukpS_kOKZ4rqA==/-Y6IEuVzbvGcozyoeNelTZw==/oat/arm/base.vdex (tensor.Classifier.recognizeImage)  #00  pc 0x00000000004e2e93  /apex/com.android.art/lib/libart.so (MterpInvokeVirtualQuick)  #00  pc 0x00000000000d2394  /apex/com.android.art/lib/libart.so (mterp_op_invoke_virtual_quick)  #00  pc 0x0000000000262b88  /data/app/~~fnfQMxtYKukpS_kOKZ4rqA==/-Y6IEuVzbvGcozyoeNelTZw==/oat/arm/base.vdex (tensorflow.HairDetectorNew$$ExternalSyntheticLambda0.run)  #00  pc 0x0000000000230391  /apex/com.android.art/lib/libart.so (art::interpreter::Execute(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame&, art::JValue, bool, bool) (.llvm.270406214262585567))  #00  pc 0x0000000000236b01  /apex/com.android.art/lib/libart.so (art::interpreter::EnterInterpreterFromEntryPoint(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*))  #00  pc 0x00000000004ce7db  /apex/com.android.art/lib/libart.so (artQuickToInterpreterBridge)  #00  pc 0x00000000000d8761  /apex/com.android.art/lib/libart.so (art_quick_to_interpreter_bridge)  #00  pc 0x00000000001113f9  /apex/com.android.art/javalib/arm/boot.oat (java.util.concurrent.Executors$RunnableAdapter.call)  #00  pc 0x0000000000154463  /apex/com.android.art/javalib/arm/boot.oat (java.util.concurrent.FutureTask.run)  #00  pc 0x0000000000187257  /apex/com.android.art/javalib/arm/boot.oat (java.util.concurrent.ThreadPoolExecutor.runWorker)  #00  pc 0x0000000000184de7  /apex/com.android.art/javalib/arm/boot.oat (java.util.concurrent.ThreadPoolExecutor$Worker.run)  #00  pc 0x00000000000dfad1  /apex/com.android.art/javalib/arm/boot.oat (java.lang.Thread.run)  #00  pc 0x00000000000d3bd5  /apex/com.android.art/lib/libart.so (art_quick_invoke_stub_internal)  #00  pc 0x00000000004eb7e3  /apex/com.android.art/lib/libart.so (art_quick_invoke_stub)  #00  pc 0x000000000012bf2d  /apex/com.android.art/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*))  #00  pc 0x00000000003f8b27  /apex/com.android.art/lib/libart.so (art::JValue art::InvokeVirtualOrInterfaceWithJValues<art::ArtMethod*>(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, art::ArtMethod*, jvalue const*))  #00  pc 0x00000000003f8c37  /apex/com.android.art/lib/libart.so (art::JValue art::InvokeVirtualOrInterfaceWithJValues<_jmethodID*>(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, _jmethodID*, jvalue const*))  #00  pc 0x000000000043a165  /apex/com.android.art/lib/libart.so (art::thread::CreateCallback(void*))  #00  pc 0x00000000000aacf3  /apex/com.android.runtime/lib/bionic/libc.so (__pthread_start(void*))  #00  pc 0x0000000000064063  /apex/com.android.runtime/lib/bionic/libc.so (__start_thread)```</details>
"
57669,0,0,0,0,1,peppermenta,0,"title:Segmentation Fault on importing Tensorflow description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.4.1### Custom CodeYes### OS Platform and DistributionDebian GNU/Linux 10### Mobile deviceDebian GNU/Linux 10### Python version3.9.7### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version11.0### GPU model and memory_No response_### Current Behaviour?```shellThe bug occurs when importing both tensorflow and torchvision in my current environment. When importing tensorflow before torchvision, there is no bug. However, when importing tensorflow _after_ importing torchvision, a segmentation fault occurs.Conda environment YAML:name: test_envchannels:  - pytorch  - nvidia  - conda-forge  - defaultsdependencies:  - _libgcc_mutex=0.1=conda_forge  - _openmp_mutex=4.5=1_llvm  - anyio=3.3.0=py39hf3d152e_0  - argon2-cffi=20.1.0=py39h3811e60_2  - async_generator=1.10=py_0  - attrs=21.2.0=pyhd8ed1ab_0  - babel=2.9.1=pyh44b312d_0  - backcall=0.2.0=pyh9f0ad1d_0  - backports=1.0=py_2  - backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0  - blas=1.0=mkl  - bleach=4.1.0=pyhd8ed1ab_0  - brotlipy=0.7.0=py39h3811e60_1001  - ca-certificates=2021.10.8=ha878542_0  - certifi=2021.10.8=py39hf3d152e_0  - cffi=1.14.6=py39h4bc2ebd_1  - chardet=4.0.0=py39hf3d152e_1  - charset-normalizer=2.0.0=pyhd8ed1ab_0  - colorama=0.4.4=pyh9f0ad1d_0  - cryptography=3.4.7=py39hbca0aa6_0  - cudatoolkit=11.0.221=h6bb024c_0  - cycler=0.10.0=py_2  - dbus=1.13.6=h48d8840_2  - debugpy=1.4.1=py39he80948d_0  - decorator=5.1.0=pyhd8ed1ab_0  - defusedxml=0.7.1=pyhd8ed1ab_0  - entrypoints=0.3=pyhd8ed1ab_1003  - expat=2.4.1=h9c3ff4c_0  - fontconfig=2.13.1=hba837de_1005  - freetype=2.10.4=h0708190_1  - gettext=0.19.8.1=h73d1719_1007  - glib=2.68.4=h9c3ff4c_1  - glib-tools=2.68.4=h9c3ff4c_1  - gst-plugins-base=1.14.0=hbbd80ab_1  - gstreamer=1.14.0=h28cd5cc_2  - icu=58.2=hf484d3e_1000  - idna=3.1=pyhd3deb0d_0  - importlib-metadata=4.8.1=py39hf3d152e_0  - ipykernel=6.4.1=py39hef51801_0  - ipython=7.27.0=py39hef51801_0  - ipython_genutils=0.2.0=py_1  - jedi=0.18.0=py39hf3d152e_2  - jinja2=3.0.1=pyhd8ed1ab_0  - joblib=1.0.1=pyhd8ed1ab_0  - jpeg=9b=h024ee3a_2  - json5=0.9.5=pyh9f0ad1d_0  - jsonschema=3.2.0=pyhd8ed1ab_3  - jupyter_client=7.0.3=pyhd8ed1ab_0  - jupyter_core=4.8.1=py39hf3d152e_0  - jupyter_server=1.11.0=pyhd8ed1ab_0  - jupyterlab=3.1.12=pyhd8ed1ab_0  - jupyterlab_pygments=0.1.2=pyh9f0ad1d_0  - jupyterlab_server=2.8.1=pyhd8ed1ab_0  - kiwisolver=1.3.2=py39h1a9c180_0  - lcms2=2.12=h3be6417_0  - ld_impl_linux-64=2.36.1=hea4e1c9_2  - libblas=3.9.0=11_linux64_mkl  - libcblas=3.9.0=11_linux64_mkl  - libffi=3.4.2=h9c3ff4c_2  - libgcc-ng=11.2.0=h1d223b6_8  - libgfortran-ng=11.2.0=h69a702a_8  - libgfortran5=11.2.0=h5c6108e_8  - libglib=2.68.4=h174f98d_1  - libiconv=1.16=h516909a_0  - liblapack=3.9.0=11_linux64_mkl  - libllvm11=11.1.0=hf817b99_2  - libpng=1.6.37=h21135ba_2  - libsodium=1.0.18=h36c2ea0_1  - libstdcxx-ng=11.2.0=he4da1e4_8  - libtiff=4.2.0=h85742a9_0  - libuuid=2.32.1=h7f98852_1000  - libwebp-base=1.2.1=h7f98852_0  - libxcb=1.13=h7f98852_1003  - libxml2=2.9.12=h03d6c58_0  - llvm-openmp=12.0.1=h4bd325d_1  - llvmlite=0.37.0=py39h1bbdace_0  - lz4-c=1.9.3=h9c3ff4c_1  - markupsafe=2.0.1=py39h3811e60_0  - matplotlib=3.4.3=py39hf3d152e_1  - matplotlib-base=3.4.3=py39h2fa2bec_1  - matplotlib-inline=0.1.3=pyhd8ed1ab_0  - mistune=0.8.4=py39h3811e60_1004  - mkl=2021.3.0=h726a3e6_557  - mkl-service=2.4.0=py39h3811e60_0  - mkl_fft=1.3.0=py39h42c9631_2  - mkl_random=1.2.2=py39hde0f152_0  - nbclassic=0.3.2=pyhd8ed1ab_0  - nbclient=0.5.4=pyhd8ed1ab_0  - nbconvert=6.1.0=py39hf3d152e_1  - nbformat=5.1.3=pyhd8ed1ab_0  - ncurses=6.2=h58526e2_4  - nest-asyncio=1.5.1=pyhd8ed1ab_0  - ninja=1.10.2=h4bd325d_0  - notebook=6.4.4=pyha770c72_0  - numba=0.54.1=py39h56b8d98_0  - numpy=1.20.3=py39hf144106_0  - numpy-base=1.20.3=py39h74d4b33_0  - olefile=0.46=pyh9f0ad1d_1  - openjpeg=2.4.0=hb52868f_1  - openssl=1.1.1l=h7f98852_0  - packaging=21.0=pyhd8ed1ab_0  - pandoc=2.14.2=h7f98852_0  - pandocfilters=1.5.0=pyhd8ed1ab_0  - parso=0.8.2=pyhd8ed1ab_0  - pcre=8.45=h9c3ff4c_0  - pexpect=4.8.0=pyh9f0ad1d_2  - pickleshare=0.7.5=py_1003  - pillow=8.3.1=py39h2c7a002_0  - pip=21.2.4=pyhd8ed1ab_0  - prometheus_client=0.11.0=pyhd8ed1ab_0  - prompt-toolkit=3.0.20=pyha770c72_0  - pthread-stubs=0.4=h36c2ea0_1001  - ptyprocess=0.7.0=pyhd3deb0d_0  - pycparser=2.20=pyh9f0ad1d_2  - pygments=2.10.0=pyhd8ed1ab_0  - pynndescent=0.5.4=pyh6c4a22f_0  - pyopenssl=20.0.1=pyhd8ed1ab_0  - pyparsing=2.4.7=pyh9f0ad1d_0  - pyqt=5.9.2=py39h2531618_6  - pyrsistent=0.17.3=py39h3811e60_2  - pysocks=1.7.1=py39hf3d152e_3  - python=3.9.7=hb7a2778_1_cpython  - python-dateutil=2.8.2=pyhd8ed1ab_0  - python_abi=3.9=2_cp39  - pytorch=1.7.1=py3.9_cuda11.0.221_cudnn8.0.5_0  - pytz=2021.1=pyhd8ed1ab_0  - pyzmq=22.3.0=py39h37b5a0c_0  - qt=5.9.7=h5867ecd_1  - readline=8.1=h46c0cb4_0  - requests=2.26.0=pyhd8ed1ab_0  - requests-unixsocket=0.2.0=py_0  - scikit-learn=0.24.2=py39h7c5d8c9_1  - scipy=1.7.1=py39hee8e79c_0  - send2trash=1.8.0=pyhd8ed1ab_0  - setuptools=58.0.4=py39hf3d152e_1  - sip=4.19.13=py39h2531618_0  - six=1.16.0=pyh6c4a22f_0  - sniffio=1.2.0=py39hf3d152e_1  - sqlite=3.36.0=h9cd32fc_1  - tbb=2021.3.0=h4bd325d_0  - terminado=0.12.1=py39hf3d152e_0  - testpath=0.5.0=pyhd8ed1ab_0  - threadpoolctl=2.2.0=pyh8a188c0_0  - tk=8.6.11=h27826a3_1  - torchaudio=0.7.2=py39  - torchvision=0.8.2=py39_cu110  - tornado=6.1=py39h3811e60_1  - tqdm=4.62.3=pyhd8ed1ab_0  - traitlets=5.1.0=pyhd8ed1ab_0  - typing_extensions=3.10.0.0=pyha770c72_0  - tzdata=2021a=he74cb21_1  - umap-learn=0.5.1=py39hf3d152e_1  - urllib3=1.26.6=pyhd8ed1ab_0  - wcwidth=0.2.5=pyh9f0ad1d_2  - webencodings=0.5.1=py_1  - websocket-client=0.57.0=py39hf3d152e_4  - wheel=0.37.0=pyhd8ed1ab_1  - xorg-libxau=1.0.9=h7f98852_0  - xorg-libxdmcp=1.1.3=h7f98852_0  - xz=5.2.5=h516909a_1  - zeromq=4.3.4=h9c3ff4c_1  - zipp=3.5.0=pyhd8ed1ab_0  - zlib=1.2.11=h516909a_1010  - zstd=1.4.9=ha95c52a_0  - pip:    - clip==1.0    - ftfy==6.0.3    - jupyterlab-vim==0.14.5    - regex==2021.8.28prefix: /home/ubuntu/anaconda3/envs/test_env```### Standalone code to reproduce the issue```shell#Need to import in this order to reproduce the bugimport torchvisionimport tensorflow```### Relevant log output_No response_</details>
"
57666,1,774,0,0,0,willi3by,0,"title:Model Trains on CPU but not on GPU - TensorDot/MatMul error description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.9.1### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 22.04### Mobile device_No response_### Python version3.9.12### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versionCUDA=11.2, cuDNN=8.1### GPU model and memoryRTX 8000 48GB ### Current Behaviour?```shellI am trying to train a model using the GPUs on my computer. The model trains as expected on the CPU but I get the following error when trying to use the GPU:Node: 'model/final_output/Tensordot/MatMul'2 root error(s) found.  (0) INTERNAL:  Blas xGEMV launch failed : a.shape=[1,2628288,150], b.shape=[1,150,1], m=2628288, n=1, k=150	 [[{{node model/final_output/Tensordot/MatMul}}]]	 [[Nadam/Nadam/group_deps/_111]]  (1) INTERNAL:  Blas xGEMV launch failed : a.shape=[1,2628288,150], b.shape=[1,150,1], m=2628288, n=1, k=150	 [[{{node model/final_output/Tensordot/MatMul}}]]0 successful operations.0 derived errors ignored. [Op:__inference_train_function_29232]```I have tried multiple versions of tensorflow gpu (2.5 through 2.10) and get the same error. I would expect the network to train the same on GPU as on CPU.```### Standalone code to reproduce the issue```shellThe network that I have created is a bit cumbersome for posting on this platform. I am happy to send files if needed.```### Relevant log output```shellNode: 'model/final_output/Tensordot/MatMul'2 root error(s) found.  (0) INTERNAL:  Blas xGEMV launch failed : a.shape=[1,2628288,150], b.shape=[1,150,1], m=2628288, n=1, k=150	 [[{{node model/final_output/Tensordot/MatMul}}]]	 [[Nadam/Nadam/group_deps/_111]]  (1) INTERNAL:  Blas xGEMV launch failed : a.shape=[1,2628288,150], b.shape=[1,150,1], m=2628288, n=1, k=150	 [[{{node model/final_output/Tensordot/MatMul}}]]0 successful operations.0 derived errors ignored. [Op:__inference_train_function_29232]```</details>
"
57664,1,0,0,0,0,Co1lin,0,"title:Inconsistant behavior of Conv2D between eager mode and tracing description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.11.0-dev20220910### Custom CodeNo### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?If the input to Conv2D is too small, in graph execution mode, a ValueError will be raised when tracing. However, if we only use eager mode to run, it will output a tensor with dim = 0. So there's an inconsistency under two modes.If we only create a conv layer and directly use it to compute something, it works well as the eager mode.```pythonconv = layers.Conv2D(1, 2, 1, autocast=False)x = tf.random.normal([2, 1, 2, 2])print(conv(x)) # no error```I think the behavior should be consistent in all modes.### Standalone code to reproduce the issue```pythonimport tensorflow as tffrom keras import layersclass MyModule(tf.Module):    def __init__(self):        self.conv = layers.Conv2D(1, 2, 1, autocast=False)        @tf.function    def __call__(self, x):        return self.conv(x)if __name__ == '__main__':    model = MyModule()    tf.config.run_functions_eagerly(True)    x = tf.random.normal([2, 1, 2, 2])    print(model(x)) # tf.Tensor([], shape=(2, 0, 1, 1), dtype=float32)    tf.config.run_functions_eagerly(False)    x = tf.random.normal([2, 1, 2, 2])    print(model(x)) # Error when tracing      model.__call__.get_concrete_function(x) # Same error if we call this instead of the last line```### Relevant log output```pythontf.Tensor([], shape=(2, 0, 1, 1), dtype=float32) # eagerly running works fineTraceback (most recent call last):  File ""/home/colin/code/test/conv.py"", line 21, in <module>    model.__call__.get_concrete_function(x)  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py"", line 1233, in get_concrete_function    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py"", line 1213, in _get_concrete_function_garbage_collected    self._initialize(args, kwargs, add_initializers_to=initializers)  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py"", line 778, in _initialize    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 166, in _get_concrete_function_internal_garbage_collected    concrete_function, _ = self._maybe_define_concrete_function(args, kwargs)  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 161, in _maybe_define_concrete_function    return self._maybe_define_function(args, kwargs)  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 364, in _maybe_define_function    concrete_function = self._create_concrete_function(args, kwargs)  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 288, in _create_concrete_function    func_graph_module.func_graph_from_py_func(  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py"", line 1281, in func_graph_from_py_func    func_outputs = python_func(*func_args, **func_kwargs)  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py"", line 679, in wrapped_fn    out = weak_wrapped_fn().__wrapped__(*args, **kwds)  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 449, in bound_method_wrapper    return wrapped_fn(*args, **kwargs)  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py"", line 1267, in autograph_handler    raise e.ag_error_metadata.to_exception(e)  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py"", line 1256, in autograph_handler    return autograph.converted_call(  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 439, in converted_call    result = converted_f(*effective_args, **kwargs)  File ""/tmp/__autograph_generated_file6e7qw11z.py"", line 12, in tf____call__    retval_ = ag__.converted_call(ag__.ld(self).conv, (ag__.ld(x),), None, fscope)  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 377, in converted_call    return _call_unconverted(f, args, kwargs, options)  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 459, in _call_unconverted    return f(*args)  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler    raise e.with_traceback(filtered_tb) from None  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 1967, in _create_c_op    raise ValueError(e.message)ValueError: in user code:    File ""/home/colin/code/test/conv.py"", line 10, in __call__  *        return self.conv(x)    File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler  **        raise e.with_traceback(filtered_tb) from None    ValueError: Exception encountered when calling layer ""conv2d"" ""                 f""(type Conv2D).        Negative dimension size caused by subtracting 2 from 1 for '{{node conv2d/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=""VALID"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](x, conv2d/Conv2D/ReadVariableOp)' with input shapes: [2,1,2,2], [2,2,2,1].        Call arguments received by layer ""conv2d"" ""                 f""(type Conv2D):      闂?inputs=tf.Tensor(shape=(2, 1, 2, 2), dtype=float32)```</details>
"
57625,1,3429,17,0,0,fregire,0,"title:ImportError: cannot import name 'test' from partially initialized module 'tensorflow._api.v2.__internal__' description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.9.2### Custom CodeYes### OS Platform and DistributionDebian GNU/Linux 11 (bullseye)### Mobile device_No response_### Python version3.9.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellI'm using tensorflow in docker container, but i get this error when trying to import this module.Error can be solved if i install another or same version of this module (but for production its bad solution)I have installed this packages (pip freeze):absl-py==1.2.0anyio==3.4.0asgi-lifespan==1.0.1asgiref==3.4.1astroid==2.11.7astunparse==1.6.3asyncio-dgram==2.1.2attrs==22.1.0autoflake==1.4bandit==1.7.4black==22.3.0cachetools==5.2.0certifi==2021.10.8charset-normalizer==2.0.7click==8.1.3dataclasses==0.6deap==1.3.1dill==0.3.5.1fastapi==0.73.0flatbuffers==1.12gast==0.4.0gitdb==4.0.9GitPython==3.1.27google-auth==2.11.0google-auth-oauthlib==0.4.6google-pasta==0.2.0grpcio==1.48.1h11==0.12.0h5py==3.7.0httpcore==0.14.7httptools==0.3.0httpx==0.22.0idna==3.3importlib-metadata==4.12.0iniconfig==1.1.1isort==5.10.1joblib==0.17.0keras==2.9.0Keras-Preprocessing==1.1.2lazy-object-proxy==1.7.1libclang==14.0.6lxml==4.7.1Markdown==3.4.1MarkupSafe==2.1.1mccabe==0.7.0mypy==0.942mypy-extensions==0.4.3numpy==1.21.3oauthlib==3.2.0opt-einsum==3.3.0packaging==21.3pandas==1.3.4pathspec==0.10.1pbr==5.10.0platformdirs==2.5.2plotly==4.8.1pluggy==1.0.0protobuf==3.19.4py==1.11.0pyasn1==0.4.8pyasn1-modules==0.2.8pydantic==1.9.0pyflakes==2.5.0pylint==2.13.2pyparsing==3.0.9pytest==7.1.1pytest-asyncio==0.18.3pytest-timeout==2.1.0python-dateutil==2.8.2python-dotenv==0.19.2pytz==2021.3PyYAML==6.0requests==2.26.0requests-oauthlib==1.3.1retrying==1.3.3rfc3986==1.5.0rsa==4.9scapy==2.4.5scikit-learn==0.24.2scipy==1.7.2six==1.16.0smmap==5.0.0sniffio==1.2.0starlette==0.17.1stevedore==4.0.0stopit==1.1.2tensorboard==2.9.1tensorboard-data-server==0.6.1tensorboard-plugin-wit==1.8.1tensorflow==2.9.2tensorflow-estimator==2.9.0tensorflow-io-gcs-filesystem==0.26.0termcolor==1.1.0threadpoolctl==3.0.0toml==0.10.2tomli==2.0.1TPOT==0.11.5tqdm==4.62.3types-PyYAML==6.0.5typing-extensions==3.10.0.2update-checker==0.18.0urllib3==1.26.7uvicorn==0.17.1uvloop==0.16.0vulture==2.3watchgod==0.7websockets==10.1Werkzeug==2.2.2wrapt==1.14.1zipp==3.8.1```### Standalone code to reproduce the issue```shellroot@c666258f6b9d:/app# python3Python 3.9.9 (main, Mar 28 2022, 09:18:32) [GCC 10.2.1 20210110] on linuxType ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> import tensorflow```### Relevant log output```shell2022-09-06 09:42:52.620866: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory2022-09-06 09:42:52.620890: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.Traceback (most recent call last):  File ""<stdin>"", line 1, in <module>  File ""/usr/local/lib/python3.9/site-packages/tensorflow/__init__.py"", line 45, in <module>    from ._api.v2 import __internal__  File ""/usr/local/lib/python3.9/site-packages/tensorflow/_api/v2/__internal__/__init__.py"", line 22, in <module>    from . import testImportError: cannot import name 'test' from partially initialized module 'tensorflow._api.v2.__internal__' (most likely due to a circular import) (/usr/local/lib/python3.9/site-packages/tensorflow/_api/v2/__internal__/__init__.py)```</details>
"
57612,1,28783,54,0,0,wangjia184,0,"title:OnDevice prediction produces same result for all inputs in a batch description:  ### Issue TypeBug### Sourcebinary### Tensorflow Version2.9.2### Custom CodeNo### OS Platform and DistributionMac M1, Windows x64### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?The model works well in python.  But when the model is exported as a SavedModel, and `predict` concrete function is called from C API,  all the inputs in the same batch produce exact same result.  Tested in Mac M1 and Windows x64, same behaviour.### Standalone code to reproduce the issue```pythonimport tensorflow as tfimport tensorflow_probability as tfpprint( tf.__version__ )def create_model(board_width, board_height):    class RenjuModel(tf.Module):        def __init__(self):            l2_penalty_beta = 1e-4            # Define the tensorflow neural network            # 1. Input:            self.inputs = tf.keras.Input( shape=(4, board_height, board_width), dtype=tf.dtypes.float32, name=""input"")            self.transposed_inputs = tf.keras.layers.Lambda( lambda x: tf.transpose(x, [0, 2, 3, 1]) )(self.inputs)            # 2. Common Networks Layers            self.conv1 = tf.keras.layers.Conv2D( name=""conv1"",                filters=32,                kernel_size=(3, 3),                padding=""same"",                data_format=""channels_last"",                activation=tf.keras.activations.relu,                kernel_regularizer=tf.keras.regularizers.L2(l2_penalty_beta)                )(self.transposed_inputs)            self.conv2 = tf.keras.layers.Conv2D( name=""conv2"",                 filters=64,                 kernel_size=(3, 3),                 padding=""same"",                 data_format=""channels_last"",                 activation=tf.keras.activations.relu,                kernel_regularizer=tf.keras.regularizers.L2(l2_penalty_beta)                )(self.conv1)            self.conv3 = tf.keras.layers.Conv2D( name=""conv3"",                filters=128,                kernel_size=(3, 3),                padding=""same"",                data_format=""channels_last"",                activation=tf.keras.activations.relu,                kernel_regularizer=tf.keras.regularizers.L2(l2_penalty_beta)                )(self.conv2)            # 3-1 Action Networks            self.action_conv = tf.keras.layers.Conv2D( name=""action_conv"",                filters=4,                kernel_size=(1, 1),                padding=""same"",                data_format=""channels_last"",                activation=tf.keras.activations.relu,                kernel_regularizer=tf.keras.regularizers.L2(l2_penalty_beta)                )(self.conv3)            # flatten tensor            self.action_conv_flat = tf.keras.layers.Reshape( (-1, 4 * board_height * board_width), name=""action_conv_flat""             )(self.action_conv)            # 3-2 Full connected layer, the output is the log probability of moves            # on each slot on the board            self.action_fc = tf.keras.layers.Dense( board_height * board_width,                activation=tf.nn.log_softmax,                name=""action_fc"",                kernel_regularizer=tf.keras.regularizers.L2(l2_penalty_beta)                )(self.action_conv_flat)            # 4 Evaluation Networks            self.evaluation_conv = tf.keras.layers.Conv2D( name=""evaluation_conv"",                filters=2,                kernel_size=(1, 1),                padding=""same"",                data_format=""channels_last"",                activation=tf.keras.activations.relu,                kernel_regularizer=tf.keras.regularizers.L2(l2_penalty_beta)                )(self.conv3)            self.evaluation_conv_flat = tf.keras.layers.Reshape( (-1, 2 * board_height * board_width),                name=""evaluation_conv_flat""                 )(self.evaluation_conv)            self.evaluation_fc1 = tf.keras.layers.Dense( 64,                activation=tf.keras.activations.relu,                name=""evaluation_fc1"",                kernel_regularizer=tf.keras.regularizers.L2(l2_penalty_beta)                )(self.evaluation_conv_flat)            self.evaluation_fc2 = tf.keras.layers.Dense( 1,                 activation=tf.keras.activations.tanh,                name=""evaluation_fc2"",                kernel_regularizer=tf.keras.regularizers.L2(l2_penalty_beta)                )(self.evaluation_fc1)            self.outputs = tf.keras.layers.Concatenate()([self.action_fc, self.evaluation_fc2])                            self.model = tf.keras.Model(inputs=self.inputs, outputs=self.outputs, name=""renju_model"")            self.model.summary()             self.lr = tf.Variable(0.002, trainable=False, dtype=tf.dtypes.float32)            @tf.function(input_signature=[ tf.TensorSpec([None, 1, board_height * board_width + 1], tf.float32),                tf.TensorSpec([None, 1, board_height * board_width + 1], tf.float32)            ])            def custom_loss(labels, predictions):                act_probs_labels, value_labels = tf.split(labels, [board_height*board_width, -1], axis=2)                act_probs_predictions, value_predictions = tf.split(predictions, [board_height*board_width, -1], axis=2)                #tf.print(act_probs_labels, summarize=-1)                #tf.print(value_labels, summarize=-1)                #tf.print(act_probs_predictions, summarize=-1)                #tf.print(value_predictions, summarize=-1)                         value_loss = tf.reduce_mean( tf.losses.mean_squared_error(value_labels, value_predictions) )                policy_loss = tf.negative(tf.reduce_mean(                    tf.reduce_sum(tf.multiply(act_probs_labels, act_probs_predictions), 2)))                total_loss = policy_loss + value_loss                tf.print( ""value_loss="", value_loss, "" policy_loss="", policy_loss, "" total_loss="", total_loss )                return total_loss            self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = self.lr),                    loss=custom_loss,                    metrics=['accuracy'])        @tf.function(input_signature=[            tf.TensorSpec([None, 4, board_height, board_width], tf.float32),        ])        def predict(self, state_batch):            if tf.shape(state_batch)[0] > 1:                tf.print(state_batch, summarize=-1)            predictions = self.model(state_batch)            if tf.shape(state_batch)[0] > 1:                tf.print(predictions, summarize=-1)            return tf.split(predictions, [board_height*board_width, -1], axis=2)        @tf.function(input_signature=[tf.TensorSpec(shape=[None, 4, board_height, board_width],  dtype=tf.float32),                                   tf.TensorSpec(shape=[None, 1, board_height * board_width],  dtype=tf.float32),                                  tf.TensorSpec(shape=[None, 1, 1],  dtype=tf.float32),                                  tf.TensorSpec(shape=[1],  dtype=tf.float32) ])        def train(self, state_batch, prob_batch, score_batch, lr):            labels = tf.concat( [prob_batch, score_batch], axis=2)            self.lr.assign(tf.gather(lr, 0))            with tf.GradientTape() as tape:                predictions = self.model(state_batch, training=True)  # Forward pass                # the loss function is configured in `compile()`                loss = self.model.compiled_loss(labels, predictions, regularization_losses=self.model.losses)             gradients = tape.gradient(loss, self.model.trainable_variables)            self.model.optimizer.apply_gradients(                zip(gradients, self.model.trainable_variables))            entropy = tf.negative(tf.reduce_mean(               tf.reduce_sum(tf.exp(predictions) * predictions, 2)))            return (loss, entropy)                @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])        def save(self, checkpoint_path):            tensor_names = [weight.name for weight in self.model.weights]            tensors_to_save = [weight.read_value() for weight in self.model.weights]            tf.raw_ops.Save(                filename=checkpoint_path, tensor_names=tensor_names,                data=tensors_to_save, name='save')            return checkpoint_path        @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])        def restore(self, checkpoint_path):            restored_tensors = {}            for var in self.model.weights:                restored = tf.raw_ops.Restore( file_pattern=checkpoint_path, tensor_name=var.name, dt=var.dtype, name='restore')                var.assign(restored)                restored_tensors[var.name] = restored            return checkpoint_path        @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.float32)])        def random_choose_with_dirichlet_noice(self, probs):            concentration = 0.3*tf.ones(tf.size(probs))            dist = tfp.distributions.Dirichlet(concentration)            p = 0.75*probs + 0.25*dist.sample(1)[0]            samples = tf.random.categorical(tf.math.log([p]), 1)            return samples[0] # selected index    return RenjuModel()model = create_model( 15, 15)model.model.save('renju_15x15_model',         save_format='tf',         signatures={            'predict': model.predict.get_concrete_function(),             'train' : model.train.get_concrete_function(),             'save' : model.save.get_concrete_function(),            'restore' : model.restore.get_concrete_function(),            'random_choose_with_dirichlet_noice' : model.random_choose_with_dirichlet_noice.get_concrete_function()         })```If calling `predict` method from python, you can clearly see the outputs of each input in the same batch are different.```pythoninput = [    [        [            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],        ],        [            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],        ],        [            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],        ],        [            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],        ],    ],        [        [            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],        ],        [            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],        ],        [            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],        ],        [            [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],        ],    ]]model.predict.get_concrete_function()( tf.convert_to_tensor(input) )```But if trying to call `predict` method of SavedModel from C API. The outputs are exactly the same for multiple inputs in the same batch.### Relevant log outputThe `predict` method prints out the input and output via `tf.print````python        def predict(self, state_batch):            if tf.shape(state_batch)[0] > 1:                tf.print(state_batch, summarize=-1)            predictions = self.model(state_batch)            if tf.shape(state_batch)[0] > 1:                tf.print(predictions, summarize=-1)            return tf.split(predictions, [board_height*board_width, -1], axis=2)```And the following logs are from stdout when calling from C, produced by the `tf.print` above.```[[[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]  [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]] [[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]]][[[-16.6252575 -16.6252575 -16.554945 -16.6252575 -16.5581188 -16.6252575 -16.5571423 -16.6252575 -16.5570812 -16.6252575 -16.5592785 -16.6252575 -16.5568371 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.556715 -16.5604382 -16.5638561 -16.1153088 -16.5592785 -16.5548229 -16.5536633 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.5546398 -16.6252575 -16.6251965 -16.5716686 -16.3776989 -15.2738171 -15.9649181 -16.549757 -16.2080822 -15.4714489 -16.3686047 -16.5711803 -16.6251965 -16.6252575 -16.5547 -16.6252575 -16.6252575 -16.57057 -16.5050793 -16.4986095 -16.1685314 -15.5485363 -15.7727184 -16.2866344 -16.1884899 -16.2140026 -16.5062389 -16.570631 -16.6252575 -16.6252575 -16.5578747 -16.5541515 -16.4574718 -16.237318 -12.7237072 -14.7688122 -15.8670177 -3.9945817 -14.1392956 -13.9726696 -14.495863 -16.5278454 -16.3515148 -16.5543957 -16.5592785 -16.6252575 -16.5604382 -16.3097668 -16.0515881 -10.2605724 -2.71552896 -3.54908609 -5.74897623 -3.83827066 -4.30714273 -13.6384411 -16.1675549 -15.1023083 -16.5556774 -16.6252575 -16.5547 -16.5589123 -16.4617443 -16.3551769 -15.2829113 -5.34351969 -3.03394938 -1.94203043 -4.53840494 -3.27900553 -15.9859142 -16.3760509 -16.4613781 -16.5639782 -16.554945 -16.6252575 -16.4714489 -16.5536022 -15.8650646 -4.17939615 -5.1966691 -2.17225504 -2.46821451 -2.10627604 -4.61872721 -3.8862443 -15.8424816 -16.5542126 -15.9301281 -16.6252575 -16.5547 -16.5638561 -15.9878063 -16.3649426 -14.6254406 -4.10206461 -4.06068277 -3.20484781 -4.07081461 -5.61872721 -15.9856701 -16.383131 -16.4535046 -16.5583019 -16.5559216 -16.6252575 -16.5608654 -16.3747082 -16.4586315 -14.030653 -3.08790445 -3.91151285 -6.01582193 -3.30915689 -3.78974771 -11.8531628 -16.1242809 -16.1317883 -16.5604382 -16.6252575 -16.5598888 -16.5536633 -16.4726696 -16.5026379 -14.5284557 -10.9210339 -15.3974743 -4.61561441 -15.4461193 -11.3503551 -12.4599743 -16.4962292 -16.3061047 -16.5563488 -16.5615978 -16.6252575 -16.6252575 -16.57057 -16.5019054 -16.5019054 -13.358778 -16.1637096 -15.8333263 -16.3820934 -16.0954113 -16.1836071 -16.5053844 -16.5707531 -16.6252575 -16.6252575 -16.5546398 -16.6252575 -16.6251965 -16.570509 -16.4332409 -15.9699841 -16.4579 -16.4486217 -15.909193 -16.3141 -15.8626842 -16.5714855 -16.6251965 -16.6252575 -16.5547619 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.5536633 -16.5536022 -16.5640392 -15.6290417 -16.5579967 -16.5569592 -16.5561047 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.5554943 -16.6252575 -16.5639172 -16.6252575 -16.5551281 -16.6252575 -16.5547 -16.6252575 -16.5608654 -16.6252575 -16.5559216 -16.6252575 -16.6252575 0.242623821]] [[-16.6252575 -16.6252575 -16.554945 -16.6252575 -16.5581188 -16.6252575 -16.5571423 -16.6252575 -16.5570812 -16.6252575 -16.5592785 -16.6252575 -16.5568371 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.556715 -16.5604382 -16.5638561 -16.1153088 -16.5592785 -16.5548229 -16.5536633 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.5546398 -16.6252575 -16.6251965 -16.5716686 -16.3776989 -15.2738171 -15.9649181 -16.549757 -16.2080822 -15.4714489 -16.3686047 -16.5711803 -16.6251965 -16.6252575 -16.5547 -16.6252575 -16.6252575 -16.57057 -16.5050793 -16.4986095 -16.1685314 -15.5485363 -15.7727184 -16.2866344 -16.1884899 -16.2140026 -16.5062389 -16.570631 -16.6252575 -16.6252575 -16.5578747 -16.5541515 -16.4574718 -16.237318 -12.7237072 -14.7688122 -15.8670177 -3.9945817 -14.1392956 -13.9726696 -14.495863 -16.5278454 -16.3515148 -16.5543957 -16.5592785 -16.6252575 -16.5604382 -16.3097668 -16.0515881 -10.2605724 -2.71552896 -3.54908609 -5.74897623 -3.83827066 -4.30714273 -13.6384411 -16.1675549 -15.1023083 -16.5556774 -16.6252575 -16.5547 -16.5589123 -16.4617443 -16.3551769 -15.2829113 -5.34351969 -3.03394938 -1.94203043 -4.53840494 -3.27900553 -15.9859142 -16.3760509 -16.4613781 -16.5639782 -16.554945 -16.6252575 -16.4714489 -16.5536022 -15.8650646 -4.17939615 -5.1966691 -2.17225504 -2.46821451 -2.10627604 -4.61872721 -3.8862443 -15.8424816 -16.5542126 -15.9301281 -16.6252575 -16.5547 -16.5638561 -15.9878063 -16.3649426 -14.6254406 -4.10206461 -4.06068277 -3.20484781 -4.07081461 -5.61872721 -15.9856701 -16.383131 -16.4535046 -16.5583019 -16.5559216 -16.6252575 -16.5608654 -16.3747082 -16.4586315 -14.030653 -3.08790445 -3.91151285 -6.01582193 -3.30915689 -3.78974771 -11.8531628 -16.1242809 -16.1317883 -16.5604382 -16.6252575 -16.5598888 -16.5536633 -16.4726696 -16.5026379 -14.5284557 -10.9210339 -15.3974743 -4.61561441 -15.4461193 -11.3503551 -12.4599743 -16.4962292 -16.3061047 -16.5563488 -16.5615978 -16.6252575 -16.6252575 -16.57057 -16.5019054 -16.5019054 -13.358778 -16.1637096 -15.8333263 -16.3820934 -16.0954113 -16.1836071 -16.5053844 -16.5707531 -16.6252575 -16.6252575 -16.5546398 -16.6252575 -16.6251965 -16.570509 -16.4332409 -15.9699841 -16.4579 -16.4486217 -15.909193 -16.3141 -15.8626842 -16.5714855 -16.6251965 -16.6252575 -16.5547619 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.5536633 -16.5536022 -16.5640392 -15.6290417 -16.5579967 -16.5569592 -16.5561047 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.5554943 -16.6252575 -16.5639172 -16.6252575 -16.5551281 -16.6252575 -16.5547 -16.6252575 -16.5608654 -16.6252575 -16.5559216 -16.6252575 -16.6252575 0.242623821]]]```
"
57611,0,4455,26,0,0,mu40,0,"title:XLA bug for ReLU after convolution description:### Issue TypeBug### Sourcebinary, official 2.9.1-gpu Docker image### Tensorflow Version2.9.1### Custom CodeNo### OS Platform and DistributionLinux CentOS Stream release 8### Python version3.8.10### CUDA/cuDNN version11.2### Current Behaviour?With XLA enabled, ReLU after a convolution causes a CUDNN_STATUS_INTERNAL_ERROR. The error disappears when ReLU is replaced with LeakyReLU.In 2019, a similar issue was resolved for JAX by updating XLA (https://github.com/google/jax/issues/1049). The bug comes up when running the below Python code in the official tensorflow/tensorflow:2.9.1-gpu Docker image from May 2022:```shellsingularity pull docker://tensorflow/tensorflow:2.9.1-gpusingularity exec -e --nv tensorflow_2.9.1-gpu.sif python ./relu.py```### Standalone code to reproduce the issue```pythonimport osimport tensorflow as tfos.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2'model = tf.keras.Sequential()model.add(tf.keras.layers.Conv1D(filters=1, kernel_size=1))model.add(tf.keras.layers.ReLU())model.compile(loss='MSE')x = tf.ones(shape=(1, 32, 1))model.fit(x, y=x, epochs=2)```### Relevant log output```2022-09-04 09:21:09.113717: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2022-09-04 09:21:17.262140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 29503 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.02022-09-04 09:21:17.290599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30988 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1c:00.0, compute capability: 7.02022-09-04 09:21:17.291712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 30988 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1d:00.0, compute capability: 7.02022-09-04 09:21:17.292349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30988 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1e:00.0, compute capability: 7.02022-09-04 09:21:17.954855: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1750] (One-time warning): Not using XLA:CPU for cluster.If you want XLA:CPU, do one of the following: - set the TF_XLA_FLAGS to include ""--tf_xla_cpu_global_jit"", or - set cpu_global_jit to true on this session's OptimizerOptions, or - use experimental_jit_scope, or - use tf.function(jit_compile=True).To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as aproper command-line flag, not via TF_XLA_FLAGS).Epoch 1/22022-09-04 09:21:18.525364: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f12e801d660 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:2022-09-04 09:21:18.525426: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.02022-09-04 09:21:18.525438: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (1): Tesla V100-SXM2-32GB, Compute Capability 7.02022-09-04 09:21:18.525449: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (2): Tesla V100-SXM2-32GB, Compute Capability 7.02022-09-04 09:21:18.525459: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (3): Tesla V100-SXM2-32GB, Compute Capability 7.02022-09-04 09:21:18.657529: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.2022-09-04 09:21:23.648975: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 81002022-09-04 09:21:36.916665: I tensorflow/compiler/jit/xla_compilation_cache.cc:478] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.1/1 [==============================] - 19s 19s/step - loss: 1.0000Epoch 2/2Traceback (most recent call last):  File ""./relu.py"", line 12, in <module>    model.fit(x, y=x, epochs=2)  File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 67, in error_handler    raise e.with_traceback(filtered_tb) from None  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 54, in quick_execute    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,tensorflow.python.framework.errors_impl.UnknownError: Graph execution error:CUDNN_STATUS_INTERNAL_ERRORin tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'         [[{{node cluster_0_1/xla_run}}]] [Op:__inference_train_function_502]```</details>
"
57579,1,1486,0,0,0,Ricky-KLA,0,"title:tf.linalg.solve produces incorrect output when compiled using XLA description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.9.1### Custom CodeNo### OS Platform and DistributionLinux Ubuntu 20.04.4 LTS### Mobile device_No response_### Python version3.8.10### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shelltf.linalg.solve when compiled with XLA produces different (incorrect) output when compared to a standalone TF execution.In the sample code below, the solveEquation function solves for Ax = B and returns Ax. This function is run with and without XLA enabled. Both executions produce very different outputs!```### Standalone code to reproduce the issue```shellimport numpy as npimport tensorflow as tfdata_type = tf.float32def solveEquation(A, B):    ATA = tf.matmul(A, A, transpose_a=True)    ATB = tf.matmul(A, B, transpose_a=True)    output = tf.linalg.solve(ATA, ATB)    return (tf.matmul(A, output))P = 2Q = 3R = 5np.random.seed(0)mat1 = tf.constant(np.random.random((P, Q)), dtype=data_type)mat2 = tf.constant(np.random.random((P, R)), dtype=data_type)output_standard = solveEquation(mat1, mat2)tf_func_xla = tf.function(func=solveEquation,                           input_signature=[tf.TensorSpec(shape=(P,Q),dtype=data_type),                                           tf.TensorSpec(shape=(P,R),dtype=data_type)],                           jit_compile=True)output_xla = tf_func_xla(mat1, mat2)tf.print(output_standard)tf.print(output_xla)```### Relevant log output```shell# output with XLA disabled[[0.43758738 0.891772866 0.963661969 0.383441627 0.791725397] [0.528894722 0.568044662 0.92559737 0.0710358918 0.0871287584]]# output with XLA enabled[[0.274795085 0.876262307 0.82626003 0.345206 0.581315041] [0.3589876 0.553595543 0.778286219 0.0325973332 -0.126585901]]```</details>
"
57558,0,17894,0,0,0,srcarroll,0,"title:hlo-legalize-to-linalg crashes on convolution op description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow VersionSHA 551852a9ea9bf4e99856ce75c63516ad6d372239### Custom CodeNo### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?I found a bug in `hlo-legalize-to-linalg` where it will crash for certain convolution operators.  Here's one example of an op that causes the crash```shellfunc.func @main(%arg0: tensor<1x13x13x32xf32>, %arg1: tensor<1x11x11x64xf32>) -> tensor<3x3x32x64xf32> {  %0 = ""mhlo.convolution""(%arg0, %arg1) {batch_group_count = 1 : i64, dimension_numbers = #mhlo.conv<[f, 0, 1, b]x[i, 0, 1, o]->[0, 1, b, f]>, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, padding = dense<0> : tensor<2x2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<1> : tensor<2xi64>} : (tensor<1x13x13x32xf32>, tensor<1x11x11x64xf32>) -> tensor<3x3x32x64xf32>  return %0 : tensor<3x3x32x64xf32>}```The crash appears to happen on this line https://github.com/tensorflow/tensorflow/commit/763d55e7e33ca888a2aa14fa89ec413047daf1e5#diff-524cc00a6c4fc699fbad5fe46d08167e30aecdb4d19edcded46028f17f1f4837R2247.  I printed out the expressions in all 3 arguments and it appears that dstExpr still has a null in it.  Here's what they look like.```srcExprsd0d2 + d3d4 + d5d6windowExprsd0d3d5d1dstExprsd2d4<<NULL AFFINE EXPR>>d6```Run command in standalone code section on the IR provided above### Standalone code to reproduce the issue```shelltf-opt --hlo-legalize-to-linalg```### Relevant log output```shellStack dump:0.      Program arguments: tf-opt --hlo-legalize-to-linalg convolution.mlir1.      Program arguments: tf-opt --hlo-legalize-to-linalg convolution.mlir #0 0x00007f089dc9a437 llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/../../../_solib_k8/_U_S_Stensorflow_Scompiler_Smlir_Ctf-opt___Utensorflow/libtensorflow_framework.so.2+0x1382437) #1 0x00007f089dc97a35 llvm::sys::RunSignalHandlers() (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/../../../_solib_k8/_U_S_Stensorflow_Scompiler_Smlir_Ctf-opt___Utensorflow/libtensorflow_framework.so.2+0x137fa35) #2 0x00007f089dc98a85 SignalHandler(int) Signals.cpp:0:0 #3 0x00007f089c78f3c0 __restore_rt (/lib/x86_64-linux-gnu/libpthread.so.0+0x143c0) #4 0x00005616013e56aa mlir::AffineExpr::walk(std::function<void (mlir::AffineExpr)>) const (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10f026aa) #5 0x00005616013ef532 mlir::AffineMap::inferFromExprList(llvm::ArrayRef<llvm::ArrayRef<mlir::AffineExpr>>) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10f0c532) #6 0x00005615fbdcf44e mlir::mhlo::(anonymous namespace)::ConvolutionOpGeneralConversion::matchAndRewrite(mlir::mhlo::ConvolutionOp, mlir::mhlo::ConvolutionOpAdaptor, mlir::ConversionPatternRewriter&) const legalize_to_linalg.cc:0:0 #7 0x00005615fbcff44b mlir::OpConversionPattern<mlir::mhlo::ConvolutionOp>::matchAndRewrite(mlir::Operation*, llvm::ArrayRef<mlir::Value>, mlir::ConversionPatternRewriter&) const (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0xb81c44b) #8 0x000056160124d76d mlir::ConversionPattern::matchAndRewrite(mlir::Operation*, mlir::PatternRewriter&) const (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10d6a76d) #9 0x00005616012a7243 mlir::PatternApplicator::matchAndRewrite(mlir::Operation*, mlir::PatternRewriter&, llvm::function_ref<bool (mlir::Pattern const&)>, llvm::function_ref<void (mlir::Pattern const&)>, llvm::function_ref<mlir::LogicalResult (mlir::Pattern const&)>) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10dc4243)#10 0x00005616012584ec (anonymous namespace)::OperationLegalizer::legalize(mlir::Operation*, mlir::ConversionPatternRewriter&) DialectConversion.cpp:0:0#11 0x000056160125e257 (anonymous namespace)::OperationConverter::convertOperations(llvm::ArrayRef<mlir::Operation*>, llvm::function_ref<void (mlir::Diagnostic&)>) DialectConversion.cpp:0:0#12 0x000056160125e5f1 mlir::applyPartialConversion(llvm::ArrayRef<mlir::Operation*>, mlir::ConversionTarget&, mlir::FrozenRewritePatternSet const&, llvm::DenseSet<mlir::Operation*, llvm::DenseMapInfo<mlir::Operation*, void>>*) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10d7b5f1)#13 0x000056160125e677 mlir::applyPartialConversion(mlir::Operation*, mlir::ConversionTarget&, mlir::FrozenRewritePatternSet const&, llvm::DenseSet<mlir::Operation*, llvm::DenseMapInfo<mlir::Operation*, void>>*) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10d7b677)#14 0x00005615fbdd6a98 mlir::mhlo::(anonymous namespace)::HloLegalizeToLinalgPass::runOnOperation() legalize_to_linalg.cc:0:0#15 0x000056160138d792 mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10eaa792)#16 0x000056160138ea42 mlir::detail::OpToOpPassAdaptor::runPipeline(mlir::OpPassManager&, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10eaba42)#17 0x000056160138ca05 mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10ea9a05)#18 0x000056160138d3a2 mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10eaa3a2)#19 0x000056160138ea42 mlir::detail::OpToOpPassAdaptor::runPipeline(mlir::OpPassManager&, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10eaba42)#20 0x000056160138f5a6 mlir::PassManager::run(mlir::Operation*) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10eac5a6)#21 0x00005615fcf7cfd3 performActions(llvm::raw_ostream&, bool, bool, llvm::SourceMgr&, mlir::MLIRContext*, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>) (.constprop.0) MlirOptMain.cpp:0:0#22 0x00005615fcf7d526 processBuffer(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, bool, bool, bool, bool, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>, mlir::DialectRegistry&, llvm::ThreadPool*) MlirOptMain.cpp:0:0#23 0x00005615fcf7d731 mlir::LogicalResult llvm::function_ref<mlir::LogicalResult (std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>::callback_fn<mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>, mlir::DialectRegistry&, bool, bool, bool, bool, bool)::'lambda'(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>(long, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&) MlirOptMain.cpp:0:0#24 0x00005616014d9d73 mlir::splitAndProcessBuffer(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<mlir::LogicalResult (std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>, llvm::raw_ostream&, bool, bool) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10ff6d73)#25 0x00005615fcf7ca72 mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>, mlir::DialectRegistry&, bool, bool, bool, bool, bool) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0xca99a72)#26 0x00005615fcf7cb23 mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, mlir::PassPipelineCLParser const&, mlir::DialectRegistry&, bool, bool, bool, bool, bool) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0xca99b23)#27 0x00005615fcf7dfa4 mlir::MlirOptMain(int, char**, llvm::StringRef, mlir::DialectRegistry&, bool) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0xca9afa4)#28 0x00005615fb7001fb main (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0xb21d1fb)#29 0x00007f089c3b00b3 __libc_start_main /build/glibc-sMfBJT/glibc-2.31/csu/../csu/libc-start.c:342:3#30 0x00005615f3bae94e _start (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x36cb94e) #0 0x00007f089dc9a437 llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/../../../_solib_k8/_U_S_Stensorflow_Scompiler_Smlir_Ctf-opt___Utensorflow/libtensorflow_framework.so.2+0x1382437) #1 0x00007f089dc97a35 llvm::sys::RunSignalHandlers() (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/../../../_solib_k8/_U_S_Stensorflow_Scompiler_Smlir_Ctf-opt___Utensorflow/libtensorflow_framework.so.2+0x137fa35) #2 0x00007f089dc98a85 SignalHandler(int) Signals.cpp:0:0 #3 0x00007f089c78f3c0 __restore_rt (/lib/x86_64-linux-gnu/libpthread.so.0+0x143c0) #4 0x00005616013e56aa mlir::AffineExpr::walk(std::function<void (mlir::AffineExpr)>) const (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10f026aa) #5 0x00005616013ef532 mlir::AffineMap::inferFromExprList(llvm::ArrayRef<llvm::ArrayRef<mlir::AffineExpr>>) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10f0c532) #6 0x00005615fbdcf44e mlir::mhlo::(anonymous namespace)::ConvolutionOpGeneralConversion::matchAndRewrite(mlir::mhlo::ConvolutionOp, mlir::mhlo::ConvolutionOpAdaptor, mlir::ConversionPatternRewriter&) const legalize_to_linalg.cc:0:0 #7 0x00005615fbcff44b mlir::OpConversionPattern<mlir::mhlo::ConvolutionOp>::matchAndRewrite(mlir::Operation*, llvm::ArrayRef<mlir::Value>, mlir::ConversionPatternRewriter&) const (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0xb81c44b) #8 0x000056160124d76d mlir::ConversionPattern::matchAndRewrite(mlir::Operation*, mlir::PatternRewriter&) const (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10d6a76d) #9 0x00005616012a7243 mlir::PatternApplicator::matchAndRewrite(mlir::Operation*, mlir::PatternRewriter&, llvm::function_ref<bool (mlir::Pattern const&)>, llvm::function_ref<void (mlir::Pattern const&)>, llvm::function_ref<mlir::LogicalResult (mlir::Pattern const&)>) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10dc4243)#10 0x00005616012584ec (anonymous namespace)::OperationLegalizer::legalize(mlir::Operation*, mlir::ConversionPatternRewriter&) DialectConversion.cpp:0:0#11 0x000056160125e257 (anonymous namespace)::OperationConverter::convertOperations(llvm::ArrayRef<mlir::Operation*>, llvm::function_ref<void (mlir::Diagnostic&)>) DialectConversion.cpp:0:0#12 0x000056160125e5f1 mlir::applyPartialConversion(llvm::ArrayRef<mlir::Operation*>, mlir::ConversionTarget&, mlir::FrozenRewritePatternSet const&, llvm::DenseSet<mlir::Operation*, llvm::DenseMapInfo<mlir::Operation*, void>>*) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10d7b5f1)#13 0x000056160125e677 mlir::applyPartialConversion(mlir::Operation*, mlir::ConversionTarget&, mlir::FrozenRewritePatternSet const&, llvm::DenseSet<mlir::Operation*, llvm::DenseMapInfo<mlir::Operation*, void>>*) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10d7b677)#14 0x00005615fbdd6a98 mlir::mhlo::(anonymous namespace)::HloLegalizeToLinalgPass::runOnOperation() legalize_to_linalg.cc:0:0#15 0x000056160138d792 mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10eaa792)#16 0x000056160138ea42 mlir::detail::OpToOpPassAdaptor::runPipeline(mlir::OpPassManager&, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10eaba42)#17 0x000056160138ca05 mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10ea9a05)#18 0x000056160138d3a2 mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10eaa3a2)#19 0x000056160138ea42 mlir::detail::OpToOpPassAdaptor::runPipeline(mlir::OpPassManager&, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10eaba42)#20 0x000056160138f5a6 mlir::PassManager::run(mlir::Operation*) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10eac5a6)#21 0x00005615fcf7cfd3 performActions(llvm::raw_ostream&, bool, bool, llvm::SourceMgr&, mlir::MLIRContext*, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>) (.constprop.0) MlirOptMain.cpp:0:0#22 0x00005615fcf7d526 processBuffer(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, bool, bool, bool, bool, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>, mlir::DialectRegistry&, llvm::ThreadPool*) MlirOptMain.cpp:0:0#23 0x00005615fcf7d731 mlir::LogicalResult llvm::function_ref<mlir::LogicalResult (std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>::callback_fn<mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>, mlir::DialectRegistry&, bool, bool, bool, bool, bool)::'lambda'(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>(long, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&) MlirOptMain.cpp:0:0#24 0x00005616014d9d73 mlir::splitAndProcessBuffer(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<mlir::LogicalResult (std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>, llvm::raw_ostream&, bool, bool) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10ff6d73)#25 0x00005615fcf7ca72 mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>, mlir::DialectRegistry&, bool, bool, bool, bool, bool) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0xca99a72)#26 0x00005615fcf7cb23 mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, mlir::PassPipelineCLParser const&, mlir::DialectRegistry&, bool, bool, bool, bool, bool) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0xca99b23)#27 0x00005615fcf7dfa4 mlir::MlirOptMain(int, char**, llvm::StringRef, mlir::DialectRegistry&, bool) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0xca9afa4)#28 0x00005615fb7001fb main (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0xb21d1fb)#29 0x00007f089c3b00b3 __libc_start_main /build/glibc-sMfBJT/glibc-2.31/csu/../csu/libc-start.c:342:3#30 0x00005615f3bae94e _start (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x36cb94e)Segmentation fault (core dumped)```</details>
"
57534,0,0,0,0,0,VictoriaGriffith,0,"title:`tf.pad` fails when executing in `tf.autodiff.ForwardAccumulator` description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.9.1### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shell`tf.pad` fails when executing in `tf.autodiff.ForwardAccumulator` and throws `TypeError`. However, if we run `tf.pad` outside of `ForwardAccumulator` with the same input, it will pass.```### Standalone code to reproduce the issue```shellimport tensorflow as tfinput_tensor = tf.random.uniform([3, 1, 1, 1], minval=2.0, maxval=3.0, dtype=tf.float32)paddings = tf.random.uniform([4, 2], minval=0, maxval=5, dtype=tf.int64) mode = ""CONSTANT""constant_values = 0 result = tf.pad(input_tensor, paddings, mode=mode, constant_values=constant_values, ) print(result.shape) # Passtangent = tf.reshape(tf.one_hot(1, tf.size(input_tensor), dtype=input_tensor.dtype), shape=input_tensor.shape)with tf.autodiff.ForwardAccumulator(input_tensor, tangent) as acc:    result = tf.pad(input_tensor, paddings, mode=mode, constant_values=constant_values, ) # Fail```### Relevant log output```shell(8, 3, 8, 4)TypeError: Input 'y' of 'Sub' Op has type int64 that does not match type int32 of argument 'x'.```</details>
"
57529,1,963,25,0,0,cheyennee,0,"title:tf.image.convert_image_dtype dont check the validity of input description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow VersionTF2.4### Custom CodeYes### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellImages that are represented using floating point values are expected to have values in the range [0,1) on documentation. However, when image is floating point values greater than 1, the code works. It should throw an exception.```### Standalone code to reproduce the issue```shellimport tensorflow as tfresults={}try:  arg_0 = [[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]],[[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]]  dtype = tf.uint64  saturate = False  results[""res""] = tf.image.convert_image_dtype(arg_0,dtype=dtype,saturate=saturate,)except Exception as e:  results[""err""] = ""Error:""+str(e)print(results)'''{'res': <tf.Tensor: shape=(2, 2, 3), dtype=uint64, numpy=array([[[9223372036854775808, 9223372036854775808, 9223372036854775808],        [9223372036854775808, 9223372036854775808, 9223372036854775808]],       [[9223372036854775808, 9223372036854775808, 9223372036854775808],        [9223372036854775808, 9223372036854775808, 9223372036854775808]]],      dtype=uint64)>}'''```### Relevant log output_No response_</details>
"
57527,1,978,25,0,0,cheyennee,0,"title:when dtype is tf.uint64, tf.image.convert_image_dtype throws exception description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow VersionTF2.4### Custom CodeYes### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shelltf.image.convert_image_dtype supports data types (for image and dtype) of uint8, uint16, uint32, uint64, int8, int16, int32, int64, float16, float32, float64, bfloat16. But when dtype is uint64, it throws exception.```### Standalone code to reproduce the issue```shellimport tensorflow as tfresults={}try:  arg_0 = [[[1, 2, 3], [4, 5, 6]],[[7, 8, 9], [10, 11, 12]]]  dtype = tf.uint64  saturate = False  results[""res""] = tf.image.convert_image_dtype(arg_0,dtype=dtype,saturate=saturate,)except Exception as e:  results[""err""] = ""Error:""+str(e)print(results)'''{'err': ""Error:Value for attr 'T' of uint64 is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, int64, complex64, complex128\n\t; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul]""}'''```### Relevant log output_No response_</details>
"
57449,1,6057,36,0,0,JVD9kh96,0,"title:Error while using keyword arguments in tensorflow methods  description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.6.4### Custom CodeYes### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellI dont know exactly what is going on, but I was trying to write a pipline by using tf.data.Dataset.from_tensor_slices. When I try to cast the image to tf.float32, it gives me the following error:img = tf.cast(img, dtype=tf.float32)    TypeError: 'dict' object is not callablewhen I change the following line to:img = tf.cast(img, tf.float32) everything works fine! The same thing happens when I tried to write a custom loss function. It seems passing keyword argument with the keyword causes the error.```### Standalone code to reproduce the issue```shellimport tensorflow as tf images = ['./temp/image1.jpg', './temp/image2.jpg']batch_size = 64def preprocess(img_name):    img = tf.io.read_file(images[0])    img = tf.io.decode_jpeg(img)    img = tf.cast(img, dtype=tf.float32)    img = tf.divide(img, 255.0)    img = tf.image.resize(img, (256, 256))    return imgdata = tf.data.Dataset.from_tensor_slices(images[0:1])data = data.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)data = data.shuffle(batch_size*4)data = data.batch(batch_size)data = data.prefetch(tf.data.AUTOTUNE)```### Relevant log output```shell/tmp/ipykernel_17/453503734.py in <module>      8       9 data = tf.data.Dataset.from_tensor_slices(images[0:1])---> 10 data = data.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)     11 data = data.shuffle(batch_size*4)     12 data = data.batch(batch_size)/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in map(self, map_func, num_parallel_calls, deterministic)   1866           num_parallel_calls,   1867           deterministic,-> 1868           preserve_cardinality=True)   1869    1870   def flat_map(self, map_func):/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)   5022         self._transformation_name(),   5023         dataset=input_dataset,-> 5024         use_legacy_function=use_legacy_function)   5025     if deterministic is None:   5026       self._deterministic = ""default""/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)   4216         fn_factory = trace_tf_function(defun_kwargs)   4217 -> 4218     self._function = fn_factory()   4219     # There is no graph to add in eager mode.   4220     add_to_graph &= not context.executing_eagerly()/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in get_concrete_function(self, *args, **kwargs)   3149     """"""   3150     graph_function = self._get_concrete_function_garbage_collected(-> 3151         *args, **kwargs)   3152     graph_function._garbage_collector.release()  # pylint: disable=protected-access   3153     return graph_function/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)   3114       args, kwargs = None, None   3115     with self._lock:-> 3116       graph_function, _ = self._maybe_define_function(args, kwargs)   3117       seen_names = set()   3118       captured = object_identity.ObjectIdentitySet(/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)   3461    3462           self._function_cache.missed.add(call_context_key)-> 3463           graph_function = self._create_graph_function(args, kwargs)   3464           self._function_cache.primary[cache_key] = graph_function   3465 /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)   3306             arg_names=arg_names,   3307             override_flat_arg_shapes=override_flat_arg_shapes,-> 3308             capture_by_value=self._capture_by_value),   3309         self._function_attributes,   3310         function_spec=self.function_spec,/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)   1005         _, original_func = tf_decorator.unwrap(python_func)   1006 -> 1007       func_outputs = python_func(*func_args, **func_kwargs)   1008    1009       # invariant: `func_outputs` contains only Tensors, CompositeTensors,/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in wrapped_fn(*args)   4193           attributes=defun_kwargs)   4194       def wrapped_fn(*args):  # pylint: disable=missing-docstring-> 4195         ret = wrapper_helper(*args)   4196         ret = structure.to_tensor_list(self._output_structure, ret)   4197         return [ops.convert_to_tensor(t) for t in ret]/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in wrapper_helper(*args)   4123       if not _should_unpack(nested_args):   4124         nested_args = (nested_args,)-> 4125       ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)   4126       if _should_pack(ret):   4127         ret = tuple(ret)/opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)    693       except Exception as e:  # pylint:disable=broad-except    694         if hasattr(e, 'ag_error_metadata'):--> 695           raise e.ag_error_metadata.to_exception(e)    696         else:    697           raiseTypeError: in user code:    /tmp/ipykernel_17/3009962238.py:9 preprocess  *        img = tf.cast(img, dtype=tf.float32)    TypeError: 'dict' object is not callable```</details>
"
57430,0,1588,0,0,0,eeDigitalSeal,0,"title:Second-order gradient calculated in forward-mode for API `tf.linalg.det` is inaccurate relative to that in reverse-mode description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.9### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version3.8### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellSecond-order gradient calculated in forward-mode AD is different from that in reverse-mode. And the error reached 1e-1 which is not acceptable.```### Standalone code to reproduce the issue```shellimport tensorflow as tfinput = tf.constant([[0.41278124, 0.472762  ],[0.5486289,0.8008381 ]],dtype=tf.float32)with tf.GradientTape(persistent=True) as g2:    with tf.GradientTape(persistent=True) as g1:        g1.watch(input)        g2.watch(input)        res = tf.linalg.det(input)    grad = g1.gradient(res, input)grad2nd = g2.gradient(grad, input)grad2nd_fwd = []for i in range(tf.size(input)):    hessian_col = []    for j in range(tf.size(input)):        tangent_inner = tf.reshape(tf.one_hot(i, tf.size(input), dtype=input.dtype), shape=input.shape)        tangent_outer = tf.reshape(tf.one_hot(j, tf.size(input), dtype=input.dtype), shape=input.shape)        with tf.autodiff.ForwardAccumulator(input, tangent_outer) as acc_outer:            with tf.autodiff.ForwardAccumulator(input, tangent_inner) as acc_inner:                res = tf.reduce_sum(tf.linalg.det(input))            jvp = acc_inner.jvp(res)        hvp = acc_outer.jvp(jvp)        hessian_col.append(hvp)    grad2nd_fwd.append(tf.reduce_sum(hessian_col))grad2nd_fwd = tf.reshape(grad2nd_fwd, shape=input.shape)print(""reverse-mode\n"",grad2nd)print(""forward-mode\n"",grad2nd_fwd)```### Relevant log output```shellreverse-mode tf.Tensor([[ 1.0000001  -1.2048287 ] [-0.79517186  1.0000002 ]], shape=(2, 2), dtype=float32)forward-mode tf.Tensor([[ 1.        -1.000001 ] [-1.0000002  1.       ]], shape=(2, 2), dtype=float32)```</details>
"
57411,1,2286,36,0,0,ZhaoqiongZ,0,"title:LSTM layer not support bf16  description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.9.1### Custom CodeYes### OS Platform and Distribution Ubuntu 22.04.1 LTS### Mobile device_No response_### Python version3.7### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellI trid do mix_precision with keras policy, get the error ""Value passed to parameter 'input' has DataType bfloat16 not in list of allowed values: float16, float32, float64""Why not support LSTM layer BF16 ? Is it a accuracy loss concern or something else?Will it be possible to support LSTM with BF16?If not, can we add it to something like DenyList to run LSTM with FP32 automatically rather than throw this error?```### Standalone code to reproduce the issue```shellfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras import mixed_precisionfrom tensorflow.keras.layers import LSTM, Embeddingpolicy = mixed_precision.Policy('mixed_bfloat16')mixed_precision.set_global_policy(policy)vocab_size = 1000# define modeldef customer_model():    model = Sequential()    model.add(Embedding(vocab_size, 512, input_length=50))    model.add(LSTM(128, return_sequences=True))    return modeldef test_model():    model = Sequential()    model.add(Embedding(vocab_size, 128, input_length=50))    model.add(LSTM(4, dtype='float32',  return_sequences=True))        return modelmodel = customer_model()model.summary()```### Relevant log output```shellTraceback (most recent call last):  File ""reproduce_error.py"", line 23, in <module>    model = customer_model()  File ""reproduce_error.py"", line 13, in customer_model    model.add(LSTM(128, return_sequences=True))  File ""/home/sdp/miniconda3/envs/tensorflow_2.9.1/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py"", line 587, in _method_wrapper    result = method(self, *args, **kwargs)  File ""/home/sdp/miniconda3/envs/tensorflow_2.9.1/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler    raise e.with_traceback(filtered_tb) from None  File ""/home/sdp/miniconda3/envs/tensorflow_2.9.1/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py"", line 58, in _SatisfiesTypeConstraint    raise TypeError(TypeError: Exception encountered when calling layer ""lstm"" (type LSTM).Value passed to parameter 'input' has DataType bfloat16 not in list of allowed values: float16, float32, float64Call arguments received by layer ""lstm"" (type LSTM):  闂?inputs=tf.Tensor(shape=(None, 50, 512), dtype=bfloat16)  闂?mask=None  闂?training=None  闂?initial_state=None```</details>
"
57392,1,2409,277,0,0,matteopilz,0,"title:Model improves worse with GradientTape than with fit() description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.9.1### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 22.04.1### Mobile device_No response_### Python version3.8 & 3.10### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellI already opened a question on stackoverflow for this, but I believe it could be some issue with tensorflow.https://stackoverflow.com/questions/73440964/model-not-improving-with-gradienttape-but-with-model-fitThe same model using either custom training with GradientTape or keras' model.fit() will perform differently. The custom run, will stop improving earlier and will take longer to improve at all.I've also included a Google Colab notebook in order to reproduce the issue:https://colab.research.google.com/drive/1pk66rbiux5vHZcav9VNSBhdWWIhQM-nF?usp=sharing```### Standalone code to reproduce the issue```shelldef build_model(kernel_regularizer=l2(0.0001), dropout=0.001, recurrent_dropout=0.):    x1 = Input(62)    x2 = Input((62, 3))    x = Embedding(30, 100, mask_zero=True)(x1)    x = Concatenate()([x, x2])    x = Bidirectional(LSTM(500,                           return_sequences=True,                           kernel_regularizer=kernel_regularizer,                           dropout=dropout,                           recurrent_dropout=recurrent_dropout))(x)    x = Bidirectional(LSTM(500,                           return_sequences=False,                           kernel_regularizer=kernel_regularizer,                           dropout=dropout,                           recurrent_dropout=recurrent_dropout))(x)    x = Activation('softmax')(x)    x = Dense(1000)(x)    x = Dense(500)(x)    x = Dense(250)(x)    x = Dense(1, bias_initializer='ones')(x)    x = tf.math.abs(x)    return Model(inputs=[x1, x2], outputs=x)optimizer = Adam(learning_rate=0.0001)model = build_model()model.compile(optimizer=optimizer, loss='mse', metrics='mse')options = tf.data.Options()options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA dat_train = tf.data.Dataset.from_generator(    generator= lambda: <load_function()>     output_types=((tf.int32, tf.float32), tf.float32)) dat_train = dat_train.with_options(options) # keras trainingmodel.fit(dat_train, epochs=50)# custom trainingfor epoch in range(50):    for (x1, x2), y in dat_train:        with tf.GradientTape() as tape:            y_pred = model((x1, x2), training=True)            loss = model.loss(y, y_pred)        grads = tape.gradient(loss, model.trainable_variables)        model.optimizer.apply_gradients(zip(grads, model.trainable_variables))```### Relevant log output_No response_</details>
"
57363,1,5067,0,0,0,singinwhale,0,"title:model.save: Tried to export a function which references 'untracked' resource even though the tensor should be tracked description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.9.1### Custom CodeNo### OS Platform and DistributionWindows 10### Mobile device_No response_### Python version3.7.13### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version11.7.0_516.01/8100### GPU model and memory_No response_### Current Behaviour?```shellI do not understand why this particular tensor (StylePredictionModelDummy/dummy_conv/kernel:0) is not tracked. As you can see my model (style_transfer_model) is created with the Tensorflow functional API and the untracked tensor in question is instantiated as part of StylePredictionModelDummy.__init__() in line 92 and assigned as a class property to self.feature_extractor which is what the error recommends to do to track the value.The outputs of StylePredictionModelDummy(line 93) are split up(lines 100-105) and routed to the ConditionalInstanceNormalization(line 71) layers as scale and bias inside of the expand layers (line 109). So it also can't be that the tensor is just not used because it is used.I expect the tensor to be tracked and that the model can be saved.```### Standalone code to reproduce the issuehttps://colab.research.google.com/drive/1HFq2k12IGO6bsnbd8WWvg3yci6g12BD0?usp=sharing```pythonimport numpy as npimport tensorflow as tfclass DummyModel(tf.keras.Model):    feature_extractor = None    def __init__(self, name=""StylePredictionModelDummy""):        super().__init__(name=name)        self.feature_extractor = tf.keras.layers.Conv2D(1, 9, 5, padding='same', name=""dummy_conv"")    def call(self, inputs, training=None, mask=None):        x = self.feature_extractor(inputs)        return ximage_shape = (None, 960//4, 1920//4, 3)model = DummyModel()element = tf.convert_to_tensor(np.zeros((1, image_shape[1], image_shape[2], 3)))# call once to build modelresult = model(element)model.save(filepath=""%TEMP%/model"", include_optimizer=False, save_format='tf')```### Relevant log output```shellC:\condaEnvs\realtime-style-transfer\python.exe C:/projects/realtime-style-transfer/save_using_checkpoint.py -C C:\projects\realtime-style-transfer\logs\2022-08-22-10-47-13.310922\last_training_checkpoint -o C:\projects\realtime-style-transfer\temp\saved.tf tensorflow - DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client. (tpu_cluster_resolver.py:32 @ 2022-08-22 10:55:32,983)2022-08-22 10:55:35.933767: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.tensorflow - WARNING - `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default. (mobilenet_v3.py:256 @ 2022-08-22 10:55:35,939)models.stylePrediction - INFO - Using 100 style parameters (stylePrediction.py:24 @ 2022-08-22 10:55:37,877)models.stylePrediction - DEBUG - Using 192 norm parameters (stylePrediction.py:33 @ 2022-08-22 10:55:37,882)tensorflow - WARNING - `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default. (mobilenet_v3.py:256 @ 2022-08-22 10:55:37,883)root - INFO - Running inference to build model... (save_using_checkpoint.py:49 @ 2022-08-22 10:55:44,154)root - INFO - Saving model... (save_using_checkpoint.py:55 @ 2022-08-22 10:55:47,613)tensorflow - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. (saving_utils.py:328 @ 2022-08-22 10:55:47,613)absl - WARNING - Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 67). These functions will not be directly callable after loading. (save.py:233 @ 2022-08-22 10:56:36,316)Traceback (most recent call last):  File ""C:\projects\realtime-style-transfer\save_using_checkpoint.py"", line 56, in <module>    style_transfer_model.save(filepath=str(outpath), include_optimizer=False, save_format='tf')  File ""C:\condaEnvs\realtime-style-transfer\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler    raise e.with_traceback(filtered_tb) from None  File ""C:\condaEnvs\realtime-style-transfer\lib\site-packages\tensorflow\python\saved_model\save.py"", line 473, in _map_captures_to_created_tensors    raise AssertionError(AssertionError: Tried to export a function which references 'untracked' resource Tensor(""52018:0"", shape=(), dtype=resource). TensorFlow objects (e.g. tf.Variable) captured by functions must be 'tracked' by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly. See the information below:	Function name = b'__inference_signature_wrapper_53008'	Captured Tensor = <ResourceHandle(name=""Resource-218-at-0x231ff25da30"", device=""/job:localhost/replica:0/task:0/device:CPU:0"", container=""Anonymous"", type=""class tensorflow::Var"", dtype and shapes : ""[ DType enum: 1, Shape: [3,3,3,16] ]"")>	Trackable referencing this tensor = <tf.Variable 'Conv/kernel:0' shape=(3, 3, 3, 16) dtype=float32>Process finished with exit code 1```</details>
"
57357,1,1092,0,0,0,VictoriaGriffith,0,"title:`tf.nn.silu` have wrong gradient for complex input description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.9### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shell`tf.nn.silu(x)` is equal to `x * sigmoid(x)`, however, when I compute the gradient with respect to a complex tensor, the gradient of the two functions are different. I believe that the `tf.nn.silu`-gradient is wrong because it is inconsistent with the numerical gradient.```### Standalone code to reproduce the issue```shellimport tensorflow as tfinput = tf.complex([[[0.0], [0.0]]], [[[5.35], [6.61]]])print(""input: "", input)with tf.GradientTape() as t:  t.watch(input)  output1 = tf.nn.silu(input)grad1 = t.gradient(output1, input)with tf.GradientTape() as t:  t.watch(input)  output2 = input * tf.math.sigmoid(input)grad2 = t.gradient(output2, input)assert tf.experimental.numpy.allclose(output1, output2) # Assertion Passprint(grad1)print(grad2)assert tf.experimental.numpy.allclose(grad1, grad2) # AssertionError```### Relevant log output```shellinput:  tf.Tensor([[[0.+5.35j]  [0.+6.61j]]], shape=(1, 2, 1), dtype=complex64)tf.Tensor([[[0.5+1.4249809j]  [0.5+1.7798613j]]], shape=(1, 2, 1), dtype=complex64)tf.Tensor([[[0.5-1.4249809j]  [0.5-1.7798615j]]], shape=(1, 2, 1), dtype=complex64)AssertionError```</details>
"
57356,0,4109,22,0,0,iammeizu,0,"title:TF golang client memory leak description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 1 and tf 2### Custom CodeYes### OS Platform and Distributionubuntu20.04### Python version3.8### GCC/Compiler version9.3.0### CUDA/cuDNN version11.4### Current Behaviour?```shellAction1: new a tensor and use the only one tensor to do session run will not result in memory raise.Action2: new a tensor every time before seesion run will lead to memory rasie.Conclusion: NewTensor api lead to memory leakSuggestion: Release tensor obvisly instead of just set finializer```### Standalone code to reproduce the issue```shellpackage mainimport (	""flag""	""fmt""	""io/ioutil""	""log""	tf ""github.com/tensorflow/tensorflow/tensorflow/go"")func main() {	// An example for using the TensorFlow Go API for image recognition	// using a pre-trained inception model (http://arxiv.org/abs/1512.00567).	//	// Sample usage: <program> -dir=/tmp/modeldir -image=/path/to/some/jpeg	//	// The pre-trained model takes input in the form of a 4-dimensional	// tensor with shape [ BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WIDTH, 3 ],	// where:	// - BATCH_SIZE allows for inference of multiple images in one pass through the graph	// - IMAGE_HEIGHT is the height of the images on which the model was trained	// - IMAGE_WIDTH is the width of the images on which the model was trained	// - 3 is the (R, G, B) values of the pixel colors represented as a float.	//	// And produces as output a vector with shape [ NUM_LABELS ].	// output[i] is the probability that the input image was recognized as	// having the i-th label.	//	// A separate file contains a list of string labels corresponding to the	// integer indices of the output.	//	// This example:	// - Loads the serialized representation of the pre-trained model into a Graph	// - Creates a Session to execute operations on the Graph	// - Converts an image file to a Tensor to provide as input to a Session run	// - Executes the Session and prints out the label with the highest probability	//	// To convert an image file to a Tensor suitable for input to the Inception model,	// this example:	// - Constructs another TensorFlow graph to normalize the image into a	//   form suitable for the model (for example, resizing the image)	// - Creates and executes a Session to obtain a Tensor in this normalized form.	modeldir := flag.String(""dir"", """", ""Directory containing the trained model files. The directory will be created and the model downloaded into it if necessary"")	//imagefile := flag.String(""image"", """", ""Path of a JPEG-image to extract labels for"")	flag.Parse()	if *modeldir == """" {		flag.Usage()		return	}	// Load the serialized GraphDef from a file.	// modelfile, labelsfile, err := modelFiles(*modeldir)	//if err != nil {	//	log.Fatal(err)	//}	model, err := ioutil.ReadFile(*modeldir)	if err != nil {		log.Fatal(err)	}	// Construct an in-memory graph from the serialized form.	graph := tf.NewGraph()	if err := graph.Import(model, """"); err != nil {		log.Fatal(err)	}	// Create a session for inference over graph.	session, err := tf.NewSession(graph, nil)	if err != nil {		log.Fatal(err)	}	defer session.Close()	// Run inference on *imageFile.	// For multiple images, session.Run() can be called in a loop (and	// concurrently). Alternatively, images can be batched since the model	// accepts batches of image data as input.	//tensor, err := makeTensorFromImage(*imagefile)	//if err != nil {	//	log.Fatal(err)	//}	inputs := make([]string, 0)	for i := 0; i< 8;i++ {		inputs = append(inputs, ""濠电姷鏁告慨鐑藉极閹间礁纾婚柣鎰▕閻掕姤绻涢崱妯绘儎闁轰礁瀚伴弻娑㈩敃閻樻彃濮曢梺绋块閿曘儵濡甸崟顖氬唨妞ゆ劦婢€缁泛鈹戦埥鍡椾簼闁挎洏鍨藉璇测槈閵忕姈銊╂煥濠靛棙鍣规い顒€顦靛娲捶椤撶偛骞嬫繛瀛樼矎濞夋盯顢氶敐澶婇唶闁哄洨鍋ら崬鍫曟⒑缂佹ɑ鈷掓い顓炵墢閸犲﹤顓兼径瀣ф嫼?)	}	tensor"
57354,1,3461,11,0,0,NevilleMthw,0,"title:0 derived errors ignored. [Op:__inference_test_function_5397]  Function call stack: test_function -> test_function description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.4.1### Custom CodeYes### OS Platform and DistributionLinux Ubuntu### Mobile device_No response_### Python version3.8### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version11.4### GPU model and memoryNVIDIA GeForce RTX 2080 Ti### Current Behaviour?```shellI am facing an issue while trying to run my model, it seems to be an unknown error with no direct understanding of what is actually causing the error. Some answers talk about increasing batch size or adding a block of code before the model training. The training suddenely stops after a few epochs which is not supposed to happen. Instead, the model will just stop through a callback after its successful training.```### Standalone code to reproduce the issue```shellfeature_layers = [    Conv2D(32, (3, 3), input_shape=(400, 400, 3,), padding='same'),    LeakyReLU(alpha=0.1),    # model_4.add(RandomFlip(mode='horizontal_and_vertical', seed=None))    BatchNormalization(),    MaxPool2D(pool_size=(2, 2), padding='same'),    Conv2D(32, (3, 3), padding='same'),    LeakyReLU(alpha=0.1),    BatchNormalization(),    MaxPool2D(pool_size=(2, 2), padding='same'),    Conv2D(64, (3, 3), padding='same'),    LeakyReLU(alpha=0.1),    BatchNormalization(),    MaxPool2D(pool_size=(2, 2), padding='same'),    Conv2D(64, (3, 3), padding='same'),    LeakyReLU(alpha=0.1),    BatchNormalization(),    MaxPool2D(pool_size=(2, 2), padding='same'),    Conv2D(64, (3, 3), padding='same'),    LeakyReLU(alpha=0.1),    BatchNormalization(),    MaxPool2D(pool_size=(2, 2), padding='same'),    Conv2D(64, (3, 3), padding='same'),    LeakyReLU(alpha=0.1),    BatchNormalization(),    MaxPool2D(pool_size=(2, 2), padding='same'),    Conv2D(128, (3, 3), padding='same'),    LeakyReLU(alpha=0.1),    BatchNormalization(),    MaxPool2D(pool_size=(2, 2), padding='same'),    Conv2D(128, (3, 3), padding='same'),    LeakyReLU(alpha=0.1),    BatchNormalization(),    MaxPool2D(pool_size=(2, 2), padding='same'),    Conv2D(128, (3, 3), padding='same'),    LeakyReLU(alpha=0.1),    BatchNormalization(),    MaxPool2D(pool_size=(2, 2), padding='same'),    Conv2D(128, (3, 3), padding='same'),    LeakyReLU(alpha=0.1),    BatchNormalization(),    MaxPool2D(pool_size=(2, 2), padding='same'),    Conv2D(256, (3, 3), padding='same'),    LeakyReLU(alpha=0.1),    BatchNormalization(),    MaxPool2D(pool_size=(2, 2), padding='same'),    Conv2D(256, (3, 3), padding='same'),    LeakyReLU(alpha=0.1),    BatchNormalization(),    MaxPool2D(pool_size=(2, 2), padding='same'),    Conv2D(256, (3, 3), padding='same'),    LeakyReLU(alpha=0.1),    MaxPool2D(pool_size=(2, 2), padding='same'),]classification_layers = [    Flatten(),    Dropout(0.35),    Dense(512, kernel_regularizer=regularizers.l2(0.1)),    LeakyReLU(alpha=0.1),    Dropout(0.35),    Dense(4),]model = Sequential(feature_layers + classification_layers, name='Stenosis_model')model.compile(loss='mse', optimizer=Adam(learning_rate=0.0001), metrics=[tf.keras.metrics.MeanIoU(num_classes=4), tfr.keras.metrics.MeanAveragePrecisionMetric()])model.summary()keras.backend.clear_session()np.random.seed(15)tf.random.set_seed(15)model.fit(dataGenerator.flow(train_images, train_targets, batch_size=16),                            validation_data=(val_images, val_targets),                             epochs=150, callbacks=[es], verbose=1, shuffle=True)```I have also added the following code before running the model:```from tensorflow.compat.v1 import ConfigProtofrom tensorflow.compat.v1 import InteractiveSessionconfig = ConfigProto()config.gpu_options.allow_growth = Truesession = InteractiveSession(config=config)``````### Relevant log output```shellInvalidArgumentError                      Traceback (most recent call last)/home/lunet/conm/Desktop/Stenosis-Project/transfer_learning_model.ipynb Cell 19 in <cell line: 6>()      2 np.random.seed(15)      3 tf.random.set_seed(15)----> 6 model.fit(dataGenerator.flow(train_images, train_targets, batch_size=16),      7                             validation_data=(val_images, val_targets),       8                             epochs=150, callbacks=[es], verbose=1, shuffle=True)File ~/.conda/envs/stenosis/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1131, in Model.fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)   1117   self._fit_frame = tf_inspect.currentframe()   1118   self._eval_data_handler = data_adapter.DataHandler(   1119       x=val_x,   1120       y=val_y,   (...)   1129       model=self,   1130       steps_per_execution=self._steps_per_execution)-> 1131 val_logs = self.evaluate(   1132     x=val_x,   1133     y=val_y,   1134     sample_weight=val_sample_weight,   1135     batch_size=validation_batch_size or batch_size,   1136     steps=validation_steps,   1137     callbacks=callbacks,   1138     max_queue_size=max_queue_size,   1139     workers=workers,   1140     use_multiprocessing=use_multiprocessing,   1141     return_dict=True)   1142 val_logs = {'val_' + name: val for name, val in val_logs.items()}   1143 epoch_logs.update(val_logs)File ~/.conda/envs/stenosis/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1389, in Model.evaluate(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)   1387 with trace.Trace('test', step_num=step, _r=1):   1388   callbacks.on_test_batch_begin(step)-> 1389   tmp_logs = self.test_function(iterator)   1390   if data_handler.should_sync:   1391     context.async_wait()File ~/.conda/envs/stenosis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:828, in Function.__call__(self, *args, **kwds)    826 tracing_count = self.experimental_get_tracing_count()    827 with trace.Trace(self._name) as tm:--> 828   result = self._call(*args, **kwds)    829   compiler = ""xla"" if self._experimental_compile else ""nonXla""    830   new_tracing_count = self.experimental_get_tracing_count()File ~/.conda/envs/stenosis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:862, in Function._call(self, *args, **kwds)    859 self._lock.release()    860 # In this case we have not created variables on the first call. So we can    861 # run the first trace but we should fail if variables are created.--> 862 results = self._stateful_fn(*args, **kwds)    863 if self._created_variables:    864   raise ValueError(""Creating variables on a non-first call to a function""    865                    "" decorated with tf.function."")File ~/.conda/envs/stenosis/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2942, in Function.__call__(self, *args, **kwargs)   2939 with self._lock:   2940   (graph_function,   2941    filtered_flat_args) = self._maybe_define_function(args, kwargs)-> 2942 return graph_function._call_flat(   2943     filtered_flat_args, captured_inputs=graph_function.captured_inputs)File ~/.conda/envs/stenosis/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1918, in ConcreteFunction._call_flat(self, args, captured_inputs, cancellation_manager)   1914 possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)   1915 if (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE   1916     and executing_eagerly):   1917   # No tape is watching; skip to running the function.-> 1918   return self._build_call_outputs(self._inference_function.call(   1919       ctx, args, cancellation_manager=cancellation_manager))   1920 forward_backward = self._select_forward_and_backward_functions(   1921     args,   1922     possible_gradient_type,   1923     executing_eagerly)   1924 forward_function, args_with_tangents = forward_backward.forward()File ~/.conda/envs/stenosis/lib/python3.8/site-packages/tensorflow/python/eager/function.py:555, in _EagerDefinedFunction.call(self, ctx, args, cancellation_manager)...0 derived errors ignored. [Op:__inference_test_function_5397]Function call stack:test_function -> test_function```</details>
"
57222,1,2534,58,0,0,stridge-cruxml,0,"title:Saving and loading with an empty model layer description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versionv2.8.0-rc1-32-g3f878cff5b6 2.8.0### Custom CodeNo### OS Platform and DistributionUbuntu 20.04### Mobile device_No response_### Python version3.8.10### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellWhen trying to load a saved keras model containing an empty sequential model, an exception is thrown.```### Standalone code to reproduce the issue```shellimport tensorflow as tfimport numpy as npclass TestBlock(tf.keras.Model):  def __init__(self):    super().__init__()    self.shortcut = tf.keras.Sequential()    self.dense = tf.keras.layers.Dense(5)    self.relu = tf.keras.layers.ReLU()  def call(self, input_vector):    shortcut = self.shortcut(input_vector)    dense = self.dense(input_vector)    return self.relu(shortcut + dense)if __name__ == ""__main__"":  model = TestBlock()  model.build((1,5))  model.compile(loss=""categorical_crossentropy"")  checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(    filepath=""checkpoints"",    save_best_only=True,    mode=""auto"",  )  train_data = tf.data.Dataset.from_tensor_slices((np.ones((20,5)), np.zeros((20,5)))).batch(1)  model.fit(train_data,    epochs=1,    callbacks=[checkpoint_callback],  )  print(""Saving ..."")  model.save(""test_model"")  print(""Loading ..."")  loaded_model = tf.keras.models.load_model(""test_model"")```### Relevant log output```shell2022-08-19 12:17:00.131391: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2022-08-19 12:17:00.599398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6666 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:29:00.0, compute capability: 7.520/20 [==============================] - 0s 1ms/step - loss: 0.0000e+00ING:tensorflow:Can save best model only with val_loss available, skipping.Saving ...2022-08-19 12:17:01.354210: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.Loading ...Traceback (most recent call last):  File ""test_seq_model.py"", line 34, in <module>    loaded_model = tf.keras.models.load_model(""test_model"")  File ""/xxxxx/keras/utils/traceback_utils.py"", line 67, in error_handler    raise e.with_traceback(filtered_tb) from None  File ""/xxxxx/keras/saving/saved_model/load.py"", line 700, in _reconstruct_model    if config['layers'][0]['class_name'] == 'InputLayer':IndexError: list index out of range```</details>
"
57218,1,314,15,0,0,kcoul,0,"title:Can't build for iOS using CMake description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.9.x### Custom CodeNo### OS Platform and DistributionAny### Mobile deviceiOS### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellDuring linking, symbols are missing due to signpost_profiler.mm not being compiled when building for iOS using CMake. Patch will be provided.```### Standalone code to reproduce the issue```shellUse an iOS toolchain such as https://github.com/leetal/ios-cmake to build for iOS. Will fail due to missing symbols since signpost_profiler.mm is not compiled.```### Relevant log output_No response_</details>
"
57211,1,363,2,0,0,Bahar-BM,0,"title:openCL delegate issue with models that output results from their intermediate nodes description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.9.1, nightly version### Custom CodeNo### OS Platform and DistributionAndroid### Mobile devicetested on Snapdragon 888, 865, 855### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellThe openCL delegate generates all zero outputs in models that output results from their intermediate nodes. This issue does not happen with other delegates like XNNPACK.```### Standalone code to reproduce the issue```shellWe have implemented a small tool (with comprehensive documentation) to reproduce the mentioned issue. Here is the link to the repository:https://github.com/Bahar-BM/openCL_test```### Relevant log output_No response_</details>
"
57206,0,1075,1,0,1,masaton6,0,"title:analyzer_wrapper/model_analyzer.cc has typo causing C2001 error while compiling description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.9### Custom CodeNo### OS Platform and DistributionWindows 10### Mobile device_No response_### Python version3.10### Bazel version5.0.0### GCC/Compiler versionMSVC### CUDA/cuDNN version8.4.1### GPU model and memoryGeForce GTX 750 Ti### Current Behaviour?```shellWhile building, below error pop uptensorflow/lite/python/analyzer_wrapper/model_analyzer.cc(197): error C2001: newline in constantThis is caused by non ascii quotes in the line 197 and 198.```### Standalone code to reproduce the issue```shellThis is fixed by below patch. Please review.diff --git a/tensorflow/lite/python/analyzer_wrapper/model_analyzer.cc b/tensorflow/lite/python/analyzer_wrapper/model_analyzer.ccindex 370a4eff470..d4609faab33 100644--- a/tensorflow/lite/python/analyzer_wrapper/model_analyzer.cc+++ b/tensorflow/lite/python/analyzer_wrapper/model_analyzer.cc@@ -194,8 +194,8 @@ void dump_model_signature_defs(std::stringstream& out_stream,     return;   }   out_stream << kSectionSplitter;-  out_stream << ""Your TFLite model has 闂? << signatures->Length()-             << """"闂?signature_def(s).\n\n"""";+  out_stream << """"Your TFLite model has '"""" << signatures->Length()+             << """"' signature_def(s).\n\n"""";   for (int i = 0; i < signatures->Length(); ++i) {     auto* signature_def = signatures->Get(i);     out_stream << """"Signature#"""" << i << """" key: '""""```### Relevant log output_No response_</details>"
57174,1,3516,25,0,0,cheyennee,0,"title:tf.nn.embedding_lookup result on cpu inconsistent with gpu description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow VersionTF2.4### Custom CodeYes### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellThe result on CPU should be consistent with GPU.```### Standalone code to reproduce the issue```shellresults = dict()import tensorflow as tftry:  try:    with tf.device('/CPU'):      params_tensor = tf.random.uniform([2, 256], dtype=tf.float32)      params = tf.identity(params_tensor)      ids_tensor = tf.saturate_cast(tf.random.uniform([2], minval=-256, maxval=257, dtype=tf.int64), dtype=tf.int32)      ids = tf.identity(ids_tensor)      results[""res_cpu""] = tf.nn.embedding_lookup(params=params,ids=ids,)  except Exception as e:    results[""err_cpu""] = ""Error:""+str(e)  try:    with tf.device('/GPU:0'):      params = tf.identity(params_tensor)      params = tf.cast(params, tf.float32)      ids = tf.identity(ids_tensor)      ids = tf.cast(ids, tf.int32)      results[""res_gpu""] = tf.nn.embedding_lookup(params=params,ids=ids,)  except Exception as e:    results[""err_gpu""] = ""Error:""+str(e)except Exception as e:  results[""err""] = ""Error:""+str(e)print(results)'''{'err_cpu': 'Error:indices[0] = -35 is not in [0, 2) [Op:GatherV2]', 'res_gpu': <tf.Tensor: shape=(2, 256), dtype=float32, numpy=array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],      dtype=float32)>}'''CPU checks input, while GPU doesn't.```### Relevant log output_No response_</details>
"
57173,1,2481,25,0,0,cheyennee,0,"title:Inconsistent sparse_categorical_crossentropy on cpu vs gpu description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow VersionTF2.4### Custom CodeYes### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellthe result on CPU should be consistent with the result on GPU. However, I find the result on CPU is not equal to the result on GPU on some APIs.These APIs are:tf.keras.losses.sparse_categorical_crossentropytf.keras.metrics.SparseCategoricalCrossentropytf.losses.sparse_categorical_crossentropytf.metrics.SparseCategoricalCrossentropy```### Standalone code to reproduce the issue```shellresults = dict()import tensorflow as tftry:  try:    with tf.device('/CPU'):      arg_0_0 = -56      arg_0_1 = -47      arg_0 = [arg_0_0,arg_0_1,]      arg_1_0_0 = 0.05      arg_1_0_1 = 0.95      arg_1_0_2 = 0      arg_1_0 = [arg_1_0_0,arg_1_0_1,arg_1_0_2,]      arg_1_1_0 = 0.1      arg_1_1_1 = 0.8      arg_1_1_2 = 0.1      arg_1_1 = [arg_1_1_0,arg_1_1_1,arg_1_1_2,]      arg_1 = [arg_1_0,arg_1_1,]      results[""res_cpu""] = tf.keras.losses.sparse_categorical_crossentropy(arg_0,arg_1,)  except Exception as e:    results[""err_cpu""] = ""Error:""+str(e)  try:    with tf.device('/GPU:0'):      arg_0 = [arg_0_0,arg_0_1,]      arg_1_0 = [arg_1_0_0,arg_1_0_1,arg_1_0_2,]      arg_1_1 = [arg_1_1_0,arg_1_1_1,arg_1_1_2,]      arg_1 = [arg_1_0,arg_1_1,]      results[""res_gpu""] = tf.keras.losses.sparse_categorical_crossentropy(arg_0,arg_1,)  except Exception as e:    results[""err_gpu""] = ""Error:""+str(e)except Exception as e:  results[""err""] = ""Error:""+str(e)print(results)```The results is {'err_cpu': 'Error:Received a label value of -56 which is outside the valid range of [0, 3). Label values: -56 -47 [Op:SparseSoftmaxCrossEntropyWithLogits]', 'res_gpu': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([nan, nan], dtype=float32)>}.```results = dict()import tensorflow as tftry:  try:    with tf.device('/CPU'):      arg_0_0 = 28      arg_0_1 = -8      arg_0 = [arg_0_0,arg_0_1,]      arg_1_0_0 = -49.95      arg_1_0_1 = -3.05      arg_1_0_2 = False      arg_1_0 = [arg_1_0_0,arg_1_0_1,arg_1_0_2,]      arg_1_1_0 = -2.9      arg_1_1_1 = -44.2      arg_1_1_2 = -21.9      arg_1_1 = [arg_1_1_0,arg_1_1_1,arg_1_1_2,]      arg_1 = [arg_1_0,arg_1_1,]      results[""res_cpu""] = tf.losses.sparse_categorical_crossentropy(arg_0,arg_1,)  except Exception as e:    results[""err_cpu""] = ""Error:""+str(e)  try:    with tf.device('/GPU:0'):      arg_0 = [arg_0_0,arg_0_1,]      arg_1_0 = [arg_1_0_0,arg_1_0_1,arg_1_0_2,]      arg_1_1 = [arg_1_1_0,arg_1_1_1,arg_1_1_2,]      arg_1 = [arg_1_0,arg_1_1,]      results[""res_gpu""] = tf.losses.sparse_categorical_crossentropy(arg_0,arg_1,)  except Exception as e:    results[""err_gpu""] = ""Error:""+str(e)except Exception as e:  results[""err""] = ""Error:""+str(e)print(results)```The result is {'err_cpu': 'Error:Received a label value of -8 which is outside the valid range of [0, 3).  Label values: 28 -8 [Op:SparseSoftmaxCrossEntropyWithLogits]', 'res_gpu': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([nan, nan], dtype=float32)>}.CPU throws an error, while GPU runs.```### Relevant log output_No response_</details>
"
57172,1,1264,25,0,0,cheyennee,0,"title:Inconsistent tf.losses.mean_squared_error on cpu vs gpu description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow VersionTF2.4### Custom CodeYes### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellThe result on CPU should be equal to the result on GPU.```### Standalone code to reproduce the issue```shellresults = dict()import tensorflow as tftry:  try:    with tf.device('/CPU'):      arg_0_tensor = tf.saturate_cast(tf.random.uniform([2, 3], minval=-256, maxval=257, dtype=tf.int64), dtype=tf.int64)      arg_0 = tf.identity(arg_0_tensor)      arg_1_tensor = tf.random.uniform([2, 3], dtype=tf.float16)      arg_1 = tf.identity(arg_1_tensor)      results[""res_cpu""] = tf.losses.mean_squared_error(arg_0,arg_1,)  except Exception as e:    results[""err_cpu""] = ""Error:""+str(e)  try:    with tf.device('/GPU:0'):      arg_0 = tf.identity(arg_0_tensor)      arg_0 = tf.cast(arg_0, tf.int64)      arg_1 = tf.identity(arg_1_tensor)      arg_1 = tf.cast(arg_1, tf.float16)      results[""res_gpu""] = tf.losses.mean_squared_error(arg_0,arg_1,)  except Exception as e:    results[""err_gpu""] = ""Error:""+str(e)except Exception as e:  results[""err""] = ""Error:""+str(e)print(results)'''{'res_cpu': <tf.Tensor: shape=(2,), dtype=float16, numpy=array([inf, inf], dtype=float16)>, 'res_gpu': <tf.Tensor: shape=(2,), dtype=float16, numpy=array([35650., 25060.], dtype=float16)>}'''The result on CPU outputs inf array, while the result on GPU outputs float array.```### Relevant log output_No response_</details>
"
57170,0,1286,25,0,0,cheyennee,0,"title:Inconsistent tf.gather_nd behavior on cpu vs gpu description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow VersionTF2.4### Custom CodeYes### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellThe result on cpu should be consistent with the result on gpu.```### Standalone code to reproduce the issue```shellresults = dict()import tensorflow as tftry:  try:    with tf.device('/CPU'):      arg_0_tensor = tf.random.uniform([1, 8, 1, 1], dtype=tf.float32)      arg_0 = tf.identity(arg_0_tensor)      arg_1_tensor = tf.saturate_cast(tf.random.uniform([8, 2], minval=-256, maxval=257, dtype=tf.int64), dtype=tf.int64)      arg_1 = tf.identity(arg_1_tensor)      results[""res_cpu""] = tf.gather_nd(arg_0,arg_1,)  except Exception as e:    results[""err_cpu""] = ""Error:""+str(e)  try:    with tf.device('/GPU:0'):      arg_0 = tf.identity(arg_0_tensor)      arg_0 = tf.cast(arg_0, tf.float32)      arg_1 = tf.identity(arg_1_tensor)      arg_1 = tf.cast(arg_1, tf.int64)      results[""res_gpu""] = tf.gather_nd(arg_0,arg_1,)  except Exception as e:    results[""err_gpu""] = ""Error:""+str(e)except Exception as e:  results[""err""] = ""Error:""+str(e)print(results)'''{'err_cpu': 'Error:indices[7] = [-232, -191] does not index into param shape [1,8,1,1] [Op:GatherNd]', 'res_gpu': <tf.Tensor: shape=(8, 1, 1), dtype=float32, numpy=array([[[0.]],       [[0.]],       [[0.]],       [[0.]],       [[0.]],       [[0.]],       [[0.]],       [[0.]]], dtype=float32)>}'''```### Relevant log output_No response_</details>
"
57167,1,1268,25,0,0,cheyennee,0,"title:tf.nn.conv2d error message is inconsistent with documentation description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow VersionTF2.4### Custom CodeYes### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shell'SAME' or 'VALID' can be set to parameter 'padding' of tf.nn.conv2d  in documentation. However, the error message of tf.nn.conv2d shows that 'EXPLICIT' is also an allowd value of 'padding'. Actually, when 'padding' takes 'EXPLICIT', code doesn't work. So the error message is inconsistent with the documentation.```### Standalone code to reproduce the issue```shellimport tensorflow as tfresults={}try:  arg_0 = tf.random.uniform([1, 8, 8, 1], dtype=tf.float32)  arg_1 = tf.random.uniform([1, 1, 1, 3], dtype=tf.float32)  strides = 1  padding = ""valid""  results[""res""] = tf.nn.conv2d(arg_0,arg_1,strides=strides,padding=padding,)except Exception as e:  results[""err""] = ""Error:""+str(e)print(results)'''{'err': 'Error:Value for attr \'padding\' of ""valid"" is not in the list of allowed values: ""SAME"", ""VALID"", ""EXPLICIT""\n\t; NodeDef: {{node Conv2D}}; Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[""SAME"", ""VALID"", ""EXPLICIT""]; attr=explicit_paddings:list(int),default=[]; attr=data_format:string,default=""NHWC"",allowed=[""NHWC"", ""NCHW""]; attr=dilations:list(int),default=[1, 1, 1, 1]> [Op:Conv2D]'}'''```### Relevant log output_No response_</details>
"
57155,1,0,0,0,0,bluetail14,0,"title:RuntimeError: Data adapters should be mutually exclusive for handling inputs in Streamlit description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.9.1### Custom CodeYes### OS Platform and DistributionWindows 10 21H2### Mobile device_No response_### Python version3.9.12### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versionCuda 11.2.2/ cuDNN v8.1.0.77### GPU model and memoryGeForce NVidia RTX 3060  16GB RAM### Current Behaviour?```shellI am trying to run a classifier with a keras CNN model within Streamlit and it gives me the following error.RuntimeError: Data adapters should be mutually exclusive for handling inputs. Found multiple adapters [<class 'keras.engine.data_adapter.GenericArrayLikeDataAdapter'>, <class 'keras.engine.data_adapter.GeneratorDataAdapter'>] to handle input: <class 'streamlit.delta_generator.DeltaGenerator'>, <class 'NoneType'>I have uploaded cnn_ltsm.h5, image20.jpg and image21.jpg to my github, if someone wants to reproduce my problem. GitHub - bluetail14/Thermal_data_images_project```### Standalone code to reproduce the issue```shell# My code runs fine in Jupyter notebook.import osimport numpy as npfrom PIL import Image import tensorflow as tffrom tensorflow.keras.models import load_modeldef load_image(image):    image = image.resize((224,224))    img_array = np.array(image)/255 # a normalised 2D array                    img_array = img_array.reshape(-1, 224, 224, 3)   # to shape as (1, 224, 224, 3)    return img_arrayim_path = 'C:\\Users\\..\\oneman\\image21.jpg'img = load_image(Image.open(im_path))model_cnn_ltsm = load_model(""C:/Users/.../Saved_models/cnn_ltsm_model.h5"")model_cnn_ltsm_ = tf.keras.models.Model(model_cnn_ltsm.inputs, model_cnn_ltsm.outputs)pred_label = model_cnn_ltsm_.predict(img)[0] if pred_label>0.5:    print('Human is detected')else:    print(""No human is detected: "")   #Output: 'Human is detected' (as expected)# However, there is a problem with the same in Streamlit.import streamlit as stimport pandas as pdimport numpy as npfrom numpy import vstackfrom PIL import Image import tensorflow as tffrom tensorflow.keras.models import load_modelst.title(""Binary Human Detection Web App"")# loading imagesdef load_image(image):    image = image.resize((224,224))    img_array = np.array(image)/255 # a normalised 2D array                    img_array = img_array.reshape(-1, 224, 224, 3)   # to shape as (1, 224, 224, 3)    return img_arrayuploaded_file = st.sidebar.file_uploader("" "",type=['jpg', 'jpeg'])    if uploaded_file is not None:        u_img = load_image(Image.open(uploaded_file))    img = st.image(u_img, 'Uploaded Image', use_column_width=True)st.sidebar.write('\n')    if st.sidebar.button(""Click Here to Predict""):        if uploaded_file is None:                st.sidebar.write(""Please upload an Image to Classify"")    else:               model_cnn = load_model(""C:/Users/.../Saved_models/cnn_ltsm_model.h5"")        model_cnn_ltsm_ = tf.keras.models.Model(model_cnn_ltsm.inputs, model_cnn_ltsm.outputs)        pred_label = model_cnn_ltsm_.predict(img)[0]         st.sidebar.header(""CNN results: "")            #st.write('Human is detected') if pred_label>0.5 else  st.write('No human is detected')            if pred_label>0.5:            st.write('Human is detected')        else:            st.write('No human is detected')```### Relevant log output```shellRuntimeError: Data adapters should be mutually exclusive for handling inputs. Found multiple adapters [<class 'keras.engine.data_adapter.GenericArrayLikeDataAdapter'>, <class 'keras.engine.data_adapter.GeneratorDataAdapter'>] to handle input: <class 'streamlit.delta_generator.DeltaGenerator'>, <class 'NoneType'>Traceback:File ""C:\Users\maria\anaconda3\envs\tfenv\lib\site-packages\streamlit\scriptrunner\script_runner.py"", line 557, in _run_script    exec(code, module.__dict__)File ""app1.py"", line 46, in <module>    pred_label = model_cnn_.predict(img)[0]File ""C:\Users\maria\anaconda3\envs\tfenv\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler    raise e.with_traceback(filtered_tb) from NoneFile ""C:\Users\maria\anaconda3\envs\tfenv\lib\site-packages\keras\engine\data_adapter.py"", line 990, in select_data_adapter    raise RuntimeError(```</details>
"
57133,0,368,15,0,0,kcoul,0,"title:[Win] fft2d tries to link m.lib description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.9.1### Custom CodeNo### OS Platform and DistributionWindows 10### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellCannot compile tensorflow lite on Windows using CMake build system. Fails at linker stage while trying to link m.lib - the math library is only suitable for linking on macOS and Linux.Will provide a patch for this issue.```### Standalone code to reproduce the issue```shellBuild on any 2.9.x branch on Windows using CMake - build will fail while trying to link non-existent m.lib at fft2d linking stage.```### Relevant log output_No response_</details>
"
57128,1,0,0,0,0,Hrayo712,0,"title:Incorrect output shape when using dilations and strides > 1 with depthwise conv description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.4 - v2.4.0-rc4-71-g582c8d236cb 2.4.0### Custom CodeNo### OS Platform and DistributionLinux Ubuntu 18.04.6 LTS### Mobile device_No response_### Python version3.8.0### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version11.0### GPU model and memory_No response_### Current Behaviour?I am building a model which uses `strides` > 1 and `dilation_rate` > 1 in some DepthwiseConv2D layers.However, with certain values, it seems like the output shape is incorrect.Note: On the documentation, I see that using dilation_rate > 1 with strides > 1 is invalid, however, no runtime error is raised, in contrast with Conv2D, where the error is raised.### Standalone code to reproduce the issue```shellimport tensorflow as tfinput = tf.keras.layers.Input(            shape=(136, 136, 72),            batch_size=1,        )x = tf.keras.layers.DepthwiseConv2D(      kernel_size=3,      strides=2,      activation=None,      use_bias=False,      padding='valid',      dilation_rate=(3, 3))(input)print(f""output shape: {x.shape}"")```The print statement shows: ```output shape: (1, 64, 64, 72)```Interestingly, when doing the following:```conv = tf.keras.layers.DepthwiseConv2D(      kernel_size=3,      strides=2,      activation=None,      use_bias=False,      padding='valid',      dilation_rate=(3, 3))conv.compute_output_shape((1, 136, 136, 72))```The output reported is: ```(1, 65, 65, 72)```Interestingly, when using dilation_rate =(2, 2) instead, the outputs reported via these two examples, match.Following the equation in: https://www.tensorflow.org/api_docs/python/tf/nn#notes_on_padding_2for valid padding, the output shape is calculated as:```output = ceil((in_height - filter_height + 1) / stride_height)```where filter_height when using dilations is computed as:```filter_height: dilation*(filter_height - 1) + 1```Following these equations, the output shape as reported by `compute_output_shape` is correct. However, it doesnt match the shape reported when actually forwarding through the layer.I also tried implementing this in Pytorch, and output shape reported matches the one reported by `compute_output_shape`This might be related to this issue: https://github.com/keras-team/keras/issues/16092Is this a bug ? I need to know how the output shape is computed, as I need to be able to calculate a certain amount of padding such to ensure the downsampling caused by the stride results in a certain shape.Thanks in advance for your support!### Relevant log output_No response_</details>
"
57103,1,0,0,0,0,TamirRozenfeld,0,"title:Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf2.9.1### Custom CodeNo### OS Platform and Distributionwindows 10### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellShould i do something with that ?Ofc i have GPU in my Computer```### Standalone code to reproduce the issue```shell'''import tensorflow as tfprint(""TensorFlow version:"", tf.__version__)'''```### Relevant log output```shellC:\Users\tamir\PycharmProjects\pythonProject\venv\Scripts\python.exe C:/Users/tamir/PycharmProjects/pythonProject/module_one.py2022-08-11 14:21:29.345331: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found2022-08-11 14:21:29.345439: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.TensorFlow version: 2.9.1Process finished with exit code 0```</details>
"
57083,0,5135,177,0,0,maxhgerlach,0,"title:Header tensorflow/tsl/platform/stack_frame.h not included with tf-nightly pip package description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versionv1.12.1-79472-g4aaefec710e 2.11.0-dev20220810### Custom CodeYes### OS Platform and Distribution_No response_### Mobile device_No response_### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?Horovod does not build with `tf-nightly` anymore, see https://github.com/horovod/horovod/issues/3641The code includes `tensorflow/core/framework/op.h` at https://github.com/horovod/horovod/blob/master/horovod/tensorflow/mpi_ops.cc#L34. Header files in `tensorflow/core` are included with the pip package. However, this ultimately includes `tensorflow/tsl/platform/stack_frame.h`, which does not come with the package.This appears to have changed with https://github.com/tensorflow/tensorflow/commit/399e4071c471f6dc47bf245f8aeb8ab0c0374fce, cc @joker-eph### Standalone code to reproduce the issue```shell#include ""tensorflow/core/framework/op.h""```### Relevant log output```shell2022-08-10T16:38:11.6788601Z #44 157.3   [ 66%] Building CXX object horovod/tensorflow/CMakeFiles/tensorflow.dir/mpi_ops.cc.o2022-08-10T16:38:11.6792823Z #44 157.3   cd /tmp/pip-req-build-ggufvm1f/build/temp.linux-x86_64-3.8/RelWithDebInfo/horovod/tensorflow && /usr/bin/c++  -DEIGEN_MPL2_ONLY=1 -DHAVE_GLOO=1 -DHAVE_MPI=1 -DTENSORFLOW_VERSION=9999999999 -Dtensorflow_EXPORTS -I/tmp/pip-req-build-ggufvm1f/third_party/HTTPRequest/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/assert/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/config/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/core/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/detail/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/iterator/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/lockfree/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/mpl/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/parameter/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/predef/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/preprocessor/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/static_assert/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/type_traits/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/utility/include -I/tmp/pip-req-build-ggufvm1f/third_party/lbfgs/include -I/tmp/pip-req-build-ggufvm1f/third_party/gloo -I/tmp/pip-req-build-ggufvm1f/third_party/flatbuffers/include -isystem /usr/local/lib/python3.8/dist-packages/tensorflow/include  -I/usr/local/lib/python3.8/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=1 -DEIGEN_MAX_ALIGN_BYTES=64  -pthread -fPIC -Wall -ftree-vectorize -mf16c -mavx -mfma -O3 -g -DNDEBUG -fPIC   -std=c++17 -o CMakeFiles/tensorflow.dir/mpi_ops.cc.o -c /tmp/pip-req-build-ggufvm1f/horovod/tensorflow/mpi_ops.cc2022-08-10T16:38:11.6795960Z #44 157.4   In file included from /tmp/pip-req-build-ggufvm1f/horovod/common/gloo/../mpi/mpi_context.h:25,2022-08-10T16:38:11.8295689Z #44 157.4                    from /tmp/pip-req-build-ggufvm1f/horovod/common/gloo/gloo_context.h:25,2022-08-10T16:38:11.8296650Z #44 157.4                    from /tmp/pip-req-build-ggufvm1f/horovod/common/gloo/gloo_controller.h:19,2022-08-10T16:38:11.8297220Z #44 157.4                    from /tmp/pip-req-build-ggufvm1f/horovod/common/gloo/gloo_controller.cc:16:2022-08-10T16:38:11.8298063Z #44 157.4   /tmp/pip-req-build-ggufvm1f/horovod/common/gloo/../mpi/../half.h: In function 闂傚倸鍊搁崐鐑芥嚄閸洖纾婚柕濞炬櫅绾惧潡鏌熺紒妯虹瑨缂佷胶骞坕d horovod::common::HalfBits2Float(const short unsigned int*, float*)闂?2022-08-10T16:38:11.8298871Z #44 157.4   /tmp/pip-req-build-ggufvm1f/horovod/common/gloo/../mpi/../half.h:76:11: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]2022-08-10T16:38:11.8299357Z #44 157.4      76 |   *res = *reinterpret_cast<float const*>(&f);2022-08-10T16:38:11.8299655Z #44 157.4         |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~2022-08-10T16:38:13.5852575Z #44 159.3   In file included from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/platform/status.h:32,2022-08-10T16:38:13.7360889Z #44 159.3                    from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/lib/core/status.h:19,2022-08-10T16:38:13.7361596Z #44 159.3                    from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/framework/resource_base.h:20,2022-08-10T16:38:13.7362253Z #44 159.3                    from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/framework/resource_handle.h:21,2022-08-10T16:38:13.7362956Z #44 159.3                    from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:32,2022-08-10T16:38:13.7363571Z #44 159.3                    from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/framework/op_def_builder.h:28,2022-08-10T16:38:13.7364233Z #44 159.3                    from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/framework/full_type_inference_util.h:23,2022-08-10T16:38:13.7364870Z #44 159.3                    from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/framework/op.h:24,2022-08-10T16:38:13.7365433Z #44 159.3                    from /tmp/pip-req-build-ggufvm1f/horovod/tensorflow/mpi_ops.cc:34:2022-08-10T16:38:13.7366144Z #44 159.3   /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/platform/stack_frame.h:19:10: fatal error: tensorflow/tsl/platform/stack_frame.h: No such file or directory2022-08-10T16:38:13.7366631Z #44 159.3      19 | #include ""tensorflow/tsl/platform/stack_frame.h""2022-08-10T16:38:13.7366941Z #44 159.3         |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~2022-08-10T16:38:13.7367206Z #44 159.3   compilation terminated.2022-08-10T16:38:13.7367601Z #44 159.3   make[2]: *** [horovod/tensorflow/CMakeFiles/tensorflow.dir/build.make:453: horovod/tensorflow/CMakeFiles/tensorflow.dir/mpi_ops.cc.o] Error 1```</details>
"
57057,1,2265,46,0,0,cgebbe,0,"title:GlobalAveragePooling2D-layer in mixed-precision mode and using CPU produces incorrect results description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versionv2.8.0-rc1-32-g3f878cff5b6 2.8.0### Custom CodeYes### OS Platform and DistributionUbuntu 20.04.4 LTS### Mobile device_No response_### Python version3.8### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellGiven a certain input, when applying a GlobalAveragePooling2D-layer in mixed-precision mode and using CPU, then the output is bogus (zeros and nans). When using GPU, all works fine.```### Standalone code to reproduce the issue```shellimport numpy as npimport tensorflow as tffrom tensorflow.keras import mixed_precision# download the file from https://github.com/cgebbe/random_files/blob/main/x.npyx = np.load(""x.npy"")assert x.shape == (1, 256, 256, 32)correct_result = np.array(    [        [            0.1556,            0.1864,            0.4644,            -0.2502,            0.4453,            0.3118,            0.835,            0.4453,            0.26,            0.452,            -0.0783,            -0.10693,            0.1621,            0.1736,            0.9214,            0.0373,            0.2294,            0.2252,            0.5415,            0.1819,            0.2678,            0.3835,            0.0973,            0.815,            -0.013306,            -0.1713,            0.1719,            0.577,            0.3047,            0.3525,            0.315,            1.783,        ]    ],    dtype=np.float16,)wrong_result = np.array(    [        [            0.0,            0.0,            0.0,            -0.0,            0.0,            0.0,            0.0,            0.0,            0.0,            0.0,            -0.0,            -0.0,            0.0,            0.0,            0.0,            0.0,            0.0,            0.0,            0.0,            0.0,            0.0,            0.0,            0.0,            0.0,            -0.0,            -0.0,            0.0,            0.0,            0.0,            0.0,            0.0,            np.nan,        ]    ],    dtype=np.float16,)policy = mixed_precision.Policy(""mixed_float16"")mixed_precision.set_global_policy(policy)new_layer = tf.keras.layers.GlobalAveragePooling2D()with tf.device(""/gpu""):    y1 = new_layer(tf.convert_to_tensor(x))    np.testing.assert_allclose(y1, correct_result)with tf.device(""/cpu""):    y2 = new_layer(tf.convert_to_tensor(x))    np.testing.assert_allclose(y2, wrong_result)```### Relevant log output_No response_</details>
"
57033,0,0,0,0,0,code4lala,0,"title:multiple definition of `tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()' description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontensorflow-2.9.1### Custom CodeNo### OS Platform and DistributionUbuntu 22.04 LTS (GNU/Linux 5.15.0-43-generic x86_64)### Mobile deviceandroid arm64### Python versionPython 3.10.4### Bazel versionbazel 5.0.0### GCC/Compiler versionndk 21.4.7075529### CUDA/cuDNN versionNA### GPU model and memoryNA### Current Behaviour?I followed the official document to build tensorflow-lite with select ops from source, ducument url: https://www.tensorflow.org/lite/guide/ops_select?hl=zh-cn#cThe document tell me that:> 婵犵數濮烽弫鎼佸磻濞戙埄鏁嬫い鎾跺枑閸欏繘鎮楅崹顐ゆ憙濠殿喗濞婇弻銈吤圭€ｎ偅鐝栫紓浣哄У濡啴寮诲☉妯锋婵☆垰鍚嬮幉鐓庮渻閵堝啫鍔氶柣妤佹崌瀵寮撮敍鍕澑闁诲函缍嗘禍鐐寸妤ｅ啯鈷戝ù鍏肩懅缁夘喖霉濠婂啰鍩ｇ€殿喖顭烽弫鎾绘偐閼碱剨绱叉繝鐢靛Т閻忔岸宕濋弽顐ょ闁割偅绻嶅〒濠氭煏閸繂鏆欓柣蹇婃櫇缁辨帗寰勭仦鐐瘎缂備緡鍠楅悷锕€顕ラ崟顖氱疀闁割煈鍋呭▍宥夋⒒娴ｇ鏆遍柣蹇撶墦瀵彃鈽夊▎鎴锤闂佽鍨庡畝鈧崬鐢告煟閻樼儤銆冮悹鈧敃鍌氱？闊洦鏌ｆ禍婊堢叓閸パ屽剰闁告梹宀搁弻宥夋寠婢舵ɑ鈻堝Δ鐘靛仦閿曘垽銆佸☉姗嗘僵妞ゆ劑鍩勫Λ婊冣攽閻樺灚鏆╅柛瀣仱瀹曞綊宕奸弴鐐殿啇闂佸啿鎼崐濠氥€呴幓鎺濇富閻庯綆浜滈銏°亜? 闂傚倸鍊搁崐椋庢濮橆剦鐒介柤濮愬€栫€氬鏌ｉ弮鍌氬付缂佲偓婢舵劕绠规繛锝庡墮婵″ジ鏌?bazel build 闂傚倸鍊搁崐椋庣矆娓氣偓楠炲鏁嶉崟顓犵厯闂佸湱鍎ら〃鍛村垂閸屾稓绡€闂傚牊渚楅崕蹇曠磼閻欌偓閸ｏ綁寮婚弴銏犻唶婵犻潧娲ょ粣娑氱磼閹冪稏缂侇喗鐟╁濠氬灳閹颁礁鎮戦柟鑲╄ˉ閳ь剚鍓氬鐐節?TensorFlow Lite 闂傚倸鍊搁崐椋庣矆娓氣偓楠炴牠顢曢敃鈧悿顕€鏌熼幆鐗堫棄闁哄嫨鍎甸弻鈥愁吋鎼粹€崇闂備浇锟ラ崐鏇㈠煘閹达附鍋愰柛娆忣槹閹瑧绱撴担鎻掍壕?--define=with_select_tf_ops=true 缂傚倸鍊搁崐鎼佸磹閹间礁纾归柟闂寸绾惧湱鎲搁悧鍫濈瑲闁稿顑夐弻锝夊箛椤旂厧濡洪梺绋块閿曪箓骞夐幖浣告婵炲棙鍎冲▓銊︾箾鐎电孝妞ゆ垵鎳橀幃娆愮節閸愶缚绨婚棅顐㈡处閹告儳鐡梻浣芥〃缁€浣虹礊婵犲偆娼? 闂傚倸鍊搁崐椋庢濮橆剦鐒介柤濮愬€栫€氬鏌ｉ弮鍌氬付缂佲偓婢舵劕绠归弶鍫濆⒔缁嬬粯銇勯锝嗙闁靛洤瀚粻娑㈠箻閹碱厽锛侀梺璇插绾板秴鐣濋幖浣歌摕闁绘棁銆€閸嬫挸鈽夊▎妯煎姺闂佹椿鍘奸惌鍌炲箖瑜版帒绠涢柕濠忛檮閻濇岸姊虹拠鈥虫灓妞ゃ垹锕棟闁规儼濮ら悡鐔镐繆閵堝嫮鍔嶆い銉ョ墦閺岀喖鎼归銈嗗櫚濡ょ姷鍋涢鍛粹€﹂崸妤€顫呴柣妯挎珪瀹曟娊姊鸿ぐ鎺濇濠电偐鍋撳Δ鐘靛仦閹瑰洭鐛幒鎴旀斀闁搞儜灞绢唵 TensorFlow 闂傚倸鍊风粈渚€骞栭位鍥敃閿曗偓閻ょ偓绻濇繝鍌滃缂佲偓婢跺绻嗛柕鍫濇噺閸ｆ椽鏌涚€Ｑ勬珚闁哄被鍊栭幈銊╁箛椤戣棄浜炬俊銈呭暟娑撳秹鏌￠崘銊у闁绘挶鍎甸弻锟犲炊椤垶鐣舵繛瀛樼矊婢х晫妲愰幘瀛樺闂傚牊绋戦～鍛繆閻愬瓨缍戦柟鑺ョ矌閸掓帡顢橀姀鈥充簻闁诲繐绻嬮悞锕傚棘閳ь剙鈹戦悩鍨毄濠殿喖顕埀顒佽壘閸㈣尙鍙?tensorflow/lite/delegates/flex:delegate闂傚倸鍊搁崐椋庢濮橆剦鐒界憸宥堢亱闂佸搫鍟悧濠囧磿?tried the first, then I got a successful result but also a tensorflow lite shared library which didn't contain select ops, so I tried the second.I wrote the following build script to cross-compile tensorflow 2.9.1``` sh#!/usr/bin/env bashset -evxtf=tensorflow-2.9.1if [ -d ""$tf"" ]; then    rm -rf $tffitar -xf tensorflow-2.9.1.tar.gzpushd tensorflow-2.9.1    cat > .tf_configure.bazelrc << EOFbuild --action_env ANDROID_NDK_HOME=""$ANDROID_NDK_HOME""build --action_env ANDROID_NDK_API_LEVEL=""33""build --action_env ANDROID_BUILD_TOOLS_VERSION=""33.0.0""build --action_env ANDROID_SDK_API_LEVEL=""33""build --action_env ANDROID_SDK_HOME=""$ANDROID_SDK_HOME""EOF    cat .tf_configure.bazelrc    sed -i '1215i ""//tensorflow/lite/delegates/flex:delegate"",' tensorflow/lite/BUILD    bazel \        build \        -c opt \        --fat_apk_cpu=arm64-v8a \        --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \        --config=android_arm64 \        --config=monolithic \        --verbose_failures \        //tensorflow/lite:libtensorflowlite.sopopd```and it produces the following failing result:```# Configuration: 5b254af6e42f0099eacf2893d40a00ee4bd5ca890db4dfcdc666730641843978# Execution platform: @local_execution_config_platform//:platformbazel-out/arm64-v8a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops_all_linked.pic.lo(register.pic.o): In function `~__base':/proc/self/cwd/tensorflow/lite/kernels/register.cc:36: multiple definition of `tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'bazel-out/arm64-v8a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops.pic.a(register.pic.o):/proc/self/cwd/tensorflow/lite/kernels/register.cc:36: first defined herebazel-out/arm64-v8a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops_all_linked.pic.lo(register.pic.o): In function `~__base':/proc/self/cwd/tensorflow/lite/kernels/register.cc:36: multiple definition of `tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'bazel-out/arm64-v8a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops.pic.a(register.pic.o):/proc/self/cwd/tensorflow/lite/kernels/register.cc:36: first defined hereclang: error: linker command failed with exit code 1 (use -v to see invocation)Target //tensorflow/lite:libtensorflowlite.so failed to buildINFO: Elapsed time: 106.618s, Critical Path: 39.19sINFO: 4 processes: 2 internal, 2 local.FAILED: Build did NOT complete successfully```Then I took a look at tensorflow/lite/BUILD and found a workaround solution which can solve the double link error: remove the `//tensorflow/lite/kernels:builtin_ops_all_linked` dependency in the deps of `tflite_cc_shared_object`.So the following building script works:``` sh#!/usr/bin/env bashset -evxtf=tensorflow-2.9.1if [ -d ""$tf"" ]; then    rm -rf $tffitar -xf tensorflow-2.9.1.tar.gzpushd tensorflow-2.9.1    cat > .tf_configure.bazelrc << EOFbuild --action_env ANDROID_NDK_HOME=""$ANDROID_NDK_HOME""build --action_env ANDROID_NDK_API_LEVEL=""33""build --action_env ANDROID_BUILD_TOOLS_VERSION=""33.0.0""build --action_env ANDROID_SDK_API_LEVEL=""33""build --action_env ANDROID_SDK_HOME=""$ANDROID_SDK_HOME""EOF    cat .tf_configure.bazelrc    sed -i '1215d' tensorflow/lite/BUILD    sed -i '1215i ""//tensorflow/lite/delegates/flex:delegate"",' tensorflow/lite/BUILD    bazel \        build \        -c opt \        --fat_apk_cpu=arm64-v8a \        --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \        --config=android_arm64 \        --config=monolithic \        --verbose_failures \        //tensorflow/lite:libtensorflowlite.sopopd```### Standalone code to reproduce the issue```shellsee the above Current Behaviour section```### Relevant log output```shellsee the above Current Behaviour section```</details>
"
57029,1,425,0,0,0,fuzzyswan,0,"title:`tf.bitwise.right_shift` have different behaviors in cpu and gpu description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.9.1### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?When `y` is negative for `tf.bitwise.right_shift`, according to the [documentation](https://www.tensorflow.org/api_docs/python/tf/bitwise/right_shift) ""the result is implementation defined"". Currently in the code example, when `y=-1`, the results on CPU and GPU are not the same, I wonder if this is a bug and the implementation on cpu and gpu should be the same?### Standalone code to reproduce the issue```shellimport tensorflow as tfwith tf.device(""gpu""):  X = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.int32)  Z = tf.bitwise.right_shift(X, -1)  print(Z)with tf.device(""cpu""):  X = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.int32)  Z = tf.bitwise.right_shift(X, -1)  print(Z)```### Relevant log output```shelltf.Tensor([[0 0 0] [0 0 0]], shape=(2, 3), dtype=int32)tf.Tensor([[1 2 3] [4 5 6]], shape=(2, 3), dtype=int32)```</details>
"
57026,0,423,8,0,1,tc-wolf,0,"title:Reducing Ops in Hexagon Delegate Use Wrong Axis Index When Input Tensor <4D description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.9.1### Custom CodeNo### OS Platform and DistributionUbuntu 20.04 LTS### Mobile device_No response_### Python version3.10### Bazel version5.0.0### GCC/Compiler version9.4.0### CUDA/cuDNN versionN/A### GPU model and memoryN/A### Current Behaviour?If there is a mean-reducing op that is executed by the Hexagon Delegate, the axis indices are unchanged from the original inputs.  This causes a problem if the tensor being reduced is of rank < 4.Because Qualcomm's nnlib needs 4D input tensors, the TF code [adds dummy dimensions of size 1](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/builders/op_builder.h#L177-L184) to the shape.  But since the reduction axis is unchanged, the nnlib implementation thinks the wrong axis is [being reduced over](https://source.codeaurora.org/quic/hexagon_nn/nnlib/tree/hexagon/ops/src/op_reducing_mean.c#n268) and so fails a sanity check on the shape of the output tensor.I.e. on CPU the (correct) behavior is:Input tensor shape -> [1, 48, 768]Axis = 2Output tensor shape -> [1, 48, 1]but in the hexagon delegate we pad the dimensions to make it 4D so we get:Input tensor shape -> [1, 1, 48, 768]Axis = 2Output tensor shape -> [1, 1, 1, 768]I've started on a fix in https://github.com/tensorflow/tensorflow/pull/57021 but I need some help with it - this approach creates a new tensor from the original axes tensor, increments the value by the number of dummy dimensions, and then tries to add this new tensor instead of the original.This runs (as long as the axes_tensor is a list, not a scalar), but it leaks memory, and there is an error relating to trying to add a duplicate tensor (`ERROR: Trying to add duplicate tensor without overwrite, tflite_tensor_id 1, hexagon_node_id 16, hexagon_node_output_id 0`).  I can't modify the original axes tensor directly because it has a read-only allocation type, and trying causes a segmentation fault.### Standalone code to reproduce the issue1. Create dummy tflite model using https://colab.research.google.com/drive/1cyjF6dx31T-NxmPgFwOdAXPQcUYD7685?usp=sharing2. Run with `./benchmark_model_plus_flex --graph=dummy_model_int8.tflite --use_hexagon=true` on a device that can use the Hexagon delegate.### Relevant log outputThe dummy model in the final layers reduces from a tensor of shape (32, 12) to (32, 1) - but because the reduction axis is 1, the downstream code is expected to reduce over (1, 1, 32, 12) for an output of (1, 1, 32, 12).```shell----------------Timestamp: Sat Aug  6 03:20:01 2022Loghexagon/ops/src/op_reducing_mean.c:268:Output tensor is of size 32, but expected output tensor of size 384hexagon/src/execute.c:167:execute() failed on node id=f err=-1hexagon/src/interface.c:1297:fail in execute_inner()----------------ERROR: Failed: Failed to execute graph..ERROR: Node number 4 (TfLiteHexagonDelegate) failed to invoke.```</details>
"
57015,1,2077,84,0,0,FragrantRookie,0,"title:When I set unroll of LSTM to True, conversion problems occurs. description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.9### Custom CodeYes### OS Platform and Distributionwindows10### Mobile device_No response_### Python version3.9### Current Behaviour?```shellMy model uses the LSTM layer.During training,parameters of LSTM, unroll=False and stateful=False.After training, I convert the model to tflite.If I still set unroll=False and stateful=False during conversion, then there is a UnidirectionalSequenceLSTM layer in the tflite file. Unfortunately, I need to use tensorflow lite for micro. UnidirectionalSequenceLSTM layer is not supported in tensorflow lite for micro now.So I need to set unroll=True and stateful=False during conversion.This setting can avoid using UnidirectionalSequenceLSTM.Test result displays that tensorflow lite for micro support the model with parameters unroll=True and stateful=False. However,the result of inference with ""unroll=True and stateful=False"" is different from that of inference with ""unroll=False and stateful=False"".How to make the two results consistent.```### Standalone code to reproduce the issue```shellDuring training:  P = Dense(units=self.nbin, activation='tanh')(P)  P = Dense(units=self.nbin, activation='tanh')(P)  P = LSTM(units=self.nbin, activation='tanh', return_sequences=True, stateful=False)(P)  P = Dense(units=self.nbin, activation='softplus')(P)  Be = Lambda(self.forward)(e)  E = Dense(units=self.nbin, activation='linear')(Be)  E = LSTM(units=self.nbin, activation='tanh', return_sequences=True, stateful=False)(E)  Z = E*P  Z = Dense(units=self.wlen_z, activation='linear')(Z)During converting:  P = Dense(units=self.nbin, activation='tanh')(P)  P = Dense(units=self.nbin, activation='tanh')(P)  P = LSTM(units=self.nbin, activation='tanh', return_sequences=True, unroll=True,stateful=False)(P)  P = Dense(units=self.nbin, activation='softplus')(P)  Be = Lambda(self.forward)(e)  E = Dense(units=self.nbin, activation='linear')(Be)  E = LSTM(units=self.nbin, activation='tanh', return_sequences=True, unroll=True,stateful=False)(E)  Z = E*P  Z = Dense(units=self.wlen_z, activation='linear')(Z)```### Relevant log output```shellThe result of inference with ""unroll=True and stateful=False"" is different from that of inference with ""unroll=False and stateful=False"".```</details>
"
57003,0,2356,7,0,0,hosford42,0,"title:MatrixTriangularSolve error: ""InvalidArgumentError: Input matrix is not invertible."" description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versionv2.7.0-rc1-69-gc256c071bb2### Custom CodeNo### OS Platform and DistributionUbuntu 20.04.4 LTS### Mobile devicen/a### Python version3.8.10### Bazel versionn/a### GCC/Compiler versionn/a### CUDA/cuDNN versionn/a### GPU model and memoryn/a### Current Behaviour?When a matrix is not invertible, I get an exception, `tensorflow.python.framework.errors_impl.InvalidArgumentError: Input matrix is not invertible. [Op:MatrixTriangularSolve]`, while computing the gradient.I expected the gradient to be filled with NaN values in the appropriate locations, as occurs on the forward pass.In the code sample I provide below, note that there are two sets of data stored in `x`. The first one has an ill-defined cholesky decomposition of the covariance matrix, whereas the second one is fine. I could still use the gradient of the second one to adjust my parameters, despite it being in the same batch with the first one, but I am prevented from doing so because the MatrixTriangularSolve operation raises a hard exception rather than filling the appropriate portion of the gradient tensor with NaNs (as is the typical behavior for other ops). Even if I split the data into separate batches, Keras will still choke on this hard exception and fail to complete the training epoch.If MatrixTriangularSolve returns NaNs in the gradient, users can add their own code to assert that NaNs are not present in the gradient and raise a hard exception, if that is the desired behavior. Or, importantly, we have the option to use a function with a custom gradient on the input tensor and block the NaN gradients from corrupting our parameters while continuing to train. But as long as MatrixTriangularSolve raises an exception itself, users are left with little to no control over how the problem is handled.### Standalone code to reproduce the issue```python3import tensorflow as tfimport tensorflow_probability as tfpx = tf.Variable([[[-0.96169615, 0.03600049, -0.86897445],                  [0.14993548, 0.32782674, 0.05467033],                  [0.70350194, 0.45990896, 0.24882722]],                 [[0.19966352, 0.22277904, 0.18279016],                  [0.996189, 0.8356074, 0.32451844],                  [0.9954922, 0.95547783, 0.7614579]]])with tf.GradientTape() as tape:    y = tfp.stats.cholesky_covariance(x, sample_axis=1)    tf.print(y)g = tape.gradient(y, x)```### Relevant log output```python32022-08-03 15:25:26.880273: W tensorflow/core/kernels/linalg/cholesky_op.cc:56] Cholesky decomposition was not successful. Eigen::LLT failed with error code 1. Filling lower-triangular output with NaNs.Traceback (most recent call last):  File ""/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py"", line 3331, in run_code    exec(code_obj, self.user_global_ns, self.user_ns)  File ""<ipython-input-106-5ae22a61c2ce>"", line 10, in <module>    g = tape.gradient(y, x)  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/backprop.py"", line 1084, in gradient    flat_grad = imperative_grad.imperative_grad(  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/imperative_grad.py"", line 71, in imperative_grad    return pywrap_tfe.TFE_Py_TapeGradient(  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/backprop.py"", line 159, in _gradient_function    return grad_fn(mock_op, *out_grads)  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/linalg_grad.py"", line 471, in _CholeskyGrad    l_inverse = linalg_ops.matrix_triangular_solve(l,  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler    raise e.with_traceback(filtered_tb) from None  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 7107, in raise_from_not_ok_status    raise core._status_to_exception(e) from None  # pylint: disable=protected-accesstensorflow.python.framework.errors_impl.InvalidArgumentError: Input matrix is not invertible. [Op:MatrixTriangularSolve][[[nan 0 -2.78880322e+11]  [nan nan 0]  [nan nan nan]] [[0.375321597 0 0]  [0.317106605 0.049177371 0]  [0.169663742 0.178507909 0.000427246094]]]```</details>
"
56979,1,688,2,0,0,rajdeep-biswas,0,"title:Colab GPU runtime ignores Graph execution error with `sparse_categorical_crossentropy` description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2### Custom CodeNo### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellSetting output layer units to be a number that is lower than possible class labels, should make `sparse_categorical_crossentropy` throw a `Graph execution error` at the fit() step - which works as expected on normal runtime, however, switching the Google Colab runtime to GPU, the error is ignored and training proceeds as normal (with nan loss and 0 accuracy).```### Standalone code to reproduce the issue```shellGPU runtime should throw the same error as well instead of letting training proceed. I have created a well-documented notebook that can demonstrate / reproduce this issue and have [linked it here](https://colab.research.google.com/github/rajdeep-biswas/sparse-categorical-crossentropy/blob/master/sparcat.ipynb)..```### Relevant log output_No response_</details>
"
56976,1,0,0,0,0,FenrirVonDerNebelungs,0,"title:Could not load dynamic library 'cudart64_110.dll' description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.9.1### Custom CodeNo### OS Platform and DistributionWindows 10### Mobile device_No response_### Python version3.10### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version11.2/8.1### GPU model and memory_No response_### Current Behaviour?```shellThe load of cuda for tensorflow fails with the following error:c:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\bin>pythonPython 3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)] on win32Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> import tensorflow2022-08-01 17:30:54.607827: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found2022-08-01 17:30:54.608106: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.>>>-----------This error appears to come from: tensorflow/tensorflow/core/platform/windows/load_library.ccline: 43LoadLibraryExW(ws_file_name.c_str(), NULL, LOAD_WITH_ALTERED_SEARCH_PATH)Technically this should not be failing. Since CUDA 11.2 is loaded, the path variable is set correctly, and the file cudart64_110.dll is present. (see ""log"" output)The load even fails when python is started from within the bin directory of CUDA\v11.2. Is it possible that an extremely long path to the tensorflow python module could cause LoadLibraryEx to fail?-----------```### Standalone code to reproduce the issue```shellThe issue may be related to the length of the python path:C:\Users\uuuuuuu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packagesLoaded tensorflow using pip. Loaded CUDA 11.2, loaded cnDNN 8.1 and copied the files into the CUDA 11.2 directory.I was NOT able to reproduce it on my other machine. However windows version is home instead of pro and python is installed under a different path.This issue is not consistently reproduceable. Works fine on home computer but not on office computer.```### Relevant log output```shellTensor flow is in the following directory:C:\Users\uuuuuuu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages>dir Volume in drive C has no label. Volume Serial Number is 4298-C2D1 Directory of C:\Users\uuuuuuu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages08/01/2022  01:59 PM    <DIR>          .08/01/2022  01:59 PM    <DIR>          ..06/20/2022  10:30 AM    <DIR>          absl06/20/2022  10:30 AM    <DIR>          absl_py-1.1.0.dist-info06/20/2022  10:30 AM    <DIR>          astunparse06/20/2022  10:30 AM    <DIR>          astunparse-1.6.3.dist-info06/20/2022  10:30 AM    <DIR>          cachetools06/20/2022  10:30 AM    <DIR>          cachetools-5.2.0.dist-info06/20/2022  10:30 AM    <DIR>          certifi06/20/2022  10:30 AM    <DIR>          certifi-2022.6.15.dist-info06/20/2022  10:29 AM             8,731 CHANGELOG.md06/20/2022  10:30 AM    <DIR>          charset_normalizer06/20/2022  10:30 AM    <DIR>          charset_normalizer-2.0.12.dist-info06/20/2022  10:29 AM    <DIR>          clang06/20/2022  10:29 AM    <DIR>          flatbuffers06/20/2022  10:29 AM    <DIR>          flatbuffers-1.12.dist-info06/20/2022  10:30 AM    <DIR>          gast06/20/2022  10:30 AM    <DIR>          gast-0.4.0.dist-info06/20/2022  10:30 AM    <DIR>          google06/20/2022  10:30 AM               539 google_auth-2.8.0-py3.10-nspkg.pth06/20/2022  10:30 AM    <DIR>          google_auth-2.8.0.dist-info06/20/2022  10:30 AM    <DIR>          google_auth_oauthlib06/20/2022  10:30 AM    <DIR>          google_auth_oauthlib-0.4.6.dist-info06/20/2022  10:30 AM    <DIR>          google_pasta-0.2.0.dist-info06/20/2022  10:30 AM    <DIR>          grpc06/20/2022  10:30 AM    <DIR>          grpcio-1.46.3.dist-info06/20/2022  10:30 AM    <DIR>          h5py06/20/2022  10:30 AM    <DIR>          h5py-3.7.0.dist-info06/20/2022  10:30 AM    <DIR>          idna06/20/2022  10:30 AM    <DIR>          idna-3.3.dist-info06/20/2022  10:29 AM    <DIR>          keras06/20/2022  10:29 AM    <DIR>          keras-2.9.0.dist-info06/20/2022  10:30 AM    <DIR>          keras_preprocessing06/20/2022  10:30 AM    <DIR>          Keras_Preprocessing-1.1.2.dist-info06/20/2022  10:29 AM    <DIR>          libclang-14.0.1.dist-info06/20/2022  10:29 AM               577 LICENSE06/20/2022  10:30 AM    <DIR>          markdown06/20/2022  10:30 AM    <DIR>          Markdown-3.3.7.dist-info05/26/2022  11:16 AM    <DIR>          mysql05/26/2022  11:15 AM    <DIR>          mysqlclient-2.1.0.dist-info05/26/2022  11:15 AM    <DIR>          MySQLdb05/26/2022  11:16 AM    <DIR>          mysqlx05/26/2022  11:16 AM    <DIR>          mysql_connector_python-8.0.29.dist-info06/20/2022  10:29 AM    <DIR>          numpy06/20/2022  10:30 AM    <DIR>          numpy-1.22.4.dist-info06/20/2022  10:29 AM    <DIR>          oauthlib06/20/2022  10:29 AM    <DIR>          oauthlib-3.2.0.dist-info06/20/2022  10:30 AM    <DIR>          opt_einsum06/20/2022  10:30 AM    <DIR>          opt_einsum-3.3.0.dist-info06/20/2022  10:30 AM    <DIR>          packaging06/20/2022  10:30 AM    <DIR>          packaging-21.3.dist-info06/20/2022  10:30 AM    <DIR>          pasta08/01/2022  12:45 PM    <DIR>          pip08/01/2022  12:45 PM    <DIR>          pip-22.2.1.dist-info06/20/2022  10:29 AM               540 protobuf-3.19.4-py3.10-nspkg.pth06/20/2022  10:29 AM    <DIR>          protobuf-3.19.4.dist-info06/20/2022  10:29 AM    <DIR>          pyasn106/20/2022  10:29 AM    <DIR>          pyasn1-0.4.8.dist-info06/20/2022  10:29 AM    <DIR>          pyasn1_modules06/20/2022  10:29 AM    <DIR>          pyasn1_modules-0.2.8.dist-info06/20/2022  10:29 AM    <DIR>          pyparsing06/20/2022  10:29 AM    <DIR>          pyparsing-3.0.9.dist-info06/20/2022  10:29 AM             1,928 README.md06/20/2022  10:30 AM    <DIR>          requests06/20/2022  10:30 AM    <DIR>          requests-2.28.0.dist-info06/20/2022  10:30 AM    <DIR>          requests_oauthlib06/20/2022  10:30 AM    <DIR>          requests_oauthlib-1.3.1.dist-info06/20/2022  10:29 AM    <DIR>          rsa06/20/2022  10:29 AM    <DIR>          rsa-4.8.dist-info06/20/2022  10:29 AM    <DIR>          six-1.16.0.dist-info06/20/2022  10:29 AM            34,549 six.py08/01/2022  01:59 PM    <DIR>          tensorboard08/01/2022  01:59 PM    <DIR>          tensorboard-2.9.1.dist-info08/01/2022  01:59 PM    <DIR>          tensorboard_data_server08/01/2022  01:59 PM    <DIR>          tensorboard_data_server-0.6.1.dist-info08/01/2022  01:59 PM    <DIR>          tensorboard_plugin_wit08/01/2022  01:59 PM    <DIR>          tensorboard_plugin_wit-1.8.1.dist-info08/01/2022  01:59 PM    <DIR>          tensorflow08/01/2022  01:59 PM    <DIR>          tensorflow-2.9.1.dist-info08/01/2022  01:59 PM    <DIR>          tensorflow_estimator08/01/2022  01:59 PM    <DIR>          tensorflow_estimator-2.9.0.dist-info08/01/2022  01:59 PM    <DIR>          tensorflow_io_gcs_filesystem08/01/2022  01:59 PM    <DIR>          tensorflow_io_gcs_filesystem-0.26.0.dist-info06/20/2022  10:29 AM    <DIR>          termcolor-1.1.0-py3.10.egg-info01/13/2011  10:43 AM             5,044 termcolor.py06/20/2022  10:29 AM    <DIR>          typing_extensions-4.2.0.dist-info06/20/2022  10:29 AM            70,613 typing_extensions.py06/20/2022  10:29 AM    <DIR>          urllib306/20/2022  10:29 AM    <DIR>          urllib3-1.26.9.dist-info06/20/2022  10:29 AM    <DIR>          werkzeug06/20/2022  10:29 AM    <DIR>          Werkzeug-2.1.2.dist-info06/20/2022  10:29 AM    <DIR>          wheel06/20/2022  10:29 AM    <DIR>          wheel-0.37.1.dist-info06/20/2022  10:29 AM    <DIR>          wrapt06/20/2022  10:29 AM    <DIR>          wrapt-1.14.1.dist-info05/26/2022  11:16 AM         1,354,240 _mysqlxpb.cp310-win_amd64.pyd05/26/2022  11:16 AM            61,952 _mysql_connector.cp310-win_amd64.pyd06/20/2022  10:29 AM    <DIR>          __pycache__              10 File(s)      1,538,713 bytes              88 Dir(s)  473,428,164,608 bytes free------------------------------------------------------------------------The cuda .dll file is in the following directoy:c:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\bin>dir Volume in drive C has no label. Volume Serial Number is 4298-C2D1 Directory of c:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\bin08/01/2022  01:27 PM    <DIR>          .08/01/2022  01:27 PM    <DIR>          ..02/15/2021  05:38 AM           205,824 bin2c.exe01/29/2021  11:05 AM                66 compute-sanitizer.bat08/01/2022  12:17 PM    <DIR>          crt02/15/2021  05:38 AM           183,808 cu++filt.exe02/15/2021  12:07 AM       107,330,560 cublas64_11.dll02/15/2021  12:07 AM       175,706,112 cublasLt64_11.dll02/15/2021  05:38 AM           374,784 cuda-memcheck.exe02/15/2021  05:38 AM         4,685,824 cudafe++.exe02/15/2021  05:38 AM           393,728 cudart32_110.dll02/15/2021  05:38 AM           464,896 cudart64_110.dll08/01/2022  12:37 PM           222,720 cudnn64_8.dll08/01/2022  12:37 PM       128,429,056 cudnn_adv_infer64_8.dll08/01/2022  12:37 PM        82,672,640 cudnn_adv_train64_8.dll08/01/2022  12:37 PM       545,695,232 cudnn_cnn_infer64_8.dll08/01/2022  12:37 PM        87,374,336 cudnn_cnn_train64_8.dll08/01/2022  12:37 PM       273,139,712 cudnn_ops_infer64_8.dll08/01/2022  12:37 PM        46,076,416 cudnn_ops_train64_8.dll02/15/2021  05:38 AM       188,301,312 cufft64_10.dll02/15/2021  05:38 AM           258,560 cufftw64_10.dll02/15/2021  05:38 AM         1,314,304 cuinj64_112.dll02/15/2021  05:38 AM         2,878,464 cuobjdump.exe02/15/2021  05:38 AM        60,627,968 curand64_10.dll02/15/2021  05:38 AM       396,296,704 cusolver64_11.dll02/15/2021  05:38 AM       213,831,168 cusolverMg64_11.dll02/15/2021  05:38 AM       228,391,424 cusparse64_11.dll02/15/2021  05:38 AM           337,408 fatbinary.exe02/15/2021  05:38 AM           246,272 nppc64_11.dll02/15/2021  05:38 AM        11,890,176 nppial64_11.dll02/15/2021  05:38 AM         5,023,744 nppicc64_11.dll02/15/2021  05:38 AM         8,515,072 nppidei64_11.dll02/15/2021  05:38 AM        58,631,168 nppif64_11.dll02/15/2021  05:38 AM        29,265,920 nppig64_11.dll02/15/2021  05:38 AM         7,227,392 nppim64_11.dll02/15/2021  05:38 AM        26,726,912 nppist64_11.dll02/15/2021  05:38 AM           219,648 nppisu64_11.dll02/15/2021  05:38 AM         3,079,680 nppitc64_11.dll02/15/2021  05:38 AM        15,545,856 npps64_11.dll02/15/2021  12:07 AM           315,392 nvblas64_11.dll02/15/2021  05:38 AM         4,817,408 nvcc.exe02/15/2021  05:38 AM               334 nvcc.profile02/15/2021  05:38 AM        33,605,120 nvdisasm.exe02/15/2021  05:38 AM         3,424,256 nvjpeg64_11.dll02/15/2021  05:38 AM         8,529,920 nvlink.exe02/15/2021  05:38 AM         2,187,264 nvprof.exe02/15/2021  05:38 AM           227,840 nvprune.exe02/15/2021  05:38 AM         5,542,912 nvrtc-builtins64_112.dll08/01/2022  12:17 PM    <DIR>          nvrtc-prev02/15/2021  05:38 AM        31,991,296 nvrtc64_112_0.dll02/15/2021  05:38 AM               129 nvvp.bat02/15/2021  05:38 AM         8,402,944 ptxas.exe              48 File(s)  2,810,609,681 bytes               4 Dir(s)  473,422,077,952 bytes free----------------------------------------------------------------The environment variable path has been set correctly:c:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\bin>echo %PATH:;=&echo.%C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\binC:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\libnvvpc:\program files\graphicsmagick-1.3.36-q16C:\Program Files (x86)\NVIDIA Corporation\PhysX\CommonC:\Program Files (x86)\Common Files\Oracle\Java\javapathC:\WINDOWS\system32C:\WINDOWSC:\WINDOWS\System32\WbemC:\WINDOWS\System32\WindowsPowerShell\v1.0\C:\WINDOWS\System32\OpenSSH\C:\Program Files (x86)\Windows Kits\8.1\Windows Performance Toolkit\C:\Program Files\Microsoft SQL Server\Client SDK\ODBC\170\Tools\Binn\C:\Program Files (x86)\Microsoft SQL Server\150\Tools\Binn\C:\Program Files\Microsoft SQL Server\150\Tools\Binn\C:\Program Files\Microsoft SQL Server\150\DTS\Binn\C:\Program Files (x86)\Microsoft SQL Server\150\DTS\Binn\C:\Program Files\Azure Data Studio\binC:\Program Files\NVIDIA Corporation\NVIDIA NvDLISRC:\Program Files\Git LFSC:\Program Files\Go\binC:\Program Files\Git\cmdC:\Program Files\NVIDIA Corporation\Nsight Compute 2020.3.1\C:\Program Files\MySQL\MySQL Shell 8.0\bin\C:\Users\uuuuuu\AppData\Local\Microsoft\WindowsAppsC:\Users\uuuuuu\Dev\opencv\build\binC:\Users\uuuuuu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\ScriptsC:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\binC:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\libnvvp-------------```</details>
"
56932,1,0,0,0,0,FragrantRookie,0,"title:Tensorflow lite for microcontroller. VAR_HANDLE requires resource variables. description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.9### Custom CodeYes### OS Platform and Distributionubuntu20### Python version3.9```shellA bug happened!When I run this codeTfLiteTensor* input1 = interpreter1.input(0);the console return the following text:VAR_HANDLE requires resource variables. Please create ResourceVariables and pass it to the interpreter.Node VAR_HANDLE (number 1f) failed to prepare with status 1I have two tflite models.One of them is OK, but the other cannot run and return this text.However, I have no problem running both with tensorflow Lite```### Standalone code to reproduce the issue```shellconst tflite::Model* model1 = ::tflite::GetModel(CDEC_1_1600_Zz_tflite);  if (model1->version() != TFLITE_SCHEMA_VERSION) {    TF_LITE_REPORT_ERROR(&micro_error_reporter,                         ""Model provided is schema version %d not equal ""                         ""to supported version %d.\n"",                         model1->version(), TFLITE_SCHEMA_VERSION);  }  tflite::AllOpsResolver micro_op_resolver;  // Build an interpreter to run the model with.  tflite::MicroInterpreter interpreter1(model1, micro_op_resolver, tensor_arena,                                       tensor_arena_size,                                       &micro_error_reporter);  interpreter1.AllocateTensors();  // Get information about the memory area to use for the model's input.  TfLiteTensor* input1 = interpreter1.input(0);  TfLiteTensor* input2 = interpreter1.input(1);  TfLiteTensor* input3 = interpreter1.input(2);  TfLiteTensor* input4 = interpreter1.input(3);  TfLiteTensor* input5 = interpreter1.input(4);  TfLiteTensor* input6 = interpreter1.input(5);```### Relevant log output```shellVAR_HANDLE requires resource variables. Please create ResourceVariables and pass it to the interpreter.Node VAR_HANDLE (number 1f) failed to prepare with status 1```</details>
"
56920,1,1328,14,0,0,panhu,0,"title:Load Lambda description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.9### Custom CodeYes### OS Platform and Distributionwindows### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellHi,my code like this:The functions:    def stftLayer(self, x, mode='mag_pha'):        frames = tf.signal.frame(x, self.block_len, self.block_shift)        if self.win is not None:            frames = self.win * frames        stft_dat = tf.signal.rfft(frames)        # calculating magnitude and phase from the complex signal        output_list = []        if mode == 'mag_pha':            mag = tf.math.abs(stft_dat)            phase = tf.math.angle(stft_dat)            output_list = [mag, phase]        elif mode == 'real_imag':            real = tf.math.real(stft_dat)            imag = tf.math.imag(stft_dat)            output_list = [real, imag]            # returning magnitude and phase as list        return output_listIn model:...real,imag = Lambda(self.stftLayer,arguments = {'mode':'real_imag'})(time_dat)...Where the error occurred闂傚倸鍊搁崐鐑芥倿閿旈敮鍋撶粭娑樻噽閻瑩鏌熼悜姗嗘闁轰礁妫欓妵鍕敂閸℃劒绮歟l = tf.keras.models.load_model(modelparh,custom_objects={""stftLayer"":stftLayer})```### Standalone code to reproduce the issue```shellWhen i want to load the model,have a issus:TypeError: Exception encountered when calling layer ""lambda"" (type Lambda).stftLayer() missing 1 required positional argument: 'x'Call arguments received by layer ""lambda"" (type Lambda):  闂?inputs=tf.Tensor(shape=(8, None), dtype=float32)  闂?mask=None  闂?training=None```### Relevant log output_No response_</details>
"
56910,1,0,0,0,0,will-cern,0,"title:unexpected value of binary_crossentropy loss function in network with  description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.8.2### Custom CodeNo### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellBasically I am seeing binary_crossentropy loss evaluate to a completely unexpected number when using it on a network that isn't outputting just a single logit. Below is a simple example of a network with two outputs which will correspond to the probability of each class. I wanted to check the calculated loss was what I would expect it to be. All this works fine for the categorical_crossentropy loss function (which is the more natural choice in this case, I accept), but given the code lets you do binary_crossentropy here too I wanted to understand what it's doing. But the meaning of the value seems hard to decipher. Is there a bug or other explanation for the difference in values???```### Standalone code to reproduce the issue```shellfrom tensorflow import kerasimport numpy as nploss_func = keras.losses.BinaryCrossentropy()nn = keras.Sequential([  keras.layers.Dense(2**8, input_shape=(1,), activation='relu'),  keras.layers.Dense(2, activation='softmax')])nn.compile(loss=loss_func,optimizer='adam')train_x = np.array([0.4])train_y = np.array([[0,1]])print(nn.predict(train_x))print(""Evaluated loss = "",nn.evaluate(train_x,train_y))print(""Function loss = "",loss_func(train_y,nn.predict(train_x)).numpy())print(""Manual loss = "",np.average( -train_y*np.log(nn.predict(train_x)) -(1-train_y)*np.log(1. - nn.predict(train_x)) ))``````### Relevant log output```shell[[0.5152152  0.48478484]]1/1 [==============================] - 0s 92ms/step - loss: 0.7085Evaluated loss =  0.7084982991218567Function loss =  0.72405Manual loss =  0.7240501642227173The evaluated loss value makes no sense compared to the function value and the manually calculated value (which is just equal to -log(0.48478484) as it should be```</details>
"
56906,0,555,13,0,0,Wanzizhu,0,"title:[C API] Index error in ShapeInference C API description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.9.1### Custom CodeNo### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shell**Error**: When using shape inference c api, `TF_ShapeInferenceContextGetInput` and `TF_ShapeInferenceContextSetOutput`, even the index is not out of range, there is an out of range error. **Cause**: Checking the source code, we can see the condition `0 < i || i >= cc_ctx->num_inputs()`, which is wrong obviously.**Source code**: Here is source code [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/ops.cc#L146)```### Standalone code to reproduce the issue```shellNo standalone test needed```### Relevant log output```shellCheck failed: TF_OK == TF_GetCode(status) (0 vs. 3)```</details>
"
56894,1,1776,160,0,1,cianciosa,0,"title:SlurmClusterResolver cannot be configured to use no GPUs description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.9.1### Custom CodeNo### OS Platform and DistributionLinux custom cray distribution### Mobile deviceN/A### Python versionConda evn installed python 3.10### Bazel versionN/A### GCC/Compiler versionN/A### CUDA/cuDNN versionN/A### GPU model and memoryN/A### Current Behaviour?```shellAttempting to use zero GPU's causes an errorTraceback (most recent call last):  File ""test.py"", line 17, in <module>    strategy = tensorflow.distribute.MultiWorkerMirroredStrategy(cluster_resolver=cluster_resolver)  File ""/global/homes/m/markcian/.conda/envs/parallel_tensorflow/lib/python3.10/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 189, in __init__    communication_options=communication_options))  File ""/global/homes/m/markcian/.conda/envs/parallel_tensorflow/lib/python3.10/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 327, in __init__    self._initialize_strategy(self._cluster_resolver)  File ""/global/homes/m/markcian/.conda/envs/parallel_tensorflow/lib/python3.10/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 338, in _initialize_strategy    if cluster_resolver.cluster_spec().as_dict():  File ""/global/homes/m/markcian/.conda/envs/parallel_tensorflow/lib/python3.10/site-packages/tensorflow/python/distribute/cluster_resolver/slurm_cluster_resolver.py"", line 327, in cluster_spec    range(num_tasks), range(0, self._gpus_per_node, self._gpus_per_task)):ValueError: range() arg 3 must not be zero```If I attempt to set gpus_per_task to 1 to fix this error, ```Traceback (most recent call last):  File ""test.py"", line 6, in <module>    auto_set_gpu=False)  File ""/global/homes/m/markcian/.conda/envs/parallel_tensorflow/lib/python3.10/site-packages/tensorflow/python/distribute/cluster_resolver/slurm_cluster_resolver.py"", line 271, in __init__    raise RuntimeError('Requested more GPUs per node then available.')RuntimeError: Requested more GPUs per node then available.```The expected behavior is that I would be able to configure tensorflow to not use GPUs and that it won't even attempt to use them if there are no GPUs on the node especially when installed with `pip install tensorflow_cpu`.```### Standalone code to reproduce the issue```shellimport tensorflowcluster_resolver = tensorflow.distribute.cluster_resolver.SlurmClusterResolver(gpus_per_node=0,                                                            gpus_per_task=0,                                                            tasks_per_node=1,                                                            auto_set_gpu=False)```### Relevant log output```shellpython -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""v2.9.0-18-gd8ce9f9c301 2.9.1```</details>
"
56890,1,202,86,0,0,shkarupa-alex,0,"title:NaN values in tf.reduce_mean + float16 + CPU description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.8.2### Custom CodeNo### OS Platform and Distribution_No response_### Mobile device_No response_### Python version3.7.13### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellWhen using CPU + fp16, tf.reduce_mean produces NaN-s.When reduce_mean placed on GPU everything works.```### Standalone code to reproduce the issue```shellhttps://colab.research.google.com/drive/1sEUNVv0knkg65JNrsHMcbdlCJGs8U8j9?usp=sharing```### Relevant log output_No response_</details>
"
56888,1,1387,0,0,0,eeDigitalSeal,0,"title:Gradients calculated in reverse mode and forward mode are not equal for `tf.compat.v1.keras.layers.GlobalAvgPool2D` description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.9### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellThe jacobian matrix calculated by reverse mode is not equal to that computed in forward mode.```### Standalone code to reproduce the issue```shellimport tensorflow as tfimport numpy as npinput = tf.constant(    [[[[0.1695373]],[[0.2855878]],[[0.67758346]],[[0.2562498]],[[0.11648738]]],     [[[0.76352525]],[[0.61349154]],[[0.12928855]],[[0.0187813]],[[0.72227335]]],     [[[0.9399148]],[[0.3782258]],[[0.4337635]],[[0.4784329]],[[0.28710878]]]], dtype=tf.float32)globalAvgPool2d = tf.compat.v1.keras.layers.GlobalAvgPool2D(data_format=""channels_last"", keepdims=[2,2])with tf.GradientTape() as g:    g.watch(input)    res_backward = globalAvgPool2d(input)jacob = g.jacobian(res_backward,input)grad_fwd_arr = []for i in range(tf.size(input)):    tangents = tf.reshape(tf.one_hot(i,tf.size(input)),shape=input.shape)    with tf.autodiff.ForwardAccumulator(input, tangents) as acc:        res_forward = globalAvgPool2d(input)        jvp = acc.jvp(res_forward)        grad_fwd_arr.append(jvp)jacob_fwd = tf.reshape(tf.convert_to_tensor(grad_fwd_arr),shape=jacob.shape)np.testing.assert_allclose(jacob,jacob_fwd)```### Relevant log output```shellAssertionError: Not equal to tolerance rtol=1e-07, atol=0Mismatched elements: 20 / 45 (44.4%)Max absolute difference: 0.2Max relative difference: 1. x: array([[[[[[[[0.2]],            [[0.2]],... y: array([[[[[[[[0.2]],            [[0. ]],...```</details>
"
56880,1,1931,15,1,1,Apprisco,0,"title:[No fix, use horovod.] Manual device placement for distributed training isn't async like TF1 description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.9.12### Custom CodeNo### OS Platform and DistributionWindows 11### Mobile device_No response_### Python version3.9.12### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version11.2/8.6### GPU model and memory3090, 24GB### Current Behaviour?```shellAs I am not able to use a tensorflow dataset for my specific application (migrated from v1, would rather keep the custom loader that works in parallel very well, tuned to specific machines.), I was looking into V1-esque distributed training strategies.In V1, we would simply loop over gpus and redo the computation while splitting the inputs in quarters.That is what I did in tf2, and that is what the manual placement guide says..debugging.set_log_device_placement(True)gpus = tf.config.list_logical_devices('GPU')if gpus:  # Replicate your computation on multiple GPUs  c = []  for gpu in gpus:    with tf.device(gpu.name):      a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])      b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])      c.append(tf.matmul(a, b))  with tf.device('/CPU:0'):    matmul_sum = tf.add_n(c)  print(matmul_sum)However, the amount of time the computation takes on multiple gpus is simply longer than the time that takes on a single gpu.So I printed what happens after each gpu's computations are finished, and as I expected, the processes are not running async.I even accumulated the gradients to compile them later, but it did not fix the problem.Is there any way to make distributed training work without a tf.dataset? I've already gone through so many issues to make this work, including the following (not even a distributed issue)https://github.com/tensorflow/tensorflow/issues/56878```### Standalone code to reproduce the issue```shell.debugging.set_log_device_placement(True)gpus = tf.config.list_logical_devices('GPU')if gpus:  # Replicate your computation on multiple GPUs  c = []  for gpu in gpus:    with tf.device(gpu.name):      a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])      b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])      c.append(tf.matmul(a, b))  with tf.device('/CPU:0'):    matmul_sum = tf.add_n(c)  print(matmul_sum)```### Relevant log output_No response_</details>
"
56867,1,1901,4,0,0,lucasfbn,0,"title:Loss stagnates after first episode on random dataset description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.9.1### Custom CodeNo### OS Platform and Distribution_No response_### Mobile device_No response_### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?#### TL:DRLoss stagnated after first episode on real-world dataset. Tried same model on a random dataset. Loss stagnated as well. Seems to be related to kernel size. Why is the network seemingly unable to learn anything past the first episode when the kernel size increases above a certain threshold?Test code/notebook link below.[Notebook link](https://colab.research.google.com/drive/1sC7KQJwbTPiQ9ZBWZBCzY3if4bSuRzNq?usp=sharing)#### ContextI'm trying to solve a real-world problem with a simple 1D CNN implemented in TensorFlow. However, the loss ""converged""/stayed constant after the first episode for some reason.After tinkering around with the actual dataset, I tried training the model with a randomly created dataset. And, low and behold, the loss stagnated at the exact same value as it would with the actual dataset.This seems to be related to the kernel size. However, I can't really make sense of why it results in the exact same constant loss despite being fed two completely different datasets.#### QuestionWhat is the intuition behind this? Why is the network seemingly unable to learn anything past the first episode when the kernel size increases above a certain threshold?Note that I get the exact same loss for a real-world dataset, so the problem isn't related to the fact that the example dataset is made up and random.Thank you for taking the time.### Standalone code to reproduce the issue[Notebook link](https://colab.research.google.com/drive/1sC7KQJwbTPiQ9ZBWZBCzY3if4bSuRzNq?usp=sharing)```pythonimport numpy as npimport tensorflow as tfdef network(seq_len, n_features):    model = tf.keras.models.Sequential()    model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=128, activation='relu',                                     input_shape=(seq_len, n_features)))    model.add(tf.keras.layers.Flatten())    model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))    return modeldef make_dataset(n, seq_len, n_features, classes):    x = np.random.random((n, seq_len, n_features))    y = np.random.randint(0, classes, (n,))    return x, y, seq_len, n_features# create random dataset with# - 10000 sequences, each of which with 2400 timesteps and one feature# - two classes (0 and 1)x, y, seq_len, n_features = make_dataset(n=10000, seq_len=2400, n_features=1, classes=2)model = network(seq_len, n_features)model.compile(optimizer=tf.keras.optimizers.Adam(),                  loss=tf.keras.losses.BinaryCrossentropy())model.fit(x, y, epochs=10, batch_size=64)```### Relevant log output```shellEpoch 1/10157/157 [==============================] - 28s 179ms/step - loss: 0.7059Epoch 2/10157/157 [==============================] - 27s 172ms/step - loss: 0.6932Epoch 3/10157/157 [==============================] - 26s 168ms/step - loss: 0.6931Epoch 4/10157/157 [==============================] - 29s 182ms/step - loss: 0.6931Epoch 5/10157/157 [==============================] - 28s 176ms/step - loss: 0.6931Epoch 6/10157/157 [==============================] - 28s 180ms/step - loss: 0.6931Epoch 7/10157/157 [==============================] - 27s 173ms/step - loss: 0.6931Epoch 8/10157/157 [==============================] - 27s 172ms/step - loss: 0.6931Epoch 9/10157/157 [==============================] - 27s 172ms/step - loss: 0.6931Epoch 10/10157/157 [==============================] - 27s 173ms/step - loss: 0.6931```</details>
"
56861,0,12700,269,0,0,elfringham,0,"title:Unit test quantization_ops:quantization_ops_test fails on mkl_aarch64 description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiongit HEAD### Custom CodeNo### OS Platform and DistributionCentOS 7### Mobile devicen/a### Python version3.8.10### Bazel version5.1.1### GCC/Compiler version10.2.1### CUDA/cuDNN versionn/a### GPU model and memoryn/a### Current Behaviour?```shellUnit test //tensorflow/python/kernel_tests/quantization_ops:quantization_ops_test fails with segfault introduced by https://github.com/tensorflow/tensorflow/commit/7cdf9d4d2083b739ec81cfdace546b0c99f50622```### Standalone code to reproduce the issue```shellbazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --noremote_accept_cached --config=nonccl --config=mkl_aarch64 --copt=""-mtune=generic"" --copt=""-march=armv8-a"" --copt=""-O3"" --test_env=TF_ENABLE_ONEDNN_OPTS=1 --copt=""-fopenmp"" --linkopt=""-lgomp"" --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --verbose_failures --build_tests_only --jobs=75 -- //tensorflow/python/kernel_tests/quantization_ops:quantization_ops_test```### Relevant log output```shell==================== Test output for //tensorflow/python/kernel_tests/quantization_ops:quantization_ops_test:2022-07-22 11:25:14.622796: I tensorflow/core/util/util.cc:175] Experimental oneDNN custom operations are on. If you experience issues, please turn them off by setting the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.Running tests under Python 3.8.13: /tmp/workspace/venv-cp38-cp38/bin/python3[ RUN      ] FakeQuantWithMinMaxVarsOpTest.test_invalid_inputsINFO:tensorflow:Running test_invalid_inputs in GRAPH mode.I0722 11:25:15.747941 281472890588256 test_util.py:1490] Running test_invalid_inputs in GRAPH mode.WARNING:tensorflow:From /opt/python/cp38-cp38/lib/python3.8/contextlib.py:83: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.Instructions for updating:Use `self.session()` or `self.cached_session()` instead.W0722 11:25:15.748297 281472890588256 deprecation.py:350] From /opt/python/cp38-cp38/lib/python3.8/contextlib.py:83: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.Instructions for updating:Use `self.session()` or `self.cached_session()` instead.INFO:tensorflow:time(__main__.FakeQuantWithMinMaxVarsOpTest.test_invalid_inputs): 0.08sI0722 11:25:15.824039 281472890588256 test_util.py:2460] time(__main__.FakeQuantWithMinMaxVarsOpTest.test_invalid_inputs): 0.08sINFO:tensorflow:Running test_invalid_inputs in EAGER mode.I0722 11:25:15.824797 281472890588256 test_util.py:1499] Running test_invalid_inputs in EAGER mode.INFO:tensorflow:time(__main__.FakeQuantWithMinMaxVarsOpTest.test_invalid_inputs): 0.05sI0722 11:25:15.870929 281472890588256 test_util.py:2460] time(__main__.FakeQuantWithMinMaxVarsOpTest.test_invalid_inputs): 0.05s[       OK ] FakeQuantWithMinMaxVarsOpTest.test_invalid_inputs[ RUN      ] FakeQuantWithMinMaxVarsOpTest.test_session[  SKIPPED ] FakeQuantWithMinMaxVarsOpTest.test_session[ RUN      ] FakeQuantWithMinMaxVarsPerChannelOpTest.test_invalid_inputsINFO:tensorflow:Running test_invalid_inputs in GRAPH mode.I0722 11:25:15.872137 281472890588256 test_util.py:1490] Running test_invalid_inputs in GRAPH mode.INFO:tensorflow:time(__main__.FakeQuantWithMinMaxVarsPerChannelOpTest.test_invalid_inputs): 0.01sI0722 11:25:15.883851 281472890588256 test_util.py:2460] time(__main__.FakeQuantWithMinMaxVarsPerChannelOpTest.test_invalid_inputs): 0.01sINFO:tensorflow:Running test_invalid_inputs in EAGER mode.I0722 11:25:15.884400 281472890588256 test_util.py:1499] Running test_invalid_inputs in EAGER mode.INFO:tensorflow:time(__main__.FakeQuantWithMinMaxVarsPerChannelOpTest.test_invalid_inputs): 0.01sI0722 11:25:15.890146 281472890588256 test_util.py:2460] time(__main__.FakeQuantWithMinMaxVarsPerChannelOpTest.test_invalid_inputs): 0.01s[       OK ] FakeQuantWithMinMaxVarsPerChannelOpTest.test_invalid_inputs[ RUN      ] FakeQuantWithMinMaxVarsPerChannelOpTest.test_session[  SKIPPED ] FakeQuantWithMinMaxVarsPerChannelOpTest.test_session[ RUN      ] QuantizeDownAndShrinkRangeOpTest.test_invalid_inputsINFO:tensorflow:Running test_invalid_inputs in GRAPH mode.I0722 11:25:15.891172 281472890588256 test_util.py:1490] Running test_invalid_inputs in GRAPH mode.INFO:tensorflow:time(__main__.QuantizeDownAndShrinkRangeOpTest.test_invalid_inputs): 0.0sI0722 11:25:15.895756 281472890588256 test_util.py:2460] time(__main__.QuantizeDownAndShrinkRangeOpTest.test_invalid_inputs): 0.0sINFO:tensorflow:Running test_invalid_inputs in EAGER mode.I0722 11:25:15.896286 281472890588256 test_util.py:1499] Running test_invalid_inputs in EAGER mode.INFO:tensorflow:time(__main__.QuantizeDownAndShrinkRangeOpTest.test_invalid_inputs): 0.02sI0722 11:25:15.911795 281472890588256 test_util.py:2460] time(__main__.QuantizeDownAndShrinkRangeOpTest.test_invalid_inputs): 0.02s[       OK ] QuantizeDownAndShrinkRangeOpTest.test_invalid_inputs[ RUN      ] QuantizeDownAndShrinkRangeOpTest.test_session[  SKIPPED ] QuantizeDownAndShrinkRangeOpTest.test_session[ RUN      ] QuantizedAddOpTest.test_invalid_inputsINFO:tensorflow:Running test_invalid_inputs in GRAPH mode.I0722 11:25:15.912965 281472890588256 test_util.py:1490] Running test_invalid_inputs in GRAPH mode.INFO:tensorflow:time(__main__.QuantizedAddOpTest.test_invalid_inputs): 0.01sI0722 11:25:15.919473 281472890588256 test_util.py:2460] time(__main__.QuantizedAddOpTest.test_invalid_inputs): 0.01sINFO:tensorflow:Running test_invalid_inputs in EAGER mode.I0722 11:25:15.920017 281472890588256 test_util.py:1499] Running test_invalid_inputs in EAGER mode.INFO:tensorflow:time(__main__.QuantizedAddOpTest.test_invalid_inputs): 0.01sI0722 11:25:15.933510 281472890588256 test_util.py:2460] time(__main__.QuantizedAddOpTest.test_invalid_inputs): 0.01s[       OK ] QuantizedAddOpTest.test_invalid_inputs[ RUN      ] QuantizedAddOpTest.test_session[  SKIPPED ] QuantizedAddOpTest.test_session[ RUN      ] QuantizedAvgPoolingOpTest.test_invalid_inputsINFO:tensorflow:Running test_invalid_inputs in GRAPH mode.I0722 11:25:15.934696 281472890588256 test_util.py:1490] Running test_invalid_inputs in GRAPH mode.INFO:tensorflow:time(__main__.QuantizedAvgPoolingOpTest.test_invalid_inputs): 0.01sI0722 11:25:15.941932 281472890588256 test_util.py:2460] time(__main__.QuantizedAvgPoolingOpTest.test_invalid_inputs): 0.01sINFO:tensorflow:Running test_invalid_inputs in EAGER mode.I0722 11:25:15.942471 281472890588256 test_util.py:1499] Running test_invalid_inputs in EAGER mode.Fatal Python error: Segmentation faultCurrent thread 0x0000ffff83a84c60 (most recent call first):  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/org_tensorflow/tensorflow/python/eager/execute.py"", line 54 in quick_execute  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/gen_nn_ops.py"", line 6987 in quantized_avg_pool_eager_fallback  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/gen_nn_ops.py"", line 6934 in quantized_avg_pool  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.py"", line 170 in test_invalid_inputs  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1504 in run_eagerly  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1520 in decorated  File ""/opt/python/cp38-cp38/lib/python3.8/unittest/case.py"", line 633 in _callTestMethod  File ""/opt/python/cp38-cp38/lib/python3.8/unittest/case.py"", line 676 in run  File ""/opt/python/cp38-cp38/lib/python3.8/unittest/case.py"", line 736 in __call__  File ""/opt/python/cp38-cp38/lib/python3.8/unittest/suite.py"", line 122 in run  File ""/opt/python/cp38-cp38/lib/python3.8/unittest/suite.py"", line 84 in __call__  File ""/opt/python/cp38-cp38/lib/python3.8/unittest/suite.py"", line 122 in run  File ""/opt/python/cp38-cp38/lib/python3.8/unittest/suite.py"", line 84 in __call__  File ""/opt/python/cp38-cp38/lib/python3.8/unittest/runner.py"", line 176 in run  File ""/opt/python/cp38-cp38/lib/python3.8/unittest/main.py"", line 271 in runTests  File ""/opt/python/cp38-cp38/lib/python3.8/unittest/main.py"", line 101 in __init__  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/absl_py/absl/testing/absltest.py"", line 2537 in _run_and_get_tests_result  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/absl_py/absl/testing/absltest.py"", line 2568 in run_tests  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/absl_py/absl/testing/absltest.py"", line 2156 in _run_in_app  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/absl_py/absl/testing/absltest.py"", line 2049 in main  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 51 in g_main  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/absl_py/absl/app.py"", line 258 in _run_main  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/absl_py/absl/app.py"", line 312 in run  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 60 in main_wrapper  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py"", line 503 in benchmarks_main  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 62 in main  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.py"", line 347 in <module>================================================================================Target //tensorflow/python/kernel_tests/quantization_ops:quantization_ops_test up-to-date:  bazel-bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_testINFO: Elapsed time: 156.845s, Critical Path: 120.79sINFO: 216 processes: 1 internal, 215 local.INFO: Build completed, 1 test FAILED, 216 total actions//tensorflow/python/kernel_tests/quantization_ops:quantization_ops_test  FAILED in 3 out of 3 in 2.7s```</details>
"
56840,0,2601,269,0,0,elfringham,0,"title:Unit test failure on high CPU core count machines description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiongit HEAD### Custom CodeNo### OS Platform and DistributionCentOS 7### Mobile devicen/a### Python version3.8.10### Bazel version5.1.1### GCC/Compiler version10.2.1### CUDA/cuDNN versionn/a### GPU model and memoryn/a### Current Behaviour?```shell//tensorflow/python/data/experimental/kernel_tests/service:cross_trainer_cache_test fails if there are more than 48 CPU cores in the machine being used to test.```### Standalone code to reproduce the issue```shellbazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=2 --test_output=all --cache_test_results=no --config=nonccl --copt=-mtune=generic --copt=-march=armv8-a --copt=-O3 --verbose_failures --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --build_tests_only -- //tensorflow/python/data/experimental/kernel_tests/service:cross_trainer_cache_test```### Relevant log output```shellFAIL: testConcurrentReaders_test_mode_graph_tfapiversion_2 (__main__.CrossTrainerCacheTest)CrossTrainerCacheTest.testConcurrentReaders_test_mode_graph_tfapiversion_2testConcurrentReaders_test_mode_graph_tfapiversion_2(mode='graph', tf_api_version=2)----------------------------------------------------------------------Traceback (most recent call last):  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/cross_trainer_cache_test.runfiles/absl_py/absl/testing/parameterized.py"", line 314, in bound_param_test    return test_method(self, **testcase_params)  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/cross_trainer_cache_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 362, in decorated    execute_test_method()  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/cross_trainer_cache_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 345, in execute_test_method    test_method(**kwargs_to_pass)  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/cross_trainer_cache_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/cross_trainer_cache_test.py"", line 97, in testConcurrentReaders    self.assertEqual(self.evaluate(iterators[j]()), i)AssertionError: 9 != 0----------------------------------------------------------------------```</details>
"
56830,1,1069,0,0,0,VictoriaGriffith,0,"title:`tf.math.reduce_std` have wrong gradient when input has only one element description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.9### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 20.04### Mobile devicen/a### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellA bug happened when compute the gradient of `tf.math.reduce_std`. When the input `x` is a shape `[1]` tensor, the reduced standard deviation is always `0`, so the gradient should be `0` (the numerical gradient computed by `tf.test.compute_gradient` is also `0`). However, the current gradient wrongly gives `nan`.```### Standalone code to reproduce the issue```shellimport tensorflow as tfx = tf.random.uniform([1], minval=-1, maxval=1, dtype=tf.float64)print(x)with tf.GradientTape(persistent=False,) as g:  g.watch(x)  y = tf.math.reduce_std(x, axis=0, keepdims=True)  print(y)print(""gradient"", g.gradient(y, x))@tf.functiondef test_func(x):  return tf.math.reduce_std(x, axis=0, keepdims=True)theoretical, numerical = tf.test.compute_gradient(test_func, [x])print(""theoretical gradient"", theoretical)print(""numerical gradient"", numerical)```### Relevant log output```shelltf.Tensor([-0.83151082], shape=(1,), dtype=float64)tf.Tensor([0.], shape=(1,), dtype=float64)gradient tf.Tensor([nan], shape=(1,), dtype=float64)theoretical gradient (array([[nan]]),)numerical gradient (array([[0.]]),)```</details>
"
56822,1,0,125,0,0,Claeb101,0,"title:tf.linalg.eigh provides inaccurate results, conflicting with tf.linalg.eig description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.8.2### Custom CodeNo### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?I am working on a project in which I need to compute the eigenvalues of each matrix element in a batch. While implementing, I discovered that `tf.linalg.eigh` was returning different results from `tf.linalg.eig` to statistical significance. Weirdly, both `tf.linalg.eig` and `tf.linalg.eigvals` return the same values as do `tf.linalg.eigh` and `tf.linalg.eigvals`.Matrix `[[1, 0], [1, 2]]` has eigenvalues = 2, 1 with eigenvectors = (0, 1) and (-1, 1) respectively. Both `eig` and `eigvals` get this correct, but both `eigh` and `eigvalsh` incorrectly return the values `0.38196601, 2.61803399`.### Standalone code to reproduce the issueSee the following for the above buggy example.https://colab.research.google.com/drive/1iJ14rLtDJLcM_ZvshCTkv0Y4vnJVN8vQ?usp=sharing### Relevant log output_No response_</details>
"
56808,0,626,2,1,0,Bahar-BM,0,"title:openCL delegate generates '0' and 'random' values with 'tf.stack' description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.9.1, nightly version### Custom CodeNo### OS Platform and DistributionAndroid### Mobile devicetested on Snapdragon 888, 865### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellOur models with 'tf.stack' ('Pack' in the tflite version) nodes generate wrong results with the openCL delegate. Our experiments show that the output of a 'Pack' node in a tflite model contains lots of 'zeros' and random values when we use openCL delegate. This issue does not happen with other delegates like XNNPACK.This issue is very similar to the issue that we reported before here:https://github.com/tensorflow/tensorflow/issues/56732```### Standalone code to reproduce the issue```shellWe have implemented a small tool to reproduce the mentioned issue with the 'tf.stack' node. Here is the link to the repository:https://github.com/Bahar-BM/tflite-test```### Relevant log output_No response_</details>
"
56796,1,403,0,0,0,VictoriaGriffith,0,"title:`l2_normalize` has wrong gradient when input is zero description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.9### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellWhen the input of `tf.nn.l2_normalize` is a zero tensor, `tf.GradientTape` outputs a very large gradient `1000000`.```### Standalone code to reproduce the issue```shellimport tensorflow as tfx = tf.random.uniform([1,1], maxval=0,dtype=tf.float64)with tf.GradientTape(persistent=True,) as g:  g.watch(x)  y = tf.nn.l2_normalize(x)print(""gradient"", g.gradient(y, x))```### Relevant log output```shellgradient tf.Tensor([[1000000.]], shape=(1, 1), dtype=float64)```</details>
"
56773,1,2708,0,0,1,pimorandi,0,"title:Error during fit on distributed dataset with multiple GPUs. ""ValueError: When providing a distributed dataset, you must specify the number of steps to run."" description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.9### Custom CodeNo### OS Platform and DistributionLinux Debian 4.19.194-3### Mobile device_No response_### Python version3.8.10### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version11.7 / 8.3### GPU model and memoryTesla V100-PCIE-32GB x2### Current Behaviour?```shellWhen trying to train a model on multiple GPUs, we get a ValueError at the start of fit method.The error is caused by the dataset distribution, done through MirroredStrategy and strategy.experimental_distribute_dataset(). If the distribution is removed from the code, no error occurs, but the dataset is not distributed.```### Standalone code to reproduce the issue```shellimport tensorflow as tfdef _parse_function(example):    _float_feature = tf.io.FixedLenFeature([], tf.float32, default_value=0.0)    feature_description = {        'f1': _float_feature,        'f2': _float_feature,        'f3': _float_feature,        'f4': _float_feature,        'f5': _float_feature,        'label': tf.io.FixedLenFeature([], tf.int64, default_value=0),    }    samples = tf.io.parse_example(example, feature_description)    label = samples['label']    features = tf.stack([            samples['f1'],            samples['f2'],            samples['f3'],            samples['f4'],            samples['f5']],            axis=1)    return (features, label)gpus = tf.config.list_logical_devices('GPU')strategy = tf.distribute.MirroredStrategy(gpus)batch_size_per_replica = 256batch_size = batch_size_per_replica * strategy.num_replicas_in_synctrain_filename = './training_data.tfrec'train_dataset = tf.data.TFRecordDataset([train_filename]        ).batch(batch_size        ).map(_parse_function)val_filename = './val_data.tfrec'val_dataset = tf.data.TFRecordDataset([val_filename]        ).batch(batch_size        ).map(_parse_function)train_dataset = strategy.experimental_distribute_dataset(train_dataset)val_dataset = strategy.experimental_distribute_dataset(val_dataset)with strategy.scope():    mdl = tf.keras.Sequential([        tf.keras.layers.InputLayer(input_shape=(5,)),        tf.keras.layers.Dense(5),        tf.keras.layers.Dense(1, activation='sigmoid')        ])    mdl.compile(tf.keras.optimizers.Adam(),        loss=tf.keras.losses.BinaryCrossentropy())h = mdl.fit(        train_dataset,         validation_data=val_dataset,        verbose=0,        epochs=50,        batch_size=batch_size,        )```### Relevant log output```shellTraceback (most recent call last):  File ""main.py"", line 76, in <module>    h = mdl.fit(  File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 67, in error_handler    raise e.with_traceback(filtered_tb) from None  File ""/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py"", line 755, in _validate_args    raise ValueError(""When providing a distributed dataset, you must ""ValueError: When providing a distributed dataset, you must specify the number of steps to run.```</details>
"
56763,1,14708,0,0,0,NiuKeke,0,"title:label_image:undefined reference to `tensorflow::str_util::EndsWith(absl::lts_20211102::string_view, absl::lts_20211102::string_view)' description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.9### Custom CodeNo### OS Platform and Distributionlinux Ubuntu 20.04### Mobile device_No response_### Python version3.8### Bazel version5.1.1### GCC/Compiler version9.4### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellWhen I built the label_image example, it just didn't work like:[1/1] Linking CXX executable test_tensorflowFAILED: test_tensorflow : && /usr/bin/c++ -O3 -DNDEBUG  CMakeFiles/test_tensorflow.dir/main.cpp.o -o test_tensorflow -L/home/nkk/nkk/nkk/work/zht/src/../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow   -L/usr/local/lib -Wl,-rpath,/home/nkk/nkk/nkk/work/zht/src/../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow:/usr/local/lib:/home/nkk/nkk/nkk/work/zht/src/../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow/lite  /usr/local/lib/libabsl_bad_any_cast_impl.a  /usr/local/lib/libabsl_bad_optional_access.a  /usr/local/lib/libabsl_bad_variant_access.a  /usr/local/lib/libabsl_base.a  /usr/local/lib/libabsl_city.a  /usr/local/lib/libabsl_civil_time.a  /usr/local/lib/libabsl_cord.a  /usr/local/lib/libabsl_cord_internal.a  /usr/local/lib/libabsl_cordz_functions.a  /usr/local/lib/libabsl_cordz_handle.a  /usr/local/lib/libabsl_cordz_info.a  /usr/local/lib/libabsl_cordz_sample_token.a  /usr/local/lib/libabsl_debugging_internal.a  /usr/local/lib/libabsl_demangle_internal.a  /usr/local/lib/libabsl_examine_stack.a  /usr/local/lib/libabsl_exponential_biased.a  /usr/local/lib/libabsl_failure_signal_handler.a  /usr/local/lib/libabsl_flags.a  /usr/local/lib/libabsl_flags_commandlineflag.a  /usr/local/lib/libabsl_flags_commandlineflag_internal.a  /usr/local/lib/libabsl_flags_config.a  /usr/local/lib/libabsl_flags_internal.a  /usr/local/lib/libabsl_flags_marshalling.a  /usr/local/lib/libabsl_flags_parse.a  /usr/local/lib/libabsl_flags_private_handle_accessor.a  /usr/local/lib/libabsl_flags_program_name.a  /usr/local/lib/libabsl_flags_reflection.a  /usr/local/lib/libabsl_flags_usage.a  /usr/local/lib/libabsl_flags_usage_internal.a  /usr/local/lib/libabsl_graphcycles_internal.a  /usr/local/lib/libabsl_hash.a  /usr/local/lib/libabsl_hashtablez_sampler.a  /usr/local/lib/libabsl_int128.a  /usr/local/lib/libabsl_leak_check.a  /usr/local/lib/libabsl_leak_check_disable.a  /usr/local/lib/libabsl_log_severity.a  /usr/local/lib/libabsl_low_level_hash.a  /usr/local/lib/libabsl_malloc_internal.a  /usr/local/lib/libabsl_periodic_sampler.a  /usr/local/lib/libabsl_random_distributions.a  /usr/local/lib/libabsl_random_internal_distribution_test_util.a  /usr/local/lib/libabsl_random_internal_platform.a  /usr/local/lib/libabsl_random_internal_pool_urbg.a  /usr/local/lib/libabsl_random_internal_randen.a  /usr/local/lib/libabsl_random_internal_randen_hwaes.a  /usr/local/lib/libabsl_random_internal_randen_hwaes_impl.a  /usr/local/lib/libabsl_random_internal_randen_slow.a  /usr/local/lib/libabsl_random_internal_seed_material.a  /usr/local/lib/libabsl_random_seed_gen_exception.a  /usr/local/lib/libabsl_random_seed_sequences.a  /usr/local/lib/libabsl_raw_hash_set.a  /usr/local/lib/libabsl_raw_logging_internal.a  /usr/local/lib/libabsl_scoped_set_env.a  /usr/local/lib/libabsl_spinlock_wait.a  /usr/local/lib/libabsl_stacktrace.a  /usr/local/lib/libabsl_status.a  /usr/local/lib/libabsl_statusor.a  /usr/local/lib/libabsl_str_format_internal.a  /usr/local/lib/libabsl_strerror.a  /usr/local/lib/libabsl_strings.a  /usr/local/lib/libabsl_strings_internal.a  /usr/local/lib/libabsl_symbolize.a  /usr/local/lib/libabsl_synchronization.a  /usr/local/lib/libabsl_throw_delegate.a  /usr/local/lib/libabsl_time.a  /usr/local/lib/libabsl_time_zone.a  ../../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow//libtensorflow_cc.so  ../../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow//libtensorflow_framework.so.2  ../../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow//lite/libtensorflowlite.so && :/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `ReadTensorFromImageFile(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, int, float, float, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*)':main.cpp:(.text+0x2b7b): undefined reference to `tensorflow::str_util::EndsWith(absl::lts_20211102::string_view, absl::lts_20211102::string_view)'/usr/bin/ld: main.cpp:(.text+0x2bc1): undefined reference to `tensorflow::str_util::EndsWith(absl::lts_20211102::string_view, absl::lts_20211102::string_view)'/usr/bin/ld: main.cpp:(.text+0x2bf3): undefined reference to `tensorflow::str_util::EndsWith(absl::lts_20211102::string_view, absl::lts_20211102::string_view)'/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `tensorflow::Status tensorflow::errors::NotFound<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*>(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*)':main.cpp:(.text._ZN10tensorflow6errors8NotFoundIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_EEENS_6StatusEDpT_[_ZN10tensorflow6errors8NotFoundIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_EEENS_6StatusEDpT_]+0x3b4): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_20211102::string_view)'/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `tensorflow::Status tensorflow::errors::DataLoss<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*, unsigned long, char const*, unsigned long>(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*, unsigned long, char const*, unsigned long)':main.cpp:(.text._ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_[_ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_]+0x96b): undefined reference to `tensorflow::strings::internal::CatPieces[abi:cxx11](std::initializer_list<absl::lts_20211102::string_view>)'/usr/bin/ld: main.cpp:(.text._ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_[_ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_]+0x98a): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_20211102::string_view)'/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `main':main.cpp:(.text.startup+0x6ab): undefined reference to `tensorflow::io::internal::JoinPathImpl[abi:cxx11](std::initializer_list<absl::lts_20211102::string_view>)'/usr/bin/ld: main.cpp:(.text.startup+0x87f): undefined reference to `tensorflow::io::internal::JoinPathImpl[abi:cxx11](std::initializer_list<absl::lts_20211102::string_view>)'collect2: error: ld returned 1 exit statusninja: build stopped: subcommand failed.```### Standalone code to reproduce the issue```shellif (tensorflow::str_util::EndsWith(tensorflow::StringPiece(file_name.c_str(), file_name.size()),                                       tensorflow::StringPiece("".png"", std::string("".png"").size()))) {        image_reader = DecodePng(root.WithOpName(""png_reader""), file_reader,                                 DecodePng::Channels(wanted_channels));    } else if (tensorflow::str_util::EndsWith(tensorflow::StringPiece(file_name.c_str()),                                              tensorflow::StringPiece("".gif""))) {        // gif decoder returns 4-D tensor, remove the first dim        image_reader =                Squeeze(root.WithOpName(""squeeze_first_dim""),                        DecodeGif(root.WithOpName(""gif_reader""), file_reader));    } else if (tensorflow::str_util::EndsWith(tensorflow::StringPiece(file_name.c_str()),                                              tensorflow::StringPiece("".bmp""))) {        image_reader = DecodeBmp(root.WithOpName(""bmp_reader""), file_reader);    } else {        // Assume if it's neither a PNG nor a GIF then it must be a JPEG.        image_reader = DecodeJpeg(root.WithOpName(""jpeg_reader""), file_reader,                                  DecodeJpeg::Channels(wanted_channels));    }```### Relevant log output```shell[1/1] Linking CXX executable test_tensorflowFAILED: test_tensorflow : && /usr/bin/c++ -O3 -DNDEBUG  CMakeFiles/test_tensorflow.dir/main.cpp.o -o test_tensorflow -L/home/nkk/nkk/nkk/work/zht/src/../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow   -L/usr/local/lib -Wl,-rpath,/home/nkk/nkk/nkk/work/zht/src/../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow:/usr/local/lib:/home/nkk/nkk/nkk/work/zht/src/../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow/lite  /usr/local/lib/libabsl_bad_any_cast_impl.a  /usr/local/lib/libabsl_bad_optional_access.a  /usr/local/lib/libabsl_bad_variant_access.a  /usr/local/lib/libabsl_base.a  /usr/local/lib/libabsl_city.a  /usr/local/lib/libabsl_civil_time.a  /usr/local/lib/libabsl_cord.a  /usr/local/lib/libabsl_cord_internal.a  /usr/local/lib/libabsl_cordz_functions.a  /usr/local/lib/libabsl_cordz_handle.a  /usr/local/lib/libabsl_cordz_info.a  /usr/local/lib/libabsl_cordz_sample_token.a  /usr/local/lib/libabsl_debugging_internal.a  /usr/local/lib/libabsl_demangle_internal.a  /usr/local/lib/libabsl_examine_stack.a  /usr/local/lib/libabsl_exponential_biased.a  /usr/local/lib/libabsl_failure_signal_handler.a  /usr/local/lib/libabsl_flags.a  /usr/local/lib/libabsl_flags_commandlineflag.a  /usr/local/lib/libabsl_flags_commandlineflag_internal.a  /usr/local/lib/libabsl_flags_config.a  /usr/local/lib/libabsl_flags_internal.a  /usr/local/lib/libabsl_flags_marshalling.a  /usr/local/lib/libabsl_flags_parse.a  /usr/local/lib/libabsl_flags_private_handle_accessor.a  /usr/local/lib/libabsl_flags_program_name.a  /usr/local/lib/libabsl_flags_reflection.a  /usr/local/lib/libabsl_flags_usage.a  /usr/local/lib/libabsl_flags_usage_internal.a  /usr/local/lib/libabsl_graphcycles_internal.a  /usr/local/lib/libabsl_hash.a  /usr/local/lib/libabsl_hashtablez_sampler.a  /usr/local/lib/libabsl_int128.a  /usr/local/lib/libabsl_leak_check.a  /usr/local/lib/libabsl_leak_check_disable.a  /usr/local/lib/libabsl_log_severity.a  /usr/local/lib/libabsl_low_level_hash.a  /usr/local/lib/libabsl_malloc_internal.a  /usr/local/lib/libabsl_periodic_sampler.a  /usr/local/lib/libabsl_random_distributions.a  /usr/local/lib/libabsl_random_internal_distribution_test_util.a  /usr/local/lib/libabsl_random_internal_platform.a  /usr/local/lib/libabsl_random_internal_pool_urbg.a  /usr/local/lib/libabsl_random_internal_randen.a  /usr/local/lib/libabsl_random_internal_randen_hwaes.a  /usr/local/lib/libabsl_random_internal_randen_hwaes_impl.a  /usr/local/lib/libabsl_random_internal_randen_slow.a  /usr/local/lib/libabsl_random_internal_seed_material.a  /usr/local/lib/libabsl_random_seed_gen_exception.a  /usr/local/lib/libabsl_random_seed_sequences.a  /usr/local/lib/libabsl_raw_hash_set.a  /usr/local/lib/libabsl_raw_logging_internal.a  /usr/local/lib/libabsl_scoped_set_env.a  /usr/local/lib/libabsl_spinlock_wait.a  /usr/local/lib/libabsl_stacktrace.a  /usr/local/lib/libabsl_status.a  /usr/local/lib/libabsl_statusor.a  /usr/local/lib/libabsl_str_format_internal.a  /usr/local/lib/libabsl_strerror.a  /usr/local/lib/libabsl_strings.a  /usr/local/lib/libabsl_strings_internal.a  /usr/local/lib/libabsl_symbolize.a  /usr/local/lib/libabsl_synchronization.a  /usr/local/lib/libabsl_throw_delegate.a  /usr/local/lib/libabsl_time.a  /usr/local/lib/libabsl_time_zone.a  ../../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow//libtensorflow_cc.so  ../../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow//libtensorflow_framework.so.2  ../../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow//lite/libtensorflowlite.so && :/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `ReadTensorFromImageFile(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, int, float, float, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*)':main.cpp:(.text+0x2b7b): undefined reference to `tensorflow::str_util::EndsWith(absl::lts_20211102::string_view, absl::lts_20211102::string_view)'/usr/bin/ld: main.cpp:(.text+0x2bc1): undefined reference to `tensorflow::str_util::EndsWith(absl::lts_20211102::string_view, absl::lts_20211102::string_view)'/usr/bin/ld: main.cpp:(.text+0x2bf3): undefined reference to `tensorflow::str_util::EndsWith(absl::lts_20211102::string_view, absl::lts_20211102::string_view)'/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `tensorflow::Status tensorflow::errors::NotFound<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*>(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*)':main.cpp:(.text._ZN10tensorflow6errors8NotFoundIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_EEENS_6StatusEDpT_[_ZN10tensorflow6errors8NotFoundIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_EEENS_6StatusEDpT_]+0x3b4): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_20211102::string_view)'/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `tensorflow::Status tensorflow::errors::DataLoss<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*, unsigned long, char const*, unsigned long>(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*, unsigned long, char const*, unsigned long)':main.cpp:(.text._ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_[_ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_]+0x96b): undefined reference to `tensorflow::strings::internal::CatPieces[abi:cxx11](std::initializer_list<absl::lts_20211102::string_view>)'/usr/bin/ld: main.cpp:(.text._ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_[_ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_]+0x98a): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_20211102::string_view)'/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `main':main.cpp:(.text.startup+0x6ab): undefined reference to `tensorflow::io::internal::JoinPathImpl[abi:cxx11](std::initializer_list<absl::lts_20211102::string_view>)'/usr/bin/ld: main.cpp:(.text.startup+0x87f): undefined reference to `tensorflow::io::internal::JoinPathImpl[abi:cxx11](std::initializer_list<absl::lts_20211102::string_view>)'collect2: error: ld returned 1 exit statusninja: build stopped: subcommand failed.```</details>
"
56755,0,0,0,0,0,Wyverald,0,"title:TF Windows build failing on Bazel CI with Bazel HEAD description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versionlast_green### Custom CodeNo### OS Platform and DistributionWindows### Mobile device_No response_### Python version_No response_### Bazel versionHEAD### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?Build breaks (see error log at https://buildkite.com/bazel/bazel-at-head-plus-downstream/builds/2547#0181f78a-6047-4ece-9101-20a934b42e61)### Standalone code to reproduce the issue```shellN/A```### Relevant log output```shelltensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(434): error C2672: 'llvm::formatv': no matching overloaded function foundtensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(435): error C2893: Failed to specialize function template 'llvm::formatv_object<unknown-type> llvm::formatv(const char *,Ts &&...)'external/llvm-project/llvm/include\llvm/Support/FormatVariadic.h(251): note: see declaration of 'llvm::formatv'tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(435): note: With the following template arguments:tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(435): note: 'Ts={const absl::lts_20211102::string_view &, size_t}'tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(433): error C2672: 'tensorflow::errors::InvalidArgument': no matching overloaded function foundtensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(486): error C2672: 'mlir::Operation::getAttrOfType': no matching overloaded function foundtensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(487): error C2440: 'initializing': cannot convert from 'const absl::lts_20211102::string_view' to 'mlir::StringAttr'tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(487): note: No user-defined-conversion operator available that can perform this conversion, or the operator cannot be calledtensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(502): error C2672: 'mlir::Operation::getAttrOfType': no matching overloaded function foundtensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(502): error C2440: 'initializing': cannot convert from 'const absl::lts_20211102::string_view' to 'mlir::StringAttr'tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(502): note: No user-defined-conversion operator available that can perform this conversion, or the operator cannot be calledtensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(503): error C3536: 'topology_attr': cannot be used before it is initializedtensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(506): error C2672: 'mlir::Operation::getAttrOfType': no matching overloaded function foundtensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(507): error C2440: 'initializing': cannot convert from 'const absl::lts_20211102::string_view' to 'mlir::StringAttr'tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(507): note: No user-defined-conversion operator available that can perform this conversion, or the operator cannot be calledtensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(508): error C3536: 'device_assignment_attr': cannot be used before it is initializedtensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(509): error C2672: 'llvm::formatv': no matching overloaded function foundtensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(510): error C2893: Failed to specialize function template 'llvm::formatv_object<unknown-type> llvm::formatv(const char *,Ts &&...)'external/llvm-project/llvm/include\llvm/Support/FormatVariadic.h(251): note: see declaration of 'llvm::formatv'tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(510): note: With the following template arguments:tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(510): note: 'Ts={const absl::lts_20211102::string_view &}'tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(514): error C2664: 'tensorflow::StatusOr<llvm::SmallVector<int64_t,8>> tensorflow::GetDeviceCoordinates(mlir::ArrayAttr)': cannot convert argument 1 from 'int' to 'mlir::ArrayAttr'tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(514): note: No constructor could take the source type, or constructor overload resolution was ambiguoustensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(423): note: see declaration of 'tensorflow::GetDeviceCoordinates'tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(516): error C3536: 'status_or_device_coodinates': cannot be used before it is initializedtensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(523): error C2660: 'tensorflow::GetTPUCompilationAndExecutionDevices': function does not take 3 argumentstensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(444): note: see declaration of 'tensorflow::GetTPUCompilationAndExecutionDevices'tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(527): error C3536: 'status_or_tpu_device_assignment': cannot be used before it is initializedtensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(531): error C2530: 'tpu_device_assignment': references must be initializedtensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(533): error C3536: 'tpu_device_assignment': cannot be used before it is initializedTarget //tensorflow/tools/pip_package:build_pip_package failed to build```</details>
"
56748,1,2722,10,0,0,TWTcodeKing,0,"title:some numpy operation(FFT,FFT2d,RFFT,RFFT2d) failed when building a model  description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.2### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 18.04### Mobile device_No response_### Python version3.7### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version11.0/8.0### GPU model and memoryNVIDIA A6000 /48G### Current Behaviour?```shellI want to use API ""tf.signal.rfft2d"" to create a model, and when I set the shape of the input tensor as (None,None,3), i.e., dynamic shape, tf.signal.rfft2d produce something abnormal. The shape of the output of tf.signal.rfft2d will be [None,None,None,None]. This won't happen when the shape of the input tensor is a constant, e.g.,(1024,1024,3), but this constrains the flexibility of the model. I've tried to use tf.keras.layers.Lambda and tf.raw_ops.RFFT2d but all failed. I also spotted that even if the model can successfully be complied with the input shape as [None,None,3], it failed to produce real value when accepting real data. I'm wondering whether it's because tf.signal.rfft2d only accept tensor or numpy object whose shape has a real value instead of None. Who has met this problem before?```### Standalone code to reproduce the issue```shellthis is my code:def ResBLock_do_fft_bench(inputs,filters):    axis = 1    x = K.permute_dimensions(inputs,(0,3,1,2))    x = tf.keras.layers.Lambda(lambda v:tf.raw_ops.RFFT2D(input=v,fft_length=(tf.shape(v)[2],tf.shape(v)[3]//2+1)))(x)    y_imag = tf.compat.v1.imag(x)    y_real = tf.compat.v1.real(x)    y_f = tf.concat([y_real, y_imag], axis=axis)    y_f = tf.keras.layers.Permute((2,3,1))(y_f)    y = BasicConv_do(y_f,filters*2,kernel_size=1,stride=1,relu=True)    y = BasicConv_do(y,filters*2,kernel_size=1,stride=1,relu=False)    y = tf.keras.layers.Permute((3,1,2))(y)    y_real, y_imag = tf.split(y, 2, axis=axis)    y = tf.complex(y_real, y_imag)    y = tf.keras.layers.Lambda(lambda v:tf.raw_ops.IRFFT2D(input=v,fft_length=(-1,-1)))(y)    y = tf.keras.layers.Permute((2,3,1))(y)    res = BasicConv_do(inputs,filters,kernel_size=3,stride=1,relu=True)    res = BasicConv_do(res,filters,kernel_size=3,stride=1,relu=False)    return res + inputs + ydef FFT_Block():    input = tf.keras.layers.Input(shape = [None,None,32],name='blur_image')    out = ResBLock_do_fft_bench(input,32)    model = tf.keras.models.Model(inputs=input,outputs=out,name='test')    return model```### Relevant log output```shellthe shape of the x before fft is:(None, 32, None, None)after fft:(None, 32, None, None).When the model takes a real value,this happens:Traceback (most recent call last):  File ""D:\Anaconda\envs\AIM\lib\site-packages\tensorflow\python\ops\gen_spectral_ops.py"", line 906, in irfft2d    tld.op_callbacks, input, fft_length, ""Treal"", Treal)tensorflow.python.eager.core._FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.if I change tf.raw_ops.RFFT2D to tf.signal.rfft2d, there will be another bug:the shape of the x before fft:(None, 32, None, None)after fft:(None, None, None, None)```</details>
"
56737,1,997,0,0,0,eeDigitalSeal,0,"title:Bug in `tf.autodiff.ForwardAccumulator` for `tf.signal.mdct` description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.9### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellGradient in forward mode is inconsistent with that in backward mode for API `tf.signal.mdct````### Standalone code to reproduce the issue```shellimport tensorflow as tfsignals = tf.constant([3.3425903], dtype=tf.float32)frame_length = 49152with tf.GradientTape(persistent=True,) as g:      g.watch(signals)      res = tf.reduce_sum(tf.signal.mdct(signals, frame_length, window_fn=None, pad_end=True, norm=None))      print(res)print(""gradient in reverse mode: "", g.gradient(res, signals))with tf.autodiff.ForwardAccumulator(signals,tf.constant([1.],shape=[1],dtype=tf.float32)) as acc:        res = tf.reduce_sum(tf.signal.mdct(signals, frame_length, window_fn=None, pad_end=True, norm=None, ))        print(res)print(""gradient in forward mode: "", acc.jvp(res))```### Relevant log output```shelltf.Tensor(11.523682, shape=(), dtype=float32)gradient in reverse mode:  tf.Tensor([1.0006392], shape=(1,), dtype=float32)tf.Tensor(11.523682, shape=(), dtype=float32)gradient in forward mode:  tf.Tensor(3.4477844, shape=(), dtype=float32)```</details>
"
56732,0,727,2,0,0,Bahar-BM,0,"title:openCL delegate generates '0' and 'inf' values with reduce_sum description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.8, tf 2.9### Custom CodeNo### OS Platform and DistributionAndroid### Mobile devicetested on Snapdragon 888, 865### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellMy models with 'tf.math.reduce_sum' ('Sum' in the tflite version) nodes generate wrong results with the openCL delegate. My experiments show that the output of a 'Sum' node in a tflite model contains lots of 'zeros' and 'inf' values when we use openCL delegate. This issue does not happen with other delegates like XNNPACK.```### Standalone code to reproduce the issue```shellHere is a very simple model with a reduce_sum node in the structure:x0 =  Input(shape=(23, 512))x1 = tf.math.reduce_sum(x0, axis=-1)model = Model([x0], [x1], name='test')model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])model.save('reduce_sum.h5')The tflite version of this model generates zeros and inf values with the openCL delegate.```### Relevant log output_No response_</details>
"
56713,0,32158,269,0,0,elfringham,0,"title:Numpy 1.23.0 causes unit test failures description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiongit HEAD### Custom CodeNo### OS Platform and DistributionCentOS 7### Mobile devicen/a### Python version3.8.10### Bazel version5.1.1### GCC/Compiler version10.2.1### CUDA/cuDNN versionn/a### GPU model and memoryn/a### Current Behaviour?```shellThe latest release of numpy ie 1.23.0 is now causing unit test failures. See https://github.com/tensorflow/tensorflow/actions/runs/2547650056```### Standalone code to reproduce the issue```shellpython -m pip install numpy==1.23.0bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=1 --test_output=all --cache_test_results=no --noremote_accept_cached --config=nonccl --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-requires-gpu --verbose_failures --build_tests_only -- //tensorflow/python/kernel_tests/control_flow:scan_ops_test_cpu //tensorflow/python/kernel_tests/array_ops:pad_op_test_cpu //tensorflow/python/kernel_tests/array_ops:concat_op_test_cpu //tensorflow/python/kernel_tests/array_ops:array_ops_test_cpu //tensorflow/python/kernel_tests/array_ops:pad_op_test_gpu //tensorflow/python/kernel_tests/array_ops:concat_op_test_gpu //tensorflow/python/kernel_tests/array_ops:split_op_test_cpu //tensorflow/python/kernel_tests/array_ops:array_ops_test_gpu //tensorflow/python/kernel_tests/array_ops:split_op_test_gpu //tensorflow/python/kernel_tests/array_ops:slice_op_test_gpu //tensorflow/python/kernel_tests/array_ops:slice_op_test_cpu //tensorflow/python/kernel_tests/control_flow:scan_ops_test_gpu```### Relevant log output```shellSample faliure======================================================================ERROR: test1D (__main__.CumprodTest)CumprodTest.test1D----------------------------------------------------------------------Traceback (most recent call last):  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated    return f(self, *args, **kwargs)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 251, in test1D    self._compareAll(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 219, in _compareAll    self._compare(x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 210, in _compare    np_out = handle_options(np.cumprod, x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options    x = numpy_reverse(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse    return x[ix]IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices======================================================================ERROR: test2D (__main__.CumprodTest)CumprodTest.test2D----------------------------------------------------------------------Traceback (most recent call last):  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated    return f(self, *args, **kwargs)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 258, in test2D    self._compareAll(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 219, in _compareAll    self._compare(x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 210, in _compare    np_out = handle_options(np.cumprod, x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options    x = numpy_reverse(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse    return x[ix]IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices======================================================================ERROR: test3D (__main__.CumprodTest)CumprodTest.test3D----------------------------------------------------------------------Traceback (most recent call last):  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated    return f(self, *args, **kwargs)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 265, in test3D    self._compareAll(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 219, in _compareAll    self._compare(x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 210, in _compare    np_out = handle_options(np.cumprod, x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options    x = numpy_reverse(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse    return x[ix]IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices======================================================================ERROR: test6D (__main__.CumprodTest)CumprodTest.test6D----------------------------------------------------------------------Traceback (most recent call last):  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated    return f(self, *args, **kwargs)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 272, in test6D    self._compareAll(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 219, in _compareAll    self._compare(x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 210, in _compare    np_out = handle_options(np.cumprod, x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options    x = numpy_reverse(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse    return x[ix]IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices======================================================================ERROR: testEmpty (__main__.CumprodTest)CumprodTest.testEmpty----------------------------------------------------------------------Traceback (most recent call last):  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated    return f(self, *args, **kwargs)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 226, in testEmpty    self._compareAll(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 219, in _compareAll    self._compare(x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 210, in _compare    np_out = handle_options(np.cumprod, x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options    x = numpy_reverse(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse    return x[ix]IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices======================================================================ERROR: testNaN (__main__.CumprodTest)CumprodTest.testNaN----------------------------------------------------------------------Traceback (most recent call last):  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated    return f(self, *args, **kwargs)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 244, in testNaN    self._compareAll(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 219, in _compareAll    self._compare(x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 210, in _compare    np_out = handle_options(np.cumprod, x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options    x = numpy_reverse(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse    return x[ix]IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices======================================================================ERROR: test1D (__main__.CumsumTest)CumsumTest.test1D----------------------------------------------------------------------Traceback (most recent call last):  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated    return f(self, *args, **kwargs)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 118, in test1D    self._compareAll(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 86, in _compareAll    self._compare(x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 77, in _compare    np_out = handle_options(np.cumsum, x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options    x = numpy_reverse(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse    return x[ix]IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices======================================================================ERROR: test2D (__main__.CumsumTest)CumsumTest.test2D----------------------------------------------------------------------Traceback (most recent call last):  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated    return f(self, *args, **kwargs)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 125, in test2D    self._compareAll(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 86, in _compareAll    self._compare(x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 77, in _compare    np_out = handle_options(np.cumsum, x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options    x = numpy_reverse(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse    return x[ix]IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices======================================================================ERROR: test3D (__main__.CumsumTest)CumsumTest.test3D----------------------------------------------------------------------Traceback (most recent call last):  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated    return f(self, *args, **kwargs)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 132, in test3D    self._compareAll(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 86, in _compareAll    self._compare(x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 77, in _compare    np_out = handle_options(np.cumsum, x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options    x = numpy_reverse(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse    return x[ix]IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices======================================================================ERROR: test6D (__main__.CumsumTest)CumsumTest.test6D----------------------------------------------------------------------Traceback (most recent call last):  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated    return f(self, *args, **kwargs)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 139, in test6D    self._compareAll(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 86, in _compareAll    self._compare(x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 77, in _compare    np_out = handle_options(np.cumsum, x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options    x = numpy_reverse(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse    return x[ix]IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices======================================================================ERROR: testEmpty (__main__.CumsumTest)CumsumTest.testEmpty----------------------------------------------------------------------Traceback (most recent call last):  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated    return f(self, *args, **kwargs)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 93, in testEmpty    self._compareAll(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 86, in _compareAll    self._compare(x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 77, in _compare    np_out = handle_options(np.cumsum, x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options    x = numpy_reverse(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse    return x[ix]IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices======================================================================ERROR: testLarge (__main__.CumsumTest)CumsumTest.testLarge----------------------------------------------------------------------Traceback (most recent call last):  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated    return f(self, *args, **kwargs)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2157, in decorated    return func(self, *args, **kwargs)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 146, in testLarge    self._compareAll(x, 0)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 86, in _compareAll    self._compare(x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 77, in _compare    np_out = handle_options(np.cumsum, x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options    x = numpy_reverse(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse    return x[ix]IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices======================================================================ERROR: testNaN (__main__.CumsumTest)CumsumTest.testNaN----------------------------------------------------------------------Traceback (most recent call last):  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated    return f(self, *args, **kwargs)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 111, in testNaN    self._compareAll(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 86, in _compareAll    self._compare(x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 77, in _compare    np_out = handle_options(np.cumsum, x, axis, exclusive, reverse)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options    x = numpy_reverse(x, axis)  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse    return x[ix]IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices----------------------------------------------------------------------Ran 29 tests in 4.799sFAILED (errors=13, skipped=2)================================================================================```</details>
"
56710,1,2212,0,0,0,Poiu19,0,"title:Greater(nextafter(x), x) returns False description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.9.1### Custom CodeNo### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version3.8### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellGreater(nextafter(x), x) returns False```### Standalone code to reproduce the issue```shellimport tensorflow as tfimport numpy as npinput = tf.constant([0.0, 0.25, np.finfo(tf.float32.as_numpy_dtype).tiny], dtype=tf.float32)nextafter = tf.math.nextafter(input, tf.dtypes.as_dtype(tf.float32).max)greater = tf.math.greater(nextafter, input)print(""input"", input)print(""nextafter"", nextafter)print(""greater"", greater)assert all(greater)```### Relevant log output```shellinput tf.Tensor([0.0000000e+00 2.5000000e-01 1.1754944e-38], shape=(3,), dtype=float32)nextafter tf.Tensor([1.4012985e-45 2.5000003e-01 1.1754945e-38], shape=(3,), dtype=float32)greater tf.Tensor([False  True  True], shape=(3,), dtype=bool)collected 0 items / 1 errors                                                                                                                                                                                                 =========================================================================================================== ERRORS ===========================================================================================================_______________________________________________________________________________________________________ ERROR repro.py _______________________________________________________________________________________________________repro.py:12: in <module>    assert all(greater)E   assert FalseE    +  where False = all(<tf.Tensor: shape=(3,), dtype=bool, numpy=array([False,  True,  True])>)================================================================================================== short test summary info ===================================================================================================ERROR repro.py - assert False!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 errors during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!================================================================================================== 1 error in 5.84 seconds ===================================================================================================```</details>
"
56700,1,2176,139,0,0,balvisio,0,"title:Saving and Loading a Dataset returns a different elements description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.9.1### Custom CodeNo### OS Platform and DistributionMacOS Monterey### Mobile device_No response_### Python version3.7### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?I am trying to save and subsequently load a dataset. I am observing that after loading the dataset the elements of it are many times different from the saved one.In order to run the code I have two text files in the following structure with contents:```shelltest |-neg->file.txt: Once again Mr. Costner has dragged out a movie |-pos->file.txt: I went and saw this movie last ```I am expecting that the two loops should print an identical Tensor.### Standalone code to reproduce the issue```pythonimport tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras import layersimport pickledef create_int_ds(test_ds):    max_sequence_length = 10    max_tokens = 20000    text_vectorization = layers.TextVectorization(        max_tokens=max_tokens,        output_mode=""int"",        output_sequence_length=max_sequence_length,    )        text_only_train_ds = test_ds.map(lambda x, y: x)    text_vectorization.adapt(text_only_train_ds)    int_test_ds = test_ds.map(        lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)    return int_test_dsbatch_size = 1test_ds = keras.utils.text_dataset_from_directory(""test"", batch_size=batch_size)int_test_ds = create_int_ds(test_ds)take_test_ds = int_test_ds.take(1)print(f""type of dataset: {type(take_test_ds)}"")print(f""spec: {take_test_ds.element_spec}"")for e in take_test_ds:    print(e)# Savetf.data.experimental.save(take_test_ds, ""int_test_ds"")with open('element_spec', 'wb') as out_:  # also save the element_spec to disk for future loading    pickle.dump(take_test_ds.element_spec, out_)# Loadwith open('element_spec', 'rb') as in_:    es = pickle.load(in_)new_take_test_ds = tf.data.experimental.load(""int_test_ds"", element_spec=es)print(type(new_take_test_ds)) for e in new_take_test_ds:    print(e) # Shoud print the same as the previous for loop```### Relevant log output```type of dataset: <class 'tensorflow.python.data.ops.dataset_ops.TakeDataset'>spec: (TensorSpec(shape=(None, 10), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))(<tf.Tensor: shape=(1, 10), dtype=int64, numpy=array([[105, 183, 111,  16, 138, 159,  41,  10,   8, 147]])>, <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>)<class 'tensorflow.python.data.experimental.ops.io._LoadDataset'>(<tf.Tensor: shape=(1, 10), dtype=int64, numpy=array([[  4,  64,   7,  39,  11,   8, 121, 108, 184, 175]])>, <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>)```
"
56694,0,540,0,0,0,VictoriaGriffith,0,"title:`tf.image.rgb_to_hsv` fails on back prop when dtype is double description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.9### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellCannot compute the gradient for `tf.image.rgb_to_hsv` with `float64` inputs. It is working fine with forward pass.```### Standalone code to reproduce the issue```shellimport tensorflow as tfimages = tf.random.uniform([5, 5, 3], dtype=tf.float64)x = tf.image.rgb_to_hsv(images) # passwith tf.GradientTape(persistent=True,) as g:  g.watch(images)  x = tf.image.rgb_to_hsv(images, )grad = g.gradient(x, images) # InvalidArgumentError```### Relevant log output```shellInvalidArgumentError: cannot compute Mul as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:Mul]```</details>
"
56690,1,328,94,0,0,GF-Huang,0,"title:TF 2.9.1 GPU not detected description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow VersionTF 2.9.1### Custom CodeNo### OS Platform and DistributionWin10 21H2 x64### Mobile device_No response_### Python version3.9.13### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versioncudatoolkit=11.2.2 cudnn=8.1.0.77### GPU model and memory_No response_### Current Behaviour?```shellI install TF 2.9.1 by:conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0python3 -m pip install tensorflow# Verify install:python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""```TF did not detect GPU.```### Standalone code to reproduce the issue```shell`python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""` output empty.```### Relevant log output_No response_</details>
"
56689,1,925,0,0,0,maidang-ee,0,"title:disable_resource_variables is deprecated description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.6.0### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 16.04### Mobile device_No response_### Python version3.8.10### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellA bug happened!I was running my code normally but then all of sudden I received a warning that disturbing my code from compiling and my program completely stop T.TMy source code was written in tf v1 and I've been running it on tf v2 so apparently I need 'tf.disable_resource_variables()' for my programme to work.```### Standalone code to reproduce the issue```shellI received this WARNING. ""WARNING:tensorflow:From /home/mdee/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.Instructions for updating:non-resource variables are not supported in the long term""This is exactly what I received and I read from the tensorflow.org website that if my code needs tf.disable_resource_variables() to be called to work properly, I should file a bug.Please let me know how can I run my code smoothly again.```### Relevant log output_No response_</details>
"
56687,1,13898,42,0,0,enor2017,0,"title:python/kernal_tests/linalg/tridiagonal_solve_op_test.py test fail description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.9.1### Custom CodeNo### OS Platform and DistributionUbuntu 18.04.5 LTS (on colab)### Mobile deviceN/A### Python version3.7.13### Bazel versionN/A### GCC/Compiler versionN/A### CUDA/cuDNN versionCUDA V11.1.105### GPU model and memory_No response_### Current Behaviour?```shell6 tests in python/kernel_tests/linalg/tridiagonal_solve_op_test.py fail, the error is:AttributeError: 'TridiagonalSolveOpTest' object has no attribute 'pivoting'.```### Standalone code to reproduce the issue```shellUse pytest to run the test.`pytest tridiagonal_solve_op_test.py`Or, please find gist here: https://colab.research.google.com/drive/1un1vsyolGCLz6DJLvnR3FHOSRH_i_0id?usp=sharing```### Relevant log output```shell============================= test session starts ==============================platform linux -- Python 3.7.13, pytest-3.6.4, py-1.11.0, pluggy-0.7.1rootdir: /content, inifile:plugins: typeguard-2.7.1collected 49 items                                                             tridiagonal_solve_op_test.py ..............FFFF...............F......F.. [ 87%].....s                                                                   [100%]=================================== FAILURES ===================================____________ TridiagonalSolveOpTest.testCompactFormatAllDimsUnknown ____________self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testCompactFormatAllDimsUnknown>args = (), kwargs = {}    def decorated(self, *args, **kwargs):      if context.executing_eagerly():        with context.graph_mode():>         return f(self, *args, **kwargs)/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/test_util.py:1625: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ tridiagonal_solve_op_test.py:570: in testCompactFormatAllDimsUnknown    expected=_sample_result)_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testCompactFormatAllDimsUnknown>diags_shape = [None, None], rhs_shape = [None]diags_feed = array([[ 2,  1,  4,  0],       [ 1,  3,  2,  2],       [ 0,  1, -1,  1]])rhs_feed = array([1, 2, 3, 4]), expected = array([-9,  5, -4,  4])diags_format = 'compact'    def _testWithPlaceholders(self,                              diags_shape,                              rhs_shape,                              diags_feed,                              rhs_feed,                              expected,                              diags_format=""compact""):      if context.executing_eagerly():        return      diags = array_ops.placeholder(dtypes.float64, shape=diags_shape)      rhs = array_ops.placeholder(dtypes.float64, shape=rhs_shape)      if test_util.is_xla_enabled() and self.pivoting:        # Pivoting is not supported by xla backends.        return      x = linalg_impl.tridiagonal_solve(>         diags, rhs, diags_format, partial_pivoting=self.pivoting)E     AttributeError: 'TridiagonalSolveOpTest' object has no attribute 'pivoting'tridiagonal_solve_op_test.py:558: AttributeError___________ TridiagonalSolveOpTest.testCompactFormatUnknownBatchSize ___________self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testCompactFormatUnknownBatchSize>args = (), kwargs = {}    def decorated(self, *args, **kwargs):      if context.executing_eagerly():        with context.graph_mode():>         return f(self, *args, **kwargs)/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/test_util.py:1625: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ tridiagonal_solve_op_test.py:597: in testCompactFormatUnknownBatchSize    expected=np.array([_sample_result, -2 * _sample_result]))_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testCompactFormatUnknownBatchSize>diags_shape = [None, 3, 4], rhs_shape = [None, 4]diags_feed = array([[[ 2,  1,  4,  0],        [ 1,  3,  2,  2],        [ 0,  1, -1,  1]],       [[-2, -1, -4,  0],        [-1, -3, -2, -2],        [ 0, -1,  1, -1]]])rhs_feed = array([[1, 2, 3, 4],       [2, 4, 6, 8]])expected = array([[ -9,   5,  -4,   4],       [ 18, -10,   8,  -8]])diags_format = 'compact'    def _testWithPlaceholders(self,                              diags_shape,                              rhs_shape,                              diags_feed,                              rhs_feed,                              expected,                              diags_format=""compact""):      if context.executing_eagerly():        return      diags = array_ops.placeholder(dtypes.float64, shape=diags_shape)      rhs = array_ops.placeholder(dtypes.float64, shape=rhs_shape)      if test_util.is_xla_enabled() and self.pivoting:        # Pivoting is not supported by xla backends.        return      x = linalg_impl.tridiagonal_solve(>         diags, rhs, diags_format, partial_pivoting=self.pivoting)E     AttributeError: 'TridiagonalSolveOpTest' object has no attribute 'pivoting'tridiagonal_solve_op_test.py:558: AttributeError__________ TridiagonalSolveOpTest.testCompactFormatUnknownMatrixSize ___________self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testCompactFormatUnknownMatrixSize>args = (), kwargs = {}    def decorated(self, *args, **kwargs):      if context.executing_eagerly():        with context.graph_mode():>         return f(self, *args, **kwargs)/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/test_util.py:1625: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ tridiagonal_solve_op_test.py:579: in testCompactFormatUnknownMatrixSize    expected=_sample_result)_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testCompactFormatUnknownMatrixSize>diags_shape = [3, None], rhs_shape = [4]diags_feed = array([[ 2,  1,  4,  0],       [ 1,  3,  2,  2],       [ 0,  1, -1,  1]])rhs_feed = array([1, 2, 3, 4]), expected = array([-9,  5, -4,  4])diags_format = 'compact'    def _testWithPlaceholders(self,                              diags_shape,                              rhs_shape,                              diags_feed,                              rhs_feed,                              expected,                              diags_format=""compact""):      if context.executing_eagerly():        return      diags = array_ops.placeholder(dtypes.float64, shape=diags_shape)      rhs = array_ops.placeholder(dtypes.float64, shape=rhs_shape)      if test_util.is_xla_enabled() and self.pivoting:        # Pivoting is not supported by xla backends.        return      x = linalg_impl.tridiagonal_solve(>         diags, rhs, diags_format, partial_pivoting=self.pivoting)E     AttributeError: 'TridiagonalSolveOpTest' object has no attribute 'pivoting'tridiagonal_solve_op_test.py:558: AttributeError___________ TridiagonalSolveOpTest.testCompactFormatUnknownRhsCount ____________self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testCompactFormatUnknownRhsCount>args = (), kwargs = {}    def decorated(self, *args, **kwargs):      if context.executing_eagerly():        with context.graph_mode():>         return f(self, *args, **kwargs)/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/test_util.py:1625: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ tridiagonal_solve_op_test.py:588: in testCompactFormatUnknownRhsCount    expected=np.transpose([_sample_result, 2 * _sample_result]))_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testCompactFormatUnknownRhsCount>diags_shape = [3, 4], rhs_shape = [4, None]diags_feed = array([[ 2,  1,  4,  0],       [ 1,  3,  2,  2],       [ 0,  1, -1,  1]])rhs_feed = array([[1, 2],       [2, 4],       [3, 6],       [4, 8]])expected = array([[ -9, -18],       [  5,  10],       [ -4,  -8],       [  4,   8]])diags_format = 'compact'    def _testWithPlaceholders(self,                              diags_shape,                              rhs_shape,                              diags_feed,                              rhs_feed,                              expected,                              diags_format=""compact""):      if context.executing_eagerly():        return      diags = array_ops.placeholder(dtypes.float64, shape=diags_shape)      rhs = array_ops.placeholder(dtypes.float64, shape=rhs_shape)      if test_util.is_xla_enabled() and self.pivoting:        # Pivoting is not supported by xla backends.        return      x = linalg_impl.tridiagonal_solve(>         diags, rhs, diags_format, partial_pivoting=self.pivoting)E     AttributeError: 'TridiagonalSolveOpTest' object has no attribute 'pivoting'tridiagonal_solve_op_test.py:558: AttributeError____________ TridiagonalSolveOpTest.testMatrixFormatWithUnknownDims ____________self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testMatrixFormatWithUnknownDims>args = (), kwargs = {}    def decorated(self, *args, **kwargs):      if context.executing_eagerly():        with context.graph_mode():>         return f(self, *args, **kwargs)/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/test_util.py:1625: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ tridiagonal_solve_op_test.py:617: in testMatrixFormatWithUnknownDims    test_with_matrix_shapes(matrix_shape=[4, 4], rhs_shape=[None, None])tridiagonal_solve_op_test.py:615: in test_with_matrix_shapes    diags_format=""matrix"")_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testMatrixFormatWithUnknownDims>diags_shape = [4, 4], rhs_shape = [None, None]diags_feed = array([[ 1,  2,  0,  0],       [ 1,  3,  1,  0],       [ 0, -1,  2,  4],       [ 0,  0,  1,  2]])rhs_feed = array([[1, 2],       [2, 4],       [3, 6],       [4, 8]])expected = array([[ -9, -18],       [  5,  10],       [ -4,  -8],       [  4,   8]])diags_format = 'matrix'    def _testWithPlaceholders(self,                              diags_shape,                              rhs_shape,                              diags_feed,                              rhs_feed,                              expected,                              diags_format=""compact""):      if context.executing_eagerly():        return      diags = array_ops.placeholder(dtypes.float64, shape=diags_shape)      rhs = array_ops.placeholder(dtypes.float64, shape=rhs_shape)      if test_util.is_xla_enabled() and self.pivoting:        # Pivoting is not supported by xla backends.        return      x = linalg_impl.tridiagonal_solve(>         diags, rhs, diags_format, partial_pivoting=self.pivoting)E     AttributeError: 'TridiagonalSolveOpTest' object has no attribute 'pivoting'tridiagonal_solve_op_test.py:558: AttributeError___________ TridiagonalSolveOpTest.testSequenceFormatWithUnknownDims ___________self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testSequenceFormatWithUnknownDims>args = (), kwargs = {}    def decorated(self, *args, **kwargs):      if context.executing_eagerly():        with context.graph_mode():>         return f(self, *args, **kwargs)/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/test_util.py:1625: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testSequenceFormatWithUnknownDims>    @test_util.run_deprecated_v1    def testSequenceFormatWithUnknownDims(self):      if context.executing_eagerly():        return      if test_util.is_xla_enabled() and self.pivoting:        # Pivoting is not supported by xla backends.        return      superdiag = array_ops.placeholder(dtypes.float64, shape=[None])      diag = array_ops.placeholder(dtypes.float64, shape=[None])      subdiag = array_ops.placeholder(dtypes.float64, shape=[None])      rhs = array_ops.placeholder(dtypes.float64, shape=[None])          x = linalg_impl.tridiagonal_solve((superdiag, diag, subdiag),                                        rhs,                                        diagonals_format=""sequence"",>                                       partial_pivoting=self.pivoting)E     AttributeError: 'TridiagonalSolveOpTest' object has no attribute 'pivoting'tridiagonal_solve_op_test.py:643: AttributeError=============================== warnings summary ===============================tridiagonal_solve_op_test.py::TridiagonalSolveOpTest::testAdjointRhs  /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py:102: ComplexWarning: Casting complex values to real discards the imaginary part    return ops.EagerTensor(value, ctx.device_name, dtype)tridiagonal_solve_op_test.py::TridiagonalSolveOpTest::testConjugateRhs  /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py:102: ComplexWarning: Casting complex values to real discards the imaginary part    return ops.EagerTensor(value, ctx.device_name, dtype)tridiagonal_solve_op_test.py::TridiagonalSolveOpTest::testConjugateRhsWithRhsAsVector  /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py:102: ComplexWarning: Casting complex values to real discards the imaginary part    return ops.EagerTensor(value, ctx.device_name, dtype)-- Docs: http://doc.pytest.org/en/latest/warnings.html========= 6 failed, 42 passed, 1 skipped, 3 warnings in 11.45 seconds ==========```</details>
"
56680,0,2532,0,0,0,Poiu19,0,"title:Conv2D returns empty tensor of type float32 in case of bfloat16 input. description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.9.1### Custom CodeNo### OS Platform and DistributionLinux### Mobile deviceLinux Ubuntu 20.04### Python version3.8### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellConv2D returns empty tensor of type float32 in case of bfloat16 input.```### Standalone code to reproduce the issue```shellimport tensorflow as tfimport numpy as npfor dtype in [tf.float32, tf.bfloat16]:    strides = tf.constant([1, 1, 1, 1], dtype=tf.int32)    input =   tf.constant(np.full(fill_value=1., shape=[1, 56, 56, 1]), dtype=dtype)    filters = tf.constant(np.full(fill_value=1., shape=[1, 56, 1, 1]), dtype=dtype)    padding = ""SAME""    output = tf.nn.conv2d(input, filters, strides, padding)    print("""")    print(""dtype:        "", dtype)    print(""Input size:   "", input.shape.num_elements())    print(""Output size:  "", output.shape.num_elements())    print(""Output dtype: "", output.dtype)    assert output.shape.num_elements() > 0```### Relevant log output```shelldtype:         <dtype: 'float32'>Input size:    3136Output size:   3136Output dtype:  <dtype: 'float32'>dtype:         <dtype: 'bfloat16'>Input size:    3136Output size:   0Output dtype:  <dtype: 'float32'>collected 0 items / 1 errors                                                                                                                                                                                                          =============================================================================================================== ERRORS ================================================================================================================______________________________________________________________________________________________________ ERROR collecting repro.py ______________________________________________________________________________________________________repro.py:20: in <module>    assert output.shape.num_elements() > 0E   assert 0 > 0E    +  where 0 = <bound method TensorShape.num_elements of TensorShape([0])>()E    +    where <bound method TensorShape.num_elements of TensorShape([0])> = TensorShape([0]).num_elementsE    +      where TensorShape([0]) = <tf.Tensor: shape=(0,), dtype=float32, numpy=array([], dtype=float32)>.shape!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 errors during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!======================================================================================================= 1 error in 2.33 seconds =======================================================================================================```</details>
"
56673,0,0,0,0,0,elfringham,0,"title:Unit test fails to build, others fail on AARCH64 after XLA CPU backend enhancements description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiongit HEAD### Custom CodeNo### OS Platform and DistributionCentOS 7### Mobile devicen/a### Python version3.8.10### Bazel version5.1.1### GCC/Compiler version10.2.1### CUDA/cuDNN versionn/a### GPU model and memoryn/a### Current Behaviour?```shellCompiling with --config=mkl_aarch64 the unit test //tensorflow/python/tools:aot_compiled_test fails to build withERROR: /tmp/workspace/tensorflow-oneDNN-ACL-git/tensorflow/python/tools/BUILD:498:11: Linking tensorflow/python/tools/aot_compiled_test failed: (Exit 1): gcc failed: error executing command(cd /root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow &&exec env -LD_LIBRARY_PATH=/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64/dyninst:/opt/rh/devtoolset-10/root/usr/lib/dyninst:/usr/local/lib64PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-5.1.1-linux-arm64/bin:/tmp/workspace/venv-cp38-cp38/bin:/opt/rh/devtoolset-10/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binPWD=/proc/self/cwdPYTHON_BIN_PATH=/tmp/workspace/venv-cp38-cp38/bin/python3PYTHON_LIB_PATH=/tmp/workspace/venv-cp38-cp38/lib/python3.8/site-packagesTF2_BEHAVIOR=1/opt/rh/devtoolset-10/root/usr/bin/gcc @bazel-out/aarch64-opt/bin/tensorflow/python/tools/aot_compiled_test-2.params)Configuration: 3f75d29a2f4aef411fd2f59b82df7437222f52171a4aa87840162de83074dda9Execution platform: @local_execution_config_platform//:platformbazel-out/aarch64-opt/bin/_solib_aarch64/libtensorflow_Spython_Stools_Slibaot_Ucompiled_Ux_Umatmul_Uy_Ularge_Umultithreaded.so: error: undefined reference to '__xla_cpu_runtime_ACLMatMulF32'collect2: error: ld returned 1 exit statusTarget //tensorflow/python/tools:aot_compiled_test failed to buildINFO: Elapsed time: 430.002s, Critical Path: 383.37sINFO: 509 processes: 6 internal, 503 local.FAILED: Build did NOT complete successfully//tensorflow/python/tools:aot_compiled_test FAILED TO BUILDFAILED: Build did NOT complete successfullyAlso other unit tests now fail.//tensorflow/compiler/tests:conv2d_test_cpu_mlir_bridge_test//tensorflow/compiler/xla/service/cpu/tests:cpu_eigen_dot_operation_test//tensorflow/compiler/tests:depthwise_conv_op_test_cpu//tensorflow/compiler/tests:depthwise_conv_op_test_cpu_mlir_bridge_test//tensorflow/compiler/tests:conv2d_test_cpu//tensorflow/compiler/xla/tests:convolution_dimension_numbers_test_cpu//tensorflow/compiler/xla/tests:convolution_variants_test_cpu//tensorflow/compiler/xla/tests:convolution_test_cpu//tensorflow/compiler/xla/tests:conv_depthwise_backprop_filter_test_cpu```### Standalone code to reproduce the issue```shellbazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --noremote_accept_cached --config=nonccl --config=mkl_aarch64 --copt=""-mtune=generic"" --copt=""-march=armv8-a"" --copt=""-O3"" --test_env=TF_ENABLE_ONEDNN_OPTS=1 --copt=""-fopenmp"" --linkopt=""-lgomp"" --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --verbose_failures --jobs=75 --build_tests_only -- //tensorflow/python/tools:aot_compiled_test```### Relevant log output```shellhttps://ci.linaro.org/job/ldcg-python-manylinux-tensorflow-onednn-nightly/108/consoleTextERROR: /tmp/workspace/tensorflow-oneDNN-ACL-git/tensorflow/python/tools/BUILD:498:11: Linking tensorflow/python/tools/aot_compiled_test failed: (Exit 1): gcc failed: error executing command      (cd /root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow && \      exec env - \        LD_LIBRARY_PATH=/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64/dyninst:/opt/rh/devtoolset-10/root/usr/lib/dyninst:/usr/local/lib64 \        PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-5.1.1-linux-arm64/bin:/tmp/workspace/venv-cp37-cp37m/bin:/opt/rh/devtoolset-10/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \        PWD=/proc/self/cwd \        PYTHON_BIN_PATH=/tmp/workspace/venv-cp37-cp37m/bin/python3 \        PYTHON_LIB_PATH=/tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages \        TF2_BEHAVIOR=1 \      /opt/rh/devtoolset-10/root/usr/bin/gcc @bazel-out/aarch64-opt/bin/tensorflow/python/tools/aot_compiled_test-2.params)    # Configuration: 4e7de1e342fde1b2946298612567ae8e6e1dc07899c92f6eb687f4d6e2fb7650    # Execution platform: @local_execution_config_platform//:platform    bazel-out/aarch64-opt/bin/_solib_aarch64/libtensorflow_Spython_Stools_Slibaot_Ucompiled_Ux_Umatmul_Uy_Ularge_Umultithreaded.so: error: undefined reference to '__xla_cpu_runtime_ACLMatMulF32'    collect2: error: ld returned 1 exit status    INFO: Elapsed time: 2135.511s, Critical Path: 497.42s    INFO: 20956 processes: 9447 internal, 11509 local.    FAILED: Build did NOT complete successfully    FAILED: Build did NOT complete successfully```</details>
"
56672,1,0,0,0,0,yonikremer,0,"title:AutoGraph could not transform function description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.9.1### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 18 (colab)### Mobile device_No response_### Python version3.7.13### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version11.2### GPU model and memoryNVIDIA Tesla T4### Current Behaviour?```shellI get the following warnings: WARNING:tensorflow:AutoGraph could not transform <function train at 0x7f0be336b5f0> and will run it as-is.Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.Cause: name 'fscope' is not definedTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert```### Standalone code to reproduce the issue```shell@tf.functiondef contains_pad(inp: tf.Tensor):    tf.debugging.assert_type(inp, tf.int32)    bool_ten = tf.math.equal(inp, pad_ten)    nonzero_count = tf.math.count_nonzero(bool_ten)    return nonzero_count > 0@tf.function(input_signature=(tf.TensorSpec(shape=[batch_size, None], dtype=tf.int32),                              tf.TensorSpec(shape=[batch_size, set_size], dtype=tf.int32)))def train_step(inp: tf.Tensor, outp: tf.Tensor) -> tf.Tensor:    with tf.GradientTape() as tape:        pred: tf.Tensor = model([inp, outp], training=True)         loss_val: tf.Tensor = tf.keras.losses.sparse_categorical_crossentropy(y_true = outp, y_pred = pred, from_logits = False)    grads: tf.RaggedTensor = tape.gradient(loss_val, model.trainable_weights)    optimizer.apply_gradients(zip(grads, model.trainable_weights))    return tf.math.reduce_mean(loss_val)@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size, max_seq_len], dtype=tf.int32)])def train(batch: tf.Tensor) -> tf.TensorSpec(shape=[], dtype=tf.keras.backend.floatx()):    per_generation_loss: tf.Tensor = tf.zeros([num_sets], dtype=tf.keras.backend.floatx())    i = 0    while i < num_sets:        # The input is of size set_size-TAKE_TO_ACCOUNT        already_predicted: int = i * (set_size + 1)        start_from: int = max(0, already_predicted - max_seq_len)        inp: tf.Tensor = batch[:, start_from:(i + 1) * set_size]        tf.debugging.assert_type(inp, tf.int32)        have_pad = tf.map_fn(contains_pad, inp, dtype=tf.bool, parallel_iterations=16)        if tf.get_static_value(tf.math.reduce_all(have_pad)):            break        outp: tf.TensorSpec(shape=[batch_size, set_size]) = batch[:, (i + 1) * set_size:(i + 2) * set_size]        loss_val: tf.TensorSpec(shape=[], dtype=tf.keras.backend.floatx()) = val_step(inp, outp)        one_hot: tf.TensorSpec(shape=[num_sets], dtype=tf.keras.backend.floatx())        one_hot = tf.one_hot([i], num_sets, dtype=tf.keras.backend.floatx()) * loss_val        per_generation_loss += one_hot        i += 1    return tf.math.reduce_mean(per_generation_loss[:i])sam = tf.ramndom.uniform(shape=[64, 265], dtype=tf.int32)train(sam)```### Relevant log output_No response_</details>
"
56669,1,1589,0,0,0,Li-Jiajie,0,"title:_self_tracked_trackables in base_layer.py should not record it self description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.5.1### Custom CodeNo### OS Platform and Distributionlinux### Mobile device_No response_### Python version3.6.5### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?My class inherited ""tf.keras.Model""闂傚倸鍊搁崐鐑芥倿閿旈敮鍋撶粭娑樻噽閻瑩鏌熸潏楣冩闁稿鏅犻弻锝堫槾闁绘鐡玼se of some history reason (compatible history code), there is a class variable assigned as ""self"". like:class Model(tf.keras.Model):  def __init__(self, name):    super().__init__()    self.model = self  # here    xxxxxxxbut it will occur infinite loop while some function use 'trainable_variables'the source is in trainning.py/trainable_weights:    for trackable_obj in self._self_tracked_trackables:      trainable_variables += trackable_obj.trainable_variablesif self is in _self_tracked_trackables, here will infinite loop_self_tracked_trackables was constructed in base_layer, maybe the _self_tracked_trackables  should filter it self?### Standalone code to reproduce the issue```shellclass Model(tf.keras.Model):   def __init__(self, name: str = None):        super().__init__()        self.dnn_model = self        print('here', self.trainable_variables)  # heretest = Model(""test"")```### Relevant log output```shellFile ""/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 2021, in trainable_weights    trainable_variables += trackable_obj.trainable_variables  File ""/usr/local/lib64/python3.6/site-packages/tensorflow/python/training/tracking/data_structures.py"", line 816, in __getattribute__    return object.__getattribute__(self, name)  File ""/usr/local/lib64/python3.6/site-packages/tensorflow/python/training/tracking/data_structures.py"", line 285, in trainable_variables    return self.trainable_weights  File ""/usr/local/lib64/python3.6/site-packages/tensorflow/python/training/tracking/data_structures.py"", line 816, in __getattribute__    return object.__getattribute__(self, name)  File ""/usr/local/lib64/python3.6/site-packages/tensorflow/python/training/tracking/data_structures.py"", line 242, in trainable_weights    for obj in self._values:  File ""/usr/local/lib64/python3.6/site-packages/tensorflow/python/training/tracking/data_structures.py"", line 816, in __getattribute__    return object.__getattribute__(self, name)  File ""/usr/local/lib64/python3.6/site-packages/tensorflow/python/training/tracking/data_structures.py"", line 841, in _values    ordered = list(zip(*sorted(self.items(), key=lambda it: it[0])))RecursionError: maximum recursion depth exceeded while calling a Python object```</details>
"
56623,1,0,0,0,0,psunn,0,"title:TF test failure: //tensorflow/lite/kernels/shim:shape_test description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versionlatest master branch### Custom CodeNo</details>### Current Behaviour?```shellTEST //tensorflow/lite/kernels/shim:shape_test failing on TF upstream/master:tensorflow/lite/kernels/shim/shape_test.cc:74: FailureExpected equality of these values:  ""[]""  Shape({}).ToString()    Which is: ""?""tensorflow/lite/kernels/shim/shape_test.cc:81: FailureValue of: Shape({}).FullyDefined()  Actual: falseExpected: true```### Standalone code to reproduce the issue```shellbazel test //tensorflow/lite/kernels/shim:shape_test --test_output=all```
"
56603,1,531,13,0,0,hshazly,0,"title:MKL use cannot be disabled  description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow VersionTF 2.8.0### Custom CodeNo### OS Platform and DistributionLinux CentOS 8### Mobile device_No response_### Python version3.8.8### Bazel version4.2.1### GCC/Compiler version8.2.1### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellI'm trying to disable the use of JIT kernels entirely. Exporting the environment variable TF_ENABLE_ONEDNN_OPTS=0 does not seem to have any effect, TF still calls: jit_avx512_common_gemm_f32.cpp from _pywrap_tensorflow_internal.so.P.S.: TensorFlow is built without --config=MKL```### Standalone code to reproduce the issue```shellimport tensorflow as tfdef test():    x = [1]*100    W = tf.Variable(tf.ones(shape=(100,100)), name=""W"")    b = tf.Variable(tf.zeros(shape=(100)), name=""b"")    return W * x + bif __name__ == ""__main__"":    test()```### Relevant log output_No response_</details>
"
56529,1,3518,1,0,0,GChaitanya2001,0,"title:Tensor Flow gives different results on INTEL and AMD CPUs description:Click to expand!   ### Issue TypeBug### Source<s>source</s>### Tensorflow Version2.3### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 18.04### Mobile device_No response_### Python version>= 3.6 ### Bazel version<s>0.26.1</s>### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?I tried running a sample sqrt computation on INTEL and AMD CPUs. I used a tolerance of 0.0000001 i.e., the values are printed in the log output, only if the difference between corresponding values is >= 0.0000001.The values obtained on INTEL and AMD CPUs are not matching.#### Note: * Please refer this discussion for more details, https://discuss.tensorflow.org/t/tf-sqrt-computations-are-different-on-two-diff-cpu-architectures-intel-and-amd/10208* the raw files generated by numpy sqrt computation by the provided script has no difference across both the architectures.<s> Steps to reproduce building TF from source: ~~* Used tensorflow 1.15.5 which is built from source ( [Build from source | TensorFlow](https://www.tensorflow.org/install/source#configuration_options)).* Disabled all supports during ./configure step (XLA, CUDA, etc.,).* Provided 闂?march=x86-64闂?for --copt and --host_copt flags while doing bazel build and set --config=v1. </s>### Standalone code to reproduce the issue######  test.py```shellimport numpy as npimport tensorflow.compat.v1 as tfprint(tf.__version__)tf.disable_v2_behavior()# float32 NumPy arraya = np.arange(100, dtype=np.float32)# The same array with the same dtype in TensorFlowa_tf = tf.constant(a, dtype=tf.float32)# Square root with NumPysqrt = np.sqrt(a)sqrt.tofile('../np_exp/sqrt.raw')# Square root with TensorFlowwith tf.Session() as sess:    sqrt_tf = sess.run(tf.sqrt(a_tf))    sqrt_tf.tofile('./sqrt.raw')```###### compare.py - script to compare raw files```shellimport csvimport osimport argparseimport jsonimport numpy as npimport math# import cv2result_data = ''tolrence = 0.0000001np.set_printoptions(precision=17, suppress=True)def csv_reader(abc1, abc2, ):    reader1 = np.fromfile(abc1, dtype=np.float32)    reader2 = np.fromfile(abc2, dtype=np.float32)    #print(reader1)    i = 0    c = 0    count = 0    anti_count = 0    isNan = 0    try:       for (index1, index2) in zip(np.nditer(reader1), np.nditer(reader2)):           i = i + 1           if math.isnan(index2):                continue           if abs(index1 - index2) >= tolrence:               count = count + 1               if count != 0:                 print(str(index1) + "" "" + str(index2) + "" "" + str(i))           else:                anti_count = anti_count + 1       if count != 0:             print(abc1)             print(abc2)             print(""Total Matches :  "", anti_count, ""Mismatches  : "", count)    except:        pass      def list_files_from_dir(root):    all_raw_list = list()    for path, subdirs, files in os.walk(root):        for name in files:            if name not in ['model.cpp', 'model.bin', 'model_net.json']:                 all_raw_list.append(os.path.join(path, name))    return all_raw_listdef compare_all(dir1, dir2):    list1 = list_files_from_dir(dir1)    list2 = list_files_from_dir(dir2)    list1.sort()    list2.sort()    for i in range(len(list1)):        csv_reader(list1[i], list2[i])if __name__ == ""__main__"":    parser = argparse.ArgumentParser(        description='parser')    parser.add_argument('-i1', '--input1', help='The path to the directory containing raw files.', required=True)    parser.add_argument('-i2', '--input2', help='The path to the directory containing raw files', required=True)    args = parser.parse_args()    input1 = args.input1    input2 = args.input2    compare_all(input1, input2)```## Sample commands to run* python3 test.py                        => script to run the sqrt computation* python3 compare.py -i1 ./AMD -i2 ./INTEL     => compares the raw files generated by AMD and INTEL (transfer the raw files to one of the machines, ./AMD folder has raw files generated on AMD and ./INTEL has raw files generated on INTEL)### Relevant log output```shell1.4142135 1.4142134 32.0 1.9999999 52.4494898 2.4494896 72.828427 2.8284268 93.0 2.9999998 103.162278 3.1622777 113.6055512 3.6055508 144.0 3.9999998 174.5825763 4.5825753 224.6904163 4.6904154 234.795831 4.7958307 244.8989797 4.898979 255.099019 5.0990186 275.1961527 5.196152 285.385165 5.3851647 305.656854 5.6568537 335.7445626 5.744562 345.830952 5.830951 356.0 5.9999995 376.0827627 6.0827622 386.324556 6.3245554 416.557439 6.557438 447.0000005 6.9999995 507.0710683 7.0710673 517.2111025 7.2111015 537.2801104 7.280109 547.416199 7.4161983 568.0 7.9999995 658.062258 8.062257 668.124039 8.124038 678.306624 8.306623 708.42615 8.426149 728.5440035 8.544003 748.6602545 8.660254 769.0 8.999999 829.055386 9.055385 839.110435 9.110433 849.165153 9.165151 859.273619 9.273618 879.380833 9.380831 899.486833 9.486834 919.539392 9.5393915 929.591662 9.591661 93./AMD/sqrt.raw./INTEL/sqrt.rawTotal Matches :   57 Mismatches  :  43```</details>
"
56506,1,350,10,0,0,DLumi,0,"title:tf.keras.backend.ctc_decode merging repeated characters by default description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.7.0### Custom CodeNo### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellThe current built-in keras wrapper function does not have an optional agrument ""merge_repeated"" to pass along to the underlying tensorflow function. I believe this issue was solved in standalone keras as per https://github.com/keras-team/keras/issues/12238```### Standalone code to reproduce the issue```shellCheck https://github.com/keras-team/keras/issues/12238 for the reproduction code```### Relevant log output_No response_</details>
"
56500,1,13040,17,0,0,GravermanDev,0,"title: OverflowError: int too large to convert to float description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.9.1### Custom CodeYes### OS Platform and DistributionWindows 1-### Mobile device_No response_### Python version3.9.5### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellI wanted to train my machine learning model on this environment, unfortunately this bug appeared. I don't know if this is my issue or bug in Tensorflow's code```### Standalone code to reproduce the issue```shellfrom collections import Counterfrom keras import layersimport numpy as npimport tensorflow as tffrom rl.agents.dqn import DQNAgentfrom rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicyfrom rl.memory import SequentialMemoryimport kerasimport randomimport gymclass Env(gym.Env):    def __init__(self):        self.observation_space = gym.spaces.MultiDiscrete([50 for _ in range(9)] + [60000, 60000, 60000])        self.action_space = gym.spaces.Discrete(12)        self.stepsLeft = 20000        self.state = self.reset()        # Step mapping is a varible showing what each action does, this prevents wall of 12 if statements        # first 3 numbers are the row you take, second 3 is the one you perform the operation on, 7th number means either adding or substacting        self.StepMapping = {0: [0, 1, 2, 3, 4, 5, -1], 1: [0, 1, 2, 6, 7, 8, -1], 2: [3, 4, 5, 0, 1, 2, -1],                            3: [3, 4, 5, 6, 7, 8, -1], 4: [6, 7, 8, 0, 1, 2, -1], 5: [6, 7, 8, 3, 4, 5, 1],                            6: [0, 1, 2, 3, 4, 5, 1], 7: [0, 1, 2, 6, 7, 8, 1], 8: [3, 4, 5, 0, 1, 2, 1],                            9: [3, 4, 5, 6, 7, 8, 1], 10: [6, 7, 8, 0, 1, 2, 1], 11: [6, 7, 8, 3, 4, 5, 1]}    def step(self, action):        reward = -1        done = False        self.stepsLeft -= 1        self.state[self.StepMapping[action][3]] += self.state[self.StepMapping[action][0]] * self.StepMapping[action][6]        self.state[self.StepMapping[action][4]] += self.state[self.StepMapping[action][1]] * self.StepMapping[action][6]        self.state[self.StepMapping[action][5]] += self.state[self.StepMapping[action][2]] * self.StepMapping[action][6]        # this wierd reward system should make AI lean towards the solution and not astronomically big numbers        wholeSystem = abs(self.state[0]) + abs(self.state[1]) + abs(self.state[2]) + abs(self.state[3]) + abs(            self.state[4]) + abs(self.state[5]) + abs(self.state[6]) + abs(self.state[7]) + abs(self.state[8])        if wholeSystem > 10000: reward = -10        if wholeSystem < 400: reward = 1        if wholeSystem < 200: reward = 5        if wholeSystem < 100: reward = 10        if wholeSystem < 50: reward = 20        if self.stepsLeft == 0:            done = True        if self.done():            done = True            reward = 100000            print(""holy shit"")        return self.state, reward, done, {}    def reset(self):        # in case you don't understand my genius way of calling varibles:        # lsq - left side of quation, rsq - right side of quation        x, y, z = random.randint(0, 200), random.randint(0, 200), random.randint(0, 200)        lsq = [random.randint(0, 49), random.randint(0, 49), random.randint(0, 49),               random.randint(0, 49), random.randint(0, 49), random.randint(0, 49),               random.randint(0, 49), random.randint(0, 49), random.randint(0, 49)]        rsq = [lsq[0] * x + lsq[1] * y + lsq[2] * z,               lsq[3] * x + lsq[4] * y + lsq[5] * z,               lsq[6] * x + lsq[7] * y + lsq[8] * z]        return lsq + rsq    def done(self):        state = [[self.state[0], self.state[1], self.state[2]], [self.state[3], self.state[4], self.state[5]], [self.state[6], self.state[7], self.state[8]]]        found = [0, 1, 2]        savedIndexes = []        try:            for i in range(3):                if Counter(state[i])[0] == 2:                    found.pop(found.index(2))                    savedIndexes = np.where(np.array(state[i]) == 0)[0].tolist()            for i in range(3):                if 0 not in state[i]: found.pop(found.index(0))                if Counter(state[i])[0] == 1:                    if state[i].index(0) in savedIndexes:                        found.pop(found.index(1))                    else: return False        except ValueError: return False        if not found: return True        else: return Falseenv = Env()model = keras.Sequential()model.add(layers.Flatten(input_shape=(1, 12)))model.add(layers.Dense(256, activation='relu', batch_input_shape=(12,)))model.add(layers.Dense(128, activation='relu'))model.add(layers.Dense(12))model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),              optimizer=keras.optimizers.Adam(lr=0.001), metrics=[""accuracy""])policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1.0, value_min=0.1, value_test=0.2, nb_steps=10000)memory = SequentialMemory(limit=50000, window_length=1)DQN = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=12, nb_steps_warmup=10, target_model_update=1e-2)DQN.compile(tf.keras.optimizers.Adam(learning_rate=1e-4), metrics=['mae'])DQN.fit(env, nb_steps=2000000, visualize=False, verbose=1)model.save('workingmodel.h5')scores = DQN.test(env, nb_episodes=1000000, visualize=True)print(np.mean(scores.history['episode_reward']))```### Relevant log output```shellD:\PycharmProjects\AI\venv\lib\site-packages\keras\optimizers\optimizer_v2\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.  super(Adam, self).__init__(name, **kwargs)Training for 2000000 steps ...Interval 1 (0 steps performed)D:\PycharmProjects\AI\venv\lib\site-packages\keras\engine\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.  updates=self.state_updates,    1/10000 [..............................] - ETA: 14:50 - reward: 1.0000D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 10 + 1) instead  batch_idxs = np.random.random_integers(low, high - 1, size=size)   12/10000 [..............................] - ETA: 6:11 - reward: -0.6667D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 11 + 1) instead  batch_idxs = np.random.random_integers(low, high - 1, size=size)D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead  batch_idxs = np.random.random_integers(low, high - 1, size=size)D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 13 + 1) instead  batch_idxs = np.random.random_integers(low, high - 1, size=size)D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 14 + 1) instead  batch_idxs = np.random.random_integers(low, high - 1, size=size)D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 15 + 1) instead  batch_idxs = np.random.random_integers(low, high - 1, size=size)D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 16 + 1) instead  batch_idxs = np.random.random_integers(low, high - 1, size=size)D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 17 + 1) instead  batch_idxs = np.random.random_integers(low, high - 1, size=size)   29/10000 [..............................] - ETA: 3:25 - reward: -0.8621D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 18 + 1) instead  batch_idxs = np.random.random_integers(low, high - 1, size=size)D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 19 + 1) instead  batch_idxs = np.random.random_integers(low, high - 1, size=size)D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 20 + 1) instead  batch_idxs = np.random.random_integers(low, high - 1, size=size)D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 21 + 1) instead  batch_idxs = np.random.random_integers(low, high - 1, size=size)D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 22 + 1) instead  batch_idxs = np.random.random_integers(low, high - 1, size=size)D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 23 + 1) instead  batch_idxs = np.random.random_integers(low, high - 1, size=size)D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 24 + 1) instead  batch_idxs = np.random.random_integers(low, high - 1, size=size)D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 25 + 1) instead  batch_idxs = np.random.random_integers(low, high - 1, size=size)D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 26 + 1) instead  batch_idxs = np.random.random_integers(low, high - 1, size=size)D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 27 + 1) instead  batch_idxs = np.random.random_integers(low, high - 1, size=size)D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 28 + 1) instead  batch_idxs = np.random.random_integers(low, high - 1, size=size)D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 29 + 1) instead  batch_idxs = np.random.random_integers(low, high - 1, size=size)D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 30 + 1) instead  batch_idxs = np.random.random_integers(low, high - 1, size=size)D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 31 + 1) instead  batch_idxs = np.random.random_integers(low, high - 1, size=size) 6720/10000 [===================>..........] - ETA: 25s - reward: -9.9231Traceback (most recent call last):File ""D:\PycharmProjects\AI\venv\lib\site-packages\IPython\core\interactiveshell.py"", line 3398, in run_code    exec(code_obj, self.user_global_ns, self.user_ns)  File ""<ipython-input-3-0e6266aa34dd>"", line 1, in <cell line: 1>    runfile('D:/PycharmProjects/AI/Linear Algebra AI.py', wdir='D:/PycharmProjects/AI')  File ""D:\JetBrains\PyCharm Community Edition 2021.2\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_umd.py"", line 198, in runfile    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script  File ""D:\JetBrains\PyCharm Community Edition 2021.2\plugins\python-ce\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile    exec(compile(contents+""\n"", file, 'exec'), glob, loc)  File ""D:/PycharmProjects/AI/Linear Algebra AI.py"", line 95, in <module>    DQN.fit(env, nb_steps=2000000, visualize=False, verbose=1)  File ""D:\PycharmProjects\AI\venv\lib\site-packages\rl\core.py"", line 168, in fit    action = self.forward(observation)  File ""D:\PycharmProjects\AI\venv\lib\site-packages\rl\agents\dqn.py"", line 224, in forward    q_values = self.compute_q_values(state)  File ""D:\PycharmProjects\AI\venv\lib\site-packages\rl\agents\dqn.py"", line 68, in compute_q_values    q_values = self.compute_batch_q_values([state]).flatten()  File ""D:\PycharmProjects\AI\venv\lib\site-packages\rl\agents\dqn.py"", line 63, in compute_batch_q_values    q_values = self.model.predict_on_batch(batch)  File ""D:\PycharmProjects\AI\venv\lib\site-packages\keras\engine\training_v1.py"", line 1200, in predict_on_batch    outputs = self.predict_function(inputs)  File ""D:\PycharmProjects\AI\venv\lib\site-packages\keras\backend.py"", line 4269, in __call__    array_vals.append(np.asarray(value,OverflowError: int too large to convert to float```</details>
"
56498,1,1559,0,0,0,pranshupant,0,"title:tf.raw_ops.DepthwiseConv2dNative ignores dilation values description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.9### Custom CodeNo### OS Platform and DistributionLinux Debian 11### Mobile device_No response_### Python version3.9.2### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellAs per the raw_ops documentation https://www.tensorflow.org/api_docs/python/tf/raw_ops/DepthwiseConv2dNativethe DepthWiseConv2dNative op supports dilated convolutions. However, in practice, the operator seems to ignore the dilation values. Given any value for dilations, the output corresponds to dilation = [1, 1, 1, 1].```### Standalone code to reproduce the issue```shellimport tensorflow as tfimport numpy as np# CustomLayer using DepthwiseConv2dNative operatorclass CustomDepthwiseConv2DLayer(tf.keras.layers.Layer):    def __init__(self):        super(CustomDepthwiseConv2DLayer, self).__init__()    def build(self, input_shape):        pass    def call(self, input, filter, strides, padding, explicit_paddings, dilations):        return tf.raw_ops.DepthwiseConv2dNative(    input=input,    filter=filter,    strides=strides,    padding=padding,    explicit_paddings=explicit_paddings,    dilations=dilations)# Modelinputs = tf.keras.Input(shape=(3, 3, 3))filter_shape = (2, 2, 3, 1)filter = tf.ones(filter_shape)strides = [1, 1, 1, 1]padding = ""VALID""explicit_paddings = []# Enter any dilation value here. It has no effect!dilations = [1, 1, 1, 1]#dilations = [1, 2, 2, 1]operator = CustomDepthwiseConv2DLayer()outputs = operator(inputs, filter, strides, padding, explicit_paddings, dilations)model = tf.keras.Model(inputs=inputs, outputs=outputs)# Predictionx = np.arange(1, 28)x = np.reshape(x, (1, 3, 3, 3))x = tf.convert_to_tensor(x, dtype=tf.float32)y = model.predict(x)print(y.shape)print(y)```### Relevant log output_No response_</details>
"
56490,0,2959,298,0,0,EnricoMi,0,"title:tf.data.experimental.service does not stop with repeated dataset description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.7.0 and 2.9.1### Custom CodeNo### OS Platform and DistributionLinux Ubuntu 22.04### Mobile device_No response_### Python version3.8.13### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?After consuming a repeated dataset, the `DispatchServer` and `WorkerServer` are shutdown, but some thread keeps running that tries to reconnect to either the dispatcher or worker. This prevents the entire process from terminating (thread seems not to be a daemon thread, likely the gRPC thread: https://github.com/grpc/grpc-java/issues/80).Setting `WorkerConfig(dispatcher_timeout_ms=1000)` (timeout is 1s) makes the thread giving up at some point, but shutdown still takes 20 seconds.### Standalone code to reproduce the issue```pythonimport threadingfrom itertools import isliceimport tensorflow as tfdef in_thread(fn, args=()):    bg = threading.Thread(target=fn, args=args)    bg.daemon = True    bg.start()    return bgdef consume(ds):    for _ in islice(ds.as_numpy_iterator(), 10):        print('.', end='')    print()dispatcher = tf.data.experimental.service.DispatchServer()dispatcher_address = dispatcher.target.split(""://"")[1]worker = tf.data.experimental.service.WorkerServer(    tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address))print('distribute dataset')dataset = tf.data.Dataset.range(1024).repeat().batch(128).apply(    tf.data.experimental.service.distribute(processing_mode='distributed_epoch', service=dispatcher.target))print('consume dataset')thread = in_thread(consume, (dataset, ))thread.join()print('dataset consumed')```### Relevant log output```2022-06-17 11:53:03.722050: I tensorflow/core/data/service/dispatcher_impl.cc:189] Running with fault_tolerant_mode=False. The dispatcher will not be able to recover its state on restart.2022-06-17 11:53:03.722067: I tensorflow/core/data/service/server_lib.cc:64] Started tf.data DispatchServer running at 0.0.0.0:339632022-06-17 11:53:03.723049: I tensorflow/core/data/service/worker_impl.cc:147] Worker registered with dispatcher running at localhost:339632022-06-17 11:53:03.723113: I tensorflow/core/data/service/server_lib.cc:64] Started tf.data WorkerServer running at 0.0.0.0:383132022-06-17 11:53:03.723501: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.distribute datasetconsume dataset..........2022-06-17 11:53:03.889622: I tensorflow/core/data/service/server_lib.cc:76] Shut down DispatchServer server running at port 339632022-06-17 11:53:03.889894: I tensorflow/core/data/service/server_lib.cc:76] Shut down WorkerServer server running at port 38313dataset consumed2022-06-17 11:53:04.192806: I tensorflow/core/data/service/grpc_util.cc:68] Failed to get next split: UNAVAILABLE: Failed to get split: failed to connect to all addresses. Will retry in 100ms.2022-06-17 11:53:04.293259: I tensorflow/core/data/service/grpc_util.cc:68] Failed to get next split: UNAVAILABLE: Failed to get split: failed to connect to all addresses. Will retry in 118ms.2022-06-17 11:53:04.411498: I tensorflow/core/data/service/grpc_util.cc:68] Failed to get next split: UNAVAILABLE: Failed to get split: failed to connect to all addresses. Will retry in 129ms.2022-06-17 11:53:04.540736: I tensorflow/core/data/service/grpc_util.cc:68] Failed to get next split: UNAVAILABLE: Failed to get split: failed to connect to all addresses. Will retry in 215ms.```</details>
"
56470,1,539,0,0,0,ValiullinAlbert,0,"title:Different results on 2.9.1 and 2.8.0 description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.8.0, tf 2.9.1### Custom CodeYes### OS Platform and DistributionWindows 11 x86### Mobile device_No response_### Python version3.8.10### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versionCUDA 11.3, cuDNN 8.2.1### GPU model and memoryNVIDIA GeForce RTX 3060 Laptop GPU, 6gb### Current Behaviour?```shellThe model has different results on 2.8.0 and 2.9.1 versions:model = keras.applications.EfficientNetB0(classes=1000)-----2.8.0-----print(model.predict(np.zeros((1, 224, 224, 3)))[0][0])### result is 0.0010812443-----2.9.1-----print(model.predict(np.zeros((1, 224, 224, 3)))[0][0])### result is 0.00066214177The same happens with other models```### Standalone code to reproduce the issue```shellimport numpy as npfrom tensorflow import kerasmodel = keras.applications.EfficientNetB0(classes=1000)print(model.predict(np.zeros((1, 224, 224, 3)))[0][0])```### Relevant log output_No response_</details>
"
56444,1,0,0,0,1,IgnacioHeredia,0,"title:Installing Python editable modules in Tensorflow Docker containers description: ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.6.1### Custom CodeNo### OS Platform and DistributionUbuntu 20.04### Python version3.8.10### Current Behaviour?Hi all,I found a very intriguing bug in TF docker images. I've spent a couple of days on it but I'm still clueless.When you install a Python module in TF docker images in **editable** mode (`-e` flag), the module is not listed in pip. The module however is still importable from python (`python3 -c 'import samplemod'`). Another (related) symptom that something is wrong is that the entrypoints defined in that module are not found either (which is the reason I found the bug in the first place).When the module is installed normally (no `-e` flag), everything works as usual, and pip finds the module.The bug first appeared in TF 2.6.1 and has remained since. TF 2.6.1 was the first version that upgraded to Ubuntu 20.04 (and python 3.8) so I suspected that it could be related to that. So I checked by building directly from Ubuntu base images. Here are the results:* ubuntu:18.04 :heavy_check_mark:```Ubuntu 18.04.6 LTSPython 3.6.9pip 21.3.1 from /usr/local/lib/python3.6/dist-packages/pip (python 3.6)$ pip3 list -ePackage Version Editable project location------- ------- -------------------------sample  0.1.0   /samplemod```* ubuntu:20.04 :x:```Ubuntu 20.04.4 LTSPython 3.8.10pip 22.1.2 from /usr/local/lib/python3.8/dist-packages/pip (python 3.8)$ pip3 list -e```* tensorflow/tensorflow:2.6.0 :heavy_check_mark:```Ubuntu 18.04.5Python 3.6.9pip 21.3.1 from /usr/local/lib/python3.6/dist-packages/pip (python 3.6)$ pip3 list -ePackage Version Editable project location------- ------- -------------------------sample  0.1.0   /samplemod```* tensorflow/tensorflow:2.6.1 :x:```Ubuntu 20.04.4 LTSPython 3.8.10pip 22.1.2 from /usr/local/lib/python3.8/dist-packages/pip (python 3.8)$ pip3 list -e```So it looks indeed that is is related to Ubuntu/python, but I'm still opening an issue here because it looks like something that needs to be fixed when using TF docker images. You can find here the issues I opened in [AskUbuntu](https://askubuntu.com/questions/1413875/installing-python-editable-modules-in-ubuntu-20-04-docker-containers) and [pip](https://github.com/pypa/pip/issues/11189).Thanks,Ignacio### Standalone code to reproduce the issueI attach a _minimal_ Dockerfile to reproduce the issue:```dockerfile#ARG base=ubuntu:18.04  # WORKS#ARG base=ubuntu:20.04  # DOES NOT WORK#ARG base=tensorflow/tensorflow:2.6.0  # WORKSARG base=tensorflow/tensorflow:2.6.1  # DOES NOT WORKFROM ${base}# Refresh cache and install gitRUN DEBIAN_FRONTEND=noninteractive apt-get update && \    apt-get install -y --no-install-recommends git# Install Python for Ubuntu images# Python versions mimic corresponding TF imagesARG base  # https://stackoverflow.com/a/56748289RUN if [ ""$base"" = ""ubuntu:18.04"" ]; then \        apt-get install -y --no-install-recommends python3.6 python3 python3-pip; \    elif [ ""$base"" = ""ubuntu:20.04"" ]; then \        apt-get install -y --no-install-recommends python3.8 python3 python3-pip; \    fi         # Update pipRUN pip3 install --upgrade pip setuptools wheel# Install sample Python module in **editable** (-e) modeRUN git clone https://github.com/navdeep-G/samplemod && \    cd  samplemod && \    pip3 install -e . && \    cd ..# Check behaviourRUN cat /etc/issueRUN python3 --versionRUN pip3 --versionRUN python3 -c ""import samplemod""RUN pip3 list -e```
"
56427,1,2170,10,0,0,Zhaopudark,0,"title:Confusing wranings when `tf.data.dataset` description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf2.9### Custom CodeNo### OS Platform and DistributionWindows 11### Mobile deviceN/A### Python version3.10.4### Bazel versionN/A### GCC/Compiler versionN/A### CUDA/cuDNN version11.7/8.4### GPU model and memoryRTX3090 24GB### Current Behaviour?```shellJust follow `tf.data.Dataset`'s documents, using the `tf.data.Dataset.cache`'s example code' to test the `cache()` functionbut there will be some confusing warnings arising, such as    1. when establish the cache file for the first time, I got:`W tensorflow/core/kernels/data/cache_dataset_ops.cc:296] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.`   2. when use the cache file, I got:`W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled`I want to know why these wranings appera even completely consistent with the document code, or how can I use `tf.data.Dataset.cache` correctly. Thanks!```### Standalone code to reproduce the issue```shellimport tensorflow as tfphysical_devices = tf.config.experimental.list_physical_devices(device_type='GPU')tf.config.experimental.set_memory_growth(physical_devices[0], True)dataset = tf.data.Dataset.range(5)dataset = dataset.map(lambda x: x**2)dataset = dataset.cache(tf.constant(""./cache/file""))# The first time reading through the data will generate the data using# `range` and `map`.list(dataset.as_numpy_iterator())# Subsequent iterations read from the cache.list(dataset.as_numpy_iterator())```### Relevant log output```shellW tensorflow/core/kernels/data/cache_dataset_ops.cc:296] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.or2022-06-11 20:45:07.212334: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled2022-06-11 20:45:07.216874: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled```</details>
"
56425,1,1657,14,0,0,ivankrylatskoe,0,"title:TFLite - different inference results between Android and python description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versionpython: 2.7.0; android: 2.4.0### Custom CodeNo### OS Platform and DistributionWindows 10; Android 10### Mobile deviceRedmi Note 9### Python version3.7.9### CUDA/cuDNN versioncuda_11.2.r11.2### Current Behaviour?```shellThe same tflite model gives different results on Android and in python using the same input data.Expected behaviour: the results are same.```### Standalone code to reproduce the issue```shellGoogle Colab code:https://colab.research.google.com/drive/1J_gE3JYiqS9MNml9phFiryznI-Bcc3-MAndroid code: https://github.com/ivankrylatskoe/tfbugModel file and input data file can be downloaded from here:https://drive.google.com/file/d/1Ed07YKIZh69Rmso3EL9a7t4dCacJqEs2/view?usp=sharinghttps://drive.google.com/file/d/1p2iAJVT8Wc46ho5Hh29O7n4bwLD38h3O/view?usp=sharingor from here:https://github.com/ivankrylatskoe/tfbug/tree/main/app/src/main/assets```### Relevant log output```shellPython output:File input.dat checksum: 99cc12fcae36b2b07ce8e3afe8c1ed86File model.tflite checksum: eb302aa27bd246fb2759e768ad3c3bc0Model output:[[ 12  13  14  16  10 253] [ 12  13  15  17  10 253] [ 12  13  15  17  10 253] [ 18  17  16  18  10 253] [ 18  17  18  21  10 253] [ 18  15  20  22  10 253] [ 25  18  15  16  10 253] [ 25  18  16  18  10 253] [ 25  18  17  19  10 253] [ 32  18  15  17  10 253]]Android output:2022-06-10 23:52:46.146 20317-20317/com.android.tfbug I/TFBug: File input.dat md5: 99cc12fcae36b2b07ce8e3afe8c1ed862022-06-10 23:52:46.252 20317-20317/com.android.tfbug I/TFBug: File model.tflite md5: eb302aa27bd246fb2759e768ad3c3bc02022-06-10 23:52:46.587 20317-20317/com.android.tfbug I/TFBug: Model output:     12 14 14 16 10 253     12 14 15 17 10 253     12 14 15 17 10 253     18 17 16 18 10 253     18 17 18 21 10 253     18 15 20 23 10 253     25 18 15 16 10 253     25 18 16 18 10 253     25 18 17 19 10 253     32 18 15 16 10 253```</details>
"
56424,1,944,2,0,0,JinkaiGUAN,0,"title:tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a NewWriteableFile: description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf2.5### Custom CodeYes### OS Platform and DistributionWindows 10### Mobile device_No response_### Python version3.8.5### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellfor name, model in models.items():                                if model is not None:                                    name_ = ""best_model_"" + name                                    # model.save_weights(os.path.join(weights_dir, name_), save_format='tf')                                    model.save(os.path.join(weights_dir, name_), save_format=""tf"")The above is the code I used to save my model, and there is a problem named a subclassing model. It seems that TensorFlow did not solve it, and gives the error below,```shelltensorflow.python.framework.errors_impl.NotFoundError: Failed to create a NewWriteableFile: with_embedded_layer\sub_data\best_model_label_embedder\variables\variables_temp/part-00000-of-00001.data-00000-of-00001.tempstate7900589316842660676 : The system cannot find the path specified.```I did not know why Tensorflow introduces a wrong path delimiter in windows, i.e., `/` in the above path.```### Standalone code to reproduce the issue```shelltensorflow.python.framework.errors_impl.NotFoundError: Failed to create a NewWriteableFile: with_embedded_layer\sub_data\best_model_label_embedder\variables\variables_temp/part-00000-of-00001.data-00000-of-00001.tempstate7900589316842660676 : The system cannot find the path specified.```### Relevant log output_No response_</details>
"
56420,1,0,0,0,0,guweixin,0,"title:Configuration Error: --define PYTHON_BIN_PATH='C:/Users/user/AppData/Local/Programs/Python/Python39/python.exe' is not executable. Is it the python binary? description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf2.9.1### Custom CodeNo### OS Platform and Distributionwin10### Mobile device_No response_### Python version3.9### Bazel version5.1.1### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellA bug happened!bazel build -c opt --config=monolithic --define=with_select_tf_ops=true tensorflow/lite/delegates/flex:tensorflowlite_flexWARNING: The following configs were expanded more than once: [monolithic]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.INFO: Options provided by the client:  Inherited 'common' options: --isatty=1 --terminal_columns=190INFO: Reading rc options for 'build' from d:\ai_framework\tensorflow_src\.bazelrc:  Inherited 'common' options: --experimental_repo_remote_execINFO: Options provided by the client:  'build' options: --python_path=C:/Users/user/AppData/Local/Programs/Python/Python39/python.exeINFO: Reading rc options for 'build' from d:\ai_framework\tensorflow_src\.bazelrc:  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_libraryINFO: Reading rc options for 'build' from d:\ai_framework\tensorflow_src\.tf_configure.bazelrc:  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/user/AppData/Local/Programs/Python/Python39/python.exe --action_env PYTHON_LIB_PATH=C:/Users/user/AppData/Local/Programs/Python/Python39/lib/site-packages --python_path=C:/Users/user/AppData/Local/Programs/Python/Python39/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=trueINFO: Reading rc options for 'build' from d:\ai_framework\tensorflow_src\.bazelrc:  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utilsINFO: Found applicable config definition build:short_logs in file d:\ai_framework\tensorflow_src\.bazelrc: --output_filter=DONT_MATCH_ANYTHINGINFO: Found applicable config definition build:v2 in file d:\ai_framework\tensorflow_src\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1INFO: Found applicable config definition build:monolithic in file d:\ai_framework\tensorflow_src\.bazelrc: --define framework_shared_object=falseINFO: Found applicable config definition build:windows in file d:\ai_framework\tensorflow_src\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=falseINFO: Found applicable config definition build:monolithic in file d:\ai_framework\tensorflow_src\.bazelrc: --define framework_shared_object=falseINFO: Repository local_execution_config_python instantiated at:  D:/ai_framework/tensorflow_src/WORKSPACE:15:14: in <toplevel>  D:/ai_framework/tensorflow_src/tensorflow/workspace2.bzl:870:19: in workspace  D:/ai_framework/tensorflow_src/tensorflow/workspace2.bzl:91:27: in _tf_toolchains  D:/ai_framework/tensorflow_src/tensorflow/tools/toolchains/remote_config/configs.bzl:6:28: in initialize_rbe_configs  D:/ai_framework/tensorflow_src/tensorflow/tools/toolchains/remote_config/rbe_config.bzl:158:27: in _tensorflow_local_configRepository rule local_python_configure defined at:  D:/ai_framework/tensorflow_src/third_party/py/python_configure.bzl:280:41: in <toplevel>ERROR: An error occurred during the fetch of repository 'local_execution_config_python':   Traceback (most recent call last):        File ""D:/ai_framework/tensorflow_src/third_party/py/python_configure.bzl"", line 213, column 22, in _create_local_python_repository                _check_python_bin(repository_ctx, python_bin)        File ""D:/ai_framework/tensorflow_src/third_party/py/python_configure.bzl"", line 145, column 25, in _check_python_bin                auto_config_fail(""--define %s='%s' is not executable. Is it the python binary? %s"" % (        File ""D:/ai_framework/tensorflow_src/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail                fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg))Error in fail: Configuration Error: --define PYTHON_BIN_PATH='C:/Users/user/AppData/Local/Programs/Python/Python39/python.exe' is not executable. Is it the python binary? <unknown object com.google.devtools.build.lib.bazel.repository.starlark.StarlarkRepositoryContext>ERROR: D:/ai_framework/tensorflow_src/WORKSPACE:15:14: fetching local_python_configure rule //external:local_execution_config_python: Traceback (most recent call last):        File ""D:/ai_framework/tensorflow_src/third_party/py/python_configure.bzl"", line 213, column 22, in _create_local_python_repository                _check_python_bin(repository_ctx, python_bin)        File ""D:/ai_framework/tensorflow_src/third_party/py/python_configure.bzl"", line 145, column 25, in _check_python_bin                auto_config_fail(""--define %s='%s' is not executable. Is it the python binary? %s"" % (        File ""D:/ai_framework/tensorflow_src/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail                fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg))Error in fail: Configuration Error: --define PYTHON_BIN_PATH='C:/Users/user/AppData/Local/Programs/Python/Python39/python.exe' is not executable. Is it the python binary? <unknown object com.google.devtools.build.lib.bazel.repository.starlark.StarlarkRepositoryContext>ERROR: Analysis of target '//tensorflow/lite/delegates/flex:tensorflowlite_flex' failed; build aborted: Configuration Error: --define PYTHON_BIN_PATH='C:/Users/user/AppData/Local/Programs/Python/Python39/python.exe' is not executable. Is it the python binary? <unknown object com.google.devtools.build.lib.bazel.repository.starlark.StarlarkRepositoryContext>INFO: Elapsed time: 0.226sINFO: 0 processes.FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)    Fetching @local_config_python; fetching```### Standalone code to reproduce the issue```shellpython configure.pybazel build -c opt --config=monolithic --define=with_select_tf_ops=true tensorflow/lite/delegates/flex:tensorflowlite_flex```### Relevant log output_No response_</details>
"
56419,1,2921,6,0,0,FengMu1995,0,"title:error! when inferencing my model with tf-lite c++Api. it is well with tf-lite pythonApi description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow VersioncApi: tf2.8    pythonApi:tf2.8### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 18.04### Mobile device_No response_### Python version3.6### Bazel version_No response_### GCC/Compiler version7.5.0### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellwhen I inferencing my model with tflite python, it could run successfully!But when I inferencing my model with tflite c, it reported:""""""ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_selectERROR: Node number 333 (FlexConv2D) failed to prepare.""""""```### Standalone code to reproduce the issue```shellconst char* filename = ""./rvm_f16.tflite"";const char* image_path = ""./meeting.png"";    int src_h = 144;    int src_w = 256;    int r1i_h = 72;    int r1i_w = 128;    int r2i_h = 36;    int r2i_w = 64;    int r3i_h = 18;    int r3i_w = 32;    int r4i_h = 9;    int r4i_w = 16;    //read image     cv::Mat srcImage = cv::imread(image_path, 1);     cv::Mat priImage;     cv::resize(srcImage, priImage, cv::Size(src_w, src_h), 0, 0, cv::INTER_NEAREST);     priImage.convertTo(priImage, CV_32FC3, 1.0f/255);    // Load model    std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(filename);    TFLITE_RVM_CHECK(model != nullptr);    // Build the interpreter    tflite::ops::builtin::BuiltinOpResolver resolver;    tflite::InterpreterBuilder builder(*model, resolver);    std::unique_ptr<tflite::Interpreter> interpreter;    builder(&interpreter);    TFLITE_RVM_CHECK(interpreter != nullptr);    // Allocate tensor buffers.    std::vector<int> src_size = {1, src_h, src_w, 3};    std::vector<int> r1i_size = {1, r1i_h, r1i_w, 16};    std::vector<int> r2i_size = {1, r2i_h, r2i_w, 20};    std::vector<int> r3i_size = {1, r3i_h, r3i_w, 40};    std::vector<int> r4i_size = {1, r4i_h, r4i_w, 64};    interpreter->ResizeInputTensor(interpreter->inputs()[0], r2i_size);//2    interpreter->ResizeInputTensor(interpreter->inputs()[1], r4i_size);//4    interpreter->ResizeInputTensor(interpreter->inputs()[2], src_size);//0    interpreter->ResizeInputTensor(interpreter->inputs()[3], r1i_size);//1    interpreter->ResizeInputTensor(interpreter->inputs()[4], r3i_size);//3    interpreter->AllocateTensors();    float* src = interpreter->typed_input_tensor<float>(2);// std::cout<<interpreter->inputs().size()<<std::endl;    float* r1i = interpreter->typed_input_tensor<float>(3);    float* r2i = interpreter->typed_input_tensor<float>(0);    float* r3i = interpreter->typed_input_tensor<float>(4);    float* r4i = interpreter->typed_input_tensor<float>(1);    memcpy(src, priImage.data, src_h*src_w*3*sizeof(float));    memset(r1i, 0.0f, sizeof(float)*r1i_h*r1i_w*16);    memset(r2i, 0.0f, sizeof(float)*r2i_h*r2i_w*20);    memset(r3i, 0.0f, sizeof(float)*r3i_h*r3i_w*40);    memset(r4i, 0.0f, sizeof(float)*r4i_h*r4i_w*64);    interpreter->Invoke();```### Relevant log output_No response_</details>
"
56414,0,3383,10,0,0,jesnie,0,"title:`reduce_retracing` causing: ""Called a function referencing variables which have been deleted..."" description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.9.1### Custom CodeYes### OS Platform and DistributionUbuntu 18.04.6 LTS### Mobile device_No response_### Python version3.10.4### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellWhen I run the below code I get the error:""Called a function referencing variables which have been deleted. This likely means that function-local variables were created and not referenced elsewhere in the program. This is generally a mistake; consider storing variables in an object attribute on first call.""I do not get the error if:1: I use an older version of TensorFlow (with experimental_relax_shapes instead of reduce_retracing)2: I set `reduce_retracing=False`.3: I remove one of the `tf.function` calls.I'm not sure exactly what's going on here, but there is some kind of interaction that is unstable.```### Standalone code to reproduce the issue```shellimport tensorflow as tfreduce_retracing = Truev = tf.Variable(0.0)@tf.function(reduce_retracing=reduce_retracing)def f(x: tf.Tensor) -> tf.Tensor:    return x@tf.function(reduce_retracing=reduce_retracing)def loss() -> tf.Tensor:    return f(v)optimiser = tf.keras.optimizers.SGD()optimiser.minimize(loss, var_list=[v])optimiser.minimize(loss, var_list=[v])```### Relevant log output```shell2022-06-09 15:38:18.788974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2022-06-09 15:38:18.792122: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory2022-06-09 15:38:18.792632: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.Skipping registering GPU devices...2022-06-09 15:38:18.792867: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.Traceback (most recent call last):  File ""/home/jesper/src/GPflow/tf29_error.py"", line 16, in <module>    optimiser.minimize(loss, var_list=[v])  File ""/home/jesper/src/GPflow/.venv/max310/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py"", line 537, in minimize    grads_and_vars = self._compute_gradients(  File ""/home/jesper/src/GPflow/.venv/max310/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py"", line 581, in _compute_gradients    loss = loss()  File ""/home/jesper/src/GPflow/.venv/max310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler    raise e.with_traceback(filtered_tb) from None  File ""/home/jesper/src/GPflow/.venv/max310/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py"", line 587, in deref    raise AssertionError(AssertionError: Called a function referencing variables which have been deleted. This likely means that function-local variables were created and not referenced elsewhere in the program. This is generally a mistake; consider storing variables in an object attribute on first call.```</details>
"
56410,1,593,89,0,0,AdityaPunetha,0,"title:Tanh giving out of range output. description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.8.2### Custom CodeNo### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellThe tanh activation function is supposed to give an output in range [-1,1] but in this instance the output would sometimes have value in range [-9.999...,9.9999...]. Below I have attached a link to a repo where you can see the model architecture, the input and the output of the same, the model is also saved and uploaded in the same repo. The notebook was ran in colab and local environment where I faced similar result```### Standalone code to reproduce the issue```shellhttps://colab.research.google.com/github/AdityaPunetha/TensorFlow-TanH-Issue/blob/main/TanH_issue.ipynbhttps://github.com/AdityaPunetha/TensorFlow-TanH-Issue```### Relevant log output_No response_</details>
"
56398,1,1740,62,0,0,xiongma,0,"title:put two models in another model, the two models variable names aren't under the another model name scope description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.5.0### Custom CodeYes### OS Platform and Distributionmac os/ linux### Mobile device_No response_### Python version3.8.5### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellput two models in another model, the two models variable names aren't under the another model name scope.1. class clip inherit from keras.Model2. class CLIPTextTransformer inherit from keras.Model3. class CLIPVisionTrransformer inherit from keras.Model```### Standalone code to reproduce the issue```shellthe source code as followclass CLIP(PreTrainModel):    """"""    CLIP    """"""    config_cls = CLIPConfig    def __init__(self, name: str = 'clip123', **kwargs):        super(CLIP, self).__init__(name=name, **kwargs)        self.text_transformer = CLIPTextTransformer(config=self.config.text_transformer, name='text_transformer')        self.vision_transformer = CLIPVisionTrransformer(config=self.config.vision_transformer,                                                         name='vision_transformer')        self.visual_projection = tf.keras.layers.Dense(units=self.config.projection_dim,                                                       kernel_initializer=get_initializer(                                                           self.vision_transformer.config.hidden_size ** -0.5),                                                       use_bias=False,                                                       name='visual_projection')        self.text_projection = tf.keras.layers.Dense(units=self.config.projection_dim,                                                     kernel_initializer=get_initializer(                                                         self.text_transformer.config.hidden_size ** -0.5),                                                     use_bias=False,                                                     name='text_projection')``````### Relevant log output_No response_</details>
"
56396,1,28180,90,0,0,ghost,0,"title:No gradients provided for any variable description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.9.1### Custom CodeYes### OS Platform and DistributionmacOS Monterey 12.4### Mobile device_No response_### Python version3.10.4### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?[distilbert-base-uncased.txt](https://huggingface.co/distilbert-base-uncased/blob/main/vocab.txt)Loss is provided and I get an error. To make it work, modify the commented line and replace `return loss` with `return 'mse'` and it should work fine.### Standalone code to reproduce the issue```shellimport randomimport stringimport numpy as npimport tensorflow as tfimport tensorflow_models as tfmfrom keras import Modelfrom keras.layers import Dense, Dropout, Inputfrom transformers import TFAutoModeldef calculate_loss(pad_val, max_val):    def loss(y_true, y_pred):        pad_indices = tf.where(y_true == pad_val)        y_true = tf.tensor_scatter_nd_update(            y_true,            pad_indices,            tf.ones(tf.shape(pad_indices)[0], y_true.dtype) * max_val,        )        actual_orders = tf.argsort(tf.argsort(y_true))        predicted_orders = tf.argsort(tf.argsort(y_pred))        return tf.reduce_sum(tf.abs(actual_orders - predicted_orders))    return loss  # change this with 'mse' and the error is gonedef create_model(input_shape, bert_head):    ids = Input(input_shape, dtype='int32', name='input_ids')    masks = Input(input_shape, dtype='int32', name='attention_mask')    x0 = dict(input_ids=ids, attention_mask=masks)    x = bert_head(x0)[0]    x = Dropout(0.2)(x)    x = Dense(64)(x)    x = Dropout(0.2)(x)    output = Dense(1, 'sigmoid')(x)    return Model(x0, output)class DatasetGen:    def __init__(self, tokenizer, packer, min_seq_length, max_seq_length):        self.tokenizer = tokenizer        self.packer = packer        self.min_seq_length = min_seq_length        self.max_seq_length = max_seq_length    def process_xy(self, x, y):        tokens = self.tokenizer(x)        x = self.packer(tokens)        x['input_ids'] = x['input_word_ids']        x['attention_mask'] = x['input_mask']        del x['input_word_ids']        del x['input_type_ids']        del x['input_mask']        return x, tf.expand_dims(y, -1)    def dummy_dataset(self, batch_size=4, size=1000):        s = string.ascii_letters + 10 * ' '        xs = []        ys = []        for _ in range(size):            x, y = [], []            seq_length = random.randint(self.min_seq_length, self.max_seq_length)            for i in range(seq_length):                x.append(''.join(random.choice(s) for _ in range(50, 150)))                y.append(i)            pad_size = self.max_seq_length - seq_length            xs.append(np.pad(np.array(x), [0, pad_size]))            ys.append(np.pad(np.array(y), [0, pad_size]))        return (            tf.data.Dataset.from_tensor_slices((xs, ys))            .batch(batch_size)            .map(self.process_xy)        )if __name__ == '__main__':    pretrained_model = 'distilbert-base-uncased'    cache_dir = 'transformers-cache'    head = TFAutoModel.from_pretrained(pretrained_model, cache_dir=cache_dir)    tokenizer = tfm.nlp.layers.BertTokenizer(        vocab_file='distilbert-base-uncased.txt'    )    packer = tfm.nlp.layers.BertPackInputs(        50, special_tokens_dict=tokenizer.get_special_tokens_dict()    )    data_gen = DatasetGen(tokenizer, packer, 10, 50)    dataset = data_gen.dummy_dataset()    model = create_model((None,), head)    model.compile('adam', calculate_loss(0, 1))    model.fit(dataset, batch_size=4)```### Relevant log output```shell/usr/local/opt/python@3.10/bin/python3.10 /Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py --multiprocess --qt-support=auto --client 127.0.0.1 --port 49349 --file ""/Users/user/Library/Application Support/JetBrains/PyCharm2022.1/scratches/scratch_1.py""Connected to pydev debugger (build 221.5787.24)2022-06-08 04:01:25.576194: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_layer_norm', 'vocab_projector', 'activation_13']- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.WARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32WARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32Traceback (most recent call last):  File ""/usr/local/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler    raise e.with_traceback(filtered_tb) from None  File ""/var/folders/_0/8cr6pwvs1jg99fgtf2_33lsm0000gn/T/__autograph_generated_file56mk7ojv.py"", line 15, in tf__train_function    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)ValueError: in user code:    File ""/usr/local/lib/python3.10/site-packages/keras/engine/training.py"", line 1051, in train_function  *        return step_function(self, iterator)    File ""/usr/local/lib/python3.10/site-packages/keras/engine/training.py"", line 1040, in step_function  **        outputs = model.distribute_strategy.run(run_step, args=(data,))    File ""/usr/local/lib/python3.10/site-packages/keras/engine/training.py"", line 1030, in run_step  **        outputs = model.train_step(data)    File ""/usr/local/lib/python3.10/site-packages/keras/engine/training.py"", line 893, in train_step        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)    File ""/usr/local/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py"", line 539, in minimize        return self.apply_gradients(grads_and_vars, name=name)    File ""/usr/local/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py"", line 640, in apply_gradients        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)    File ""/usr/local/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/utils.py"", line 73, in filter_empty_gradients        raise ValueError(f""No gradients provided for any variable: {variable}. ""    ValueError: No gradients provided for any variable: (['tf_distil_bert_model/distilbert/embeddings/word_embeddings/weight:0', 'tf_distil_bert_model/distilbert/embeddings/position_embeddings/embeddings:0', 'tf_distil_bert_model/distilbert/embeddings/LayerNorm/gamma:0', 'tf_distil_bert_model/distilbert/embeddings/LayerNorm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/q_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/q_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/k_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/k_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/v_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/v_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/out_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/out_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/sa_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/sa_layer_norm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin1/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin1/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin2/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin2/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/output_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/output_layer_norm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/q_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/q_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/k_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/k_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/v_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/v_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/out_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/out_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/sa_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/sa_layer_norm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin1/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin1/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin2/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin2/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/output_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/output_layer_norm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/q_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/q_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/k_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/k_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/v_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/v_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/out_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/out_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/sa_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/sa_layer_norm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin1/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin1/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin2/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin2/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/output_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/output_layer_norm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/q_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/q_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/k_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/k_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/v_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/v_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/out_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/out_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/sa_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/sa_layer_norm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin1/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin1/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin2/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin2/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/output_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/output_layer_norm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/q_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/q_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/k_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/k_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/v_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/v_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/out_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/out_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/sa_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/sa_layer_norm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin1/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin1/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin2/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin2/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/output_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/output_layer_norm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/q_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/q_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/k_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/k_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/v_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/v_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/out_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/out_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/sa_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/sa_layer_norm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin1/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin1/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin2/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin2/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/output_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/output_layer_norm/beta:0', 'dense/kernel:0', 'dense/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'tf_distil_bert_model/distilbert/embeddings/word_embeddings/weight:0' shape=(30522, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/embeddings/position_embeddings/embeddings:0' shape=(512, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/q_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/q_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/k_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/k_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/v_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/v_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/out_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/out_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/sa_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/sa_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin1/kernel:0' shape=(768, 3072) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin1/bias:0' shape=(3072,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin2/kernel:0' shape=(3072, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin2/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/output_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/output_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/q_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/q_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/k_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/k_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/v_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/v_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/out_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/out_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/sa_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/sa_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin1/kernel:0' shape=(768, 3072) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin1/bias:0' shape=(3072,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin2/kernel:0' shape=(3072, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin2/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/output_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/output_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/q_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/q_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/k_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/k_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/v_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/v_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/out_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/out_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/sa_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/sa_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin1/kernel:0' shape=(768, 3072) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin1/bias:0' shape=(3072,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin2/kernel:0' shape=(3072, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin2/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/output_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/output_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/q_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/q_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/k_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/k_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/v_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/v_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/out_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/out_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/sa_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/sa_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin1/kernel:0' shape=(768, 3072) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin1/bias:0' shape=(3072,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin2/kernel:0' shape=(3072, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin2/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/output_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/output_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/q_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/q_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/k_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/k_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/v_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/v_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/out_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/out_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/sa_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/sa_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin1/kernel:0' shape=(768, 3072) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin1/bias:0' shape=(3072,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin2/kernel:0' shape=(3072, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin2/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/output_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/output_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/q_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/q_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/k_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/k_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/v_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/v_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/out_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/out_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/sa_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/sa_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin1/kernel:0' shape=(768, 3072) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin1/bias:0' shape=(3072,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin2/kernel:0' shape=(3072, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin2/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/output_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/output_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'dense/kernel:0' shape=(768, 64) dtype=float32>), (None, <tf.Variable 'dense/bias:0' shape=(64,) dtype=float32>), (None, <tf.Variable 'dense_1/kernel:0' shape=(64, 1) dtype=float32>), (None, <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32>)).python-BaseExceptionProcess finished with exit code 137 (interrupted by signal 9: SIGKILL)```</details>
"
56393,1,833,249,0,1,Young768,0,"title:DTensor pack/unpack APIs used high device memory description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf.2.9.1### Custom CodeNo### OS Platform and DistributionLinux Ubuntu 20.04.4### Mobile device_No response_### Python version3.8.10### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memoryV100### Current Behaviour?Hi, DTensor developersI was testing the DTensor features on ResNet-50 on imagenet. I started with the simple data parallel. To do data parallel, I fully replicated the weights and only shard the input data. The input batch sharding is handled by  method. However, I can only run the model with a very small global batch, i.e., 64. If I increased the #step-per-epoch, this number will be further reduced to 32. When I used a larger batch size, i.e. 128, from the TF-profiler, I saw that the used device memory kept accumulating and then it hit the OOM within 110 steps.I spent some times and figured out that ```repack_batch``` method could cause high device memory. If I moved the below [code](https://github.com/Young768/test-dtensor/blob/main/resnet-opt.py#L757-L762) out from the for loop and used a fixed sharded input batch, I can successfully run the model with a global batch size of 1024. I would think that TF would reuse the allocation for the every new batch. But here it seems TF didn't do that. Would you please have a look? Thanks.```x = next(train_iter)images, labels = ximages, labels = repack_batch(        images, labels, image_layout, label_layout)t_x = (images, labels)total_loss += train_step(t_x)```### Standalone code to reproduce the issueThis is the [repo](https://github.com/Young768/test-dtensor/blob/main/resnet-opt.py) I used.### Relevant log output```shell2022-05-26 19:20:17.146997: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] total_region_allocated_bytes_: 15348793344 memory_limit_: 15348793344 available bytes: 0 curr_region_allocation_bytes_: 171798691842022-05-26 19:20:17.147010: I tensorflow/core/common_runtime/bfc_allocator.cc:1103] Stats: Limit:                     15348793344InUse:                     15344515072MaxInUse:                  15347394560NumAllocs:                       23834MaxAllocSize:               2512388096Reserved:                            0PeakReserved:                        0LargestFreeBlock:                    0```</details>
"
56387,1,195,54,0,0,MovsisyanM,0,"title:VRAM-hungry LSTM monster description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.8### Custom CodeNo### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version3.8.10### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version11.6### GPU model and memory3x RTX 3090 (24Gb of VRAM each)### Current Behaviour?```shellA bug happened!Compiling a simple model with an lstm cell fills the VRAM to the maximum, before even fitting.```### Standalone code to reproduce the issue```shellhttps://github.com/MovsisyanM/sandbox/blob/main/random/LSTM-hell.ipynb```### Relevant log output_No response_</details>
"
56385,1,6801,10,0,1,Zhaopudark,0,"title:Dataset Pipeline determinism when reload, using `batch` `prefetch` description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf2.9### Custom CodeNo### OS Platform and DistributionWindows 11### Mobile deviceNo### Python version3.10.4### Bazel versionN/A### GCC/Compiler versionN/A### CUDA/cuDNN version11.7/8.4### GPU model and memoryRTX3090 24GB### Current Behaviour?```shellFor some reason, I have to built a cascade tf.data.dataset like `dataset_B(dataset_A)`, where the dataset_A deal with `tf.string` datas (contains description of data, such as paths, operands for data augmentation), saving the iter state into checkpoint, dataset_B do some mapping (such as getting real data arrays from paths, augmenting data by operands). In this way, I can easily condouct `shuffle` in dataset_A and without loading whole and huge real data into `shuffle buffsize`. I can easily  save the data's iter state by `https://www.tensorflow.org/guide/data#iterator_checkpointing` and reload the data with `checkpoint` to deal with any unexpected stop of my algorithmmy. It looked perfect, and I did it. But there are still shortcomings. If I use `batch` or `prefetch` to deal with dataset_B, and if I do not use `tf.config.experimental.enable_op_determinism()`, the reload behavior will be non-deterministic. I will show this bug in following simple codes. If there is no bug, `buf1`, `buf2` and `buf3` should have the same output. But unfortunately, since `buf2` is the output by using `batch` and `perfetch` and without  `tf.config.experimental.enable_op_determinism()`, it did more iterations before unexpected termination, leading data shift when reload. But this bug disappear when use `tf.config.experimental.enable_op_determinism()`I hope this bug will be fixed, i.e., I want data will not shift after reload, even thouth I do not use `tf.config.experimental.enable_op_determinism()`.```### Standalone code to reproduce the issue```shellimport tempfileimport tensorflow as tfphysical_devices = tf.config.experimental.list_physical_devices(device_type='GPU')tf.config.experimental.set_memory_growth(physical_devices[0], True)datas = {""test"":[str(item) for item in  range(100)]} def mapfunc(x):    def py_func(inp):        return int(inp.numpy())    k = list(x.keys())    v = list(x.values())    y = tf.py_function(py_func,inp=v,Tout=[tf.float32])    return dict(zip(k,y))# buf1-(without batch,perfetch)with tempfile.TemporaryDirectory() as dir_name:    dataset_A = tf.data.Dataset.from_tensor_slices(datas)    def wrapper(iterator):        def gen():            yield from iterator        dataset_B = tf.data.Dataset.from_generator(gen,output_signature=({""test"":tf.TensorSpec(shape=[],dtype=tf.string)})).map(map_func=mapfunc)        return dataset_B # without batch,perfetch    step = tf.Variable(0)    iterator = iter(dataset_A)     checkpoint = tf.train.Checkpoint(iterator=iterator,step=step)    ckpt_manager = tf.train.CheckpointManager(checkpoint=checkpoint,directory=dir_name,max_to_keep=3,step_counter=step,checkpoint_interval=10)    buf1 = []    for s,item  in zip(range(step.numpy()+1,8+1),wrapper(iterator)):        step.assign(s)        buf1.append((step.numpy(),tf.reduce_mean(item[""test""]).numpy()))        ckpt_manager.save(check_interval=True,checkpoint_number=step)        if step.numpy()>=5: # mimic unexpected stop            break    ckpt_manager.restore_or_initialize()    for s,item  in zip(range(step.numpy()+1,8+1),wrapper(iterator)):        step.assign(s)        buf1.append((step.numpy(),tf.reduce_mean(item[""test""]).numpy()))        ckpt_manager.save(check_interval=True,checkpoint_number=step)    #buf2-(with batch,perfetch)with tempfile.TemporaryDirectory() as dir_name:    dataset_A = tf.data.Dataset.from_tensor_slices(datas)    def wrapper(iterator):        def gen():            yield from iterator        dataset_B = tf.data.Dataset.from_generator(gen,output_signature=({""test"":tf.TensorSpec(shape=[],dtype=tf.string)})).map(map_func=mapfunc)        return dataset_B.batch(1).prefetch(tf.data.AUTOTUNE) # with batch,perfetch    step = tf.Variable(0)    iterator = iter(dataset_A)     checkpoint = tf.train.Checkpoint(iterator=iterator,step=step)    ckpt_manager = tf.train.CheckpointManager(checkpoint=checkpoint,directory=dir_name,max_to_keep=3,step_counter=step,checkpoint_interval=10)    buf2 = []    for s,item  in zip(range(step.numpy()+1,8+1),wrapper(iterator)):        step.assign(s)        buf2.append((step.numpy(),tf.reduce_mean(item[""test""]).numpy()))        ckpt_manager.save(check_interval=True,checkpoint_number=step)        if step.numpy()>=5: # mimic unexpected stop            break    ckpt_manager.restore_or_initialize()    for s,item  in zip(range(step.numpy()+1,8+1),wrapper(iterator)):        step.assign(s)        buf2.append((step.numpy(),tf.reduce_mean(item[""test""]).numpy()))        ckpt_manager.save(check_interval=True,checkpoint_number=step)    #buf3-(use determinism and with batch,perfetch)tf.keras.utils.set_random_seed(0)tf.config.experimental.enable_op_determinism() # determinismwith tempfile.TemporaryDirectory() as dir_name:    dataset_A = tf.data.Dataset.from_tensor_slices(datas)    def wrapper(iterator):        def gen():            yield from iterator        dataset_B = tf.data.Dataset.from_generator(gen,output_signature=({""test"":tf.TensorSpec(shape=[],dtype=tf.string)})).map(map_func=mapfunc)        return dataset_B.batch(1).prefetch(tf.data.AUTOTUNE) # with batch,perfetch    step = tf.Variable(0)    iterator = iter(dataset_A)     checkpoint = tf.train.Checkpoint(iterator=iterator,step=step)    ckpt_manager = tf.train.CheckpointManager(checkpoint=checkpoint,directory=dir_name,max_to_keep=3,step_counter=step,checkpoint_interval=10)    buf3 = []    for s,item  in zip(range(step.numpy()+1,8+1),wrapper(iterator)):        step.assign(s)        buf3.append((step.numpy(),tf.reduce_mean(item[""test""]).numpy()))        ckpt_manager.save(check_interval=True,checkpoint_number=step)        if step.numpy()>=5: # mimic unexpected stop            break    ckpt_manager.restore_or_initialize()    for s,item  in zip(range(step.numpy()+1,8+1),wrapper(iterator)):        step.assign(s)        buf3.append((step.numpy(),tf.reduce_mean(item[""test""]).numpy()))        ckpt_manager.save(check_interval=True,checkpoint_number=step)print(""buf1 without batch,perfetch (index,data):"")print(buf1)print(""buf2 with batch,perfetch (index,data):"")print(buf2)print(""buf3 use determinism and with batch,perfetch (index,data):"")print(buf3)```### Relevant log output```shellbuf1 without batch,perfetch (index,data):[(1, 0.0), (2, 1.0), (3, 2.0), (4, 3.0), (5, 4.0), (2, 1.0), (3, 2.0), (4, 3.0), (5, 4.0), (6, 5.0), (7, 6.0), (8, 7.0)]buf2 with batch,perfetch (index,data):[(1, 0.0), (2, 1.0), (3, 2.0), (4, 3.0), (5, 4.0), (2, 3.0), (3, 4.0), (4, 5.0), (5, 6.0), (6, 7.0), (7, 8.0), (8, 9.0)]buf3 use determinism and with batch,perfetch (index,data):[(1, 0.0), (2, 1.0), (3, 2.0), (4, 3.0), (5, 4.0), (2, 1.0), (3, 2.0), (4, 3.0), (5, 4.0), (6, 5.0), (7, 6.0), (8, 7.0)]```</details>
"
56378,1,0,0,0,0,jq460,0,"title:ImportError: cannot import name 'full_type_pb2' description:### Issue TypeBug### Sourcebinary### Tensorflow Versiontf.2.6### Custom CodeYes### OS Platform and DistributionWindows10 x64### Mobile device_No response_### Python version3.6.8### Current Behaviour?```shellWhen import tensorflow raise ImportError: cannot import name 'full_type_pb2'```### Standalone code to reproduce the issue```shelluse anaconda-4.12.0 to create python env.Package                 Version----------------------- ------------absl-py                 0.15.0astunparse              1.6.3atomicwrites            1.4.0attrs                   21.4.0cached-property         1.5.2cachetools              4.2.4certifi                 2021.10.8cffi                    1.15.0charset-normalizer      2.0.10clang                   5.0cloudpickle             2.0.0colorama                0.4.4configparser            3.7.5cycler                  0.11.0dataclasses             0.8deap                    1.3.1decorator               4.4.2enum34                  1.1.10esdk-obs-python         3.21.8flatbuffers             1.12future                  0.18.2gast                    0.4.0google-auth             1.35.0google-auth-oauthlib    0.4.6google-pasta            0.2.0grpcio                  1.43.0h5py                    3.1.0hyperopt                0.2.7idna                    3.3importlib-metadata      4.8.3importlib-resources     5.4.0joblib                  1.1.0jsonschema              3.2.0keras                   2.6.0Keras-Preprocessing     1.1.2kiwisolver              1.3.1lightgbm                3.3.2lxml                    4.9.0Markdown                3.3.6matplotlib              3.3.4mip                     1.13.0more-itertools          8.12.0mpmath                  1.2.1networkx                2.5.1numpy                   1.19.5oauthlib                3.1.1opt-einsum              3.3.0packaging               21.3pandas                  1.1.5Pillow                  8.4.0pip                     21.3.1pluggy                  0.13.1protobuf                4.21.0py                      1.11.0py4j                    0.10.9.5pyaml                   21.10.1pyasn1                  0.4.8pyasn1-modules          0.2.8pycparser               2.21pycryptodome            3.10.1pyparsing               3.0.7pyrsistent              0.18.0pytest                  4.6.11python-dateutil         2.8.2python-docx             0.8.11python-intervals        1.10.0.post1pytz                    2021.3PyWavelets              1.1.1PyYAML                  6.0requests                2.26.0requests-oauthlib       1.3.0rsa                     4.8ruamel.yaml             0.16.13ruamel.yaml.clib        0.2.6scikit-learn            0.23.2scikit-optimize         0.8.1scipy                   1.4.1setuptools              47.1.1simon                   1.0.8six                     1.15.0sympy                   1.9tensorboard             2.6.0tensorboard-data-server 0.6.1tensorboard-plugin-wit  1.8.1tensorflow              2.6.2tensorflow-estimator    2.6.0termcolor               1.1.0threadpoolctl           3.1.0tqdm                    4.64.0typing-extensions       3.7.4.3urllib3                 1.26.8wcwidth                 0.2.5Werkzeug                2.0.2wheel                   0.37.1wincertstore            0.2wrapt                   1.12.1xgboost                 1.4.2zipp                    3.6.0```### Relevant log outputTraceback (most recent call last):  File ""D:\solution_utils.py"", line 17, in <module>    import tensorflow as tf  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\__init__.py"", line 41, in <module>    from tensorflow.python.tools import module_util as _module_util  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\__init__.py"", line 46, in <module>    from tensorflow.python import data  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\data\__init__.py"", line 25, in <module>    from tensorflow.python.data import experimental  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\data\experimental\__init__.py"", line 97, in <module>    from tensorflow.python.data.experimental import service  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\data\experimental\service\__init__.py"", line 353, in <module>    from tensorflow.python.data.experimental.ops.data_service_ops import distribute  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\data\experimental\ops\data_service_ops.py"", line 26, in <module>    from tensorflow.python.data.experimental.ops import compression_ops  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\data\experimental\ops\compression_ops.py"", line 20, in <module>    from tensorflow.python.data.util import structure  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\data\util\structure.py"", line 26, in <module>    from tensorflow.python.data.util import nest  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\data\util\nest.py"", line 40, in <module>    from tensorflow.python.framework import sparse_tensor as _sparse_tensor  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\framework\sparse_tensor.py"", line 28, in <module>    from tensorflow.python.framework import constant_op  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\framework\constant_op.py"", line 29, in <module>    from tensorflow.python.eager import execute  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\eager\execute.py"", line 28, in <module>    from tensorflow.python.framework import ops  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\framework\ops.py"", line 54, in <module>    from tensorflow.python.framework import cpp_shape_inference_pb2  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\framework\cpp_shape_inference_pb2.py"", line 16, in <module>    from tensorflow.core.framework import full_type_pb2 as tensorflow_dot_core_dot_framework_dot_full__type__pb2ImportError: cannot import name 'full_type_pb2'
"
56367,0,533,55,0,0,NobuoTsukamoto,0,"title:[TensorFlow Lite label_image]  Abort occurs with xnnpack_delegate option. description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versionv2.9.1 (or master)### Custom CodeNo### OS Platform and DistributionRaspberry Pi OS (bullseye)### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler versiongcc version 10.2.1 20210110 (Debian 10.2.1-6) ### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellBuild tensorflow-lite and label_image from source (with CMake).Execute with the `--xnnpack_delegate` option.label_image \  --tflite_model /tmp/mobilenet_v1_1.0_224.tflite \  --labels /tmp/labels.txt \  --image tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp \  --xnnpack_delegate 1Type mismatch while accessing parameter.Abort``````### Standalone code to reproduce the issue```shellClone repository.git clone -b v2.9.1 https://github.com/tensorflow/tensorflow.git```Build tensorflow-lite and label_image```mkdir build && cd buildcmake ../tensorflow/tensorflow/lite/cmake --build . -j$(nproc)cmake --build . -j$(nproc) -t label_image```Download tflite model```wget https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgztar xf mobilenet_v1_1.0_224.tgzwget  https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_1.0_224_frozen.tgztar xf mobilenet_v1_1.0_224_frozen.tgzcp mobilenet_v1_1.0_224/labels.txt ./```Exec label_image (with use_xnnpack option)```./examples/label_image/label_image \  --tflite_model ./mobilenet_v1_1.0_224.tflite \  --labels ./labels.txt \  --image ../tensorflow/tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp``````### Relevant log output_No response_</details>
"
56365,1,1295,84,0,0,FragrantRookie,0,"title:There was a problem quantifying the model with two inputs which have different dimensions. description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.9### Custom CodeYes### OS Platform and Distributionwindows10### Mobile device_No response_### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellA bug happened!My model have two inputs.The first input have dimension [1,1,257],the second input have dimension [1,2,128,2].I want to quantify my model with int16 input/output type and int8 weights type.When I use mycode to quantify my model, Error occurred.```### Standalone code to reproduce the issue```shelldef representative_dataset():    for _ in range(100):      data1 = np.random.rand(1, 2, 128,2)      data2 = np.random.rand(1, 1, 257)      #data3 = {'input_2': [data2.astype(np.float32)], 'input_3': [data1.astype(np.float32)]}      data3 = [[data2.astype(np.float32)], [data1.astype(np.float32)]]      yield data3if use_dynamic_range_quant:    converter.optimizations = [tf.lite.Optimize.DEFAULT]    converter.target_spec.supported_ops =\      [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]    converter.inference_input_type = tf.int16    converter.inference_output_type = tf.int16    converter.representative_dataset = representative_datasettflite_model = converter.convert()```### Relevant log output```shellThe error like this:  File ""D:\ProgramFiles\Anaconda3\lib\site-packages\tensorflow\lite\python\optimize\calibrator.py"", line 129, in <listcomp>    self._calibrator.Prepare([list(s.shape) for s in input_array])AttributeError: 'list' object has no attribute 'shape'```</details>
"
56363,1,0,0,0,0,bluetail14,0,"title:Keras/tensorflow runs only one epoch description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.6.0### Custom CodeYes### OS Platform and DistributionWindows 10 21H2### Mobile device_No response_### Python version3.9.12### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versionCuda 11.2.2_461.33/ cuDNN v8.1.1### GPU model and memoryGeForce NVidia RTX 3060  16GB RAM### Current Behaviour?```shellMy tensforflow 2 is gpu-enabled and had been running model training with no issues until last week. Then the issue occurred - it runs only one epoch and stops.I have all libraries installed via one of Anaconda's virtual environments. when I ran 'install -c anaconda tensorflow-gpu' , I actually got installed the following versions: tensorflow: 2.6.0,  cudatoolkit-11.3.1, cudnn-8.2.1I have tried rolling back to the previous Nvidia driver version (from 512.95 to 512.15), and restoring to an earlier point in Windows when it worked.```### Standalone code to reproduce the issue```shelltf.test.is_built_with_cuda()Out: Truephysical_devices = tf.config.list_physical_devices('GPU')physical_devicesOut: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]with open('./Data/train.csv') as f:    data = f.readlines()   def read_csv(filename):    sentences = []    labels = []    with open(filename, 'r') as csvfile:                reader = csv.reader(csvfile, delimiter=',')        for row in reader:            sentences.append(row[0])            labels.append(row[1])    return sentences, labels## some data preprocessing...## glove embeddingsGLOVE_EMBEDDINGS = {}with open(GLOVE_FILE) as f:    for line in f:        values = line.split()        word = values[0]        coefs = np.asarray(values[1:], dtype='float32')        GLOVE_EMBEDDINGS[word] = coefsEMBEDDING_DIM = 50# Initialize an empty numpy array with the appropriate sizeEMBEDDINGS_MATRIX = np.zeros((VOCAB_SIZE+1, EMBEDDING_DIM))# Iterate all of the words in the vocabulary and if the vector representation for # each word exists within GloVe's representations, save it in the EMBEDDINGS_MATRIX arrayfor word, i in word_index.items():    embedding_vector = GLOVE_EMBEDDINGS.get(word)    if embedding_vector is not None:        EMBEDDINGS_MATRIX[i] = embedding_vector## hyperparametersEMBEDDING_DIM = 50MAXLEN = 500 #1000, 1400VOCAB_SIZE =  33713DENSE1_DIM = 64DENSE2_DIM = 32DENSE3_DIM = 16LSTM1_DIM = 32 LSTM2_DIM = 16WD = 0.001FILTERS = 64  KERNEL_SIZE = 5## Model Definition model_lstm = tf.keras.Sequential([    tf.keras.layers.Embedding(VOCAB_SIZE+1, EMBEDDING_DIM, input_length=MAXLEN,weights=[EMBEDDINGS_MATRIX], trainable=False),    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM1_DIM, dropout=0.5, kernel_regularizer = regularizers.l2(WD),return_sequences=True)),    tf.keras.layers.Dense(DENSE2_DIM, activation='relu'),        tf.keras.layers.Conv1D(FILTERS, KERNEL_SIZE, activation='relu', ),    tf.keras.layers.Dropout(0.1),    tf.keras.layers.GlobalAveragePooling1D(),     tf.keras.layers.Dense(DENSE3_DIM, activation='relu'),      tf.keras.layers.Dense(1, activation='sigmoid')])# Set the training parametersmodel_lstm.compile(loss='binary_crossentropy',                   optimizer=tf.keras.optimizers.Adam(),                    metrics=[tf.keras.metrics.BinaryAccuracy()])# Print the model summarymodel_lstm.summary()## training the modelhistory_lstm = model_lstm.fit(sent_tok_train, labels_train,  validation_data=(sent_tok_val, labels_val), verbose =2)Out: 44/44 - 8s - loss: 0.7946 - binary_accuracy: 0.5629 - val_loss: 0.7613 - val_binary_accuracy: 0.5250```### Relevant log output```shellNone. no error messages at all```</details>
"
56359,1,4533,4,0,0,feiticeir0,0,"title:Simple audio recognition: Recognizing keywords -  'images' must have either 3 or 4 dimensions. description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.7.0### Custom CodeNo### OS Platform and DistributionMacOS Monterey 12.4### Mobile device_No response_### Python versionPython 3.10.4### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellI'm following the TensorFlow tutorial on Simple audio recognition: Recognizing keywordsand when training the model, after the first EPOCH, I get the following error:Epoch 1/10100/100 [==============================] - ETA: 0s - loss: 1.7273 - accuracy: 0.3780WARNING:tensorflow:Model was constructed with shape (None, 124, 129, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 124, 129, 1), dtype=tf.float32, name='input_1'), name='input_1', description=""created by layer 'input_1'""), but it was called on an input with incompatible shape (None, None).Traceback (most recent call last):  File ""/Users/feiticeir0/scripts/audio_recognition/audio_classification.py"", line 283, in <module>    history = model.fit(  File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler    raise e.with_traceback(filtered_tb) from None  File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py"", line 1129, in autograph_handler    raise e.ag_error_metadata.to_exception(e)ValueError: in user code:    File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/engine/training.py"", line 1366, in test_function  *        return step_function(self, iterator)    File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/engine/training.py"", line 1356, in step_function  **        outputs = model.distribute_strategy.run(run_step, args=(data,))    File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/engine/training.py"", line 1349, in run_step  **        outputs = model.test_step(data)    File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/engine/training.py"", line 1303, in test_step        y_pred = self(x, training=False)    File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler        raise e.with_traceback(filtered_tb) from None    ValueError: Exception encountered when calling layer ""resizing"" (type Resizing).        'images' must have either 3 or 4 dimensions.        Call arguments received:      闂?inputs=tf.Tensor(shape=(None, None), dtype=float32)```### Standalone code to reproduce the issue```shellJust follow the code in the tutorial.```### Relevant log output```shellEpoch 1/10100/100 [==============================] - ETA: 0s - loss: 1.7273 - accuracy: 0.3780WARNING:tensorflow:Model was constructed with shape (None, 124, 129, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 124, 129, 1), dtype=tf.float32, name='input_1'), name='input_1', description=""created by layer 'input_1'""), but it was called on an input with incompatible shape (None, None).Traceback (most recent call last):  File ""/Users/feiticeir0/scripts/audio_recognition/audio_classification.py"", line 283, in <module>    history = model.fit(  File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler    raise e.with_traceback(filtered_tb) from None  File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py"", line 1129, in autograph_handler    raise e.ag_error_metadata.to_exception(e)ValueError: in user code:    File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/engine/training.py"", line 1366, in test_function  *        return step_function(self, iterator)    File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/engine/training.py"", line 1356, in step_function  **        outputs = model.distribute_strategy.run(run_step, args=(data,))    File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/engine/training.py"", line 1349, in run_step  **        outputs = model.test_step(data)    File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/engine/training.py"", line 1303, in test_step        y_pred = self(x, training=False)    File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler        raise e.with_traceback(filtered_tb) from None    ValueError: Exception encountered when calling layer ""resizing"" (type Resizing).        'images' must have either 3 or 4 dimensions.        Call arguments received:      闂?inputs=tf.Tensor(shape=(None, None), dtype=float32)```</details>
"
56341,1,3010,5,0,0,bzdjordje,0,"title:Incorrect gradient of model output with respect to inputs using GradientTape description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.1.3### Custom CodeNo### OS Platform and DistributionWindows 10 Home### Mobile device_No response_### Python version3.7### Bazel versionn/a### GCC/Compiler versionn/a### CUDA/cuDNN versionCPU### GPU model and memoryn/a### Current Behaviour?```shellI am trying to take the gradient/jacobian of my model output with respect to the inputs to study the derivatives of my parameter space. This issue has come up several times in the past but I do not seem to be able to reproduce their results or solutions, if any were suggested. It seems that I am able to apply gradients a single variable output, single variable input model, however my problem is multivariate with respect to both. I am using an older version of TF for work purposes but the issue was the same for TF 2.6 as well.```### Standalone code to reproduce the issue```shellimport tensorflow as tffrom numpy import *from tensorflow.keras import Sequentialfrom tensorflow.keras.layers import Denseimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitx1 = linspace(0,4*pi,100000)x2 = linspace(2,20,100000)y = cos(x1) + x2/5model = Sequential()model.add(Dense(100, activation='elu', input_shape=(2,)))  # elu used so that smooth, continuous functions are differentiatedmodel.add(Dense(100, activation='elu'))model.add(Dense(50, activation='elu'))model.add(Dense(50, activation='elu'))model.add(Dense(10, activation='elu'))model.add(Dense(10, activation='elu'))model.add(Dense(1, activation='linear'))model.compile(loss='MSE', optimizer='adam', metrics=['accuracy'])x_in = vstack((x1,x2)).Ty_out = yx_train, x_valid, y_train, y_valid = train_test_split(x_in, y_out, test_size=0.3, shuffle= True)epochs = 100batch_size = 110h0 = model.fit(x_train, y_train, validation_data=(x_valid,y_valid), batch_size=batch_size,epochs=epochs)grado1 = zeros((100000,2))grado2 = zeros((100000,2))for i in range(0,100000):    x = tf.Variable([[x_in[i,0],x_in[i,1]]])        with tf.GradientTape(persistent=True,watch_accessed_variables=True) as t:        t.watch(x)          out1 = model(x)        out2 = tf.math.cos(x[:,0]) + x[:,1]/5  # this works using both a Variable array as well as separated taped variables, i.e., x1t, x2t    gradients1 = t.gradient(out1, x)    gradients2 = t.gradient(out2, x)    if i % 100 == 0:        print(i)    grado1[i,:] = gradients1.numpy()    grado2[i,:] = gradients2.numpy()ymodel = model.predict([x_in])for i in range(2):    plt.figure(figsize=(15,4))    ax = plt.subplot(111)    ax.plot(x_in[:,i],y_out,label='data',linewidth=5)    ax.plot(x_in[:,i],ymodel,'--',label='model',linewidth=5)    plt.ylabel('y')    plt.xlabel('x'+str(i+1))    plt.legend(loc='lower left')        ax2 = ax.twinx()    ax2.plot(x_in[:,i],grado1[:,i],'b--',label='grad1_x'+str(i+1)+' model')    ax2.plot(x_in[:,i],grado2[:,i],'g',label='grad2_x'+str(i+1)+' autodiff formula')    if i == 0:        ax2.plot(x_in[:,0],-sin(x_in[:,0]),'r--',label='grad2_x'+str(i+1)+' explicit formula')    elif i == 1:        ax2.plot(x_in[:,1],1/5*ones(len(x_in[:,1])),'r--',label='grad2_x'+str(i+1)+' explicit formula')    plt.ylabel('dy/dx'+str(i+1))    plt.xlabel('x'+str(i+1))    plt.legend()```### Relevant log output_No response_</details>
"
56328,0,2745,0,0,0,y165749,0,"title:clEnqueueNDRangeKernel failed on TfLiteGpuDelegate for split operation description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.9### Custom CodeNo### OS Platform and Distribution_No response_### Mobile deviceGalaxy S10 (w/ Mali G76)### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?When executing TensorFlow Lite benchmark [binary](https://www.tensorflow.org/lite/performance/measurement#native_benchmark_binary) (built from tf 2.9.1 source) with TfLiteGpuDelegate for certain models on Galaxy S10 (with Mali G76 GPU), I got the following error:> ERROR: TfLiteGpuDelegate Invoke: Failed to clEnqueueNDRangeKernel - Out of resources> ERROR: Node number 44 (TfLiteGpuDelegateV2) failed to invoke.> count=1 curr=238> Benchmarking failed.I found that [clEnqueueNDRangeKernel](https://github.com/tensorflow/tensorflow/blob/d8ce9f9c301d021a69953134185ab728c1c248d3/tensorflow/lite/delegates/gpu/cl/cl_command_queue.cc#L72) returns CL_OUT_OF_RESOURCES only for **split** operation (but working well for other operations). For example, the sixth operation of [resnext14_16x4d.tflite](https://github.com/165749/tflite-models/raw/main/models/resnext14_16x4d.tflite) is a split layer (which splits a tensor in shape [1, 58, 58, 64] into 16 tensors in shape [1, 58, 58, 4]). The clEnqueueNDRangeKernel failed for this operation, and I observed work_groups_count [2, 5, 1] and work_group_size [29, 13, 1] (which is under the limit of CL_DEVICE_MAX_WORK_GROUP_SIZE 384).  I have uploaded a bunch of tflite models with the same issue [here](https://github.com/165749/tflite-models/tree/main/models); they are actually executed smoothly on Pixel 4 (Adreno) and Galaxy A03s (PowerVR). I haven闂傚倸鍊搁崐鐑芥嚄閸洖纾婚柕濞炬櫅绾惧潡鏌＄仦璇插姕闁?confirmed on other devices with Mali GPUs.I appreciate it if any insights can be provided!### Standalone code to reproduce the issue```shell/data/local/tmp/benchmark_model --warmup_runs=1 --num_runs=1 --use_gpu=true --graph=/data/local/tmp/resnext14_16x4d.tflite```### Relevant log outputThe OpenCL information of the Galaxy S10 (Android 12, Mali G76) is attached:```shellCL_PLATFORM_NAME: ARM PlatformCL_PLATFORM_VENDOR: ARMCL_PLATFORM_VERSION: OpenCL 3.0 v1.r32p1-01bet2-mbs2v39_0.131801e953429f661ecce1d5e1d2b3efCL_PLATFORM_PROFILE: FULL_PROFILECL_PLATFORM_EXTENSIONS: cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_byte_addressable_store cl_khr_3d_image_writes cl_khr_int64_base_atomics cl_khr_int64_extended_atomics cl_khr_fp16 cl_khr_icd cl_khr_egl_image cl_khr_image2d_from_buffer cl_khr_depth_images cl_khr_subgroups cl_khr_subgroup_extended_types cl_khr_subgroup_non_uniform_vote cl_khr_subgroup_ballot cl_khr_il_program cl_khr_priority_hints cl_khr_create_command_queue cl_khr_spirv_no_integer_wrap_decoration cl_khr_extended_versioning cl_khr_device_uuid cl_arm_core_id cl_arm_printf cl_arm_non_uniform_work_group_size cl_arm_import_memory cl_arm_import_memory_dma_buf cl_arm_import_memory_host cl_arm_import_memory_android_hardware_buffer cl_arm_integer_dot_product_int8 cl_arm_job_slot_selection cl_arm_scheduling_controls cl_arm_controlled_kernel_termination cl_ext_cxx_for_openclNumber of devices available: 1 CL_DEVICE_NAME: Mali-G76 r0p0CL_DEVICE_TYPE: CL_DEVICE_TYPE_GPUCL_DEVICE_VENDOR: ARMCL_DEVICE_VERSION: OpenCL 3.0 v1.r32p1-01bet2-mbs2v39_0.131801e953429f661ecce1d5e1d2b3efCL_DRIVER_VERSION: 3.0CL_DEVICE_OPENCL_C_VERSION: OpenCL C 3.0 v1.r32p1-01bet2-mbs2v39_0.131801e953429f661ecce1d5e1d2b3efCL_DEVICE_MAX_CLOCK_FREQUENCY: 5 MHzCL_DEVICE_MAX_COMPUTE_UNITS: 12CL_DEVICE_GLOBAL_MEM_SIZE: 6969 MBCL_DEVICE_GLOBAL_MEM_CACHE_SIZE: 1024 KBCL_DEVICE_GLOBAL_MEM_CACHE_TYPE: CL_READ_WRITE_CACHECL_DEVICE_GLOBAL_MEM_CACHELINE_SIZE: 64CL_DEVICE_MAX_MEM_ALLOC_SIZE: 6969 MBCL_DEVICE_LOCAL_MEM_SIZE: 32 KBCL_DEVICE_LOCAL_MEM_TYPE: CL_GLOBALCL_DEVICE_MAX_CONSTANT_ARGS: 128CL_DEVICE_MAX_CONSTANT_BUFFER_SIZE: 7137008 KBCL_DEVICE_MAX_WORK_GROUP_SIZE: 384CL_DEVICE_MAX_WORK_ITEM_DIMENSIONS: 3CL_DEVICE_MAX_WORK_ITEM_SIZES 0: 384CL_DEVICE_MAX_WORK_ITEM_SIZES 1: 384CL_DEVICE_MAX_WORK_ITEM_SIZES 2: 384CL_DEVICE_PROFILE: FULL_PROFILECL_DEVICE_PROFILING_TIMER_RESOLUTION: 1000 nsCL_DEVICE_QUEUE_PROPERTIES: CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLECL_DEVICE_QUEUE_PROPERTIES: CL_QUEUE_PROFILING_ENABLECL_DEVICE_IMAGE_SUPPORT: 1CL_DEVICE_IMAGE2D_MAX_WIDTH: 65536CL_DEVICE_IMAGE2D_MAX_HEIGHT: 65536CL_DEVICE_IMAGE3D_MAX_WIDTH: 65536CL_DEVICE_IMAGE3D_MAX_HEIGHT: 65536CL_DEVICE_IMAGE3D_MAX_DEPTH: 65536CL_DEVICE_MAX_READ_IMAGE_ARGS: 128CL_DEVICE_MAX_WRITE_IMAGE_ARGS: 64CL_DEVICE_MAX_SAMPLERS: 16```</details>
"
56319,0,1463,0,0,0,svenhsia,0,"title:Dataset created from large ragged tensor doesn't copy values description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.9.1### Custom CodeYes### OS Platform and DistributionLinux### Mobile device_No response_### Python version3.9.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shell<tf.Tensor: shape=(256,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0.], dtype=float32)>```### Standalone code to reproduce the issue```shellimport numpy as npimport tensorflow as tfx = np.random.randn(425000, 32, 256).astype('float32')l = [250] * 1700ds = tf.data.Dataset.from_tensor_slices(tf.RaggedTensor.from_row_lengths(x, l))display(next(iter(ds))[0][0])```### Relevant log output_No response_</details>
"
56317,0,17658,269,0,1,elfringham,0,"title:Unit test //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test fails on Python 3.7, 3.8 description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiongit HEAD### Custom CodeNo### OS Platform and DistributionUbuntu 20.04### Mobile devicen/a### Python version3.8.13### Bazel version5.1.1### GCC/Compiler version10.3.0### CUDA/cuDNN versionn/a### GPU model and memoryn/a### Current Behaviour?```shellThe test fails due to Python code that is not supported in 3.7 or 3.8```### Standalone code to reproduce the issue```shellbazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --noremote_accept_cached --config=nonccl --copt=""-mtune=generic"" --copt=""-march=armv8-a"" --copt=""-O3"" --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --verbose_failures --build_tests_only -- //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test```### Relevant log output```shellbazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --noremote_accept_cached --config=nonccl --copt=""-mtune=generic"" --copt=""-march=armv8-a"" --copt=""-O3"" --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --verbose_failures --build_tests_only -- //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_testStarting local Bazel server and connecting to it...INFO: Options provided by the client:  Inherited 'common' options: --isatty=1 --terminal_columns=143INFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc:  Inherited 'common' options: --experimental_repo_remote_execINFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc:  Inherited 'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=trueINFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.tf_configure.bazelrc:  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/home/builder/1/tensorflow_build/venv_py38/bin/python3 --action_env PYTHON_LIB_PATH=/home/builder/1/tensorflow_build/venv_py38/lib/python3.8/site-packages --python_path=/home/builder/1/tensorflow_build/venv_py38/bin/python3INFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc:  Inherited 'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utilsINFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.tf_configure.bazelrc:  'test' options: --flaky_test_attempts=3 --test_size_filters=small,mediumINFO: Found applicable config definition build:short_logs in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --output_filter=DONT_MATCH_ANYTHINGINFO: Found applicable config definition build:v2 in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1INFO: Found applicable config definition test:v2 in file /home/builder/1/tensorflow_build/tensorflow-git/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1onlyINFO: Found applicable config definition build:nonccl in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --define=no_nccl_support=trueINFO: Found applicable config definition build:linux in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changesINFO: Found applicable config definition build:dynamic_kernels in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELSINFO: Analyzed target //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test (474 packages loaded, 28581 targets configured).INFO: Found 1 test target...FAIL: //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test (see /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test/test_attempts/attempt_1.log)FAIL: //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test (see /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test/test_attempts/attempt_2.log)FAIL: //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test (see /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test/test.log)FAILED: //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test (Summary)      /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test/test.log      /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test/test_attempts/attempt_1.log      /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test/test_attempts/attempt_2.logINFO: From Testing //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test:==================== Test output for //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test:WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.checkpoint_management has been moved to tensorflow.python.checkpoint.checkpoint_management. The old module will be deleted in version 2.9.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.resource has been moved to tensorflow.python.trackable.resource. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.util has been moved to tensorflow.python.checkpoint.checkpoint. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base_delegate has been moved to tensorflow.python.trackable.base_delegate. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.graph_view has been moved to tensorflow.python.checkpoint.graph_view. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.python_state has been moved to tensorflow.python.trackable.python_state. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.functional_saver has been moved to tensorflow.python.checkpoint.functional_saver. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.checkpoint_options has been moved to tensorflow.python.checkpoint.checkpoint_options. The old module will be deleted in version 2.11.Traceback (most recent call last):  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test.runfiles/org_tensorflow/tensorflow/compiler/mlir/quantization/tensorflow/python/integration_test/concurrency_test.py"", line 22, in <module>    from tensorflow.compiler.mlir.quantization.tensorflow.python import quantize_model  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test.runfiles/org_tensorflow/tensorflow/compiler/mlir/quantization/tensorflow/python/quantize_model.py"", line 312, in <module>    signature_keys: list[str],TypeError: 'type' object is not subscriptable==================================================================================================== Test output for //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test:WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.checkpoint_management has been moved to tensorflow.python.checkpoint.checkpoint_management. The old module will be deleted in version 2.9.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.resource has been moved to tensorflow.python.trackable.resource. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.util has been moved to tensorflow.python.checkpoint.checkpoint. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base_delegate has been moved to tensorflow.python.trackable.base_delegate. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.graph_view has been moved to tensorflow.python.checkpoint.graph_view. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.python_state has been moved to tensorflow.python.trackable.python_state. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.functional_saver has been moved to tensorflow.python.checkpoint.functional_saver. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.checkpoint_options has been moved to tensorflow.python.checkpoint.checkpoint_options. The old module will be deleted in version 2.11.Traceback (most recent call last):  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test.runfiles/org_tensorflow/tensorflow/compiler/mlir/quantization/tensorflow/python/integration_test/concurrency_test.py"", line 22, in <module>    from tensorflow.compiler.mlir.quantization.tensorflow.python import quantize_model  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test.runfiles/org_tensorflow/tensorflow/compiler/mlir/quantization/tensorflow/python/quantize_model.py"", line 312, in <module>    signature_keys: list[str],TypeError: 'type' object is not subscriptable==================================================================================================== Test output for //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test:WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.checkpoint_management has been moved to tensorflow.python.checkpoint.checkpoint_management. The old module will be deleted in version 2.9.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.resource has been moved to tensorflow.python.trackable.resource. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.util has been moved to tensorflow.python.checkpoint.checkpoint. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base_delegate has been moved to tensorflow.python.trackable.base_delegate. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.graph_view has been moved to tensorflow.python.checkpoint.graph_view. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.python_state has been moved to tensorflow.python.trackable.python_state. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.functional_saver has been moved to tensorflow.python.checkpoint.functional_saver. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.checkpoint_options has been moved to tensorflow.python.checkpoint.checkpoint_options. The old module will be deleted in version 2.11.Traceback (most recent call last):  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test.runfiles/org_tensorflow/tensorflow/compiler/mlir/quantization/tensorflow/python/integration_test/concurrency_test.py"", line 22, in <module>    from tensorflow.compiler.mlir.quantization.tensorflow.python import quantize_model  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test.runfiles/org_tensorflow/tensorflow/compiler/mlir/quantization/tensorflow/python/quantize_model.py"", line 312, in <module>    signature_keys: list[str],TypeError: 'type' object is not subscriptable================================================================================Target //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test up-to-date:  bazel-bin/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_testINFO: Elapsed time: 112.798s, Critical Path: 85.50sINFO: 274 processes: 19 internal, 255 local.INFO: Build completed, 1 test FAILED, 274 total actions//tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test FAILED in 3 out of 3 in 8.4s  Stats over 3 runs: max = 8.4s, min = 3.8s, avg = 5.3s, dev = 2.2s  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test/test.log  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test/test_attempts/attempt_1.log  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test/test_attempts/attempt_2.logINFO: Build completed, 1 test FAILED, 274 total actions```</details>
"
56312,1,1050,20,0,0,Mirandatz,0,"title:Training the same model on the same data yielding extremely different test accuracy description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.8.0 (From Google Colab)### Custom CodeNo### OS Platform and DistributionGoogle Colab### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versionGoogle Colab### GPU model and memory_No response_### Current Behaviour?```shellI wrote a function to load datasets (train, val, and test), create a model, train the model on the dataset train partition, and compute its accuracy on the test partition.I expected that running this function multiple times in a row would yield similar results, but they are extremely different, ranging from .20 to .70 accuracy.I tried to remove most sources of randomness, such as using the same numpy rng seed, TensorFlow rng seed, resetting the TensorFlow backend between runs, and using the same seed for shuffling the dataset, etc. The issue persists.The issue occurs both in my local machine running Tensorflow 2.10.0-dev20220424 and in the Google Colab running TensorFlow 2.8.```### Standalone code to reproduce the issue```shellI created a Google Colab notebook that reproduces the issue: https://colab.research.google.com/drive/1b2DZaAIjqqXDlQ_uJyinVxOn1z_cbAoT?usp=sharingThe dataset I am using is just CIFAR-10 with the training partition split into train and validation:https://drive.google.com/file/d/1UXJOv9MJ7bu_0woiVVp0BmcnfDzo0hpf/view?usp=sharing```### Relevant log output_No response_</details>
"
56308,1,0,0,0,0,mangelozzi,0,"title:Fails to import tensorflow description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.9.1### Custom CodeNo### OS Platform and DistributionUbuntu 20.04### Mobile device_No response_### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellWhen try to import it has a circular import error.```### Standalone code to reproduce the issue```shell$ pythonPython 3.9.13 (main, May 23 2022, 22:01:06) [GCC 9.4.0] on linuxType ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> import tensorflow as tf2022-05-31 12:09:07.100479: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory2022-05-31 12:09:07.100505: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.Traceback (most recent call last):  File ""<stdin>"", line 1, in <module>  File ""/home/michael/.venv/ml/lib/python3.9/site-packages/tensorflow/__init__.py"", line 37, in <module>    from tensorflow.python.tools import module_util as _module_util  File ""/home/michael/.venv/ml/lib/python3.9/site-packages/tensorflow/python/__init__.py"", line 37, in <module>    from tensorflow.python.eager import context  File ""/home/michael/.venv/ml/lib/python3.9/site-packages/tensorflow/python/eager/context.py"", line 25, in <module>    from absl import logging  File ""/home/michael/.venv/ml/lib/python3.9/site-packages/absl/logging/__init__.py"", line 88, in <module>    import timeit  File ""/home/michael/timeit.py"", line 1, in <module>    from timeit import timeitImportError: cannot import name 'timeit' from partially initialized module 'timeit' (most likely due to a circular import) (/home/michael/timeit.py)>>>```### Relevant log output```shellSee above```</details>
"
56307,1,13570,8,0,1,jyh2378,0,"title:tensorflow.lite.python.convert_phase.ConverterError: device assign error description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versionv2.9.0-rc2-42-g8a20d54a3c1 2.9.0### Custom CodeNo### OS Platform and DistributionLinux CentOS 7### Mobile device_No response_### Python version3.9.13### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version11.2 / 8.1.1### GPU model and memory_No response_### Current Behaviour?```shellI tried to convert the tf saved_mode(.pb) to tflite model. my computer can use gpu device, but I have error that some operation cannot assigned to gpu device. How can I fix it?```### Standalone code to reproduce the issue```shellhttps://colab.research.google.com/drive/1kDOUXMha543jKJxMl0w4CEExx7G461Q6?usp=sharing```### Relevant log output```shell2022-05-31 17:55:48.396106: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2022-05-31 17:55:54.142361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30983 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:15:00.0, compute capability: 7.02022-05-31 17:55:54.144955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30983 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:16:00.0, compute capability: 7.02022-05-31 17:55:54.147135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 30983 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3a:00.0, compute capability: 7.02022-05-31 17:55:54.149256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30983 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.02022-05-31 17:55:54.151379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 30983 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:89:00.0, compute capability: 7.02022-05-31 17:55:54.153498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 30983 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:8a:00.0, compute capability: 7.02022-05-31 17:55:54.155614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 30983 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b2:00.0, compute capability: 7.02022-05-31 17:55:54.157735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 30983 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b3:00.0, compute capability: 7.0WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1a/kernel:0' shape=(3, 3, 3, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1a/bias:0' shape=(16,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1aa/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1aa/bias:0' shape=(16,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1b/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1a/kernel:0' shape=(3, 3, 3, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1a/bias:0' shape=(16,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1aa/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1aa/bias:0' shape=(16,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1b/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1a/kernel:0' shape=(3, 3, 3, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1a/bias:0' shape=(16,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1aa/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1aa/bias:0' shape=(16,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1b/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1a/kernel:0' shape=(3, 3, 3, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1a/bias:0' shape=(16,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1aa/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1aa/bias:0' shape=(16,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1b/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().2022-05-31 17:56:01.538586: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.2022-05-31 17:56:01.538654: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.2022-05-31 17:56:01.539779: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: ./models/saved_model2022-05-31 17:56:01.562091: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve }2022-05-31 17:56:01.562154: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: ./models/saved_model2022-05-31 17:56:01.640074: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled2022-05-31 17:56:01.711920: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: fail: INVALID_ARGUMENT: Cannot assign a device for operation pwcnet/featpyr/strided_slice: {{node pwcnet/featpyr/strided_slice}} was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.         [[pwcnet/featpyr/strided_slice]]. Took 172109 microseconds.2022-05-31 17:56:01.712003: E tensorflow/compiler/mlir/tensorflow/translate/tf_mlir_translate.cc:199] Failed to load saved model v1 './models/saved_model': INVALID_ARGUMENT: Cannot assign a device for operation pwcnet/featpyr/strided_slice: {{node pwcnet/featpyr/strided_slice}} was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.         [[pwcnet/featpyr/strided_slice]]Traceback (most recent call last):  File ""/home1/irteam/users/yeonghwa.jin/tfoptflow/tfoptflow/export_tflite.py"", line 51, in <module>    export_tflite(saved_model_dir)  File ""/home1/irteam/users/yeonghwa.jin/tfoptflow/tfoptflow/export_tflite.py"", line 42, in export_tflite    tflite_model = converter.convert()  File ""/home1/irteam/.pyenv/versions/tf2.9.0_cuda11.2_cudnn8.1.1/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 929, in wrapper    return self._convert_and_export_metrics(convert_func, *args, **kwargs)  File ""/home1/irteam/.pyenv/versions/tf2.9.0_cuda11.2_cudnn8.1.1/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 908, in _convert_and_export_metrics    result = convert_func(self, *args, **kwargs)  File ""/home1/irteam/.pyenv/versions/tf2.9.0_cuda11.2_cudnn8.1.1/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 1212, in convert    return self._convert_from_saved_model(graph_def)  File ""/home1/irteam/.pyenv/versions/tf2.9.0_cuda11.2_cudnn8.1.1/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 1095, in _convert_from_saved_model    result = _convert_saved_model(**converter_kwargs)  File ""/home1/irteam/.pyenv/versions/tf2.9.0_cuda11.2_cudnn8.1.1/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py"", line 212, in wrapper    raise converter_error from None  # Re-throws the exception.  File ""/home1/irteam/.pyenv/versions/tf2.9.0_cuda11.2_cudnn8.1.1/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py"", line 205, in wrapper    return func(*args, **kwargs)  File ""/home1/irteam/.pyenv/versions/tf2.9.0_cuda11.2_cudnn8.1.1/lib/python3.9/site-packages/tensorflow/lite/python/convert.py"", line 809, in convert_saved_model    data = convert(  File ""/home1/irteam/.pyenv/versions/tf2.9.0_cuda11.2_cudnn8.1.1/lib/python3.9/site-packages/tensorflow/lite/python/convert.py"", line 311, in convert    raise converter_errortensorflow.lite.python.convert_phase.ConverterError: Cannot assign a device for operation pwcnet/featpyr/strided_slice: {{node pwcnet/featpyr/strided_slice}} was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.         [[pwcnet/featpyr/strided_slice]]```</details>
"
56293,1,1992,90,0,0,ghost,0,"title:Unable to batch dataset using `.batch` and `.padded_batch` description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.9.1### Custom CodeYes### OS Platform and DistributionmacOS Monterey 12.3.1### Mobile device_No response_### Python version3.10.4### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellI'm writing some variable length string feature to tfrecord. If the feature has the same shape for all examples, it runs perfectly fine without problems. If the shape varies, the error below is raised whenever the created tfrecord is being read.```### Standalone code to reproduce the issue```shellimport randomimport numpy as npimport tensorflow as tfdef serialize_example(writer):    # s = np.array(['aaa' for _ in range(10)])  # this works fine    s = np.array(['aaa' for _ in range(random.randint(1, 100))])    features = {        'f1': tf.train.Feature(            bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(s).numpy()])        )    }    example = tf.train.Example(features=tf.train.Features(feature=features))    writer.write(example.SerializeToString())def create_tfrecord(output_path):    with tf.io.TFRecordWriter(output_path) as writer:        for i in range(total := 100):            print(f'\rWriting example: {i + 1}/{total}', end='')            serialize_example(writer)def read_example(example, feature_map):    features = tf.io.parse_single_example(example, feature_map)    f1 = tf.sparse.to_dense(features['f1'])    f1 = tf.io.parse_tensor(f1[0], tf.string)    return f1def read_tfrecord(fp, batch_size):    files = tf.data.Dataset.list_files(fp)    dataset = files.flat_map(tf.data.TFRecordDataset)    feature_map = {        'f1': tf.io.VarLenFeature(tf.string),    }    return dataset.map(        lambda x: read_example(x, feature_map),        tf.data.experimental.AUTOTUNE,    ).batch(batch_size)  # if this is removed, both cases work fineif __name__ == '__main__':    create_tfrecord('xyz.tfrecord')    dataset = read_tfrecord('xyz.tfrecord', 8)    sample = dataset.take(1).as_numpy_iterator().next()```### Relevant log output```shelltensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [83], [batch]: [32] [Op:IteratorGetNext]```</details>
"
56282,1,2633,0,0,0,delulu,0,"title:Gets stuck in tensorflow.test.is_gpu_available() description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontensorflow_gpu-1.4.0### Custom CodeNo### OS Platform and DistributionUbuntu 18.04.5 LTS### Mobile device_No response_### Python version2.7### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shell# pythonPython 2.7.12 (default, Nov 19 2016, 06:48:10)[GCC 5.4.0 20160609] on linux2Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> import tensorflow as tf>>> tf.test.is_gpu_available()2022-05-27 04:09:29.325229: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMAGET STUCK...Normally it should return msg like ""Found device 0 with properties:""```### Standalone code to reproduce the issue```shell# pythonPython 2.7.12 (default, Nov 19 2016, 06:48:10)[GCC 5.4.0 20160609] on linux2Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> import tensorflow as tf>>> tf.test.is_gpu_available()```### Relevant log output```shellFri May 27 04:15:13 2022+-----------------------------------------------------------------------------+| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     ||-------------------------------+----------------------+----------------------+| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. ||                               |                      |               MIG M. ||===============================+======================+======================||   0  Tesla M60           Off  | 00000001:00:00.0 Off |                  Off || N/A   41C    P8    22W / 150W |      8MiB /  8129MiB |      0%      Default ||                               |                      |                  N/A |+-------------------------------+----------------------+----------------------+|   1  Tesla M60           Off  | 00000002:00:00.0 Off |                  Off || N/A   34C    P8    15W / 150W |      3MiB /  8129MiB |      0%      Default ||                               |                      |                  N/A |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes:                                                                  ||  GPU   GI   CI        PID   Type   Process name                  GPU Memory ||        ID   ID                                                   Usage      ||=============================================================================||    0   N/A  N/A     50650      C   python                              2MiB ||    0   N/A  N/A    100924      C   python                              2MiB |+-----------------------------------------------------------------------------+```</details>
"
56280,0,1318,7,0,1,plliao,0,"title:Potential race condition in nsync library that causes SIGSEGV or deadlock description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.4### Custom CodeNo### OS Platform and DistributionLinux Redhat 7.6### Mobile device_No response_### Python version3.7### Bazel version3.1.0### GCC/Compiler version7### CUDA/cuDNN version11.0/8.0.2### GPU model and memory_No response_### Current Behaviour?Our TensorFlow application which uses [nsync](https://github.com/google/nsync) for mutex/cv implementation failed because of SIGSEGV. The issue can be reproduced during thread pool destruction with thread-local [TraceMeRecorder::ThreadLocalRecorderWrapper](https://github.com/tensorflow/tensorflow/blob/efa65ea9f1fa3d0baac82366f4405f2470797e04/tensorflow/core/profiler/backends/cpu/traceme_recorder.cc#L282) used in each thread. After adding logs into nsync library, we are able to identify the root cause.#### Root causeDestruction order of thread-local variables is not deterministic in [POSIX](https://linux.die.net/man/3/pthread_key_create). Mutex or conditional variable in TensorFlow is a thread local object. If another thread local object e.g. per-thread TraceMe recorder use a lock in its destructor, the lock can already be destructed and the behavior is undefined.#### DetailsFor example, the following logs show two threads(0x7f4118748700 and 0x7f4100718700) share the same nsync waiter object (0x7f465c2c2290) and cause SIGSEGV. A nsync waiter object is a thread local object used by nsync to implement a lock. First, the waiter_destroy i.e. lock destructor was called on thread 0x7f4118748700 and the waiter 0x7f465c2c2290 had been recycled into free_waiters. At this moment, the other thread 0x7f4100718700 enter and take the waiter from free_waiters. Then, the old thread 0x7f4118748700 tries to take a lock again as the other thread-local object use a lock in its destructor. Because the thread-local state is not cleaned up in the old thread 0x7f4118748700, it assumes the waiter object is still reserved by itself. It ends up with two threads hold the same waiter object. The object was freed twice and failed the assertion in nsync_waiter_free_.```shell[nsync] waiter_destroy(internal/common.c#L150) tid 0x7f4118748700, w 0x7f465c2c2290[nsync] nsync_waiter_new_(internal/common.c#L182) tid 0x7f4100718700, w 0x7f465c2c2290[nsync] nsync_waiter_new_(internal/common.c#L171) tid 0x7f4118748700, tw 0x7f465c2c2290[nsync] nsync_waiter_new_((internal/common.c#L206) tid 0x7f4100718700, w 0x7f465c2c2290[nsync] nsync_mu_lock_slow_(internal/mu.c#L102) tid 0x7f4118748700, mu 0x12482b40, waiter 0x7f465c2c2290[nsync] nsync_mu_lock_slow_(internal/mu.c#L71) tid 0x7f4100718700, mu 0x7f4ae4593f30, waiter 0x7f465c2c2290[nsync] nsync_mu_lock_slow_(internal/mu.c#L71) tid 0x7f4118748700, mu 0x12482b40, waiter 0x7f465c2c2290# A fatal error has been detected by the Java Runtime Environment:## SIGSEGV (0xb) at pc=0x00007f4af0e74eda, pid=6, tid=0x00007f4118748700## JRE version: Java(TM) SE Runtime Environment (8.0_172-b11) (build 1.8.0_172-b11)# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.172-b11 mixed mode linux-amd64 compressed oops)# Problematic frame:# C [pywrap_tensorflow_internal.so+0xc8cbeda] nsync::nsync_waiter_free(nsync::waiter*)+0xa## Core dump written. Default location: /opt/code-fetcher-system/src/core or core.6## An error report file with more information is saved as:# /opt/code-fetcher-system/src/hs_err_pid6.log```Deadlock can also happen when the waiter is moved to another mutex before it got notified. For example, two threads above were locked on two different mutex (0x12482b40 and 0x7f4ae4593f30). In this case, the waiter semaphore is decremented twice but only incremented once.P.S. The thread-local variable that access locks in its destructor in TensorFlow is the per-thread TraceMeRecorder::ThreadLocalRecorderWrapper used in TensorFlow profiler.### Standalone code to reproduce the issueN/AThe issue was fixed in nsync upstream. See [PR](https://github.com/google/nsync/pull/12) for more details. We need help from Google to release a new nsync version and upgrade nsync version in TensorFlow. Thank you so much!### Relevant log output_No response_</details>
"
56279,0,2694,53,0,0,alexarmstrongvi,0,"title:Bad file descriptor error during cleanup of MirroredStrategy model description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.8.0### Custom CodeNo### OS Platform and DistributionLinux Red Hat### Mobile device_No response_### Python version3.9.12### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versionCUDA 11.1 / cuDNN 8.2.1.32### GPU model and memory_No response_### Current Behaviour?```shellCreating a model within the scope of a MirroredStrategy results in an OSError during memory cleanup at the end of the program. I would expect it to run without an error.Others seem to have encountered a similar issue:- https://stackoverflow.com/questions/66954494/oserror-errno-9-bad-file-descriptor-in-tensorflow-estimater-when-deploying-mo- https://sm-stackoverflow.azurefd.net/questions/71965552/problem-when-saving-model-after-mirroredstrategy-in-kerasI encountered this error following the example athttps://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit.After getting the error, I trimmed it down to the included minimal example.```### Standalone code to reproduce the issue```shell'''Environment setup>> conda create -n test_tf -c conda-forge python=3.9.12 tensorflow=2.8.0 cudnn=8.2.1.32>> export CUDA_VISIBLE_DEVICES=""""'''import osos.environ['CUDA_VISIBLE_DEVICES'] = ""0""import tensorflow as tffrom tensorflow import kerasdef main():    print('GPUs:', tf.config.list_physical_devices('GPU'))    with tf.distribute.MirroredStrategy().scope():        model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])    print('Output shape:', model.output_shape)if __name__ == '__main__':    main()```### Relevant log output```shellGPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]2022-05-21 14:16:41.133359: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2022-05-21 14:16:41.633403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0Output shape: (None, 1)Exception ignored in: <function Pool.__del__ at 0x2aabb1d85670>Traceback (most recent call last):  File ""~/.conda/envs/test_tf/lib/python3.9/multiprocessing/pool.py"", line 268, in __del__    self._change_notifier.put(None)  File ""~/.conda/envs/test_tf/lib/python3.9/multiprocessing/queues.py"", line 378, in put    self._writer.send_bytes(obj)  File ""~/.conda/envs/test_tf/lib/python3.9/multiprocessing/connection.py"", line 205, in send_bytes    self._send_bytes(m[offset:offset + size])  File ""~/.conda/envs/test_tf/lib/python3.9/multiprocessing/connection.py"", line 416, in _send_bytes    self._send(header + buf)  File ""~/.conda/envs/test_tf/lib/python3.9/multiprocessing/connection.py"", line 373, in _send    n = write(self._handle, buf)OSError: [Errno 9] Bad file descriptor```</details>
"
56266,1,666,14,0,0,bonastreyair,0,"title:New released protobuf +v3.20 causes an error when importing  description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.7.1### Custom CodeNo### OS Platform and DistributionLinux 20.04### Mobile device_No response_### Python version3.8.12### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellAfter the new protobuf release and installing tensorflow an error appears when importing it.https://pypi.org/project/protobuf/4.21.0/```### Standalone code to reproduce the issue```shellAfter installing tensorflow 2.7.1 running a simple `import tensorflow````### Relevant log output```shellTypeError: Descriptors cannot not be created directly.If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.If you cannot immediately regenerate your protos, some other possible workarounds are: 1. Downgrade the protobuf package to 3.20.x or lower. 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).```</details>
"
56253,1,4582,14,0,0,fjzhangcr,0,"title:converter.convert() to a tflite file sucessfully, but interpreter.invoke() raise RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 5 (FlexSize) failed to prepare. description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.3### Custom CodeYes### OS Platform and Distributionwindows 10### Mobile device_No response_### Python version3.7### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellA bug happened!i have a model contains codes like this:    grid_size = tf.shape(conv_output)[1:3] # shape=(13,13)    x_grid,y_grid = tf.meshgrid(tf.range(grid_size[1]), tf.range(grid_size[0]))  # shape=(13,13)i can generate tflite file sucessfullybut when i open this tflite file, and use interpreter.invoke(), the runtime error occurs:  File ""D:\Anaconda3\envs\CV_TF23_py37\lib\site-packages\tensorflow\lite\python\interpreter.py"", line 524, in invoke    self._interpreter.Invoke()RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 5 (FlexSize) failed to prepare.```### Standalone code to reproduce the issue```shellimport tensorflow as tfimport numpy as npclass CFGGG:    SAVED_MODEL_DIR='./zz_tflite_op_support/demo'    TFLITE_MODEL_FILENAME='./zz_tflite_op_support/demo.tflite'inputs=tf.keras.layers.Input((5,5,2))def pre_process(x):    grid_x,grid_y=tf.shape(x)[1:3]    # range_x=tf.range(5) # when replace 5 with grid_x闂傚倸鍊搁崐鐑芥倿閿旈敮鍋撶粭娑樻噽閻瑩鏌熸潏鎯х槣闁轰礁锕弻鐔煎箵閹哄鍋撴俊鏄砿e error occurs when invoke    # range_y=tf.range(5) # when replace 5 with grid_y闂傚倸鍊搁崐鐑芥倿閿旈敮鍋撶粭娑樻噽閻瑩鏌熸潏鎯х槣闁轰礁锕弻鐔煎箵閹哄鍋撴俊鏄砿e error occurs when invoke    range_x=tf.range(grid_x)     range_y=tf.range(grid_y)     x_grid,y_grid=tf.meshgrid(range_y,range_x)        b=tf.stack([y_grid,x_grid],axis=-1)    b=tf.cast(b,tf.float32)    y=x+b    return y    x=pre_process(inputs)outputs=tf.keras.layers.Dense(10)(x)model_debug=tf.keras.Model(inputs=inputs,outputs=x)model_debug.summary()model_debug.save(CFGGG.SAVED_MODEL_DIR)converter = tf.lite.TFLiteConverter.from_saved_model(CFGGG.SAVED_MODEL_DIR)converter.optimizations = [tf.lite.Optimize.DEFAULT]converter.target_spec.supported_types = [tf.compat.v1.lite.constants.FLOAT16]converter.target_spec.supported_ops = [    tf.lite.OpsSet.TFLITE_BUILTINS,    tf.lite.OpsSet.SELECT_TF_OPS]converter.allow_custom_ops = Truetflite_model = converter.convert()with open(CFGGG.TFLITE_MODEL_FILENAME, 'wb') as f:    f.write(tflite_model)    interpreter = tf.lite.Interpreter(model_path=CFGGG.TFLITE_MODEL_FILENAME)interpreter.allocate_tensors()input_details = interpreter.get_input_details()output_details = interpreter.get_output_details()# Explore the model input output shape.print(input_details)print(output_details)input_shape = input_details[0]['shape']print(""input shape"",input_shape)for i, output_detail in enumerate(output_details):    output_shape = output_detail['shape']    print(""No {} output shape {}"".format(i,output_shape))# Test the model on random input data.input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)interpreter.set_tensor(input_details[0]['index'], input_data)print(""input data complete: "",input_data.shape)interpreter.invoke()# The function `get_tensor()` returns a copy of the tensor data.# Use `tensor()` in order to get a pointer to the tensor.for i in range(len(output_details)):    output_data_tmp = interpreter.get_tensor(output_details[i]['index'])    output_shape_tmp = output_data_tmp.shape    print(""invoke() complete闂傚倸鍊搁崐鐑芥倿閿旈敮鍋撶粭娑樻噽閻瑩鏌熸潏鎯х槣闁轰礁妫欓妵?{} output shape {}"".format(i,output_shape_tmp))```### Relevant log output```shellINFO:tensorflow:Assets written to: ./zz_tflite_op_support/demo\assetsINFO:tensorflow:Assets written to: ./zz_tflite_op_support/demo\assets[{'name': 'input_27', 'index': 0, 'shape': array([1, 5, 5, 2]), 'shape_signature': array([-1,  5,  5,  2]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}][{'name': 'Identity', 'index': 25, 'shape': array([1, 5, 5, 2]), 'shape_signature': array([-1,  5,  5,  2]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]input shape [1 5 5 2]No 0 output shape [1 5 5 2]intpu data complete: (1, 5, 5, 2)Traceback (most recent call last):  File ""D:\OneDrive\AI_Working_Directory\prj_quickstart\zzz_debug_tflite_op_support.py"", line 103, in <module>    interpreter.invoke()  File ""D:\Anaconda3\envs\CV_TF23_py37\lib\site-packages\tensorflow\lite\python\interpreter.py"", line 524, in invoke    self._interpreter.Invoke()RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 5 (FlexSize) failed to prepare.```</details>
"
56252,1,6199,6,0,0,FengMu1995,0,"title:convert to TF-Lite ValueError: Cannot iterate over a shape with unknown rank description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf2.3.1### Custom CodeNo### OS Platform and DistributionLinux Ubuntu 18.04### Mobile device_No response_### Python version3.6### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellwhen I convert the savedmodel to tflite model, error happensValueError: Cannot iterate over a shape with unknown rank.```### Standalone code to reproduce the issue```shellimport tensorflow as tfconverter = tf.lite.TFLiteConverter.from_saved_model('model') converter.target_spec.supported_ops = [    tf.lite.OpsSet.TFLITE_BUILTINS,    tf.lite.OpsSet.SELECT_TF_OPS    ]tflite_model = converter.convert()with open('rvm.tflite', 'wb') as f:     f.write(tflite_model)```### Relevant log output```shell2022-05-25 12:59:38.001545: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory2022-05-25 12:59:38.001571: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.2022-05-25 12:59:39.165540: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory2022-05-25 12:59:39.165560: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)2022-05-25 12:59:39.165578: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (wudi-QiTianM620-N000): /proc/driver/nvidia/version does not exist2022-05-25 12:59:39.165701: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2022-05-25 12:59:39.190199: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2899885000 Hz2022-05-25 12:59:39.190662: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55dc4ea19680 initialized for platform Host (this does not guarantee that XLA will be used). Devices:2022-05-25 12:59:39.190693: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version2022-05-25 12:59:46.493553: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 02022-05-25 12:59:46.493624: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session2022-05-25 12:59:46.608109: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize2022-05-25 12:59:46.608136: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 2419 nodes (2082), 6014 edges (5672), time = 50.604ms.2022-05-25 12:59:46.608141: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 2419 nodes (0), 6014 edges (0), time = 26.182ms.2022-05-25 12:59:46.608144: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: __inference_cond_1_true_10408_92122022-05-25 12:59:46.608147: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.2022-05-25 12:59:46.608150: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.2022-05-25 12:59:46.608153: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: __inference_cond_true_8440_267092022-05-25 12:59:46.608156: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.2022-05-25 12:59:46.608158: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.2022-05-25 12:59:46.608161: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: __inference_cond_false_8441_365592022-05-25 12:59:46.608164: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.2022-05-25 12:59:46.608167: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.2022-05-25 12:59:46.608170: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: __inference_cond_1_false_10409_216972022-05-25 12:59:46.608173: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.2022-05-25 12:59:46.608176: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.Traceback (most recent call last):  File ""/home/wudi/project/project_rvm/Matting(tensorflow)/convert1.py"", line 11, in <module>    tflite_model = converter.convert()  File ""/home/wudi/Software/yes/envs/onnx2tf/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 1076, in convert    return super(TFLiteConverterV2, self).convert()  File ""/home/wudi/Software/yes/envs/onnx2tf/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 900, in convert    self).convert(graph_def, input_tensors, output_tensors)  File ""/home/wudi/Software/yes/envs/onnx2tf/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 633, in convert    **converter_kwargs)  File ""/home/wudi/Software/yes/envs/onnx2tf/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 567, in toco_convert_impl    input_tensors, output_tensors, *args, **kwargs)  File ""/home/wudi/Software/yes/envs/onnx2tf/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 458, in build_toco_convert_protos    for dim in shape:  File ""/home/wudi/Software/yes/envs/onnx2tf/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py"", line 859, in __iter__    raise ValueError(""Cannot iterate over a shape with unknown rank."")ValueError: Cannot iterate over a shape with unknown rank.```</details>
"
56239,1,0,0,0,0,sharvin-vittappan,0,"title:Memory leak in TfLiteModelCreateFromFile() in tflite v2.8.0 GNU Linux x86-64 description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.8.0### Custom CodeNo### OS Platform and DistributionLinux ubuntu 20.04### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler versiongcc version 8.3.0### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellA bug happened!I see memory leak from TfLiteModelCreateFromFile().```### Standalone code to reproduce the issue```shellWhen I compile below code##############################include <stdio.h>#include ""c_api.h""int main(){    TfLiteModel* model = NULL;    model = TfLiteModelCreateFromFile(""model.tflite"");    if (model == NULL)    {        printf(""Model opening failed\n"");    }    TfLiteModelDelete(model);        return 0;}#############################using g++ app.c -I <path to c_api.h> -L <path to libtensorflowlite_c.so >-ltensorflowlite_c```### Relevant log output```shellvalgrind --leak-check=full --track-origins=yes ./a.outHEAP SUMMARY:==24315==     in use at exit: 8 bytes in 1 blocks==24315==   total heap usage: 6 allocs, 5 frees, 72,824 bytes allocated==24315====24315== LEAK SUMMARY:==24315==    definitely lost: 0 bytes in 0 blocks==24315==    indirectly lost: 0 bytes in 0 blocks==24315==      possibly lost: 0 bytes in 0 blocks==24315==    still reachable: 8 bytes in 1 blocks==24315==         suppressed: 0 bytes in 0 blocksIt looks like object StderrReporter is not freed. ErrorReporter* DefaultErrorReporter() {  static StderrReporter* error_reporter = new StderrReporter;  return error_reporter;}```</details>
"
56235,1,607,6,0,0,v3551G,0,"title:GPU can not be identified when using Tensorflow 2.4./2.5 with CUDA11.6 description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.4### Custom CodeYes### OS Platform and DistributionUbuntu 20.04### Mobile device_No response_### Python version3.8### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version11.6/### GPU model and memoryRTX 3080   ### Current Behaviour?```shellGPU can not be identified under the tensorflow2.4.0 environment, and the code can only running using CPU. I am wandering whether Tensorflow with higher version (>=2.5) supports GPU, or whether TensorFlow 2.4/2.5 is compatible with CUDA 11.6.Note that two RTX 3080 GPUs were installed on the machine, and the CUDA and CUDNN are installed properly, since there is no bugs when running other programs written based on Pytorch and GPU can be identified.```### Standalone code to reproduce the issue```shellThis is an environmental problem. The GPU cannot be recognized using tensorflow 2.4 / 2.5 framework, but it can be recognized using pytorch.```### Relevant log output_No response_</details>
"
56226,0,0,0,0,0,andsmi97,0,"title:Cholesky decomposition half precision either does not work or wrong documentation. description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.8### Custom CodeYes### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memoryTesla T4### Current Behaviour?```shellCholesky crashes for half precision while documentation says otherwise. Quote from documentation: input	A Tensor. Must be one of the following types: float64, float32, half, complex64, complex128. Shape is [..., M, M].https://www.tensorflow.org/api_docs/python/tf/linalg/cholesky```### Standalone code to reproduce the issue```shellimport tensorflow as tfA = tf.random.normal((32,32), dtype=tf.dtypes.float16) # same for tf.dtypes.halfU = A @ tf.transpose(A)tf.linalg.cholesky(U)```### Relevant log output```shellCould not find device for node: {{node Cholesky}} = Cholesky[T=DT_HALF]All kernels registered for op Cholesky:  device='CPU'; T in [DT_COMPLEX128]  device='CPU'; T in [DT_COMPLEX64]  device='CPU'; T in [DT_DOUBLE]  device='CPU'; T in [DT_FLOAT]  device='GPU'; T in [DT_COMPLEX128]  device='GPU'; T in [DT_COMPLEX64]  device='GPU'; T in [DT_DOUBLE]  device='GPU'; T in [DT_FLOAT] [Op:Cholesky]```</details>
"
56216,1,0,0,0,0,snowuyl,0,"title:pod install command can't install TensorFlowLiteSwift 2.9.0 on MacBook Air M1 description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.9### Custom CodeNo### OS Platform and DistributionmacOS 12.3.1闂傚倸鍊搁崐鐑芥倿閿旈敮鍋撶粭娑樻噽閻瑩鏌熺€涙绠伴柤? arm64CPU)### Mobile deviceiPhone 12### Python version3.9.12### Bazel version5.0.0### GCC/Compiler versionApple clang version 13.1.6### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellpod install command can't install TensorFlowLiteSwift 2.9.0```### Standalone code to reproduce the issue```shellUsing Xcode to create TFLiteSwift projectcd TFLiteSwiftpod initAdding the following command to Podfilepod 'TensorFlowLiteSwift' '~> 2.9.0'pod install```### Relevant log output```shellpod installAnalyzing dependencies[!] Unable to find a specification for `TensorFlowLiteSwift~> 2.9.0`You have either: * out-of-date source repos which you can update with `pod repo update` or with `pod install --repo-update`. * mistyped the name or version. * not added the source repo that hosts the Podspec to your Podfile.```</details>
"
56181,1,882,73,0,0,fredo838,0,"title:`tf.image.resize` different result when inside a `tf.function` description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versionv2.9.0-rc2-42-g8a20d54a3c1 2.9.0### Custom CodeNo### OS Platform and DistributionLinux Ubuntu 18.04### Mobile device_No response_### Python version3.8### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellWhen you put a `tf.image.resize` op inside a `tf.function` whilst using `tf.RaggedTensor`s, the result changes.```### Standalone code to reproduce the issue```pythonimport numpy as npimport tensorflow as tfnp.random.seed(0)batch1 = tf.cast(tf.ragged.constant([255*np.random.uniform(size=(2000, 2000))]), tf.uint8)batch1 = tf.expand_dims(batch1, axis=-1)batch1 = tf.concat([batch1, batch1, batch1], axis=-1)sign = tf.RaggedTensorSpec((1, None, None, 3), tf.uint8, 2, tf.int64)@tf.function(input_signature=(sign,))def resize_tf(images):  return tf.image.resize(images, (50, 50)) / 255.  def resize_non_tf(images):  return tf.image.resize(images, (50, 50)) / 255.  print(tf.reduce_mean(resize_tf(batch1)))print(tf.reduce_mean(resize_non_tf(batch1)))and then run `python3 test.py````### Relevant log output```shelltf.Tensor(0.49723607, shape=(), dtype=float32)tf.Tensor(0.497236, shape=(), dtype=float32)```</details>
"
56178,1,277,3,0,0,IBSApple,0,"title:In Pose detection when I use front camera it display opposite mirror how to resolve it?  description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versionpod 'TensorFlowLiteSwift', '~> 0.0.1-nightly', :subspecs => ['CoreML', 'Metal']### Custom CodeNo### OS Platform and DistributionMacOS### Mobile deviceiPhone X### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellIn Pose detection when I use front camera it display opposite mirror how to resolve it? also camera resolution also need to clear.```### Standalone code to reproduce the issue```shellMust display proper mirror frame & resolution when front camera start ie. iPhone default camera app front camera mirror & resolution.```### Relevant log output_No response_</details>
"
56176,1,466,13,0,0,snowuyl,0,"title:Xcode 13.3.1 build simple project failed on MacBook Air M1 description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.9### Custom CodeNo### OS Platform and DistributionmacOS ### Mobile deviceiPhone 12### Python version 3.9.12### Bazel version5.0.0### GCC/Compiler versionApple clang version 13.1.6### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellXcode 13.3.1 build simple project failed on MacBook Air M1.```### Standalone code to reproduce the issue```shellgit clone https://github.com/tensorflow/tensorflow tensorflow-2.9.0_iosgit checkout r2.9cd tensorflow-2.9.0_ios/tensorflow/lite/examples/iOS/simplepod installopen simple.xcworkspaceopen simple.xcodeprojUsing Xcode to build simple project. But the following error occurred.Use of undeclared identifier 'RegisterSelectedOps'```### Relevant log output```shellUse of undeclared identifier 'RegisterSelectedOps'```</details>
"
56170,0,3635,15,0,0,leeflix,0,"title:Variable constant folding is failed. Please consider using enabling `experimental_enable_resource_variables` flag in the TFLite converter object. For example, converter.experimental_enable_resource_variables = True description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.8### Custom CodeNo### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellI am trying to export a generator model (part of GAN) with TFLite which fails. I provide a Collab to reproduce the error.```### Standalone code to reproduce the issue```shellhttps://colab.research.google.com/drive/1SPUX9b5hHw0mBeU11y1fOE6WigK3CrgU?usp=sharing# example of defining a u-net encoder-decoder generator modelfrom tensorflow.keras.initializers import RandomNormalfrom tensorflow.keras.layers import Input, Activation, BatchNormalization, Concatenate, Conv2D, Conv2DTranspose, Dropout, LeakyReLUfrom tensorflow.keras.models import Modelfrom tensorflow.keras.utils import plot_model# define an encoder blockdef define_encoder_block(layer_in, n_filters, batchnorm=True):    # weight initialization    init = RandomNormal(stddev=0.02)    # add downsampling layer    g = Conv2D(n_filters, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(layer_in)    # conditionally add batch normalization    if batchnorm:        g = BatchNormalization()(g, training=True)    # leaky relu activation    g = LeakyReLU(alpha=0.2)(g)    return g# define a decoder blockdef decoder_block(layer_in, skip_in, n_filters, dropout=True):    # weight initialization    init = RandomNormal(stddev=0.02)    # add upsampling layer    g = Conv2DTranspose(n_filters, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(layer_in)    # add batch normalization    g = BatchNormalization()(g, training=True)    # conditionally add dropout    if dropout:        g = Dropout(0.5)(g, training=True)    # merge with skip connection    g = Concatenate()([g, skip_in])    # relu activation    g = Activation('relu')(g)    return g# define the standalone generator modeldef define_generator(image_shape=(256, 256, 3)):    # weight initialization    init = RandomNormal(stddev=0.02)    # image input    in_image = Input(shape=image_shape)    # encoder model: C64-C128-C256-C512-C512-C512-C512-C512    e1 = define_encoder_block(in_image, 64, batchnorm=False)    e2 = define_encoder_block(e1, 128)    e3 = define_encoder_block(e2, 256)    e4 = define_encoder_block(e3, 512)    e5 = define_encoder_block(e4, 512)    e6 = define_encoder_block(e5, 512)    e7 = define_encoder_block(e6, 512)    # bottleneck, no batch norm and relu    b = Conv2D(512, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(e7)    b = Activation('relu')(b)    # decoder model: CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128    d1 = decoder_block(b, e7, 512)    d2 = decoder_block(d1, e6, 512)    d3 = decoder_block(d2, e5, 512)    d4 = decoder_block(d3, e4, 512, dropout=False)    d5 = decoder_block(d4, e3, 256, dropout=False)    d6 = decoder_block(d5, e2, 128, dropout=False)    d7 = decoder_block(d6, e1, 64, dropout=False)    # output    g = Conv2DTranspose(3, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(d7)    out_image = Activation('tanh')(g)    # define model    model = Model(in_image, out_image)    return model# define image shapeimage_shape = (256, 256, 3)# create the modelmodel = define_generator(image_shape)# Convert the modelconverter = tf.lite.TFLiteConverter.from_keras_model(model) # path to the SavedModel directoryconverter.target_spec.supported_ops = [  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.]converter.experimental_enable_resource_variables = Truetflite_model = converter.convert()# Save the model.with open('model.tflite', 'wb') as f:    f.write(tflite_model)``````### Relevant log output```shell---------------------------------------------------------------------------ConverterError                            Traceback (most recent call last)Input In [4], in <cell line: 10>()      5 converter.target_spec.supported_ops = [      6   tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.      7   tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.      8 ]      9 converter.experimental_enable_resource_variables = True---> 10 tflite_model = converter.convert()     12 # Save the model.     13 with open('model.tflite', 'wb') as f:File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\lite\python\lite.py:803, in _export_metrics.<locals>.wrapper(self, *args, **kwargs)    800 @functools.wraps(convert_func)    801 def wrapper(self, *args, **kwargs):    802   # pylint: disable=protected-access--> 803   return self._convert_and_export_metrics(convert_func, *args, **kwargs)File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\lite\python\lite.py:789, in TFLiteConverterBase._convert_and_export_metrics(self, convert_func, *args, **kwargs)    787 self._save_conversion_params_metric()    788 start_time = time.process_time()--> 789 result = convert_func(self, *args, **kwargs)    790 elapsed_time_ms = (time.process_time() - start_time) * 1000    791 if result:File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\lite\python\lite.py:1210, in TFLiteKerasModelConverterV2.convert(self)   1197 @_export_metrics   1198 def convert(self):   1199   """"""Converts a keras model based on instance variables.   1200    1201   Returns:   (...)   1208       Invalid quantization parameters.   1209   """"""-> 1210   saved_model_convert_result = self._convert_as_saved_model()   1211   if saved_model_convert_result:   1212     return saved_model_convert_resultFile ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\lite\python\lite.py:1192, in TFLiteKerasModelConverterV2._convert_as_saved_model(self)   1189   graph_def, input_tensors, output_tensors = (   1190       self._convert_keras_to_saved_model(temp_dir))   1191   if self.saved_model_dir:-> 1192     return super(TFLiteKerasModelConverterV2,   1193                  self).convert(graph_def, input_tensors, output_tensors)   1194 finally:   1195   shutil.rmtree(temp_dir, True)File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\lite\python\lite.py:1003, in TFLiteConverterBaseV2.convert(self, graph_def, input_tensors, output_tensors)    998   logging.info(""Using new converter: If you encounter a problem ""    999                ""please file a bug. You can opt-out ""   1000                ""by setting experimental_new_converter=False"")   1002 # Converts model.-> 1003 result = _convert_graphdef(   1004     input_data=graph_def,   1005     input_tensors=input_tensors,   1006     output_tensors=output_tensors,   1007     **converter_kwargs)   1009 return self._optimize_tflite_model(   1010     result, self._quant_mode, quant_io=self.experimental_new_quantizer)File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\lite\python\convert_phase.py:213, in convert_phase.<locals>.actual_decorator.<locals>.wrapper(*args, **kwargs)    211   else:    212     report_error_message(str(converter_error))--> 213   raise converter_error from None  # Re-throws the exception.    214 except Exception as error:    215   report_error_message(str(error))File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\lite\python\convert_phase.py:206, in convert_phase.<locals>.actual_decorator.<locals>.wrapper(*args, **kwargs)    203 @functools.wraps(func)    204 def wrapper(*args, **kwargs):    205   try:--> 206     return func(*args, **kwargs)    207   except ConverterError as converter_error:    208     if converter_error.errors:File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\lite\python\convert.py:774, in convert_graphdef(input_data, input_tensors, output_tensors, **kwargs)    771   else:    772     model_flags.output_arrays.append(util.get_tensor_name(output_tensor))--> 774 data = convert(    775     model_flags.SerializeToString(),    776     conversion_flags.SerializeToString(),    777     input_data.SerializeToString(),    778     debug_info_str=debug_info.SerializeToString() if debug_info else None,    779     enable_mlir_converter=enable_mlir_converter)    780 return dataFile ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\lite\python\convert.py:306, in convert(model_flags_str, conversion_flags_str, input_data_str, debug_info_str, enable_mlir_converter)    304     for error_data in _metrics_wrapper.retrieve_collected_errors():    305       converter_error.append_error(error_data)--> 306     raise converter_error    308 return _run_deprecated_conversion_binary(    309     model_flags_str, conversion_flags_str, input_data_str, debug_info_str)ConverterError: Variable constant folding is failed. Please consider using enabling `experimental_enable_resource_variables` flag in the TFLite converter object. For example, converter.experimental_enable_resource_variables = True```</details>
"
56169,1,1295,12,0,0,optiluca,0,"title:Unexpected output when lambda layers are used in a for loop description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.9### Custom CodeYes### OS Platform and DistributionWindows 10 x64### Mobile device_No response_### Python version3.8### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellThe attached code should produce an output that is equal to the input.  All it does is take the input vector, split it, and reconcatenate it.```### Standalone code to reproduce the issue```shellimport tensorflow as tffrom tensorflow.keras import layers, modelstest_input_data = tf.expand_dims(tf.linspace(0.0, 1.0, 6), axis=0)input_layer = layers.Input(batch_shape=(1, 6))outputs = []for i_channel in range(6):    print(f'Initialising layer that should extract element #{i_channel} - (expecting {test_input_data[0, i_channel]})')    outputs.append(layers.Lambda(lambda c: c[..., i_channel])(input_layer))cat_outputs = layers.Concatenate()(outputs)model = models.Model(inputs=input_layer, outputs=cat_outputs, name='repro_model')y = model(test_input_data)print(y)```### Relevant log output```shellInitialising layer that should extract element #0 - (expecting 0.0)Initialising layer that should extract element #1 - (expecting 0.20000000298023224)Initialising layer that should extract element #2 - (expecting 0.4000000059604645)Initialising layer that should extract element #3 - (expecting 0.6000000238418579)Initialising layer that should extract element #4 - (expecting 0.800000011920929)Initialising layer that should extract element #5 - (expecting 1.0)tf.Tensor([1. 1. 1. 1. 1. 1.], shape=(6,), dtype=float32)```</details>
"
56166,1,1190,12,0,0,GitEasonXu,0,"title:benchmark_model tool get Failed to apply XNNPACK delegate error. description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.8### Custom CodeNo### OS Platform and DistributionAndroid 11### Mobile deviceandroid_arm64### Python version_No response_### Bazel version4.2.1### GCC/Compiler version7.5.0### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?[Following this guide](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark),I successfully compiled benchmark_model tool.It's ok when I run this cmd `adb shell /data/local/tmp/benchmark/benchmark_model --graph=/data/local/tmp/benchmark/model.tflite --num_threads=1 --num_runs=1000`. But when I add `--use_xnnpack=true`, I getting shuch error:```STARTING!Log parameter values verbosely: [0]Graph: [pruning_model.tflite]Use xnnpack: [1]Loaded model pruning_model.tfliteINFO: Initialized TensorFlow Lite runtime.INFO: Created TensorFlow Lite XNNPACK delegate for CPU.XNNPACK delegate created.Failed to apply XNNPACK delegate.Benchmarking failed.```I tried to compile again with the following command, but it didn't work.```bazel build -c opt --config=android_arm64 --define tflite_with_xnnpack=true tensorflow/lite/tools/benchmark:benchmark_modelorbazel build -c opt --config=android_arm64 --define tflite_with_xnnpack=true --define xnn_eable_qs8=true tensorflow/lite/tools/benchmark:benchmark_model```### Standalone code to reproduce the issue```shellPost-training quantization codconvert = tf.lite.TFLiteConverter.from_keras_model(model)convert.optimizations = [tf.lite.Optimize.EXPERIMENTAL_SPARSITY]convert.representative_dataset = representative_datasettflite_quant_model = convert.convert()```### Relevant log output```shellSTARTING!Log parameter values verbosely: [0]Graph: [pruning_model.tflite]Use xnnpack: [1]Loaded model pruning_model.tfliteINFO: Initialized TensorFlow Lite runtime.INFO: Created TensorFlow Lite XNNPACK delegate for CPU.XNNPACK delegate created.Failed to apply XNNPACK delegate.Benchmarking failed.```</details>
"
56138,1,1837,1,0,0,wangwei-cmd,0,"title:tape.jacobian with chain rule cannot compute the gradient correctly description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf2.7### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 21.10### Mobile device_No response_### Python version3.9.7### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version11.3.1/8.1.0.77### GPU model and memory_No response_### Current Behaviour?```shellI want to use the jacobian matrix and chain-rule to compute the gradient. When the composed functions are basic arithmetics, the gradient is right. But when the composed function is a convolution, the gradient  calculated by the chain-rule is not consistent with the one outputed by 'tape.gradient'.```### Standalone code to reproduce the issue```shellimport tensorflow as tflayer = tf.keras.layers.Conv2D(1, 3, activation='relu', padding=""same"")x = tf.random.normal([2,7, 7,1])xx = tf.random.normal([2,7, 7,1])with tf.GradientTape(persistent=True) as tape1:    tape1.watch(x)    y1 = layer(x)    y2 = layer(y1)    tape1.watch(y2)    loss=tf.keras.losses.MeanSquaredError()(y2,xx)j1=tape1.jacobian(y2,x)[g_x,g_y2]=tape1.gradient(loss,[x,y2])j1_vec=tf.reshape(j1,[x.shape[0]*x.shape[1]*x.shape[2]*x.shape[3],                     y2.shape[0]*y2.shape[1]*y2.shape[2]*y2.shape[3]])g_y2_vec=tf.reshape(g_y2,[y2.shape[0]*y2.shape[1]*y2.shape[2]*y2.shape[3],1])G=tf.matmul(j1_vec,g_y2_vec)G_x=tf.reshape(G,[x.shape[0],x.shape[1],x.shape[2],x.shape[3]])e1=G_x-g_xprint('error value of e1:',tf.reduce_max(abs(e1)).numpy())with tf.GradientTape(persistent=True) as tape2:    tape2.watch(x)    y1 = x**3+100    y2 = tf.math.cos(y1)+10    tape2.watch(y2)    loss=tf.keras.losses.MeanSquaredError()(y2,xx)j2=tape2.jacobian(y2,x)[g_x,g_y2]=tape2.gradient(loss,[x,y2])j2_vec=tf.reshape(j2,[x.shape[0]*x.shape[1]*x.shape[2]*x.shape[3],                     y2.shape[0]*y2.shape[1]*y2.shape[2]*y2.shape[3]])g_y2_vec=tf.reshape(g_y2,[y2.shape[0]*y2.shape[1]*y2.shape[2]*y2.shape[3],1])G=tf.matmul(j2_vec,g_y2_vec)G_x=tf.reshape(G,[x.shape[0],x.shape[1],x.shape[2],x.shape[3]])e2=G_x-g_xprint('error value of e2:',tf.reduce_max(abs(e2)).numpy())```### Relevant log output```shellerror value of e1: 0.10533138error value of e2: 2.3841858e-07```</details>
"
56134,0,1406,21,0,1,dev0x13,0,"title:GpuCudaMallocAsyncAllocator fails if the same GPU device is initialised multiple times description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.7.1### Custom CodeNo### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versionCUDA 11.2, cuDNN 8.1.1### GPU model and memoryNVIDIA GTX 1080### Current Behaviour?When [https://github.com/tensorflow/tensorflow/blob/v2.7.1/tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc#L391](GpuCudaMallocAsyncAllocator) is used instead of the default one (either via environment variable or config setting), and the same GPU device is initialised more than once (e.g. if there are multiple TensorFlow sessions within a single process) it fails with the [""Trying to set the stream twice. This isn't supported.""](https://github.com/tensorflow/tensorflow/blob/v2.7.1/tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc#L391) error.Apparently this is happening because the `GpuCudaMallocAsyncAllocator` is global and its `cuda_stream_` field is filled on the first `SetStreamAndPreallocateMemory`. I was able to fix this by making some changes in the code, and I am going to submit a PR containing those.### Standalone code to reproduce the issue```cpp#include <tensorflow/core/public/session.h>tensorflow::GraphDef graph_def2;auto options = tensorflow::SessionOptions();options.config.mutable_gpu_options()->mutable_experimental()->set_use_cuda_malloc_async(true);tensorflow::GraphDef graph_def1;// Load graph_def1tensorflow::Session* session1 = nullptr;tensorflow::NewSession(options, &session1);session1->Create(graph_def1);tensorflow::GraphDef graph_def2;// Load graph_def2tensorflow::Session* session2 = nullptr;tensorflow::NewSession(options, &session2);session2->Create(graph_def2);```### Relevant log output```shell2022-05-17 14:17:05.253418: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:214] Using CUDA malloc Async allocator for GPU: 02022-05-17 14:17:05.253515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7234 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.12022-05-17 14:17:05.422828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7234 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.12022-05-17 14:17:05.423011: F tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:390] Trying to set the stream twice. This isn't supported.```</details>
"
56132,1,2617,4,0,0,DepenM,0,"title:SGD with momentum optimizer update fails for variables with dynamic shape description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.8.0### Custom CodeYes### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellUsing SGD with momentum optimizer on variables with dynamic shapes throws an error, while without momentum the updates work well.```### Standalone code to reproduce the issue```shellimport tensorflow as tfx = tf.Variable(tf.random.normal((32,3)), shape=[None,3])with tf.GradientTape() as tape:  x.assign(tf.random.normal((20,3)))  y = tf.reduce_sum(x)grads = tape.gradient(y, x)opt = tf.keras.optimizers.SGD(0.01, momentum = 0.9)opt.apply_gradients([[grads, x]])```### Relevant log output```shell<ipython-input-8-dddd56b45b9e> in <module>()     10 opt = tf.keras.optimizers.SGD(0.01, momentum = 0.9)     11 print(x)---> 12 opt.apply_gradients([[grads, x]])5 frames/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py in apply_gradients(self, grads_and_vars, name, experimental_aggregate_gradients)    637       # Create iteration if necessary.    638       with tf.init_scope():--> 639         self._create_all_weights(var_list)    640     641       if not grads_and_vars:/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py in _create_all_weights(self, var_list)    823     _ = self.iterations    824     self._create_hypers()--> 825     self._create_slots(var_list)    826     827   def __getattribute__(self, name):/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py in _create_slots(self, var_list)    117     if self._momentum:    118       for var in var_list:--> 119         self.add_slot(var, ""momentum"")    120     121   def _prepare_local(self, var_device, var_dtype, apply_state):/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py in add_slot(self, var, slot_name, initializer, shape)    913               dtype=var.dtype,    914               trainable=False,--> 915               initial_value=initial_value)    916       backend.track_variable(weight)    917       slot_dict[slot_name] = weight/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)    151     except Exception as e:    152       filtered_tb = _process_traceback_frames(e.__traceback__)--> 153       raise e.with_traceback(filtered_tb) from None    154     finally:    155       del filtered_tb/usr/local/lib/python3.7/dist-packages/keras/initializers/initializers_v2.py in __call__(self, shape, dtype, **kwargs)    143     if _PARTITION_SHAPE in kwargs:    144       shape = kwargs[_PARTITION_SHAPE]--> 145     return tf.zeros(shape, dtype)    146     147 ValueError: Cannot convert a partially known TensorShape (None, 3) to a Tensor.```</details>
"
56130,1,537,53,0,0,mattbarrett98,0,"title:math floordiv function gives incorrect result description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.8### Custom CodeNo### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellThe code import tensorflow as tfx = tf.constant([1e-37], dtype=tf.float32)y = tf.constant([-2.0], dtype=tf.float32)tf.math.floordiv(x, y)```works as expected, giving ```<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>```.However```import tensorflow as tfx = tf.constant([1e-38], dtype=tf.float32)y = tf.constant([-2.0], dtype=tf.float32)tf.math.floordiv(x, y)```gives ```<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.], dtype=float32)>```.Any idea why when x gets small enough it starts to output `-0.` rather than `-1.`. `1e-38` is still in range for float32 I believe so I think it should still work?```### Standalone code to reproduce the issue```shellhttps://colab.research.google.com/drive/1J583b0h11hee3UItK6l4VyaJ_hG7J4AT#scrollTo=c4-4DQ72QPZy```### Relevant log output_No response_</details>
"
56128,1,0,0,0,0,snowuyl,0,"title:Can't install TensorFlow 2.9 python package description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.9### Custom CodeNo### OS Platform and DistributionmacOS 12.3.1闂傚倸鍊搁崐鐑芥倿閿旈敮鍋撶粭娑樻噽閻瑩鏌熺€涙绠伴柤? arm64CPU)### Mobile device_No response_### Python version 3.9.12### Bazel version5.0.0### GCC/Compiler versionApple clang version 13.1.6 ### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellCan't install TensorFlow 2.9 python package on MacBook Air M1 platform.```### Standalone code to reproduce the issue```shellbazel build -c opt //tensorflow/tools/pip_package:build_pip_packagebazel-bin/tensorflow/tools/pip_package/build_pip_package .pip3 install tensorflow-2.9.0-cp39-cp39-macosx_12_0_arm64.whl```### Relevant log output```shellpip3 install tensorflow-2.9.0-cp39-cp39-macosx_12_0_arm64.whlDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621Processing ./tensorflow-2.9.0-cp39-cp39-macosx_12_0_arm64.whlCollecting gast<=0.4.0,>=0.2.1  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)Collecting typing-extensions>=3.6.6  Using cached typing_extensions-4.2.0-py3-none-any.whl (24 kB)Collecting tensorboard<2.10,>=2.9  Using cached tensorboard-2.9.0-py3-none-any.whl (5.8 MB)Collecting grpcio<2.0,>=1.24.3  Using cached grpcio-1.46.1-cp39-cp39-macosx_11_0_arm64.whlCollecting astunparse>=1.6.0  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)Requirement already satisfied: six>=1.12.0 in /opt/homebrew/lib/python3.9/site-packages/six-1.16.0-py3.9.egg (from tensorflow==2.9.0) (1.16.0)Collecting libclang>=13.0.0  Using cached libclang-14.0.1-py2.py3-none-macosx_11_0_arm64.whl (11.8 MB)Collecting opt-einsum>=2.3.2  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)ERROR: Could not find a version that satisfies the requirement tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow) (from versions: none)ERROR: No matching distribution found for tensorflow-io-gcs-filesystem>=0.23.1```</details>
"
56116,0,0,0,0,1,gunandrose4u,0,"title:Import tensorflow module error and abort to execute on Ubuntu 1804 description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf-nightly==2.10.0.dev20220515### Custom CodeNo### OS Platform and DistributionUbuntu 1804 lts### Mobile device_No response_### Python version3.7.7### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellAfter run pip install tf-nightly==2.10.0.dev20220515. Directly import tensorflow as tf, will get error message ""getInferLibVersion symbol not found."" and abort.```### Standalone code to reproduce the issue```shell1. Run pip install tf-nightly==2.10.0.dev202205152. Run python -c 'import tensorflow as tf'```### Relevant log output```shell(py376_test) azureuser@anubis-StandardD32sv4-linux-9:~/tmp/mlperf_python$ pythonPython 3.7.7 (default, May  7 2020, 21:25:33)[GCC 7.3.0] :: Anaconda, Inc. on linuxType ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> import tensorflow as tf2022-05-16 04:06:41.555118: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2022-05-16 04:06:41.665272: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.2022-05-16 04:06:41.669465: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory2022-05-16 04:06:41.669490: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.checkpoint_management has been moved to tensorflow.python.checkpoint.checkpoint_management. The old module will be deleted in version 2.9.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.resource has been moved to tensorflow.python.trackable.resource. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.util has been moved to tensorflow.python.checkpoint.checkpoint. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base_delegate has been moved to tensorflow.python.trackable.base_delegate. The old module will be deleted in version 2.11.WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.graph_view has been moved to tensorflow.python.checkpoint.graph_view. The old module will be deleted in version 2.11.2022-05-16 04:06:42.334465: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory2022-05-16 04:06:42.334493: F tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:49] getInferLibVersion symbol not found.Aborted (core dumped)```</details>
"
56109,1,2234,12,0,0,bleedingfight,0,"title:no such attribute 'shared_lib_name' in 'cc_shared_library' rule description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf master ### Custom CodeNo### OS Platform and Distribution5.15.32-1-MANJARO### Mobile device5.15.32-1-MANJARO### Python version3.10### Bazel versionbazel 5.0.0### GCC/Compiler versiongcc-10.2### CUDA/cuDNN version11.6### GPU model and memoryRTX3090 24G### Current Behaviour?```shellERROR: /home/liushuai/tensorflow/tensorflow/BUILD:993:21: //tensorflow:libtensorflow_framework.so.2.10.0: no such attribute 'shared_lib_name' in 'cc_shared_library' ruleERROR: /home/liushuai/tensorflow/tensorflow/BUILD:993:21: //tensorflow:libtensorflow_framework.so.2.10.0: no such attribute 'win_def_file' in 'cc_shared_library' ruleERROR: /home/liushuai/tensorflow/tensorflow/BUILD:993:21: //tensorflow:libtensorflow_framework.2.10.0.dylib: no such attribute 'shared_lib_name' in 'cc_shared_library' ruleERROR: /home/liushuai/tensorflow/tensorflow/BUILD:993:21: //tensorflow:libtensorflow_framework.2.10.0.dylib: no such attribute 'win_def_file' in 'cc_shared_library' ruleERROR: /home/liushuai/tensorflow/tensorflow/BUILD:993:21: //tensorflow:tensorflow_framework.dll: no such attribute 'shared_lib_name' in 'cc_shared_library' ruleERROR: /home/liushuai/tensorflow/tensorflow/BUILD:993:21: //tensorflow:tensorflow_framework.dll: no such attribute 'win_def_file' in 'cc_shared_library' ruleERROR: /home/liushuai/tensorflow/tensorflow/BUILD:1115:21: //tensorflow:libtensorflow_cc.so.2.10.0: no such attribute 'shared_lib_name' in 'cc_shared_library' ruleERROR: /home/liushuai/tensorflow/tensorflow/BUILD:1115:21: //tensorflow:libtensorflow_cc.so.2.10.0: no such attribute 'win_def_file' in 'cc_shared_library' ruleERROR: /home/liushuai/tensorflow/tensorflow/BUILD:1115:21: //tensorflow:libtensorflow_cc.2.10.0.dylib: no such attribute 'shared_lib_name' in 'cc_shared_library' ruleERROR: /home/liushuai/tensorflow/tensorflow/BUILD:1115:21: //tensorflow:libtensorflow_cc.2.10.0.dylib: no such attribute 'win_def_file' in 'cc_shared_library' ruleERROR: /home/liushuai/tensorflow/tensorflow/BUILD:1115:21: //tensorflow:tensorflow_cc.dll: no such attribute 'shared_lib_name' in 'cc_shared_library' ruleERROR: /home/liushuai/tensorflow/tensorflow/BUILD:1115:21: //tensorflow:tensorflow_cc.dll: no such attribute 'win_def_file' in 'cc_shared_library' ruleERROR: /home/liushuai/tensorflow/tensorflow/tools/pip_package/BUILD:275:10: errors encountered resolving select() keys for //tensorflow/tools/pip_package:build_pip_package```### Standalone code to reproduce the issue```shellbazel build --config=cuda  //tensorflow/tools/pip_package:build_pip_package```### Relevant log output_No response_</details>
"
56108,1,0,0,0,0,Sanjay2802,0,"title:PortAudio library not found  description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow VersionLatest ### Custom CodeNo### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellNow cant import anything it's showing ""PortAudio library not found"" error while importing```### Standalone code to reproduce the issue```shellResolve asap```### Relevant log output_No response_</details>
"
56085,1,0,0,0,0,Naegionn,0,"title:Tensorflow docker image has outdated keys description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontensorflow:latest### Custom CodeNo### OS Platform and DistributionDocker### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellWhen using a tensorflow docker image there seems to be an outdated apt repository key:```### Standalone code to reproduce the issue```shelldocker run -it  tensorflow/tensorflow:latest-gpu bashapt update```### Relevant log output```shelllukas@docker-client:~$ docker run -it  tensorflow/tensorflow:latest-gpu bashUnable to find image 'tensorflow/tensorflow:latest-gpu' locallylatest-gpu: Pulling from tensorflow/tensorflow7b1a6ab2e44d: Pull complete28a427df77d0: Pull complete7df9c933f388: Pull complete5efcf2478dc2: Pull complete3196a0117ed3: Pull complete558a8016d36f: Pull complete4187faa68339: Pull complete5cb96aedb476: Pull completed827240235b3: Pull complete75600f20c268: Pull complete097a8b939f57: Pull complete22ec0787b1d5: Pull completead4c0760f8ac: Pull completef26dd99dc5e3: Pull completeDigest: sha256:1e03623e335aac1610b1a3cfa6a96cf10156acb095287f9d6031df3980148663Status: Downloaded newer image for tensorflow/tensorflow:latest-gpu________                               __________________  __/__________________________________  ____/__  /________      ____  /  _  _ \_  __ \_  ___/  __ \_  ___/_  /_   __  /_  __ \_ | /| / /_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ //_/    \___//_/ /_//____/ \____//_/    /_/      /_/  \____/____/|__/WARNING: You are running this container as root, which can cause new files inmounted volumes to be created as the root user on your host machine.To avoid this, run the container by specifying your user's userid:$ docker run -u $(id -u):$(id -g) args...root@031b1960b4ea:/# apt updateHit:1 http://archive.ubuntu.com/ubuntu focal InReleaseGet:2 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]Get:3 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]Get:5 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InReleaseErr:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CCIgn:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InReleaseHit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  ReleaseHit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  ReleaseGet:10 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1170 kB]Get:11 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [1249 kB]Get:12 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [25.8 kB]Get:13 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [1812 kB]Get:15 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [873 kB]Get:16 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2239 kB]Get:18 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [30.3 kB]Get:19 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1156 kB]Get:20 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [26.0 kB]Get:21 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [51.2 kB]Reading package lists... DoneW: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CCE: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease' is no longer signed.N: Updating from such a repository can't be done securely, and is therefore disabled by default.N: See apt-secure(8) manpage for repository creation and user configuration details.```</details>
"
56078,0,23335,269,0,1,elfringham,0,"title:Unit test //tensorflow/tools/docs:tf_doctest is broken by protobuf exception description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiongit HEAD### Custom CodeNo### OS Platform and DistributionCentOS 7### Mobile devicen/a### Python version3.8.13### Bazel version5.1.1### GCC/Compiler version10.2.1### CUDA/cuDNN versionn/a### GPU model and memoryn/a### Current Behaviour?```shell//tensorflow/tools/docs:tf_doctest                                       FAILED in 12 out of 12 in 10.8s```### Standalone code to reproduce the issue```shellbazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --noremote_accept_cached --config=nonccl --copt=""-mtune=generic"" --copt=""-march=armv8-a"" --copt=""-O3"" --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --verbose_failures --build_tests_only --jobs=75 -- //tensorflow/tools/docs:tf_doctest```### Relevant log output```shellIntroduced by https://github.com/tensorflow/tensorflow/commit/c777808941613acebc74dbcdd5e48c4e76ac5271INFO: Options provided by the client:  Inherited 'common' options: --isatty=1 --terminal_columns=132INFO: Reading rc options for 'test' from /tmp/workspace/tensorflow-git/.bazelrc:  Inherited 'common' options: --experimental_repo_remote_execINFO: Reading rc options for 'test' from /tmp/workspace/tensorflow-git/.bazelrc:  Inherited 'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=trueINFO: Reading rc options for 'test' from /tmp/workspace/tensorflow-git/.tf_configure.bazelrc:  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/tmp/workspace/venv-cp38-cp38/bin/python3 --action_env PYTHON_LIB_PATH=/tmp/workspace/venv-cp38-cp38/lib/python3.8/site-packages --python_path=/tmp/workspace/venv-cp38-cp38/bin/python3INFO: Reading rc options for 'test' from /tmp/workspace/tensorflow-git/.bazelrc:  Inherited 'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utilsINFO: Reading rc options for 'test' from /tmp/workspace/tensorflow-git/.tf_configure.bazelrc:  'test' options: --flaky_test_attempts=3 --test_size_filters=small,mediumINFO: Found applicable config definition build:short_logs in file /tmp/workspace/tensorflow-git/.bazelrc: --output_filter=DONT_MATCH_ANYTHINGINFO: Found applicable config definition build:v2 in file /tmp/workspace/tensorflow-git/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1INFO: Found applicable config definition test:v2 in file /tmp/workspace/tensorflow-git/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1onlyINFO: Found applicable config definition build:nonccl in file /tmp/workspace/tensorflow-git/.bazelrc: --define=no_nccl_support=trueINFO: Found applicable config definition build:linux in file /tmp/workspace/tensorflow-git/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changesINFO: Found applicable config definition build:dynamic_kernels in file /tmp/workspace/tensorflow-git/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELSINFO: Analyzed target //tensorflow/tools/docs:tf_doctest (279 packages loaded, 21308 targets configured).INFO: Found 1 test target...FAIL: //tensorflow/tools/docs:tf_doctest (shard 3 of 4) (see /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_3_of_4/test_attempts/attempt_1.log)FAIL: //tensorflow/tools/docs:tf_doctest (shard 1 of 4) (see /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_1_of_4/test_attempts/attempt_1.log)FAIL: //tensorflow/tools/docs:tf_doctest (shard 4 of 4) (see /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_4_of_4/test_attempts/attempt_1.log)FAIL: //tensorflow/tools/docs:tf_doctest (shard 2 of 4) (see /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_2_of_4/test_attempts/attempt_1.log)FAIL: //tensorflow/tools/docs:tf_doctest (shard 4 of 4) (see /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_4_of_4/test_attempts/attempt_2.log)FAIL: //tensorflow/tools/docs:tf_doctest (shard 1 of 4) (see /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_1_of_4/test_attempts/attempt_2.log)FAIL: //tensorflow/tools/docs:tf_doctest (shard 3 of 4) (see /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_3_of_4/test_attempts/attempt_2.log)FAIL: //tensorflow/tools/docs:tf_doctest (shard 2 of 4) (see /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_2_of_4/test_attempts/attempt_2.log)FAIL: //tensorflow/tools/docs:tf_doctest (shard 4 of 4) (see /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_4_of_4/test.log)INFO: From Testing //tensorflow/tools/docs:tf_doctest (shard 4 of 4):==================== Test output for //tensorflow/tools/docs:tf_doctest (shard 4 of 4):2022-05-12 10:42:30.717782: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered under GRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ==================================================================================================== Test output for //tensorflow/tools/docs:tf_doctest (shard 4 of 4):2022-05-12 10:42:35.654227: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered under GRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ==================================================================================================== Test output for //tensorflow/tools/docs:tf_doctest (shard 4 of 4):2022-05-12 10:42:40.591478: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered under GRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ================================================================================FAIL: //tensorflow/tools/docs:tf_doctest (shard 3 of 4) (see /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_3_of_4/test.log)INFO: From Testing //tensorflow/tools/docs:tf_doctest (shard 3 of 4):==================== Test output for //tensorflow/tools/docs:tf_doctest (shard 3 of 4):2022-05-12 10:42:30.717687: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered under GRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ==================================================================================================== Test output for //tensorflow/tools/docs:tf_doctest (shard 3 of 4):2022-05-12 10:42:35.668010: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered under GRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ==================================================================================================== Test output for //tensorflow/tools/docs:tf_doctest (shard 3 of 4):2022-05-12 10:42:40.629291: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered under GRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ================================================================================FAIL: //tensorflow/tools/docs:tf_doctest (shard 1 of 4) (see /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_1_of_4/test.log)INFO: From Testing //tensorflow/tools/docs:tf_doctest (shard 1 of 4):==================== Test output for //tensorflow/tools/docs:tf_doctest (shard 1 of 4):2022-05-12 10:42:30.717782: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered under GRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ==================================================================================================== Test output for //tensorflow/tools/docs:tf_doctest (shard 1 of 4):2022-05-12 10:42:35.654227: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered under GRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ==================================================================================================== Test output for //tensorflow/tools/docs:tf_doctest (shard 1 of 4):2022-05-12 10:42:40.629291: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered under GRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ================================================================================FAIL: //tensorflow/tools/docs:tf_doctest (shard 2 of 4) (see /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_2_of_4/test.log)FAILED: //tensorflow/tools/docs:tf_doctest (Summary)      /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_4_of_4/test.log      /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_4_of_4/test_attempts/attempt_1.log      /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_4_of_4/test_attempts/attempt_2.log      /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_3_of_4/test.log      /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_3_of_4/test_attempts/attempt_1.log      /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_3_of_4/test_attempts/attempt_2.log      /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_1_of_4/test.log      /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_1_of_4/test_attempts/attempt_1.log      /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_1_of_4/test_attempts/attempt_2.log      /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_2_of_4/test.log      /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_2_of_4/test_attempts/attempt_1.log      /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_2_of_4/test_attempts/attempt_2.logINFO: From Testing //tensorflow/tools/docs:tf_doctest (shard 2 of 4):==================== Test output for //tensorflow/tools/docs:tf_doctest (shard 2 of 4):2022-05-12 10:42:30.717687: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered under GRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ==================================================================================================== Test output for //tensorflow/tools/docs:tf_doctest (shard 2 of 4):2022-05-12 10:42:35.770968: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered under GRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ==================================================================================================== Test output for //tensorflow/tools/docs:tf_doctest (shard 2 of 4):2022-05-12 10:42:40.991862: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered under GRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ================================================================================Target //tensorflow/tools/docs:tf_doctest up-to-date:  bazel-bin/tensorflow/tools/docs/tf_doctestINFO: Elapsed time: 681.341s, Critical Path: 425.75sINFO: 3147 processes: 456 internal, 2691 local.INFO: Build completed, 1 test FAILED, 3147 total actions//tensorflow/tools/docs:tf_doctest                                       FAILED in 12 out of 12 in 10.8s  Stats over 12 runs: max = 10.8s, min = 4.9s, avg = 6.9s, dev = 2.7s  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_4_of_4/test.log  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_4_of_4/test_attempts/attempt_1.log  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_4_of_4/test_attempts/attempt_2.log  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_3_of_4/test.log  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_3_of_4/test_attempts/attempt_1.log  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_3_of_4/test_attempts/attempt_2.log  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_1_of_4/test.log  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_1_of_4/test_attempts/attempt_1.log  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_1_of_4/test_attempts/attempt_2.log  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_2_of_4/test.log  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_2_of_4/test_attempts/attempt_1.log  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_2_of_4/test_attempts/attempt_2.logINFO: Build completed, 1 test FAILED, 3147 total actions```</details>
"
56077,0,0,0,0,0,matthewfeickert,0,"title:Breaking change in protobuf v4.21.0-rc1 breaks `import tensorflow` description:### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.9.0-rc2### Custom CodeNo### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version3.10### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?This issue is to give a warning (that might already be known in internal CI) that [`protobuf` `v4.21.0-rc1`](https://github.com/protocolbuffers/protobuf/releases/tag/v21.0-rc1) which was released today (2022-05-11) causes breaking changes in messages between Python and C++ that affect TensorFlow and will crash with```pytbAttributeError: module 'google._upb._message' has no attribute 'Message'. Did you mean: 'CMessage'?```on `import tensorflow`.This is somewhat noted in the `protobuf` release notes for Python> The C extension module for Python has been rewritten to use the upb library. This is expected to deliver significant performance benefits, especially when parsing large payloads. There are some minor breaking changes, but these should not impact most users. For more information see: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updateswhere the [Python Updates notes](https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates) mentions> Applications that rely on sharing messages between Python and C++ break in the new version. Most developers won't be affected by this, but users of `Nucleus{.external}` and possibly other libraries may be. As a workaround, you can [set an environment variable](https://developers.google.com/protocol-buffers/docs/reference/python-generated#sharing-messages) that forces the library to preserve compatibility.which finally in [Sharing Messages Between Python and C++](https://developers.google.com/protocol-buffers/docs/reference/python-generated#sharing-messages) tells us that> Prior to Python 4.21.0, Python apps could share messages with C++ using a native extension. Starting in Python 4.21.0, sharing messages between Python and C++ is not supported by the default install. To enable this capability when working with 4.x and later versions of the Python API, define the environment variable, `PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=cpp`, and ensure that the Python/C++ extension is installed.However,```console$ export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=cpp```just now leads to an```pytbImportError: cannot import name '_message' from 'google.protobuf.pyext' (/venv/lib/python3.10/site-packages/google/protobuf/pyext/__init__.py)```Additionally tagging @deannagarcia who released [`protobuf` `v4.21.0-rc1`](https://github.com/protocolbuffers/protobuf/releases/tag/v21.0-rc1).This was noticed in `pyhf`'s nightly [pre-release and HEAD of dependencies CI jobs](https://github.com/scikit-hep/pyhf/runs/6398925572?check_suite_focus=true).### Standalone code to reproduce the issueHere's a `python:3.10` Docker image exampleClick to exapnd:```console$ docker run --rm -ti python:3.10 /bin/bashroot@a0c6928654f7:/# python -m venv venv && . venv/bin/activate(venv) root@a0c6928654f7:/# python -m pip --quiet install --upgrade pip setuptools wheel(venv) root@a0c6928654f7:/# python -m pip --quiet install 'tensorflow==2.8.0'  # First show this with stable release v2.8.0(venv) root@a0c6928654f7:/# python -m pip list | grep 'tensorflow\|protobuf'protobuf                     3.20.1tensorflow                   2.8.0tensorflow-io-gcs-filesystem 0.25.0(venv) root@a0c6928654f7:/# python -c 'import tensorflow; print(tensorflow.__version__)'2022-05-12 03:21:47.631420: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory2022-05-12 03:21:47.631464: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.2.8.0(venv) root@a0c6928654f7:/# python -m pip --quiet install --pre --upgrade protobuf(venv) root@a0c6928654f7:/# python -m pip show protobufName: protobufVersion: 4.21.0rc1Summary: Home-page: Author: Author-email: License: Location: /venv/lib/python3.10/site-packagesRequires: Required-by: tensorboard, tensorflow(venv) root@a0c6928654f7:/# python -c 'import tensorflow'2022-05-12 03:23:19.770470: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory2022-05-12 03:23:19.770491: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.Traceback (most recent call last):  File ""<string>"", line 1, in <module>  File ""/venv/lib/python3.10/site-packages/tensorflow/__init__.py"", line 37, in <module>    from tensorflow.python.tools import module_util as _module_util  File ""/venv/lib/python3.10/site-packages/tensorflow/python/__init__.py"", line 37, in <module>    from tensorflow.python.eager import context  File ""/venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py"", line 29, in <module>    from tensorflow.core.framework import function_pb2  File ""/venv/lib/python3.10/site-packages/tensorflow/core/framework/function_pb2.py"", line 16, in <module>    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2  File ""/venv/lib/python3.10/site-packages/tensorflow/core/framework/attr_value_pb2.py"", line 16, in <module>    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2  File ""/venv/lib/python3.10/site-packages/tensorflow/core/framework/tensor_pb2.py"", line 16, in <module>    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2  File ""/venv/lib/python3.10/site-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2  File ""/venv/lib/python3.10/site-packages/tensorflow/core/framework/tensor_shape_pb2.py"", line 36, in <module>    _descriptor.FieldDescriptor(  File ""/venv/lib/python3.10/site-packages/google/protobuf/descriptor.py"", line 560, in __new__    _message.Message._CheckCalledFromGeneratedFile()AttributeError: module 'google._upb._message' has no attribute 'Message'. Did you mean: 'CMessage'?(venv) root@a0c6928654f7:/# export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=cpp  # try protobuf environment variable suggestion(venv) root@a0c6928654f7:/# python -c 'import tensorflow'2022-05-12 03:23:50.884921: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory2022-05-12 03:23:50.884945: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine./venv/lib/python3.10/site-packages/google/protobuf/internal/api_implementation.py:109: UserWarning: Selected implementation cpp is not available.  warnings.warn(Traceback (most recent call last):  File ""<string>"", line 1, in <module>  File ""/venv/lib/python3.10/site-packages/tensorflow/__init__.py"", line 37, in <module>    from tensorflow.python.tools import module_util as _module_util  File ""/venv/lib/python3.10/site-packages/tensorflow/python/__init__.py"", line 37, in <module>    from tensorflow.python.eager import context  File ""/venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py"", line 29, in <module>    from tensorflow.core.framework import function_pb2  File ""/venv/lib/python3.10/site-packages/tensorflow/core/framework/function_pb2.py"", line 7, in <module>    from google.protobuf import descriptor as _descriptor  File ""/venv/lib/python3.10/site-packages/google/protobuf/descriptor.py"", line 51, in <module>    from google.protobuf.pyext import _messageImportError: cannot import name '_message' from 'google.protobuf.pyext' (/venv/lib/python3.10/site-packages/google/protobuf/pyext/__init__.py)(venv) root@a0c6928654f7:/# unset PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION(venv) root@a0c6928654f7:/# python -m pip --quiet install --pre --upgrade tensorflow  # Try release candidate v2.9.0-rc2(venv) root@a0c6928654f7:/# python -m pip show tensorflowName: tensorflowVersion: 2.9.0rc2Summary: TensorFlow is an open source machine learning framework for everyone.Home-page: https://www.tensorflow.org/Author: Google Inc.Author-email: packages@tensorflow.orgLicense: Apache 2.0Location: /venv/lib/python3.10/site-packagesRequires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, keras-preprocessing, libclang, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wraptRequired-by(venv) root@a0c6928654f7:/# python -c 'import tensorflow'2022-05-12 03:26:20.957106: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory2022-05-12 03:26:20.957131: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.Traceback (most recent call last):  File ""<string>"", line 1, in <module>  File ""/venv/lib/python3.10/site-packages/tensorflow/__init__.py"", line 37, in <module>    from tensorflow.python.tools import module_util as _module_util  File ""/venv/lib/python3.10/site-packages/tensorflow/python/__init__.py"", line 37, in <module>    from tensorflow.python.eager import context  File ""/venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py"", line 29, in <module>    from tensorflow.core.framework import function_pb2  File ""/venv/lib/python3.10/site-packages/tensorflow/core/framework/function_pb2.py"", line 16, in <module>    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2  File ""/venv/lib/python3.10/site-packages/tensorflow/core/framework/attr_value_pb2.py"", line 16, in <module>    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2  File ""/venv/lib/python3.10/site-packages/tensorflow/core/framework/tensor_pb2.py"", line 16, in <module>    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2  File ""/venv/lib/python3.10/site-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2  File ""/venv/lib/python3.10/site-packages/tensorflow/core/framework/tensor_shape_pb2.py"", line 36, in <module>    _descriptor.FieldDescriptor(  File ""/venv/lib/python3.10/site-packages/google/protobuf/descriptor.py"", line 560, in __new__    _message.Message._CheckCalledFromGeneratedFile()AttributeError: module 'google._upb._message' has no attribute 'Message'. Did you mean: 'CMessage'?(venv) root@a0c6928654f7:/# export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=cpp(venv) root@a0c6928654f7:/# python -c 'import tensorflow'2022-05-12 03:27:01.166115: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory2022-05-12 03:27:01.166141: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine./venv/lib/python3.10/site-packages/google/protobuf/internal/api_implementation.py:109: UserWarning: Selected implementation cpp is not available.  warnings.warn(Traceback (most recent call last):  File ""<string>"", line 1, in <module>  File ""/venv/lib/python3.10/site-packages/tensorflow/__init__.py"", line 37, in <module>    from tensorflow.python.tools import module_util as _module_util  File ""/venv/lib/python3.10/site-packages/tensorflow/python/__init__.py"", line 37, in <module>    from tensorflow.python.eager import context  File ""/venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py"", line 29, in <module>    from tensorflow.core.framework import function_pb2  File ""/venv/lib/python3.10/site-packages/tensorflow/core/framework/function_pb2.py"", line 7, in <module>    from google.protobuf import descriptor as _descriptor  File ""/venv/lib/python3.10/site-packages/google/protobuf/descriptor.py"", line 51, in <module>    from google.protobuf.pyext import _messageImportError: cannot import name '_message' from 'google.protobuf.pyext' (/venv/lib/python3.10/site-packages/google/protobuf/pyext/__init__.py)```</details>### Relevant log output```pytbTraceback (most recent call last):  File ""<string>"", line 1, in <module>  File ""/venv/lib/python3.10/site-packages/tensorflow/__init__.py"", line 37, in <module>    from tensorflow.python.tools import module_util as _module_util  File ""/venv/lib/python3.10/site-packages/tensorflow/python/__init__.py"", line 37, in <module>    from tensorflow.python.eager import context  File ""/venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py"", line 29, in <module>    from tensorflow.core.framework import function_pb2  File ""/venv/lib/python3.10/site-packages/tensorflow/core/framework/function_pb2.py"", line 16, in <module>    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2  File ""/venv/lib/python3.10/site-packages/tensorflow/core/framework/attr_value_pb2.py"", line 16, in <module>    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2  File ""/venv/lib/python3.10/site-packages/tensorflow/core/framework/tensor_pb2.py"", line 16, in <module>    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2  File ""/venv/lib/python3.10/site-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2  File ""/venv/lib/python3.10/site-packages/tensorflow/core/framework/tensor_shape_pb2.py"", line 36, in <module>    _descriptor.FieldDescriptor(  File ""/venv/lib/python3.10/site-packages/google/protobuf/descriptor.py"", line 560, in __new__    _message.Message._CheckCalledFromGeneratedFile()AttributeError: module 'google._upb._message' has no attribute 'Message'. Did you mean: 'CMessage'?``````pytb/venv/lib/python3.10/site-packages/google/protobuf/internal/api_implementation.py:109: UserWarning: Selected implementation cpp is not available.  warnings.warn(Traceback (most recent call last):  File ""<string>"", line 1, in <module>  File ""/venv/lib/python3.10/site-packages/tensorflow/__init__.py"", line 37, in <module>    from tensorflow.python.tools import module_util as _module_util  File ""/venv/lib/python3.10/site-packages/tensorflow/python/__init__.py"", line 37, in <module>    from tensorflow.python.eager import context  File ""/venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py"", line 29, in <module>    from tensorflow.core.framework import function_pb2  File ""/venv/lib/python3.10/site-packages/tensorflow/core/framework/function_pb2.py"", line 7, in <module>    from google.protobuf import descriptor as _descriptor  File ""/venv/lib/python3.10/site-packages/google/protobuf/descriptor.py"", line 51, in <module>    from google.protobuf.pyext import _messageImportError: cannot import name '_message' from 'google.protobuf.pyext' (/venv/lib/python3.10/site-packages/google/protobuf/pyext/__init__.py)```
"
56046,0,1097,32,0,0,namrata-ibm,0,"title:Test for TStrings of type ""Offset"" returns incorrect size on s390x arch  description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.8.0### Custom CodeNo### OS Platform and DistributionLinux Ubuntu 20.04### Mobile deviceNA### Python versionPython 3.8.10### Bazel version4.2.2### GCC/Compiler version9.4.0### CUDA/cuDNN versionNA### GPU model and memoryNA### Current Behaviour?```shellTF_CTStringTest OffsetType fails as size or TF_TSTR_OFFSET are 0 on big endian.As per analysis done, TF_TString_ToInternalSizeT calls from TF_TString_ResizeUninitialized as well as in test file(https://github.com/tensorflow/tensorflow/blob/v2.8.0/tensorflow/core/platform/ctstring_test.cc#L397) returns a very high number but it is not decoded correctly.Hence all calls such as TF_TString_GetSize, TF_TString_GetType return value of 0 on s390x.Expected behaviour: This test should pass on BE systems as well.```### Standalone code to reproduce the issue```shellOn s390x, run:`bazel --host_jvm_args=""-Xms1024m"" --host_jvm_args=""-Xmx2048m"" test --strip never --jobs=10 --verbose_failures //tensorflow/core/platform:ctstring_test````### Relevant log output```shelltensorflow/core/platform/ctstring_test.cc:401: FailureExpected equality of these values:  size    Which is: 8  TF_TString_GetSize(&s71)    Which is: 0tensorflow/core/platform/ctstring_test.cc:402: FailureExpected equality of these values:  TF_TSTR_OFFSET    Which is: 2  TF_TString_GetType(&s71)    Which is: 0[  FAILED  ] TF_CTStringTest.OffsetType (0 ms)``````</details>
"
56024,0,2459,10,0,0,jesnie,0,"title:Autograph appears not to support Python variables without an initial value description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.8.0### Custom CodeYes### OS Platform and DistributionUbuntu 18.04.6 LTS### Mobile device_No response_### Python version3.10.4### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellAutograph appears not to support Python variables without an initial value.Although this is a rarely used feature of Python, I don't see why this should cause any problems. It literally has no control flow.In my below example `foo` works, but `bar` does not, despite them essentially only having formatting differences.```### Standalone code to reproduce the issue```shellimport tensorflow as tf@tf.functiondef foo(a: tf.Tensor, b: tf.Tensor) -> tf.Tensor:    result: tf.Tensor = a + b    return result@tf.functiondef bar(a: tf.Tensor, b: tf.Tensor) -> tf.Tensor:    result: tf.Tensor    result = a + b    return resultfoo(tf.constant(2), tf.constant(3))  # <-- Works.bar(tf.constant(2), tf.constant(3))  # <-- WARNING:tensorflow:AutoGraph could not transform...```### Relevant log output```shell2022-05-09 10:25:03.704416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2022-05-09 10:25:03.707662: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory2022-05-09 10:25:03.708246: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.Skipping registering GPU devices...2022-05-09 10:25:03.708548: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.WARNING:tensorflow:AutoGraph could not transform <function bar at 0x7feba9ff7130> and will run it as-is.Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.Cause: invalid value for ""node"": expected ""ast.AST"", got ""<class 'NoneType'>""; to visit lists of nodes, use ""visit_block"" insteadTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert```</details>
"
55933,0,3669,0,0,0,philippfriese,0,"title:Segfault on tf.matmul and tf.einsum with batched input tensors using intel-tensorflow-avx512 description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.8### Custom CodeYes### OS Platform and DistributionSUSE Linux Enterprise Server 15 SP3, Kernel 5.3.18-59.19### Mobile device_No response_### Python version3.9.10### Bazel version5.1.1### GCC/Compiler version7.5.0### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?Using `intel-tensorflow-avx512` either compiled manually or from the official pip package source, a segmentation fault is observed on the calls `tf.matmul` and `tf.einsum`. The log output is `Segmentation fault (core dumped)`.After calling `tf.matmul` on tensors with at least one batching dimension, a consecutive call to `tf.einsum` causes a segfault. The same happens for `tf.einsum` followed by a `tf.matmul`. Calling just `tf.matmul` or just `tf.einsum` does not trigger a segfault.This crash does not appear when using a shape such as [n,n], i.e. a shape without batching dimensions.The non-Intel tensorflow version does not result in a segfault.The section ""Standalone code to reproduce the issue"" gives a minimal example. The code within `# either #` and `# or #` refers to the two possible call orders.The section ""Relevant log output"" lists a backtrace of the dumped core, shortened for brevity. The backtrace was obtained using gdb.The self-compiled `intel-tensorflow-avx512` was compiled using ""Intel闂?Optimization for TensorFlow* Installation Guide"" [1].This bug has been observed on two separate machine, running an Intel Xeon Platinum 8360Y and Intel Xeon W-2295 respectively. The second machine runs Linux Ubuntu Server 20.04.4 LTS.[1] https://www.intel.com/content/www/us/en/developer/articles/guide/optimization-for-tensorflow-installation-guide.html### Standalone code to reproduce the issue```shellimport tensorflow as tfb = 1n = 4shape = [b,n,n]# either # print(f""Einsum A*B: {tf.einsum('...ij, ...jk -> ...ik', tf.random.normal(shape), tf.random.normal(shape)).shape}"")print(f""Matmul A*B: {tf.matmul(tf.random.normal(shape), tf.random.normal(shape)).shape}"") # <-- crash# or # print(f""Matmul A*B: {tf.matmul(tf.random.normal(shape), tf.random.normal(shape)).shape}"")print(f""Einsum A*B: {tf.einsum('...ij, ...jk -> ...ik', tf.random.normal(shape), tf.random.normal(shape)).shape}"")  # <-- crash```### Relevant log output```shell(gdb) r </path/to/script.py>Starting program: <path/to/python3> </path/to/script.py>[...]Einsum A*B: (1, 4, 4)Thread 1 ""python3"" received signal SIGSEGV, Segmentation fault.0x0000000002edf9c0 in ?? ()(gdb) bt#0  0x0000000002edf9c0 in ?? ()#1  0x00007f2b36ffa0f1 in tensorflow::CreateStream(tensorflow::MklDnnThreadPool*, dnnl::engine const&) [clone .isra.370] ()   from [...]/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so#2  0x00007f2b37003010 in tensorflow::BatchMatMulMkl<Eigen::ThreadPoolDevice, float, float, float, true>::Compute(tensorflow::OpKernelContext*) ()   from [...]/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so#3  0x00007f2b2cff5644 in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()   from [...]/site-packages/tensorflow/python/../libtensorflow_framework.so.2#4  0x00007f2b38da8dff in tensorflow::KernelAndDeviceOp::Run([...]) ()   from [...]/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so#5  0x00007f2b32c5c30c in tensorflow::EagerKernelExecute([...]) ()   from [...]/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so#6  0x00007f2b32c5d1a1 in tensorflow::ExecuteNode::Run() ()   from [...]/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so#7  0x00007f2b39232e8a in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) ()   from [...]/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so#8  0x00007f2b32c57e10 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) ()   from [...]/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so#9  0x00007f2b32c58439 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) ()   from [...]/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so#10 0x00007f2b328d4319 in tensorflow::EagerOperation::Execute(absl::lts_20210324::Span<tensorflow::AbstractTensorHandle*>, int*) ()   from [...]/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so#11 0x00007f2b38db8a29 in tensorflow::CustomDeviceOpHandler::Execute([...]) ()    from [...]/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so#12 0x00007f2b325e783d in TFE_Execute ()   from [...]/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so#13 0x00007f2b32170bfb in TFE_Py_FastPathExecute_C(_object*) ()   from [...]/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so#14 0x00007f2b2829ba05 in pybind11::cpp_function::initialize<[...]>[...] ()    from [...]/site-packages/tensorflow/python/_pywrap_tfe.so#15 0x00007f2b282c0bb0 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) ()   from [...]/site-packages/tensorflow/python/_pywrap_tfe.so#16 0x00000000005f3989 in PyCFunction_Call ()[...]#62 Py_BytesMain (argc=<optimized out>, argv=<optimized out>)    at [...]/main.c:731#63 0x00007ffff66bd34d in __libc_start_main () from /lib64/libc.so.6#64 0x000000000040106a in _start () at ../sysdeps/x86_64/start.S:120```</details>
"
55929,1,4043,3,0,0,edwinjosegeorge,0,"title:No bounding box detection when using GPU description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versionv2.8.0-rc1-32-g3f878cff5b6 2.8.0### Custom CodeYes### OS Platform and DistributionLinux Ubuntu 20.04### Mobile device_No response_### Python version3.9.7### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versionCUDA V10.1.243  cuDNN 8400### GPU model and memory_No response_### Current Behaviour?```shellI have a custom yolov4 .weights that is converted to .tf model. When run on GPU, the model only gives detection for first 2 frames. However, when the model is run on CPU, all detection are properly made.I have tried out the same at my local machine, a workstation, and at Google Colab, all results the same.```### Standalone code to reproduce the issue```shellThe code used was imported from https://github.com/theAIGuysCode/yolov4-deepsort.git We used a custom yolov4 model trained on darknet. To run our detection, we first compile .weights into .tf using *save_model.py* and then changed the path for *weights* to point to our model and updated the core/config.py *coco.names* to our custom *label.names*```### Relevant log output```shellOutput when running on GPUOpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'Frame #:  1Objects being tracked: 3FPS: 0.38Frame #:  2Objects being tracked: 0FPS: 17.86Frame #:  3Objects being tracked: 0FPS: 16.62Frame #:  4Objects being tracked: 0FPS: 16.76Frame #:  5Objects being tracked: 0FPS: 16.45Frame #:  6Objects being tracked: 0FPS: 16.78Frame #:  7Objects being tracked: 0FPS: 16.47Frame #:  8Objects being tracked: 0FPS: 15.85Frame #:  9Objects being tracked: 0.....................Output when running on CPUOpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'Frame #:  1Objects being tracked: 3FPS: 1.00Frame #:  2Objects being tracked: 3FPS: 5.62Frame #:  3Objects being tracked: 3Tracker ID: 1, Class: car,  BBox Coords (xmin, ymin, xmax, ymax): (67, 467, 146, 508)Tracker ID: 2, Class: van,  BBox Coords (xmin, ymin, xmax, ymax): (235, 455, 522, 620)Tracker ID: 3, Class: bus,  BBox Coords (xmin, ymin, xmax, ymax): (0, 435, 44, 482)FPS: 5.97Frame #:  4Objects being tracked: 3Tracker ID: 1, Class: car,  BBox Coords (xmin, ymin, xmax, ymax): (67, 467, 146, 508)Tracker ID: 2, Class: van,  BBox Coords (xmin, ymin, xmax, ymax): (220, 441, 549, 631)Tracker ID: 3, Class: bus,  BBox Coords (xmin, ymin, xmax, ymax): (0, 435, 44, 482)FPS: 5.60Frame #:  5Objects being tracked: 3Tracker ID: 1, Class: car,  BBox Coords (xmin, ymin, xmax, ymax): (68, 467, 146, 508)Tracker ID: 2, Class: van,  BBox Coords (xmin, ymin, xmax, ymax): (253, 451, 545, 618)Tracker ID: 3, Class: bus,  BBox Coords (xmin, ymin, xmax, ymax): (0, 435, 44, 482)FPS: 5.82Frame #:  6Objects being tracked: 2Tracker ID: 1, Class: car,  BBox Coords (xmin, ymin, xmax, ymax): (67, 467, 146, 508)Tracker ID: 2, Class: van,  BBox Coords (xmin, ymin, xmax, ymax): (277, 453, 555, 610)Tracker ID: 3, Class: bus,  BBox Coords (xmin, ymin, xmax, ymax): (0, 435, 44, 482)FPS: 6.07Frame #:  7Objects being tracked: 2Tracker ID: 1, Class: car,  BBox Coords (xmin, ymin, xmax, ymax): (67, 467, 146, 508)Tracker ID: 2, Class: van,  BBox Coords (xmin, ymin, xmax, ymax): (277, 446, 578, 615)FPS: 6.21Frame #:  8Objects being tracked: 2Tracker ID: 1, Class: car,  BBox Coords (xmin, ymin, xmax, ymax): (67, 467, 146, 508)Tracker ID: 2, Class: van,  BBox Coords (xmin, ymin, xmax, ymax): (297, 439, 588, 603)FPS: 6.04Frame #:  9Objects being tracked: 2Tracker ID: 1, Class: car,  BBox Coords (xmin, ymin, xmax, ymax): (67, 467, 146, 508)Tracker ID: 2, Class: van,  BBox Coords (xmin, ymin, xmax, ymax): (315, 438, 593, 593)FPS: 6.12Frame #:  10Objects being tracked: 2Tracker ID: 1, Class: car,  BBox Coords (xmin, ymin, xmax, ymax): (67, 467, 146, 508)Tracker ID: 2, Class: van,  BBox Coords (xmin, ymin, xmax, ymax): (332, 441, 596, 586)FPS: 6.27Frame #:  11Objects being tracked: 2Tracker ID: 1, Class: car,  BBox Coords (xmin, ymin, xmax, ymax): (67, 466, 147, 508)Tracker ID: 2, Class: van,  BBox Coords (xmin, ymin, xmax, ymax): (317, 428, 627, 599).....................................```</details>
"
55916,1,285,3,0,0,HotzingTone,0,"title:tfd.Normal.prob outputs probabilities greater than 1 description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.8### Custom CodeYes### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellA bug happened!tfd.Normal.prob outputs probabilities greater than 1```### Standalone code to reproduce the issue```shellimport tensorflow_probability as tfptfd = tfp.distributionsdist = tfd.Normal(loc=0.53605634, scale=0.3021255)dist.prob(0.41487068)output: <tf.Tensor: shape=(), dtype=float32, numpy=1.218389>```### Relevant log output_No response_</details>
"
55845,0,24234,269,0,1,elfringham,0,"title://tensorflow/tools/docs:tf_doctest unit test broken description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiongit HEAD### Custom CodeNo### OS Platform and DistributionUbuntu 20.04### Mobile devicen/a### Python version3.8.10### Bazel version5.1.1### GCC/Compiler version10.3.0### CUDA/cuDNN versionn/a### GPU model and memoryn/a### Current Behaviour?```shell//tensorflow/tools/docs:tf_doctest fails```### Standalone code to reproduce the issue```shellbazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --config=nonccl --copt=""-mtune=generic"" --copt=""-march=armv8-a"" --copt=""-O3"" --verbose_failures --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --strip=never --build_tests_only -- //tensorflow/tools/docs:tf_doctest```### Relevant log output```shell$ bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --config=nonccl --copt=""-mtune=generic"" --copt=""-march=armv8-a"" --copt=""-O3"" --verbose_failures --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --strip=never --build_tests_only -- //tensorflow/tools/docs:tf_doctestINFO: Options provided by the client:  Inherited 'common' options: --isatty=1 --terminal_columns=159INFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc:  Inherited 'common' options: --experimental_repo_remote_execINFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc:  Inherited 'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=trueINFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.tf_configure.bazelrc:  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/home/builder/1/tensorflow_build/venv_py38/bin/python3 --action_env PYTHON_LIB_PATH=/home/builder/1/tensorflow_build/venv_py38/lib/python3.8/site-packages --python_path=/home/builder/1/tensorflow_build/venv_py38/bin/python3INFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc:  Inherited 'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utilsINFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.tf_configure.bazelrc:  'test' options: --flaky_test_attempts=3 --test_size_filters=small,mediumINFO: Found applicable config definition build:short_logs in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --output_filter=DONT_MATCH_ANYTHINGINFO: Found applicable config definition build:v2 in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1INFO: Found applicable config definition test:v2 in file /home/builder/1/tensorflow_build/tensorflow-git/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1onlyINFO: Found applicable config definition build:nonccl in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --define=no_nccl_support=trueINFO: Found applicable config definition build:linux in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changesINFO: Found applicable config definition build:dynamic_kernels in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELSINFO: Analyzed target //tensorflow/tools/docs:tf_doctest (283 packages loaded, 14898 targets configured).INFO: Found 1 test target...FAIL: //tensorflow/tools/docs:tf_doctest (shard 3 of 4) (see /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_3_of_4/test_attempts/attempt_1.log)FAIL: //tensorflow/tools/docs:tf_doctest (shard 1 of 4) (see /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_1_of_4/test_attempts/attempt_1.log)FAIL: //tensorflow/tools/docs:tf_doctest (shard 4 of 4) (see /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_4_of_4/test_attempts/attempt_1.log)FAIL: //tensorflow/tools/docs:tf_doctest (shard 2 of 4) (see /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_2_of_4/test_attempts/attempt_1.log)FAIL: //tensorflow/tools/docs:tf_doctest (shard 3 of 4) (see /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_3_of_4/test_attempts/attempt_2.log)FAIL: //tensorflow/tools/docs:tf_doctest (shard 1 of 4) (see /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_1_of_4/test_attempts/attempt_2.log)FAIL: //tensorflow/tools/docs:tf_doctest (shard 2 of 4) (see /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_2_of_4/test_attempts/attempt_2.log)FAIL: //tensorflow/tools/docs:tf_doctest (shard 4 of 4) (see /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_4_of_4/test_attempts/attempt_2.log)FAIL: //tensorflow/tools/docs:tf_doctest (shard 2 of 4) (see /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_2_of_4/test.log)INFO: From Testing //tensorflow/tools/docs:tf_doctest (shard 2 of 4):==================== Test output for //tensorflow/tools/docs:tf_doctest (shard 2 of 4):2022-05-03 14:24:00.201237: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered underGRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ==================================================================================================== Test output for //tensorflow/tools/docs:tf_doctest (shard 2 of 4):2022-05-03 14:24:04.612762: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered underGRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ==================================================================================================== Test output for //tensorflow/tools/docs:tf_doctest (shard 2 of 4):2022-05-03 14:24:08.768859: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered underGRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ================================================================================FAIL: //tensorflow/tools/docs:tf_doctest (shard 3 of 4) (see /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_3_of_4/test.log)INFO: From Testing //tensorflow/tools/docs:tf_doctest (shard 3 of 4):==================== Test output for //tensorflow/tools/docs:tf_doctest (shard 3 of 4):2022-05-03 14:24:00.195904: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered underGRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ==================================================================================================== Test output for //tensorflow/tools/docs:tf_doctest (shard 3 of 4):2022-05-03 14:24:04.547096: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered underGRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ==================================================================================================== Test output for //tensorflow/tools/docs:tf_doctest (shard 3 of 4):2022-05-03 14:24:08.838289: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered underGRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ================================================================================FAIL: //tensorflow/tools/docs:tf_doctest (shard 4 of 4) (see /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_4_of_4/test.log)INFO: From Testing //tensorflow/tools/docs:tf_doctest (shard 4 of 4):==================== Test output for //tensorflow/tools/docs:tf_doctest (shard 4 of 4):2022-05-03 14:24:00.201237: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered underGRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ==================================================================================================== Test output for //tensorflow/tools/docs:tf_doctest (shard 4 of 4):2022-05-03 14:24:04.675745: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered underGRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ==================================================================================================== Test output for //tensorflow/tools/docs:tf_doctest (shard 4 of 4):2022-05-03 14:24:08.891334: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered underGRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ================================================================================FAIL: //tensorflow/tools/docs:tf_doctest (shard 1 of 4) (see /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_1_of_4/test.log)FAILED: //tensorflow/tools/docs:tf_doctest (Summary)      /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_2_of_4/test.log      /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_2_of_4/test_attempts/attempt_1.log      /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_2_of_4/test_attempts/attempt_2.log      /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_3_of_4/test.log      /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_3_of_4/test_attempts/attempt_1.log      /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_3_of_4/test_attempts/attempt_2.log      /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_4_of_4/test.log      /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_4_of_4/test_attempts/attempt_1.log      /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_4_of_4/test_attempts/attempt_2.log      /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_1_of_4/test.log      /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_1_of_4/test_attempts/attempt_1.log      /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_1_of_4/test_attempts/attempt_2.logINFO: From Testing //tensorflow/tools/docs:tf_doctest (shard 1 of 4):==================== Test output for //tensorflow/tools/docs:tf_doctest (shard 1 of 4):2022-05-03 14:24:00.195904: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered underGRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ==================================================================================================== Test output for //tensorflow/tools/docs:tf_doctest (shard 1 of 4):2022-05-03 14:24:04.586980: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered underGRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ==================================================================================================== Test output for //tensorflow/tools/docs:tf_doctest (shard 1 of 4):2022-05-03 14:24:08.950464: E tensorflow/core/common_runtime/session_factory.cc:48] Two session factories are being registered underGRPC_SESSION[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: tensorflow/python/framework/cpp_shape_inference.proto[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): terminate called after throwing an instance of 'google::protobuf::FatalException'  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): ================================================================================Target //tensorflow/tools/docs:tf_doctest up-to-date:  bazel-bin/tensorflow/tools/docs/tf_doctestINFO: Elapsed time: 469.600s, Critical Path: 344.39sINFO: 968 processes: 141 internal, 827 local.INFO: Build completed, 1 test FAILED, 968 total actions//tensorflow/tools/docs:tf_doctest                                       FAILED in 12 out of 12 in 9.4s  Stats over 12 runs: max = 9.4s, min = 4.1s, avg = 6.0s, dev = 2.4s  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_2_of_4/test.log  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_2_of_4/test_attempts/attempt_1.log  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_2_of_4/test_attempts/attempt_2.log  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_3_of_4/test.log  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_3_of_4/test_attempts/attempt_1.log  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_3_of_4/test_attempts/attempt_2.log  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_4_of_4/test.log  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_4_of_4/test_attempts/attempt_1.log  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_4_of_4/test_attempts/attempt_2.log  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_1_of_4/test.log  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_1_of_4/test_attempts/attempt_1.log  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/tools/docs/tf_doctest/shard_1_of_4/test_attempts/attempt_2.logINFO: Build completed, 1 test FAILED, 968 total actions```</details>
"
55835,1,0,0,0,0,leeflix,0,"title:Exporting LSTM with TFlite Converter yields model which makes bad predictions in contrast to keras model description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.8### Custom CodeNo### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellI want to export the model presented in this (https://keras.io/examples/vision/captcha_ocr/) keras tutorial in the tflite format. I found two ways to export the model:1. Setting the flag tf.lite.OpsSet.SELECT_TF_OPS2. Using the function get_concrete_functionOption 1 works in terms of correct predictions, but I would like to refrain from using the flag. With option 2 the export works but the predictions of the exported model are bad.```### Standalone code to reproduce the issue```shellYou can go through the keras notebook and add the following code for the two options.Code for option 1:model = prediction_modelconverter = tf.lite.TFLiteConverter.from_keras_model(model)converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]tflite_model = converter.convert()with open('model.tflite', 'wb') as f:    f.write(tflite_model)----------------------------------------------------------------Code for option 2:model = prediction_modelrun_model = tf.function(lambda x: model(x))concrete_func = run_model.get_concrete_function(tf.TensorSpec([1] + model.inputs[0].shape[1:], model.inputs[0].dtype))# model directory.MODEL_DIR = ""keras_lstm""model.save(MODEL_DIR, save_format=""tf"", signatures=concrete_func)converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)tflite_model = converter.convert()with open('flag_diopter_model.tflite', 'wb') as f:    f.write(tflite_model)```### Relevant log output_No response_</details>
"
55814,1,648,13,0,0,njuaplusplus,0,"title:Inconsistent results with or without tf.function description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf 2.8.0### Custom CodeNo### OS Platform and DistributionGoogle Colab### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellThe function with tf.function decorator should produce the same results the one without the decorator. In addition, adding one tf.print statement in the function with tf.function decorator will change the behavior.```### Standalone code to reproduce the issue```shellI created a google colab notebook. Please check this link.https://colab.research.google.com/drive/1FOvjw0pPZHQw2bZtuFE8fnTAZk8_n0X8?usp=sharing```### Relevant log output```shellTensorFlow version: 2.8.0loss_object tf.Tensor(26.064999, shape=(), dtype=float32)loss_object_1 tf.Tensor(nan, shape=(), dtype=float32)[[1.5240801e-14 4.78756791e-12 7.77465369e-15 ... 1 4.88135581e-29 0]]loss_object_2 tf.Tensor(nan, shape=(), dtype=float32)```</details>
"
55795,0,0,8,0,0,jgentil,0,"title:Autograph crashes when using pythonw.exe on Windows description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Version2.8.0### Custom CodeNo### OS Platform and DistributionWindows### Mobile device_No response_### Python version3.8### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?When using pythonw.exe, if local sources are not available, (e.g. only .pyc files are available), autograph causes a crash by trying to flush sys.stdout.https://github.com/tensorflow/tensorflow/blob/f39905619ddf75b31e209264673f76ac2c72030e/tensorflow/python/autograph/core/ag_ctx.py#L100-L105If the source code to Autograph itself isn't available, it emits this warning using ag_logging.warning:https://github.com/tensorflow/tensorflow/blob/f39905619ddf75b31e209264673f76ac2c72030e/tensorflow/python/autograph/utils/ag_logging.py#L141-L145For some reason, `warning()` is different than any of the other emitters and flushes the stream.  But when running in pythonw.exe, `sys.stdout` is `None`.  It checks for this using `echo_log_to_stdout` to ensure that it doesn't write to stdout, but then immediately tries to flush it.This can be remediated to indenting the call to `sys.stdout.flush()` to inside the `if echo_log_to_stdout:` check.### Standalone code to reproduce the issue1. Create a virtual environment and install TensorFlow2. Compile all of your py files in site-packages using the `compileall` module3. Delete py files from your site-packages4. Create an example tk-based script or something else that shows a GUI5. Import tensorflow in your script and execute it using `pythonw.exe` and observe the error, which will not fail when running `python.exe`### Relevant log output_No response_</details>
"
55792,0,1289,148,0,0,kaixih,0,"title:TF doesn't fuse Conv2D since v2.7 description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.8, tf 2.9, tf 2.10### Custom CodeNo### OS Platform and DistributionUbuntu 20.04### Mobile device_No response_### Python version3.9### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version11.6/8.1### GPU model and memory_No response_### Current Behaviour?```shellThe TF no longer fuse the convolution+bias+activation patterns from 2.8 and later.```### Standalone code to reproduce the issue```shellimport tensorflow as tfimport numpy as npuse_nhwc = TrueN, H, W, C = 1, 2, 3, 3k, r, s, c = 3, 2, 2, Cif use_nhwc:  x_format = 'NHWC'  x_format_keras = 'channels_last'  bias_format = 'N...C'  x_shape = (N, H, W, C)  channel_axis = -1else:  x_format = 'NCHW'  x_format_keras = 'channels_first'  bias_format = 'NC...'  x_shape = (N, C, H, W)  channel_axis = 1f_np = np.random.random([r, s, c, k]).astype(np.float32)f = tf.Variable(f_np)b_np = np.random.random([k]).astype(np.float32)b = tf.Variable(b_np)@tf.functiondef fused_conv_bias_relu(x):  y = tf.nn.conv2d(x, f, strides=(1,1), padding='SAME',                   data_format=x_format, name='xxx_cno1')  y = tf.nn.bias_add(y, b, data_format=bias_format)  y = tf.nn.relu(y)  return yinputs = tf.random.normal(x_shape)outputs = fused_conv_bias_relu(inputs)print(outputs)```### Relevant log output```shell$ TF_CPP_VMODULE=remapper=2 python demo.pyIn 2.7.1 container:...2022-04-28 20:35:13.516124: I tensorflow/core/grappler/optimizers/remapper.cc:1345] Fuse Conv2D with BiasAdd and Relu: activation=Relu bias_add=BiasAdd contraction=xxx_cno1In nightly-gpu container:...<No fusion info>```</details>
"
55753,0,320,39,0,0,kanghj,0,"title:Different behavior between `tf.experimental.numpy.(v/h/d)split` and `np.(v/h/d)split` description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.8### Custom CodeYes### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?I expected Tensorflow to throw a similar exception (`ValueError: array split does not result in an equal division`) as Numpy on invoking the experimental.numpy.*split functions (e.g. `tf.experimental.numpy.dsplit`, `tf.experimental.numpy.hsplit`, `tf.experimental.numpy.vsplit`),but currently, Tensorflow consumes all RAM and crashes.I wonder if Tensorflow should perform the validation checks in the same order as numpy (i.e., detect the invalid array split first)?### Standalone code to reproduce the issueHere's a colab notebook: https://colab.research.google.com/drive/1zyOtnyGi9KgZLskYc4SRCB74fV73rnyU?usp=sharing```pythonimport tensorflow as tfimport numpy as npx = np.arange(16.0)tf.experimental.numpy.dsplit(ary=x,indices_or_sections=256000000)  # crashes``````python# In contrast,np.dsplit(ary=np.arange(16.0),indices_or_sections=128000000) # doesn't crash, but throws an exception instead. This is the expected behavior```### Relevant log output_No response_</details>
"
55745,0,0,0,0,0,stewartmiles,0,"title:TF Lite (CMake): Does not compile on Windows description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.9.0### Custom CodeNo### OS Platform and DistributionWindows (MSVC 2019)### Mobile deviceN/A### Python versionN/A### Bazel versionN/A### GCC/Compiler versionN/A### CUDA/cuDNN versionN/A### GPU model and memoryN/A### Current Behaviour?```shellTF Lite fails to compile on Windows with MSVC 2019 due to `M_PI` being used by `tensorflow/core/lib/random/random_distributions_utils.h` without defining `_USE_MATH_DEFINES` (see https://docs.microsoft.com/en-us/cpp/c-runtime-library/math-constants?view=msvc-160).```### Standalone code to reproduce the issue```shellAttempt to compile TF Lite on Windows with CMake and it will fail to build with the following error:random_ops.ccbuild\tensorflow\tensorflow/core/lib/random/random_distributions_utils.h(78,27): error C2065: 'M_PI': undeclared identifier [build\_deps\tensorflow-build\tensorflow-lite.vcxproj]\build\tensorflow\tensorflow/core/lib/random/random_distributions_utils.h(78,15): error C2737: 'v1': const object must be initialized [build\_deps\tensorflow-build\tensorflow-lite.vcxproj]``````### Relevant log output_No response_</details>
"
55738,0,2936,4,0,0,cchan-lm,0,"title:BatchNorm with mixed_bfloat16 returns empty output description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versiontf 2.7.0### Custom CodeNo### OS Platform and DistributionLinx Ubuntu 18.04### Mobile device_No response_### Python version3.8.10### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version11.2/8.1### GPU model and memoryNVIDIA A100### Current Behaviour?```shellBatchNorm layer using `dtype=""mixed_bfloat16""` or ""bfloat16"" on an NVIDIA A100 returns an empty output with TF 2.7.0. However, when using `dtype=""float32""`, ""float16"", or ""mixed_float16"", an actual input is returned.As a side note, it is also only with `float32` that I get the message `tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8100`. `mixed_float16` gives the message `Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: A100-PCIE-40GB, compute capability 8.0`. `float16` does not have a message of its own, it seems.To minimize environmental problems, I used the `tensorflow/tensorflow:2.7.0-gpu` Docker image and ran `docker run -it --gpus all tensorflow/tensorflow:2.7.0-gpu bash`Thank you in advance for helping solve this issue!```### Standalone code to reproduce the issue```shellimport tensorflow as tfimport numpy as nptest_bn = tf.keras.layers.BatchNormalization(dtype=""mixed_bfloat16"")test_bn(np.random.randn(16, 24, 24, 32))```### Relevant log output```shellWhen using mixed_bfloat16 or bfloat16:2022-04-25 18:55:54.298268: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2022-04-25 18:55:54.845911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38403 MB memory:  -> device: 0, name: A100-PCIE-40GB, pci bus id: 0000:01:00.0, compute capability: 8.0<tf.Tensor: shape=(0,), dtype=float32, numpy=array([], dtype=float32)>When using float32:2022-04-25 18:58:51.935409: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2022-04-25 18:58:53.684462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38403 MB memory:  -> device: 0, name: A100-PCIE-40GB, pci bus id: 0000:01:00.0, compute capability: 8.02022-04-25 18:58:54.296455: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8100<tf.Tensor: shape=(16, 24, 24, 32), dtype=float32, numpy=array([[[[-1.78168976e+00,  8.01364720e-01,  3.17569762e-01, ...,          -1.08890069e+00, -4.05266196e-01,  9.75035071e-01],         [-9.24855590e-01,  5.21459460e-01,  1.35295260e+00, ...,          -1.89387918e+00,  8.45147550e-01, -7.09079564e-01],         [ 1.49759853e+00,  3.18406433e-01,  2.32584342e-01, ...,           2.76129186e-01, -2.41327822e-01, -4.68785256e-01],         ...,...```</details>
"
55724,1,2336,4,0,0,kaysagit,0,"title:AttributeError: module 'tensorflow.compat.v2.__internal__.distribute' has no attribute 'strategy_supports_no_merge_call' description:Click to expand!   ### Issue TypeBug### Sourcebinary### Tensorflow Versionv2.7.0-rc1-69-gc256c071bb2 2.7.0### Custom CodeYes### OS Platform and DistributionNAME=""CentOS Linux"" VERSION=""7 (Core)""### Mobile device_No response_### Python versionPython 3.7.13### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN versionCuda 8.x and Cuda 11.x### GPU model and memoryNVIDIA GeForce GTX 1080 Ti  11178MiB### Current Behaviour?```shellWhen I want to train my model I get the error shown below. I couldn't find any solution our hint. Can you explain why this error occurs and how to solve it?Many thanks in advance!```### Standalone code to reproduce the issue```shellThis error arises in my own code, which is a simple U-net taking in 3D numpy arrays```### Relevant log output```shellTraceback (most recent call last):  File ""./training.py"", line 299, in launch    sample_weight=sample_weight,  File ""./python3.7/site-packages/keras/engine/training.py"", line 2093, in train_on_batch    logs = self.train_function(iterator)  File ""./python3.7/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler    raise e.with_traceback(filtered_tb) from None  File ""./.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 1129, in autograph_handler    raise e.ag_error_metadata.to_exception(e)AttributeError: in user code:    File ""./lib/python3.7/site-packages/keras/engine/training.py"", line 1021, in train_function  *        return step_function(self, iterator)    File ""./lib/python3.7/site-packages/keras/engine/training.py"", line 1010, in step_function  **        outputs = model.distribute_strategy.run(run_step, args=(data,))    File ""./lib/python3.7/site-packages/keras/engine/training.py"", line 1000, in run_step  **        outputs = model.train_step(data)    File ""./lib/python3.7/site-packages/keras/engine/training.py"", line 863, in train_step        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)    File ""./lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py"", line 532, in minimize        return self.apply_gradients(grads_and_vars, name=name)    File ""./lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py"", line 668, in apply_gradients        grads_and_vars = self._aggregate_gradients(grads_and_vars)    File ""./lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py"", line 484, in _aggregate_gradients        return self.gradient_aggregator(grads_and_vars)    File ""./lib/python3.7/site-packages/keras/optimizer_v2/utils.py"", line 33, in all_reduce_sum_gradients        if tf.__internal__.distribute.strategy_supports_no_merge_call():    AttributeError: module 'tensorflow.compat.v2.__internal__.distribute' has no attribute 'strategy_supports_no_merge_call'```</details>
"
55719,1,3787,7,0,0,winginitau,0,"title:Error cascade on model.save() version 2.9.0-rc0 description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.9.0-rc0### Custom CodeNo### OS Platform and DistributionLinux Ubuntu 21.09### Mobile device_No response_### Python version3.9.7### Bazel version5.1.1### GCC/Compiler version11.2.0### CUDA/cuDNN version_No response_### GPU model and memoryIntel Core i7 980### Current Behaviour?```shellError cascade on saving model after training.Saves fine before training. Many thanksBrendan```### Standalone code to reproduce the issue```shellimport tensorflow as tffrom tensorflow import kerasimport pandas as pdprint(tf.version.GIT_VERSION, tf.version.VERSION)def baseline_model():    b_model = keras.Sequential()    b_model.add(keras.layers.Flatten(input_shape=[5]))        b_model.add(keras.layers.Dense(units=512, activation='relu', name='dense_1'))    b_model.add(keras.layers.Dropout(0.2))    b_model.add(keras.layers.Dense(units=32, activation='relu', name='dense_2'))    b_model.add(keras.layers.Dense(3, activation='softmax'))        b_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),                  loss=keras.losses.CategoricalCrossentropy(),                  metrics=[""accuracy""])    return b_modelf = {'feature1': [1, 1], 'feature2': [2, 2], 'feature3': [3, 3], 'feature4': [4, 4], 'feature5': [5, 5] }c = {'cat1': [0, 1 ], 'cat2': [1, 0], 'cat3': [0, 0] }x_train = pd.DataFrame(f)y_train = pd.DataFrame(c)                                 model = baseline_model()model.save('grrrrr_untrained')model.fit(x_train, y_train, epochs=2, verbose=1)model.save('grrrrr_trained')```### Relevant log output```shellv1.12.1-73396-g3903c35b3bf 2.9.0-rc02022-04-23 10:22:59.360155: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.Epoch 1/21/1 [==============================] - ETA: 0s - loss: 0.9775 - accuracy: 0.50001/1 [==============================] - 1s 612ms/step - loss: 0.9775 - accuracy: 0.5000Epoch 2/21/1 [==============================] - ETA: 0s - loss: 0.8341 - accuracy: 0.50001/1 [==============================] - 0s 3ms/step - loss: 0.8341 - accuracy: 0.5000Traceback (most recent call last):...RuntimeError: in user code:    RuntimeError: Mismatching ReplicaContext....ValueError: Error when tracing gradients for SavedModel....Check the error log to see the error that was raised when converting a gradient function to a concrete function. You may need to update the custom gradient, or disable saving gradients with the option tf.saved_model.SaveOptions(custom_gradients=False).	Problematic op name: Adam/IdentityN	Gradient inputs: (<tf.Tensor 'gradient_tape/sequential/dense_1/MatMul/MatMul:0' shape=(5, 512) dtype=float32>, <tf.Tensor 'gradient_tape/sequential/dense_1/BiasAdd/BiasAddGrad:0' shape=(512,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential/dense_2/MatMul/MatMul_1:0' shape=(512, 32) dtype=float32>, <tf.Tensor 'gradient_tape/sequential/dense_2/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential/dense/MatMul/MatMul_1:0' shape=(32, 3) dtype=float32>, <tf.Tensor 'gradient_tape/sequential/dense/BiasAdd/BiasAddGrad:0' shape=(3,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential/dense_1/MatMul/MatMul:0' shape=(5, 512) dtype=float32>, <tf.Tensor 'gradient_tape/sequential/dense_1/BiasAdd/BiasAddGrad:0' shape=(512,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential/dense_2/MatMul/MatMul_1:0' shape=(512, 32) dtype=float32>, <tf.Tensor 'gradient_tape/sequential/dense_2/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential/dense/MatMul/MatMul_1:0' shape=(32, 3) dtype=float32>, <tf.Tensor 'gradient_tape/sequential/dense/BiasAdd/BiasAddGrad:0' shape=(3,) dtype=float32>)```</details>
"
55715,1,0,0,0,1,KangChou,0,"title:./build_all_linux.sh  description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiontf1.15### Custom CodeNo### OS Platform and Distributionubuntu18.04### Mobile deviceubuntu18.04### Python versionpython==3.6.13### Bazel version0.26.1### GCC/Compiler version7.5### CUDA/cuDNN versionCPU### GPU model and memoryCPU### Current Behaviour?```shellA bug happened!tensorflow/contrib/makefile# ./build_all_linux.sh  inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/AppDelegate.h    inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/AppDelegate.mm     creating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/Base.lproj/  inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/Base.lproj/LaunchScreen.xib    inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/Base.lproj/Main.storyboard     creating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/Images.xcassets/   creating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/Images.xcassets/AppIcon.appiconset/  inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/Images.xcassets/AppIcon.appiconset/Contents.json    inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/Info.plist    inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/ViewController.h    inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/ViewController.m    inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/main.m    inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/test.cc    inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/test.h    inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/test_allocator.cc    inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/test_blocking_counter.cc    inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/test_data.cc    inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/test_data.h    inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/test_fixedpoint.cc    inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/test_math_helpers.cc     creating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/todo/  inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/todo/armv8-64bit-kernel-for-less-than-8-bit.txt    inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/todo/error-diffusion-experiments.txt    inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/todo/fast-gemv.txt    inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/todo/less-than-8-bit-without-requantization.txt    inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/todo/multi-threading-experiments.txt    inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/todo/neon-depth-major-sources-packing.txt    inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/todo/remove-default-template-param-values.txt    inflating: /tmp/tmp.8jIp3DIvsk/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/todo/x86-kernels.txt  downloading https://github.com/google/googletest/archive/refs/tags/release-1.8.0.tar.gzgzip: stdin: unexpected end of filetar: Child returned status 1tar: Error is not recoverable: exiting now```### Standalone code to reproduce the issue```shelltensorflow/contrib/makefile# ./build_all_linux.shgzip: stdin: unexpected end of filetar: Child returned status 1tar: Error is not recoverable: exiting now```### Relevant log output```shelltensorflow/contrib/makefile# ./build_all_linux.shgzip: stdin: unexpected end of filetar: Child returned status 1tar: Error is not recoverable: exiting now```</details>
"
55699,0,9600,269,0,0,elfringham,0,"title://tensorflow/tools/docs:tf_doctest fails on machines without GPU description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Versiongit HEAD### Custom CodeNo### OS Platform and DistributionCentOS 7### Mobile devicen/a### Python version3.7.13### Bazel version5.1.1### GCC/Compiler version10.2.1### CUDA/cuDNN versionn/a### GPU model and memoryn/a### Current Behaviour?```shellUnit test //tensorflow/tools/docs:tf_doctest fails when run on machine without GPU installed.```### Standalone code to reproduce the issue```shellbazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --noremote_accept_cached --config=nonccl --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --verbose_failures --build_tests_only -- //tensorflow/tools/docs:tf_doctest```### Relevant log output```shellMultiple instances similar to----------------------------------------------------------------------File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/types/distribute.py"", line 156, in tensorflow.python.types.distribute.DistributedValuesFailed example:    strategy = tf.distribute.MirroredStrategy([""GPU:0"", ""GPU:1""])Exception raised:    Traceback (most recent call last):      File ""/opt/python/cp37-cp37m/lib/python3.7/doctest.py"", line 1337, in __run        compileflags, 1), test.globs)      File ""<doctest tensorflow.python.types.distribute.DistributedValues[22]>"", line 1, in <module>        strategy = tf.distribute.MirroredStrategy([""GPU:0"", ""GPU:1""])      File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/distribute/mirrored_strategy.py"", line 287, in __init__        self, devices=devices, cross_device_ops=cross_device_ops)      File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/distribute/mirrored_strategy.py"", line 342, in __init__        self._initialize_strategy(devices)      File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/distribute/mirrored_strategy.py"", line 367, in _initialize_strategy        self._collective_ops = self._make_collective_ops(devices)      File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/distribute/mirrored_strategy.py"", line 385, in _make_collective_ops        collective_keys=self._collective_keys)      File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/distribute/cross_device_ops.py"", line 1104, in __init__        group_key, group_size, self._collective_keys, device, options)      File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/distribute/cross_device_utils.py"", line 271, in __init__        self._ordering_token = resource_variable_ops.ResourceVariable(0.)      File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/util/traceback_utils.py"", line 141, in error_handler        return fn(*args, **kwargs)      File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/ops/variables.py"", line 268, in __call__        return super(VariableMetaclass, cls).__call__(*args, **kwargs)      File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/ops/resource_variable_ops.py"", line 1670, in __init__        validate_shape=validate_shape,      File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/ops/resource_variable_ops.py"", line 1817, in _init_from_args        initial_value, name=""initial_value"", dtype=dtype)      File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/profiler/trace.py"", line 183, in wrapped        return func(*args, **kwargs)      File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 1640, in convert_to_tensor        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)      File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/framework/tensor_conversion_registry.py"", line 48, in _default_conversion_function        return constant_op.constant(value, dtype, name=name)      File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/framework/constant_op.py"", line 268, in constant        allow_broadcast=True)      File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/framework/constant_op.py"", line 279, in _constant_impl        return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)      File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/framework/constant_op.py"", line 304, in _constant_eager_impl        t = convert_to_eager_tensor(value, ctx, dtype)      File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/framework/constant_op.py"", line 102, in convert_to_eager_tensor        return ops.EagerTensor(value, ctx.device_name, dtype)    tensorflow.python.framework.errors_impl.InvalidArgumentError: Could not satisfy device specification '/job:localhost/replica:0/task:0/device:GPU:0'. enable_soft_placement=0. Supported device types [CPU]. All available devices [/job:localhost/replica:0/task:0/device:CPU:0].----------------------------------------------------------------------File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/types/distribute.py"", line 158, in tensorflow.python.types.distribute.DistributedValuesFailed example:    dataset_iterator = iter(strategy.experimental_distribute_dataset(dataset))Exception raised:    Traceback (most recent call last):      File ""/opt/python/cp37-cp37m/lib/python3.7/doctest.py"", line 1337, in __run        compileflags, 1), test.globs)      File ""<doctest tensorflow.python.types.distribute.DistributedValues[24]>"", line 1, in <module>        dataset_iterator = iter(strategy.experimental_distribute_dataset(dataset))    NameError: name 'strategy' is not defined----------------------------------------------------------------------File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/types/distribute.py"", line 159, in tensorflow.python.types.distribute.DistributedValuesFailed example:    per_replica_values = strategy.experimental_local_results(       distributed_values)Exception raised:    Traceback (most recent call last):      File ""/opt/python/cp37-cp37m/lib/python3.7/doctest.py"", line 1337, in __run        compileflags, 1), test.globs)      File ""<doctest tensorflow.python.types.distribute.DistributedValues[25]>"", line 1, in <module>        per_replica_values = strategy.experimental_local_results(    NameError: name 'strategy' is not defined----------------------------------------------------------------------File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/types/distribute.py"", line 161, in tensorflow.python.types.distribute.DistributedValuesFailed example:    per_replica_valuesException raised:    Traceback (most recent call last):      File ""/opt/python/cp37-cp37m/lib/python3.7/doctest.py"", line 1337, in __run        compileflags, 1), test.globs)      File ""<doctest tensorflow.python.types.distribute.DistributedValues[26]>"", line 1, in <module>        per_replica_values    NameError: name 'per_replica_values' is not defined----------------------------------------------------------------------```</details>
"
55656,1,33,0,0,0,rthadur,1,"title:test description:Click to expand!   ### Issue TypeBug### Sourcesource### Tensorflow Version2.6### Custom CodeYes### OS Platform and Distribution_No response_### Mobile device_No response_### Python version_No response_### Bazel version_No response_### GCC/Compiler version_No response_### CUDA/cuDNN version_No response_### GPU model and memory_No response_### Current Behaviour?```shellA bug happened!```### Standlone code to reproduce the issue```shelltest```### Relevant log output_No response_</details>
"
55610,0,1981,186,0,0,wenscarl,0,"title:[XLA] Different JIT compile behavior from TF2.7 description:For the customized code below, I have seen such a error at runtime when xla is turned on. This does NOT appear in TF2.7.`2022-04-13 19:49:36.873241: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at xla_ops.cc:436 : INVALID_ARGUMENT: Fail to proof the equality of two dimensions at compile time: %multiply.144 = s32[] multiply(s32[] %constant.142, s32[] %add.1), metadata={op_type=""Reshape"" op_name=""Reshape_3""} vs %add = s32[] add(s32[] %reduce.109, s32[] %constant.17)`**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution: Ubuntu 20.04.4 LTS- TensorFlow installed from (source or binary): binary- TensorFlow version:2.8.0- Python version: 3.8- GCC/Compiler version (if compiling from source): gcc 10- CUDA/cuDNN version: 11.6- GPU model and memory: V100, 32G**Standalone code to reproduce the issue**```import tensorflow as tfimport numpy as npdisplay_id_counter = tf.Variable(0, trainable=False, dtype=tf.float64)@tf.functiondef evaluation_step(x, y, predictions):    dummy_loss = 0.9    predictions = tf.reshape(predictions, [-1])    predictions = tf.cast(predictions, tf.float64)    display_ids = x    display_ids = tf.reshape(display_ids, [-1])    labels = tf.reshape(y, [-1])    sorted_ids = tf.argsort(display_ids)    display_ids = tf.gather(display_ids, indices=sorted_ids)    predictions = tf.gather(predictions, indices=sorted_ids)    labels = tf.gather(labels, indices=sorted_ids)    _, display_ids_idx, display_ids_ads_count = tf.unique_with_counts(        display_ids, out_idx=tf.int64)    pad_length = 30 - tf.reduce_max(display_ids_ads_count)    preds = tf.RaggedTensor.from_value_rowids(        predictions, display_ids_idx).to_tensor()    labels = tf.RaggedTensor.from_value_rowids(        labels, display_ids_idx).to_tensor()    labels_mask = tf.math.reduce_max(labels, 1)    preds_masked = tf.boolean_mask(preds, labels_mask)    labels_masked = tf.boolean_mask(labels, labels_mask)    labels_masked = tf.argmax(labels_masked, axis=1, output_type=tf.int32)    labels_masked = tf.reshape(labels_masked, [-1, 1])    preds_masked = tf.pad(preds_masked, [(0, 0), (0, pad_length)])    _, predictions_idx = tf.math.top_k(preds_masked, 12)    indices = tf.math.equal(predictions_idx, labels_masked)    shape = tf.cast(tf.shape(indices)[0], tf.float64)    display_id_counter.assign_add(shape)DIM = 102400tf.config.optimizer.set_jit(True)for step in range(200):    pre = np.random.random((DIM, 1))    y_tmp = np.zeros((DIM, 1), dtype=float)    num_ones = np.random.randint(1, DIM+1, 1)    id_one = np.random.randint(0, DIM, num_ones)    for i in id_one:        y_tmp[i][0] = 1.    x_tmp = np.random.randint(0, DIM, (DIM, 1), dtype=np.int64)    evaluation_step(x_tmp, y_tmp, pre)```Tracked down to commit [ac4575](https://github.com/tensorflow/tensorflow/commit/ac457560364f18f24ae506d978f2ab5f04aa501b).
"
55595,1,1804,69,0,0,dogeplusplus,0,"title:Possible shuffling issue with using `tf.data.Dataset.list_files`? description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 22.04 (Tried colab as well)- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow installed from (source or binary): pip wheel- TensorFlow version (use command below): 2.8.0 `v2.8.0-rc1-32-g3f878cff5b6 2.8.0`- Python version: 3.8.6- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source): N/A- CUDA/cuDNN version: 11.2- GPU model and memory: RTX 3090 (24GB)You can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**I'm having some weird behavior where I'm enumerating files in a directory, then generating the corresponding labels by using the map of the first dataset. However the order isn't being preserved when I zip them together. I've tried zipping a dataset generated from tensor slices and that seems to work. I've also tested this on Colab and get the same issue.EDIT: I checked the documentation and understand there's a shuffle parameter in list files which defaults to True, but I was expecting that if I'm zipping the same object with itself I should get matching pairs. Let me know if this is the intended behavior**Describe the expected behavior**I expect that if I zip a dataset with itself I should get pairs of matching items when I iterate over it.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.```import osimport tensorflow as tftf.random.set_seed(42)from tempfile import TemporaryDirectorywith TemporaryDirectory() as temp_dir:    for i in range(10):        os.mknod(os.path.join(temp_dir, f""{i}.txt""))    inputs = tf.data.Dataset.list_files(f""{temp_dir}/*"")    ds = tf.data.Dataset.zip((inputs, inputs))    for x, y in ds:        print(f""Input: {str(x)}, Output: {str(y)}"")```Gives me:```Input: tf.Tensor(b'/tmp/tmpxcqf5sg_/5.txt', shape=(), dtype=string), Output: tf.Tensor(b'/tmp/tmpxcqf5sg_/8.txt', shape=(), dtype=string)Input: tf.Tensor(b'/tmp/tmpxcqf5sg_/8.txt', shape=(), dtype=string), Output: tf.Tensor(b'/tmp/tmpxcqf5sg_/0.txt', shape=(), dtype=string)Input: tf.Tensor(b'/tmp/tmpxcqf5sg_/2.txt', shape=(), dtype=string), Output: tf.Tensor(b'/tmp/tmpxcqf5sg_/4.txt', shape=(), dtype=string)Input: tf.Tensor(b'/tmp/tmpxcqf5sg_/0.txt', shape=(), dtype=string), Output: tf.Tensor(b'/tmp/tmpxcqf5sg_/1.txt', shape=(), dtype=string)Input: tf.Tensor(b'/tmp/tmpxcqf5sg_/4.txt', shape=(), dtype=string), Output: tf.Tensor(b'/tmp/tmpxcqf5sg_/2.txt', shape=(), dtype=string)Input: tf.Tensor(b'/tmp/tmpxcqf5sg_/7.txt', shape=(), dtype=string), Output: tf.Tensor(b'/tmp/tmpxcqf5sg_/7.txt', shape=(), dtype=string)Input: tf.Tensor(b'/tmp/tmpxcqf5sg_/1.txt', shape=(), dtype=string), Output: tf.Tensor(b'/tmp/tmpxcqf5sg_/3.txt', shape=(), dtype=string)Input: tf.Tensor(b'/tmp/tmpxcqf5sg_/3.txt', shape=(), dtype=string), Output: tf.Tensor(b'/tmp/tmpxcqf5sg_/9.txt', shape=(), dtype=string)Input: tf.Tensor(b'/tmp/tmpxcqf5sg_/9.txt', shape=(), dtype=string), Output: tf.Tensor(b'/tmp/tmpxcqf5sg_/6.txt', shape=(), dtype=string)Input: tf.Tensor(b'/tmp/tmpxcqf5sg_/6.txt', shape=(), dtype=string), Output: tf.Tensor(b'/tmp/tmpxcqf5sg_/5.txt', shape=(), dtype=string)```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
55565,1,0,82,0,0,mehran66,0,"title:tf.keras.metrics.MeanIoU outcome is not improving description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10- TensorFlow installed from (source or binary): source- TensorFlow version (use command below): 2.8.0- Python version: 3.95**Describe the current behavior**I am using tf.keras.metrics.MeanIoU as a metric in a semantic segmentation problem. All of the other metrics (accuracy, precision, recall) are changing during training but MeanIoU does not change. I am using tf.keras.losses.CategoricalCrossentropy(from_logits=False) for my loss function.This issue has been reported in the following two StackOverFlow but no solution provided yet.https://stackoverflow.com/questions/71729853/tf-keras-metrics-meaniou-outcome-is-not-improving/71813168#71813168https://stackoverflow.com/questions/70848143/mean-iou-in-tensorflow-not-updating-resulting-in-correct-value/71813135#71813135Here is an example of metrics during training:Epoch 1/30Epoch 1: val_loss improved from inf to 2.39962, saving model to /content/drive/MyDrive/Colab/test/semantic_segmentation/models_trained/training-min-val_loss.hdf5269/269 - 899s - loss: 0.2277 - accuracy: 0.9108 - IoU: 0.2500 - val_loss: 2.3996 - val_accuracy: 0.9286 - val_IoU: 0.8372 - lr: 0.0010 - 899s/epoch - 3s/stepEpoch 2/30Epoch 2: val_loss did not improve from 2.39962269/269 - 853s - loss: 0.1805 - accuracy: 0.9320 - IoU: 0.2500 -  val_loss: 4.5132 - val_accuracy: 0.9244 - val_IoU: 0.8303 -  lr: 0.0010 - 853s/epoch - 3s/stepEpoch 3/30Epoch 3: val_loss did not improve from 2.39962269/269 - 817s - loss: 0.1672 - accuracy: 0.9380 - IoU: 0.2500 -  val_loss: 3.2384 - val_accuracy: 0.9198 - val_IoU: 0.8092 - lr: 0.0010 - 817s/epoch - 3s/stepEpoch 4/30Epoch 4: val_loss improved from 2.39962 to 0.36882, saving model to /content/drive/MyDrive/Colab/test/semantic_segmentation/models_trained/training-min-val_loss.hdf5269/269 - 855s - loss: 0.1643 - accuracy: 0.9390 - IoU: 0.2500 - val_loss: 0.3688 - val_accuracy: 0.9337 - val_IoU: 0.2600 -  lr: 0.0010 - 855s/epoch - 3s/stepIf I change my one hot encoded labels to a one band maks, use SparseCategoricalAccuracy and the following modified MeanIoU....it works all fine.class SparseMeanIoU(tf.keras.metrics.MeanIoU):  def __init__(self,               y_true=None,               y_pred=None,               num_classes=None,               name=None,               dtype=None):    super(SparseMeanIoU, self).__init__(num_classes = num_classes,name=name, dtype=dtype)  def update_state(self, y_true, y_pred, sample_weight=None):    y_pred = tf.math.argmax(y_pred, axis=-1)    return super().update_state(y_true, y_pred, sample_weight)**Describe the expected behavior**MeanIoU should change during training.- Do you want to contribute a PR? (yes/no): no
"
55545,1,1631,0,0,0,canbakiskan,0,"title:tf.keras.layers.BatchNormalization computes moving variances inconsistently when `fused=True` vs. when `fused=False`  description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.8.0 (gpu)- Python version: 3.10.2- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source): N/A- CUDA/cuDNN version: 11.5/8.3.1- GPU model and memory: 1080 Ti 12GB**Describe the current behavior**`tf.keras.layers.BatchNormalization` computes moving variances inconsistently when `fused=True` vs when `fused=False`. When `fused=False` it computes it with              next_variance=old_variance*momentum+batch_variance*(1-momentum)whereas when `fused=True` it computes it with:              next_variance=old_variance*momentum+batch_variance*(1-momentum)*bessel_coefficient_correction**This is a very minor bug when n is large** as bessel_coefficient_correction tends to 1. But when number of elements per channel is small it becomes noticeable. Below I gave an example code for when elements per channel is 2.**Describe the expected behavior**I would expect `fused=True` and `fused=False` to be consistent.- Do you want to contribute a PR? (yes/no): no**Solution**I think the issue lies [here](https://github.com/tensorflow/tensorflow/blob/b89a2f2a1b761ac68aa808f2bcd847314b15c6c1/tensorflow/core/kernels/fused_batch_norm_op.cc#L213). In which you can see the bessel_coefficient_correction as `rest_size_adjust`. Either that should be removed in fused batch norm or it should be added for non-fused batch norm.**Standalone code to reproduce the issue**```import tensorflow as tffrom math import sqrtwith tf.device(""/cpu:0""):# with tf.device(""/gpu:0""):    var = 5.0    std = sqrt(var)    n=2    inp = tf.random.normal((1,n,1,1))    true_mean = tf.reduce_mean(inp, axis=(0, 1, 2)).numpy()    print(""True mean: "" + str(true_mean))    true_var = tf.math.reduce_variance(inp, axis=(0, 1, 2)).numpy()    print(""True variance: "" + str(true_var))    for momentum in [0.0, 0.25, 0.75, 0.9]:        for fused in [True, False]:            layer = tf.keras.layers.BatchNormalization(                scale=True, center=True, trainable=True, momentum=momentum, fused=fused            )            initial_mean = 0.0            initial_var = 1.0            out = layer(inp, training=True)            print(""\n==========================="")            print(""Momentum: "" + str(momentum) + "" Fused: "" + str(fused))            print(""Moving mean: "" + str(layer.moving_mean.numpy()))            print(                ""Moving mean always correctly computed: ""                + str(initial_mean * momentum + true_mean * (1 - momentum))            )            print(f""Moving var computed with fused={fused}: "" + str(layer.moving_variance.numpy()))            print(                ""Moving var with Bessel correction: ""                + str(initial_var * momentum + n/(n-1) * true_var * (1 - momentum))            )            print(                ""Moving var without Bessel correction: ""                + str(initial_var * momentum + true_var * (1 - momentum))            )            print(""===========================\n"")```See this gist: https://gist.github.com/canbakiskan/9a1fe01a218277539f9602081719a1c3
"
55522,0,0,2,0,0,BenjaminWegener,0,"title:tflite gpu_delegate ""undefined symbol: glFenceSync"" description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 22.04 arm64- TensorFlow installed from (source or binary): source- TensorFlow version (use command below): 2.8.0- Python version: 3.10- Bazel version (if compiling from source): - GCC/Compiler version (if compiling from source):- CUDA/cuDNN version:- GPU model and memory: Mali G72**Describe the current behavior**i compiled tflite from source to test the gpu / gl delegate in python with`bazel build -s -c opt --copt ""-DEGL_NO_X11"" --copt=""-DMESA_EGL_NO_X11_HEADERS"" tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so`tflite inference and training works without loading the delegate, uses xnnpack delegate for cpuif loading the gpu delegate, i get following error:OSError: /home/linux/Documents/libtensorflowlite_gpu_delegate.so: undefined symbol: glFenceSync**UPDATE: the loading takes places via following code:`delegate = tf.lite.experimental.load_delegate('./libtensorflowlite_gpu_delegate.so')`the error is exactly following:OSError                                   Traceback (most recent call last)/tmp/ipykernel_24874/1769046455.py in <module>----> 1 delegate = tf.lite.experimental.load_delegate('./libtensorflowlite_gpu_delegate.so')      2 model = tf.lite.Interpreter(model_content=tflite_model, experimental_delegates=[delegate])      3       4 model.allocate_tensors()      5 infer = generator_lite.get_signature_runner(""infer"")~/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py in load_delegate(library, options)    173   """"""    174   try:--> 175     delegate = Delegate(library, options)    176   except ValueError as e:    177     raise ValueError('Failed to load delegate from {}\n{}'.format(~/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py in __init__(self, library, options)     81                          'due to missing immediate reference counting.')     82 ---> 83     self._library = ctypes.pydll.LoadLibrary(library)     84     self._library.tflite_plugin_create_delegate.argtypes = [     85         ctypes.POINTER(ctypes.c_char_p),/usr/lib/python3.10/ctypes/__init__.py in LoadLibrary(self, name)    450     451     def LoadLibrary(self, name):--> 452         return self._dlltype(name)    453     454     __class_getitem__ = classmethod(_types.GenericAlias)/usr/lib/python3.10/ctypes/__init__.py in __init__(self, name, mode, handle, use_errno, use_last_error, winmode)    372     373         if handle is None:--> 374             self._handle = _dlopen(self._name, mode)    375         else:    376             self._handle = handle`OSError: ./libtensorflowlite_gpu_delegate.so: undefined symbol: glFenceSync`--> This is on Ubuntu 22.04 arm64--> on Ubuntu 22.04 x64 the Error is:`OSError: ./libtensorflowlite_gpu_delegate.so: undefined symbol: glDeleteBuffers` ****Describe the expected behavior**should work with gpu delegate?**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
55512,0,1188,5,0,0,nitsanh,0,"title:Converting models to TFLite is not deterministic description:**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 11.5.2- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.7.1- Python version: 3.8.8- GPU model and memory: AMD Radeon Pro 5300M 4 GB, Intel UHD Graphics 630 1536 M**Describe the current behavior**I have a keras model that I convert into a tflite model using `tf.lite.TFLiteConverter.from_keras_model`.I then save the converted tflite model. I noticed that the output of the conversion is not bit-exact. That means if I save the same converted tflite model twice, the two files may have different hashes.**Describe the expected behavior**I would expect the output tflite model file to always have the same hash. I need this so I will be able to know if the model changed.**Standalone code to reproduce the issue**This behavior doesn't reproduce every time, but it happens pretty often```import osimport tensorflow as tfdef create_model():    inputs = tf.keras.Input(shape=[5, 5, 3])    x = tf.keras.layers.Conv2D(32, 3, strides=2, padding=""same"")(inputs)    x = tf.keras.layers.BatchNormalization()(x)    model = tf.keras.Model(inputs, x)    return modelmodel = create_model()converter = tf.lite.TFLiteConverter.from_keras_model(model)converter.experimental_new_converter = Trueconverter.optimizations = [tf.lite.Optimize.DEFAULT]converter.target_spec.supported_types = [tf.float16]tflite_model1 = converter.convert()tflite_model2 = converter.convert()with open(""model1.tflite"", ""wb"") as f:    f.write(tflite_model1)with open(""model2.tflite"", ""wb"") as f:    f.write(tflite_model2)print(os.system(""diff model1.tflite model2.tflite""))converter = tf.lite.TFLiteConverter.from_keras_model(model)converter.experimental_new_converter = Trueconverter.optimizations = [tf.lite.Optimize.DEFAULT]converter.target_spec.supported_types = [tf.float16]tflite_model3 = converter.convert()with open(""model3.tflite"", ""wb"") as f:    f.write(tflite_model3)print(os.system(""diff model1.tflite model3.tflite""))```
"
55503,0,1251,99,0,0,w3sip,0,"title:2.7.0: memory leak in TFLite's tflite::Interpreter::Invoke() description:Seeing a memory leak in tflite::Interpreter::Invoke().The leak is observed while running our software, wrapping TFLite 2.7.0 (built from source at that tag), on iOS 15.4 (EDIT: confirmed still leaking with 2.7.1 and 15.4.1), and using CoreML delegate.The following leaks seem to occur roughly with every call to Invoke:80 bytes chunk with the following stack:```class_createInstance		__CFAllocateObject		__NSSetI_new		-[NSSet initWithArray:range:copyItems:]		+[NSSet setWithArray:]		-[MLDictionaryFeatureProvider featureNames]		0x11e19b664		0x11e14b44c		tflite::Subgraph::Invoke()		tflite::Interpreter::Invoke()	```48 bytes chunk with the following stack:```class_createInstance		__CFAllocateObject		__NSSetI_new		-[NSSet initWithArray:range:copyItems:]		+[NSSet setWithArray:]		-[MLDictionaryFeatureProvider featureNames]		0x11e19b664		0x11e14b44c		tflite::Subgraph::Invoke()		tflite::Interpreter::Invoke()```16 bytes chunk with the following stack:```_CFCreateArrayStorage		-[NSDictionary allKeys]		-[MLDictionaryFeatureProvider featureNames]		0x11e19b664		0x11e14b44c		tflite::Subgraph::Invoke()		tflite::Interpreter::Invoke()	```32 bytes chunk with the following stack ```_objc_rootAllocWithZone		objc_alloc_init		-[NSTaggedPointerString UTF8String]		0x11e19afd0		-[MLNeuralNetworkEngine verifyInputs:error:]		-[MLNeuralNetworkEngine evaluateInputs:options:error:]		__62-[MLNeuralNetworkEngine predictionFromFeatures:options:error:]_block_invoke		0x10342e7bc		_dispatch_lane_barrier_sync_invoke_and_complete		-[MLNeuralNetworkEngine predictionFromFeatures:options:error:]		0x11e19b714		0x11e14b44c		tflite::Subgraph::Invoke()		tflite::Interpreter::Invoke()	```Last time I went hunting for memory leaks, we were using 2.5.0, and there had been no leak there.
"
55502,1,0,4,0,0,caliari-italo,0,"title:Using model.predict(X) gives me ""InternalError: Failed copying input tensor from CPU to GPU in order to run _EagerConst: Dst tensor is not initialized."" description:**System information**- OS Platform and Distribution (Linux Ubuntu 20.04):- TensorFlow installed from Docker (tensorflow/tensorflow):- TensorFlow version: 2.8.0- Python version: Python 3.8.10- GPU model and memory: NVIDIA GeForce MX110 2048 MBI have built a neural network model with TensorFlow Probability and when trying to predict values as in `pred_train = model.predict(X_train)` it gives me this error, most of the times `InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.`and sometimes it runs ok without errors.The same data/code was used with different models and works just fine. The problem seems to appear only with models built with TensorFlow Probability 
"
55475,1,665,300,0,0,foxik,0,"title:tf.map_fn on RaggedTensors crash during gradient computation on a GPU description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Colab**- TensorFlow installed from (source or binary): **binary**- TensorFlow version (use command below): **2.8**- Python version: **3.7****Describe the current behavior**When some loss (`tf.losses.SparseCategoricalCrossentropy`, `tf.losses.CategoricalCrossentropy`, `tf.losses.BinaryCrossentropy`, or `tf.losses.MeanSquaredError`) is used on Ragged tensors, which is computed via a `tf.map_fn` on a `RaggedTensor`, that the gradient computation on a GPU crashes with```Node: 'Adam/gradients/zeros_like_2'2 root error(s) found.  (0) INTERNAL:  No unary variant unary_op function found for op ZEROS_LIKE Variant type_name: RaggedTensorVariant for device type: GPU	 [[{{node Adam/gradients/zeros_like_2}}]]	 [[binary_crossentropy/map/while/loop_body_control/_124/_67]]  (1) INTERNAL:  No unary variant unary_op function found for op ZEROS_LIKE Variant type_name: RaggedTensorVariant for device type: GPU	 [[{{node Adam/gradients/zeros_like_2}}]]0 successful operations.0 derived errors ignored. [Op:__inference_train_function_16690]```The computation does not crash on a CPU and it does not crash when `tf.function`s are executed eagerly.Also, if the `tf.map_fn` is circumvented by using the following argument to compile```python  loss=lambda yt, yp: tf.losses.BinaryCrossentropy()(yt. values, yp.values)```it works on GPU without a crash.**Describe the expected behavior**The code does not crash on a GPU.- Do you want to contribute a PR? (yes/no): **no****Standalone code to reproduce the issue**A simple Colab reproducing the error is here: https://colab.research.google.com/drive/1OELAhvpQHhaz3sOYabf4SdBqKlQCjNjs?usp=sharing**Other info / logs**The `map_fn` used is here: https://github.com/keras-team/keras/blob/2db5acf3e3c5904b014cb409d3c514bef44f9640/keras/losses.py#L1408 
"
55474,1,1534,1,0,0,dmho418,0,"title:Using tf.data.Dataset.list_files prints ""unshardable source dataset"" warning description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 LTS- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): v2.8.0-rc1-32-g3f878cff5b6 2.8.0- Python version: 3.8.10**Describe the current behavior**Using distributed strategy and `tf.data.Dataset.list_files` prints a `unshardable source dataset` warning.**Describe the expected behavior**API should detect that the source dataset is files and can be sharded. Reference: https://www.tensorflow.org/tutorials/distribute/input**Standalone code to reproduce the issue**```pythonstrategy = tf.distribute.MirroredStrategy()ds = tf.data.Dataset.list_files("".*"", shuffle=False).map(lambda x: (tf.strings.length(x), tf.strings.length(x)))with strategy.scope():  dummy_model = tf.keras.Sequential()  dummy_model.add(tf.keras.layers.Dense(1, input_shape=(1,)))  dummy_model.compile(loss=""mse"")dummy_model.fit(ds.batch(4))``````2022-04-03 16:22:04.058728: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: ""TensorSliceDataset/_1""op: ""TensorSliceDataset""input: ""Placeholder/_0""attr {  key: ""Toutput_types""  value {    list {      type: DT_STRING    }  }}attr {  key: ""_cardinality""  value {    i: 1  }}attr {  key: ""is_files""  value {    b: false  }}attr {  key: ""metadata""  value {    s: ""\n\026TensorSliceDataset:350""  }}attr {  key: ""output_shapes""  value {    list {      shape {      }    }  }}experimental_type {  type_id: TFT_PRODUCT  args {    type_id: TFT_DATASET    args {      type_id: TFT_PRODUCT      args {        type_id: TFT_TENSOR        args {          type_id: TFT_STRING        }      }    }  }  args {    type_id: TFT_DATASET    args {      type_id: TFT_PRODUCT      args {        type_id: TFT_TENSOR        args {          type_id: TFT_STRING        }      }    }  }}```
"
55455,1,311,8,0,0,David-Mao,0,"title:GRU performance severely degraded inside tf.function with Apple m1 chip description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Mac OS Monterey 12.3, Metal device set to: Apple M1 Pro- TensorFlow installed from (source or binary):binary- TensorFlow version:tensorflow-deps           2.7.0                tensorflow-macos          2.8.0              tensorflow-metal          0.4.0 - Python version:3.9.12- GPU model and memory: Apple M1 Pro**Describe the current behavior**I run this simple code with a GRU layer with a `tf.function` decorator:```import tensorflow as tffrom time import timea = tf.random.truncated_normal([4, 4, 4])layer = tf.keras.layers.GRU(4) @tf.functiondef f(a):    return layer(a)start = time()for _ in range(1000):    with tf.GradientTape() as tape:        b = f(a)print(str(time() - start), ""seconds"")```its much slower (~5-10x times) than running in the eager mode. However, this bug only shows up for recurrent layers. When using Dense, the `tf.function` mode is faster than the eager mode as expected. The issue also disappeared outside `tf.GradientTape()`.I only encountered this problem in my Apple Macbook Pro with M1 chip. I tried it on a linux machine and it's ok.**Describe the expected behavior**`tf.function` should be faster (at least not several times slower) than the eager mode.**Standalone code to reproduce the issue**It cannot be reproduced on a linux machine, so no Colab notebook is available.**Other info / logs** FYI the code above runs with the warning message as follows:> 2022-03-31 14:40:34.462151: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz2022-03-31 14:40:34.463604: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.2022-03-31 14:40:34.480917: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:828] function_optimizer failed: INVALID_ARGUMENT: Input 0 of node gru_partitionedcall_10_RetVal was passed float from gru/PartitionedCall:12 incompatible with expected variant.2022-03-31 14:40:34.487243: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:828] tfg_optimizer{} failed: INVALID_ARGUMENT: Input 0 of node gru_partitionedcall_10_RetVal was passed float from gru/PartitionedCall:12 incompatible with expected variant.	when importing GraphDef to MLIR module in GrapplerHook2022-03-31 14:40:34.488903: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:828] function_optimizer failed: INVALID_ARGUMENT: Input 0 of node gru_partitionedcall_10_RetVal was passed float from gru/PartitionedCall:12 incompatible with expected variant.2022-03-31 14:40:34.494395: W tensorflow/core/common_runtime/process_function_library_runtime.cc:932] Ignoring multi-device function optimization failure: INVALID_ARGUMENT: Input 0 of node gru_partitionedcall_10_RetVal was passed float from gru/PartitionedCall:12 incompatible with expected variant.2022-03-31 14:40:34.508855: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.
"
55454,1,2065,0,1,1,VihGrimm,0,"title:Keras cant load model with tf.where when dtype is float64 description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.8 (also encountered on 2.4)- Python version: 3.7.5- Bazel version (if compiling from source): -- GCC/Compiler version (if compiling from source): -- CUDA/cuDNN version: -- GPU model and memory: -**Describe the current behavior**Loading a model that utilizes `tf.where` to replace a variable with some constant fails when all involved tensors are `float64` tensors.Possibly connected to [this issue](https://github.com/tensorflow/tensorflow/issues/47161).**Describe the expected behavior**The model loads successfuly.**Standalone code to reproduce the issue**```import tensorflow as tffrom tensorflow import kerasdtype = 'float64'some_input = keras.Input(shape=(1), dtype=dtype)some_value = keras.layers.Dense(4, activation=""relu"", dtype=dtype)(some_input)some_value = keras.layers.Dense(2, dtype=dtype)(some_value)other_value = tf.constant([0.1], dtype=dtype)mask = tf.equal([0, 1], 1)replaced_value = tf.where(mask, x=other_value, y=some_value)model = tf.keras.Model(inputs=[some_input], outputs=[replaced_value])model.save('my_test_model')loaded_model = tf.keras.models.load_model('my_test_model')```However, the model can be loaded properly when the arguments in the call to `tf.where` are switched, like this `tf.where(mask, x=some_value, y=other_value)`Setting `tf.keras.backend.set_floatx('float64')` does not prevent the error from occuring.See this [Colab notebook](https://colab.research.google.com/drive/1mii4nbvHUJHaqbG60bEm88-ttdB7AltJ).**Other info / logs**```---------------------------------------------------------------------------TypeError                                 Traceback (most recent call last)[<ipython-input-11-a4651214ca80>](https://localhost:8080/#) in <module>()----> 1 loaded_model = tf.keras.models.load_model('my_test_model')[/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py](https://localhost:8080/#) in error_handler(*args, **kwargs)     65     except Exception as e:  # pylint: disable=broad-except     66       filtered_tb = _process_traceback_frames(e.__traceback__)---> 67       raise e.with_traceback(filtered_tb) from None     68     finally:     69       del filtered_tb[/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py](https://localhost:8080/#) in _apply_op_helper(op_type_name, name, **keywords)    545     546             raise TypeError(--> 547                 f""{prefix} type ""    548                 f""{dtypes.as_dtype(attrs[input_arg.type_attr]).name} of ""    549                 f""argument '{inferred_from[input_arg.type_attr]}'."")TypeError: Exception encountered when calling layer ""tf.where_3"" (type TFOpLambda).Input 'e' of 'SelectV2' Op has type float64 that does not match type float32 of argument 't'.Call arguments received:  闂?condition=['tf.Tensor(shape=(), dtype=bool)', 'tf.Tensor(shape=(), dtype=bool)']  闂?x=['0.1']  闂?y=tf.Tensor(shape=(None, 2), dtype=float64)  闂?name=None```
"
55438,1,569,31,0,0,darrahts,0,"title:model is loaded as Loader._recreate_base_user_object.<locals>._UserObject description:python == 3.8tensorflow (gpu) == 'v2.8.0-rc1-32-g3f878cff5b6'keras == 2.8windows 10I am saving the model as both an h5 and keras format with```model.save('model.h5')model.save(model_location)type(model)   ``````keras.engine.functional.Functional```Then, I am loading the model with ```model = keras.models.load_model(model_location)# or model = tf.saved_model.load(model_location) # results in the sametype(model)``````<class 'tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject'>```There is something up with the loading process that results in the following error when calling `model.predict(X)` ```AttributeError: '_UserObject' object has no attribute 'predict'``````tf.saved_model.contains_saved_model(model_location)true```However, loading the same model saved as an h5 file ```model = keras.models.load_model('model.h5')type(model)``````keras.engine.functional.Functional```works. Thanks in advance!
"
55437,0,0,269,0,0,elfringham,0,"title:runpath not including //tensorflow/python for _dtensor_device.so description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): manylinux2014- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a- TensorFlow installed from (source or binary): source- TensorFlow version (use command below): git HEAD- Python version: 3.7.x- Bazel version (if compiling from source): 5.0.0- GCC/Compiler version (if compiling from source): 10.2.1- CUDA/cuDNN version: n/a- GPU model and memory: n/aYou can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**auditwheel fails with message about cannot find _pywrap_tensorflow_internal.so**Describe the expected behavior**auditwheel passes**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.auditwheel repair -w ../wheels/ tensorflow-pkg/tensorflow_aarch64-2.9.0-cp38-cp38-linux_aarch64.wh**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.Issue introduced with https://github.com/tensorflow/tensorflow/commit/fd94c2628c344a288ee194bcdbfe36239f5d4cdd
"
55435,1,0,1,0,0,szleb,0,"title:Loss names are not equal to the outputs dictionary keys of a model description:**System information**- Have I written custom code: yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10.0.18363- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.8.0- Python version: 3.8.10- CUDA/cuDNN version: none- GPU model and memory: none**Current behavior**The names for the single losses of a multi output model correspond to the output layer names instead to the output names provided by the keys in the outputs dictionary.I am not sure if this is the desired behavior.**Expected behavior**I would expect that the names of the losses are the same as the keys in the dictionary passed to the outputs argument when creating the model.**Standalone code to reproduce the issue**    import numpy as np    from tensorflow.keras.layers import Dense, Input    from tensorflow.keras.models import Model        first_input = Input(1)    x = Dense(100)(first_input)    x = Dense(1)(x)        second_input = Input(1)    x2 = Dense(100)(second_input)    x2 = Dense(1)(x2)        model = Model(        inputs={""first_input"": first_input, ""second_input"": second_input},        outputs={""first_output"": x, ""second_output"": x2},    )        first_input_data = np.random.random(100)    second_input_data = np.random.random(100)    first_output_data = np.random.random(100)    second_output_data = np.random.random(100)        model.compile(loss=""mse"")        model.fit(        x={""first_input"": first_input_data, ""second_input"": second_input_data},        y={""first_output"": first_output_data, ""second_output"": second_output_data},        batch_size=2,        epochs=2,    )**Other info / logs**The loss names are visible in the following logging, when running the code example:    Epoch 1/2    50/50 [==============================] - 0s 673us/step - loss: 0.2440 - dense_1_loss: 0.1308 - dense_3_loss: 0.1131    Epoch 2/2    50/50 [==============================] - 0s 592us/step - loss: 0.1934 - dense_1_loss: 0.0973 - dense_3_loss: 0.0961The problem is that I cannot see which loss belongs to which single output. Of course, my real use case is much more complicated, so that I cannot just name the output layers correctly.
"
55390,0,1062,15,0,0,zhaozheng09,0,"title:tanh(float(7~8)) = 1.0000001 in XLA description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux or mac - Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no- TensorFlow installed from (source or binary): source - TensorFlow version (use command below): 2.8 or 1.15- Python version: 3.9 or 2.7- Bazel version (if compiling from source): bazel release 5.0.0-pre.20211011.2- GCC/Compiler version (if compiling from source): Apple clang version 13.0.0 (clang-1300.0.29.3) - CUDA/cuDNN version: no- GPU model and memory: no You can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`    zsh: command not found: v1.12.1-68911-gea661077441**Describe the current behavior**The return value of tanh function is  1.0000001, but tanh can not return more than 1.**Describe the expected behavior**The return value of tanh function is 1 or less than 1.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): yes- Briefly describe your candidate solution(if contributing):There is a description in the xla comment that the xla implementation of the tanh function is copied from eigen3.(tensorflow/compiler/xla/service/llvm_ir/math_ops.h:25), but I compare the implementation of the tanh function within xla and eigen3, they not have same implementation .so I copy the implementation of tanh function from eigen3 to tensorflow ....**Standalone code to reproduce the issue**```import osos.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit --tf_xla_min_cluster_size=0'import tensorflow as tftf.compat.v1.disable_eager_execution()#with tf.compat.v1.device('gpu'):ctr_y = tf.compat.v1.random.uniform([1], minval=8, maxval=9, dtype=tf.compat.v1.float32)ctr_pred_ori = tf.compat.v1.tanh(ctr_y)session_config = tf.compat.v1.ConfigProto(allow_soft_placement=True, log_device_placement=True)session_config.graph_options.rewrite_options.disable_meta_optimizer=Truewith tf.compat.v1.Session(config=session_config) as sess:  while  True:    ret = sess.run([ctr_pred_ori])[0]    if ret > 1.0:      print(ret);```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.``` 212 [array([0.99999964], dtype=float32)]13702 [array([0.99999976], dtype=float32)] 621 [array([0.9999997], dtype=float32)]17916 [array([0.9999998], dtype=float32)]21312 [array([0.99999994], dtype=float32)]48624 [array([0.9999999], dtype=float32)]8097 [array([1.0000001], dtype=float32)]   5 [array([1.0000002], dtype=float32)]42322 [array([1.], dtype=float32)]```tanh return 1.0000001 8097 times
"
55389,1,0,275,0,0,luckysmg,0,"title:TensorflowLite Undefined symbol: _TfLiteTensorCopy description:**System information**Xcode13.3, complie app for arm64 iPhoneThe step I following is here:https://www.tensorflow.org/lite/guide/ops_select#iosDemo code is here:[iOSDemo 2.zip](https://github.com/tensorflow/tensorflow/files/8356734/iOSDemo.2.zip)**Step to reproduce issue**1. Download the zip,and `cd` to the project root folder2. Run `pod update` 3. Compile project.4. Get the error and compile failed.<img width=""352"" alt=""QQ20220327-110056@2x"" src=""https://user-images.githubusercontent.com/49340347/160264810-524cfabc-5cfd-48bd-a93c-9ff9966374a6.png"">
"
55364,0,1774,0,0,0,tonysung,0,"title:benchmark_model: Could not create Hexagon delegate: platform may not support delegate or required libraries are missing description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 11- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung S21- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): nightly**Describe the current behavior*** Follow instructions in https://www.tensorflow.org/lite/performance/measurement#native_benchmark_binary and https://www.tensorflow.org/lite/performance/hexagon_delegate to run TFLite `benchmark_model` with `use_hexagon=true`.* `adb push` contents of these to `/data/local/tmp`:  * [android_aarch64_benchmark_model](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_aarch64_benchmark_model)  * [libhexagon_interface.so ](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_aarch64_libhexagon_interface.so)  * [hexagon_nn_skel v1.20.0.1](https://storage.cloud.google.com/download.tensorflow.org/tflite/hexagon_nn_skel_v1.20.0.1.run)  * [mobilenet_v2_1.0_224_quant](https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz)* Verify with `adb shell ls /data/local/tmp`:   ```  android_aarch64_benchmark_model  libhexagon_interface.so  libhexagon_nn_skel.so  libhexagon_nn_skel_v65.so  libhexagon_nn_skel_v66.so  mobilenet_v2_1.0_224.tflite  mobilenet_v2_1.0_224_quant.tflite  ```* Run `adb shell /data/local/tmp/android_aarch64_benchmark_model --graph=/data/local/tmp/mobilenet_v2_1.0_224_quant.tflite --use_hexagon=true --hexagon_lib_path=/data/local/tmp`* Hexagon delegate fail to create:  ```  STARTING!  Log parameter values verbosely: [0]  Graph: [/data/local/tmp/mobilenet_v2_1.0_224_quant.tflite]  Use Hexagon: [1]  Hexagon lib path: [/data/local/tmp]  Loaded model /data/local/tmp/mobilenet_v2_1.0_224_quant.tflite  INFO: Initialized TensorFlow Lite runtime.  WARNING: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.  INFO: Hexagon Delegate is not supported.    loaded libcdsprpc.so  Could not create Hexagon delegate: platform may not support delegate or required libraries are missing  The input model file size (MB): 3.57776  Initialized session in 51.739ms.  Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.  count=66 first=23605 curr=6778 min=6765 max=23605 avg=7583.89 std=2197    Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.  count=82 first=6870 curr=850063 min=6765 max=850063 avg=17097.5 std=92551    Inference timings in us: Init: 51739, First inference: 23605, Warmup (avg): 7583.89, Inference (avg): 17097.5  Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.  Memory footprint delta from the start of the tool (MB): init=4.73828 overall=8.75  ``` **Describe the expected behavior**`benchmark_model` runs with hexagon delegate enabled.
"
55349,1,2403,31,1,0,DanBmh,0,"title:TF-lite conversion of complex abs layer not working with integer quantization with fallbacks description:I'm trying to optimize a speech-to-text Conformer model for tflite usage. Quantization of the model is working for _default_ optimization mode, but not when a _representative dataset_ is used. In this case it fails in inference with: `type != kTfLiteFloat32 (INT8 != FLOAT32) Node number 46 (COMPLEX_ABS) failed to prepare`Shouldn't the model use the _float32_ fallback in this case, since I'm not enforcing _int8_ operators?### 1. System information- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20:04- TensorFlow installation (pip package or built from source): Nvidia-Docker- TensorFlow library (version, if pip package or github SHA, if built from source): 2.7### 2. Failure after conversion```Transcribing the audio ...Label:             they were subjected to constant surveillance and periodic searchesTraceback (most recent call last):  File ""/Scribosermo/exporting/testing_file.py"", line 161, in <module>    main()  File ""/Scribosermo/exporting/testing_file.py"", line 155, in main    test_tflite(checkpoint_tflite_quant)  File ""/Scribosermo/exporting/testing_file.py"", line 118, in test_tflite    prediction = predict(interpreter, audio)  File ""/Scribosermo/exporting/testing_file.py"", line 92, in predict    interpreter.invoke()  File ""/usr/local/lib/python3.8/dist-packages/tflite_runtime/interpreter.py"", line 923, in invoke    self._interpreter.Invoke()RuntimeError: /workspace/tensorflow/lite/kernels/complex_support.cc:43 output-> \  type != kTfLiteFloat32 (INT8 != FLOAT32)Node number 46 (COMPLEX_ABS) failed to prepare.```### 3. Code snippets```python3# Conversion settingsdef export_tflite(model, save_path, optimize):    converter = tf.lite.TFLiteConverter.from_keras_model(model)    if optimize:        converter.optimizations = [tf.lite.Optimize.DEFAULT]        converter.representative_dataset = representative_dataset    tflite_model = converter.convert()    with open(save_path, ""wb+"") as file:        file.write(tflite_model)# Spectrogram definitiondef audio_to_spect(self, audio):        """"""Calculate the spectrogram""""""        # Pytorch uses a slightly different spectrogram calculation which is matched to librosa        # unlike the default tensorflow implementation        n_fft = 512        nbatch = tf.shape(audio)[0]        # Add center padding        signal = tf.reshape(audio, [nbatch, -1])        pad_amount = int(self.audio_window_samples // 2)        signal = tf.pad(signal, [[0, 0], [pad_amount, pad_amount]], ""REFLECT"")        signal = tf.reshape(signal, [nbatch, 1, -1])        # Calculate short-time Fourier transforms with a differnt windowing approach        f = tf.signal.frame(            signal, self.audio_window_samples, self.audio_step_samples, pad_end=False        )        w = tf.signal.hann_window(self.audio_window_samples, periodic=False)        stfts = tf.signal.rfft(f * w, fft_length=[n_fft])        # Obtain the magnitude of the STFT.        spectrogram = tf.abs(stfts) ** 2        spectrogram = tf.squeeze(spectrogram, axis=1)        return spectrogram```<br>Related issue: https://github.com/tensorflow/tensorflow/issues/53393#issuecomment-1009703703 (was closed without solution)
"
55343,1,1092,58,0,0,jayagami,0,"title:Normalization and BatchNormalization layer does not rescale or normalize inputs, am I missing anything? description:**System information**colab with tf-2.8[old-tf_env.txt](https://github.com/tensorflow/tensorflow/files/8330439/old-tf_env.txt)---**Describe the current behavior**Normalization and Batch Normalization layer does not rescale or normalize inputs as excepted, the inputs barely not changing.---**Standalone code to reproduce the issue**## Tensorflow:###  Normalization layer```pythontfdata = np.array([[1,2,3,4,5]],dtype=float)print(""tfdata:"" ,tfdata)print(""avg: "", np.average(tfdata))print(""var: "", np.var(tfdata))```>tfdata: [[1. 2. 3. 4. 5.]]avg:  3.0var:  2.0```pythonnormalization_layer = tf.keras.layers.Normalization(axis=-1)normalized_data = normalization_layer(tfdata)print(normalized_data.numpy())print(""normalized avg: "", np.average(normalized_data.numpy()))print(""normalized var: "", np.var(normalized_data.numpy()))```**output:** >[[1. 2. 3. 4. 5.]]normalized avg:  3.0normalized var:  2.0**This output is unchanged**### BatchNormalization layer```pythonbatch_normalization_layer = tf.keras.layers.BatchNormalization(axis=-1,scale=True,center=True)batch_normalized_data = batch_normalization_layer(tfdata)print(batch_normalized_data.numpy())print(""batch normalized avg: "", np.average(batch_normalized_data.numpy()))print(""batch normalized var: "", np.var(batch_normalized_data.numpy()))```**output:**>[[0.9995004 1.9990008 2.9985013 3.9980016 4.997502 ]]batch normalized avg:  2.9985013batch normalized var:  1.9980018---**This output still unchanged!**## Torch:### LayerNorm```pythontdata = torch.tensor([1,2,3,4,5],dtype=torch.float32)normalization_layer=torch.nn.LayerNorm(normalized_shape=[5])normalized_data = normalization_layer(tdata).detach().numpy()print(normalized_data)print(""normalized avg: "", np.average(normalized_data))print(""normalized var: "", np.var(normalized_data))```**output**>[-1.4142101  -0.70710504 -0.00000006  0.7071049   1.4142098 ]normalized avg:  -4.7683717e-08normalized var:  0.9999949normalized av closed to 0, and normalized var close to 1, correct answer.and BatchNormalization ... all the same
"
55340,0,543,299,0,0,bhack,1,"title:Autovectorization fail with tf.vectorized_map and range description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Colab- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: - TensorFlow installed from (source or binary):Binary- TensorFlow version (use command below):Nightly- Python version:Colab- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version:- GPU model and memory:You can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**```pythonimport tensorflow as tftf.autograph.set_verbosity(10)def outer_product(a):  for i in tf.range(10):    pass  return tf.tensordot(a, a, 0)@tf.functiondef vectorized(a):  return tf.vectorized_map(outer_product, a)  batch_size = 100a = tf.ones((batch_size, 32, 32))c = vectorized(a)assert c.shape == (batch_size, 32, 32, 32, 32)``````OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.```**Describe the expected behavior****[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
55323,1,84,78,0,0,NatanBagrov,0,"title:`tf.image.non_max_suppression_padded` pads indices with a valid index value (0) description:`tf.image.non_max_suppression` returns a vector of surviving indices of dynamic shape.To hold static shapes, you use `tf.image.non_max_suppression_padded` which returns a vector of surviving indices (and a scalar indicates the amount).This vector however, is padded with zeros, to match the input shape (dimension 0).Since [0] is a valid index in the input, I find it weird and confusing -- does the first zero encountered in the surviving indices is a survivor or just a padding?For instance, I cannot use `>=0` since that will take all indices, and I cannot use `>0` since it will skip index 0 in case it does survive```pythonreturn tf.where(tf.expand_dims(indices, axis=1) >= 0, x, tf.zeros_like(x))```I understand that this is solvable using the second returned value which indicates the amount of survivors, but I wish the vector itself would be sufficient.A current workaround is prepending a dummy box with confidence=0 just to make sure that valid indices start from [1].CoreML handles this issue by padding -1s -- which makes more sense, I guess that might be helpful here as well.TF 2.8.0
"
55309,1,0,4,0,0,caliari-italo,0,"title:Node: 'model/conv1d/Conv1D' DNN library is not found. description:**System information**- OS: Linux Ubuntu 20.04:- TensorFlow installed from pip- TensorFlow version: v2.8.0-rc1-32-g3f878cff5b6 2.8.0- Python version: Python 3.8.10- CUDA/cuDNN version: NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6   - GPU model : [GeForce GTX 1650] Hello, everyone.I'm trying to run a  convolutional neural network on tensorflow but I'm receiving the current error:> 2022-03-21 10:40:20.665473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero> 2022-03-21 10:40:20.687599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero> 2022-03-21 10:40:20.687804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero> 2022-03-21 10:40:22.544819: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.> 2022-03-21 10:40:22.545204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero> 2022-03-21 10:40:22.545428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero> 2022-03-21 10:40:22.545579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero> 2022-03-21 10:40:22.815601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero> 2022-03-21 10:40:22.815824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero> 2022-03-21 10:40:22.815975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero> 2022-03-21 10:40:22.816100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2607 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5> Epoch 1/999999> 2022-03-21 10:40:23.481069: E tensorflow/stream_executor/cuda/cuda_dnn.cc:361] Loaded runtime CuDNN library: 8.0.4 but source was compiled with: 8.1.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.> 2022-03-21 10:40:23.481615: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at conv_ops.cc:1120 : UNIMPLEMENTED: DNN library is not found.> Traceback (most recent call last):>   File ""CNN_regression.py"", line 367, in <module>>     results = pd.concat([results,main(mode)], ignore_index=True)>   File ""CNN_regression.py"", line 124, in main>     history = model.fit(X_train, Y_train,>   File ""/home/italocaliari/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler>     raise e.with_traceback(filtered_tb) from None>   File ""/home/italocaliari/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 54, in quick_execute>     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,> tensorflow.python.framework.errors_impl.UnimplementedError: Graph execution error:> > Detected at node 'model/conv1d/Conv1D' defined at (most recent call last):>     File ""CNN_regression.py"", line 367, in <module>>       results = pd.concat([results,main(mode)], ignore_index=True)>     File ""CNN_regression.py"", line 124, in main>       history = model.fit(X_train, Y_train,>     File ""/home/italocaliari/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 64, in error_handler>       return fn(*args, **kwargs)>     File ""/home/italocaliari/.local/lib/python3.8/site-packages/keras/engine/training.py"", line 1384, in fit>       tmp_logs = self.train_function(iterator)>     File ""/home/italocaliari/.local/lib/python3.8/site-packages/keras/engine/training.py"", line 1021, in train_function>       return step_function(self, iterator)>     File ""/home/italocaliari/.local/lib/python3.8/site-packages/keras/engine/training.py"", line 1010, in step_function>       outputs = model.distribute_strategy.run(run_step, args=(data,))>     File ""/home/italocaliari/.local/lib/python3.8/site-packages/keras/engine/training.py"", line 1000, in run_step>       outputs = model.train_step(data)>     File ""/home/italocaliari/.local/lib/python3.8/site-packages/keras/engine/training.py"", line 859, in train_step>       y_pred = self(x, training=True)>     File ""/home/italocaliari/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 64, in error_handler>       return fn(*args, **kwargs)>     File ""/home/italocaliari/.local/lib/python3.8/site-packages/keras/engine/base_layer.py"", line 1096, in __call__>       outputs = call_fn(inputs, *args, **kwargs)>     File ""/home/italocaliari/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 92, in error_handler>       return fn(*args, **kwargs)>     File ""/home/italocaliari/.local/lib/python3.8/site-packages/keras/engine/functional.py"", line 451, in call>       return self._run_internal_graph(>     File ""/home/italocaliari/.local/lib/python3.8/site-packages/keras/engine/functional.py"", line 589, in _run_internal_graph>       outputs = node.layer(*args, **kwargs)>     File ""/home/italocaliari/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 64, in error_handler>       return fn(*args, **kwargs)>     File ""/home/italocaliari/.local/lib/python3.8/site-packages/keras/engine/base_layer.py"", line 1096, in __call__>       outputs = call_fn(inputs, *args, **kwargs)>     File ""/home/italocaliari/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 92, in error_handler>       return fn(*args, **kwargs)>     File ""/home/italocaliari/.local/lib/python3.8/site-packages/keras/layers/convolutional.py"", line 248, in call>       outputs = self.convolution_op(inputs, self.kernel)>     File ""/home/italocaliari/.local/lib/python3.8/site-packages/keras/layers/convolutional.py"", line 233, in convolution_op>       return tf.nn.convolution(> Node: 'model/conv1d/Conv1D'> DNN library is not found.> 	 [[{{node model/conv1d/Conv1D}}]] [Op:__inference_train_function_660]I'm running the exact same code with another computer with a Nvidia MX110 and is working just fine. I think this might be a configuration/installation issue related to:>  ""Loaded runtime CuDNN library: 8.0.4 but source was compiled with: 8.1.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration."" I could not find any solution to this problem.Thank you all in advance.
"
55307,1,4450,26,0,0,ZeroExistence,0,"title:[TPU] TPUClusterResolver Can't Resolve TPU Metadata When Using Regional GKE Cluster description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): tensorflow/tensorflow:2.7.1, tensorflow/tensorflow:2.8.0- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.7.1, 2.8.0- Python version: 3.8- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version:- GPU model and memory: cloud-tpus.google.com/preemptible-v2: ""8""**Describe the current behavior**We have a GKE regional cluster with TPU support. When I tried to create a ResNet-RS using the official TPU guide, I encountered the issue with TPUClusterResolver.```Traceback (most recent call last):  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/client/client.py"", line 259, in _fetch_cloud_tpu_metadata    r = service.projects().locations().nodes().get(name=self._full_name())  File ""/usr/local/lib/python3.8/dist-packages/googleapiclient/discovery.py"", line 1044, in method    raise TypeError(TypeError: Parameter ""name"" value ""projects/[PROJECT-CODE]/locations/us-central1-b/nodes/us-central1-b/[TPU-RESOURCE]"" does not match the pattern ""^projects/[^/]+/locations/[^/]+/nodes/[^/]+$""During handling of the above exception, another exception occurred:Traceback (most recent call last):  File ""/root/models/official/vision/beta/train.py"", line 70, in <module>    app.run(main)  File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run    _run_main(main, args)  File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main    sys.exit(main(argv))  File ""/root/models/official/vision/beta/train.py"", line 50, in main    distribution_strategy = distribute_utils.get_distribution_strategy(  File ""/root/models/official/common/distribute_utils.py"", line 151, in get_distribution_strategy    cluster_resolver = tpu_initialize(tpu_address)  File ""/root/models/official/common/distribute_utils.py"", line 88, in tpu_initialize    tf.config.experimental_connect_to_cluster(cluster_resolver)  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/remote.py"", line 141, in connect_to_cluster    if cluster_spec_or_resolver.master() in _LOCAL_MASTERS:  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 256, in master    cluster_spec = self.cluster_spec()  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 330, in cluster_spec    network_endpoints = self._cloud_tpu_client.network_endpoints()  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/client/client.py"", line 346, in network_endpoints    response = self._fetch_cloud_tpu_metadata()  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/client/client.py"", line 262, in _fetch_cloud_tpu_metadata    raise ValueError(""Could not lookup TPU metadata from name '%s'. Please ""ValueError: Could not lookup TPU metadata from name 'us-central1-b/[TPU-RESOURCE]'. Please doublecheck the tpu argument in the TPUClusterResolver constructor. Exception: Parameter ""name"" value ""projects/[PROJECT-CODE]/locations/us-central1-b/nodes/us-central1-b/[TPU-RESOURCE]"" does not match the pattern ""^projects/[^/]+/locations/[^/]+/nodes/[^/]+$""```**Describe the expected behavior**The ResNet-RS job should run in TPU core without any issues.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): No- Briefly describe your candidate solution(if contributing): N/A**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.https://cloud.google.com/tpu/docs/tutorials/resnet-rs-2.xRun this guide in a regional GKE cluster with TPU support.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.```apiVersion: v1kind: Podmetadata:    endpoints.cloud-tpus.google.com/gke: grpc://10.0.0.106:8470    name.cloud-tpus.google.com/gke: us-central1-b/[TPU-RESOURCE]  name: gkespec:  containers:  - command:    - bash    - -c    - export PYTHONPATH=$PYTHONPATH:/root/models && cd /root && apt-get update &&      apt-get install -y git && git clone https://github.com/tensorflow/models.git      && pip3 install tf-models-nightly tensorflow-text-nightly && pip3 install -r      models/official/requirements.txt && python3 /root/models/official/vision/beta/train.py      --experiment=resnet_rs_imagenet --mode=train_and_eval --model_dir=$MODEL_DIR      --tpu=$TPU_NAME --config_file=/root/models/official/vision/beta/configs/experiments/image_classification/imagenet_resnetrs50_i160.yaml      --params_override=""task.train_data.input_path=$IMAGENET_DIR/train*, task.validation_data.input_path=$IMAGENET_DIR/valid*,      trainer.train_steps=100"" && sleep 900    env:    - name: IMAGENET_DIR      value: gs://cloud-tpu-test-datasets/fake_imagenet    - name: STORAGE_BUCKET      value: gs://xxxxx    - name: MODEL_DIR      value: gs://xxxxx/resnet-rs-2x_v6    - name: GOOGLE_APPLICATION_CREDENTIALS      value: /var/secrets/google/key.json    - name: KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS      valueFrom:        fieldRef:          apiVersion: v1          fieldPath: metadata.annotations['endpoints.cloud-tpus.google.com/gke-dgtest-tpu-resnetrs6']    - name: TPU_NAME      valueFrom:        fieldRef:          apiVersion: v1          fieldPath: metadata.annotations['name.cloud-tpus.google.com/gke-dgtest-tpu-resnetrs6']    image: tensorflow/tensorflow:2.8.0```
"
55305,0,239,39,0,0,kanghj,0,"title:`tf.strings.unsorted_segment_join` crashes unexpectedly when `num_segments` is negative description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below):2.8.0- Python version: 3.7.12- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source): N/A- CUDA/cuDNN version: N/A- GPU model and memory: N/A**Describe the current behavior**Given the following code snippet:```import tensorflow as tftry:  tf.strings.unsorted_segment_join(inputs=['123'],segment_ids=[0],num_segments=-1)except Exception:  print('an exception should be thrown, but unsorted_segment_join crashes')print('Not reached')```the call to `tf.strings.unsorted_segment_join`  causes a crash.**Describe the expected behavior**Since `num_segments` is negative, an exception should be thrown (perhaps an `InvalidArgumentError` or `ValueError`. The code should not crash.**Standalone code to reproduce the issue**The code snippet above should reproduce the issue. The following colab notebook (running the notebook should crash the session) demonstrates the issue: https://colab.research.google.com/drive/1zoYVGQXY9MlYgbtW4N3lC51yC8wQh5J2?usp=sharing
"
55298,1,0,0,0,0,IoanaKitsune,0,"title:org.tensorflow:tensorflow-lite-support:0.3.1 has missing files description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary):- TensorFlow version (use command below):- Python version:- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version:- GPU model and memory:You can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**When trying to deploy a TFLite model in Android Studio, I received the error ClassNotFound: about org.tensorflow.lite.support.common.SupportPreconditions.**Describe the expected behavior**No error.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.modelPath = ""local_path_of_model""AudioClassifier model = AudioClassifier.createFromFile(requireContext(), modelPath);TensorAudio tensor = model.createInputTensorAudio();AudioRecord record = model.createAudioRecord();**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.When I downgraded to org.tensorflow:tensorflow-lite-support:0.3.0, I didn't receive the error anymore.
"
55292,0,101,39,0,0,kanghj,0,"title:`tf.experimental.numpy.array` should have the same behavior as `numpy.array`, but currently crashes description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.8.0- Python version: 3.7.12- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source): N/A- CUDA/cuDNN version: N/A- GPU model and memory: N/A**Describe the current behavior**Currently, the following code snippet below leads to a crash due to the incorrect value of `ndmin`.```a = tf.constant(value=1)b = tf.constant(value=1000)tf.experimental.numpy.array(val=a,ndmin=b)```**Describe the expected behavior**It should not crash.It should have the  same behavior as numpy.array given the same inputs (in which ndmin is validated):`ValueError: ndmin bigger than allowable number of dimensions NPY_MAXDIMS (=32)`**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**The following colab demonstrates the issue:https://colab.research.google.com/drive/1Voz0WB5TCt3s5pQ2etIU3HFBZqN-YVFc?usp=sharing
"
55278,0,602,299,0,0,bhack,0,"title:tf.function condition with tensor description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Colab- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary):colab- TensorFlow version (use command below):default colab- Python version:default colab- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version:- GPU model and memory:You can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**```python @tf.functiondef test():  b = tf.math.maximum(0,1)  if 0 < b <= 2:   b = b+1  return btest().numpy()``````pythonOperatorNotAllowedInGraphError: in user code:    File ""<ipython-input-80-cfa5312e70c9>"", line 4, in test  *        if 0 < b <= 2:    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.```**Describe the expected behavior**I suppose that it will give `2` as with this little modified version:```python@tf.functiondef test():  b = tf.math.maximum(0,1)  if 0 < b and b <= 2:   b = b+1  return btest().numpy()``````python2```**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):With some hints- Briefly describe your candidate solution(if contributing):I need some pointer**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
55263,0,719,39,0,0,kanghj,0,"title:`tf.compat.v1.signal.rfft2d` and `rfft3d` lacks input validation leading to crashes description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.8.0- Python version:3.7.12- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version: 11.2 (based on a colab notebook)- GPU model and memory: Tesla T4, 15109MiB (based on a colab notebook)**Describe the current behavior**The following code snippets lead to crashes when executed:```import numpy as npimport tensorflow as tfa = np.empty([6, 0])b = np.array([1, -1])try:  tf.compat.v1.signal.rfft2d(input_tensor=a,fft_length=b)  # on a different machine: Check failed: size >= 0 (-9223372036854775808 vs. 0)  # Aborted (core dumped)except:  passprint('execution does not reach this line')```and```import numpy as npimport tensorflow as tfa = np.empty([6, 1, 1])b = np.array([1, 2, 0])try:  tf.compat.v1.signal.irfft3d(input_tensor=a,fft_length=b)  # on a different machine: failed to initialize batched cufft plan with customized allocator: Failed to make cuFFT batched plan.  # Aborted (core dumped)except:  passprint('execution does not reach this line')```In either case, the inputs do not quite make sense, and tensorflow should throw.**Describe the expected behavior**Tensorflow should throw exceptions instead of crashing.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Here is a colab notebook:https://colab.research.google.com/drive/168jYG6MqnW4jpJdIXFMUBkyiaweA43aP?usp=sharingEdit: the notebook has to be run with GPU The code snippets above should also reproduce the issue.
"
55251,0,0,269,0,0,elfringham,0,"title:Test //tensorflow/core/framework:model_test fails on AARCH64 description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a- TensorFlow installed from (source or binary): source- TensorFlow version (use command below): git HEAD- Python version: 3.8.12- Bazel version (if compiling from source): 5.0.0- GCC/Compiler version (if compiling from source): 10.2.1- CUDA/cuDNN version: n/a- GPU model and memory: n/aYou can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**Test fails with[ RUN      ] Test/AsyncUnknownRatioTest.Model/0tensorflow/core/framework/model_test.cc:462: FailureExpected equality of these values:  async_unknown_many->TotalProcessingTime( nullptr)    Which is: 109.333  ratio * (50 + 50) + 128 / 3.0    Which is: 109.333[  FAILED  ] Test/AsyncUnknownRatioTest.Model/0, where GetParam() = (1, 0) (1 ms)**Describe the expected behavior**Test passes**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): yes- Briefly describe your candidate solution(if contributing):Add some tolerance to test for equality. Debugging the test shows that there is a difference of 1.42109e-14 in the two values on AARCH64.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --config=nonccl --verbose_failures --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --jobs=75 --build_tests_only -- //tensorflow/core/framework:model_test**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.Introduced with https://github.com/tensorflow/tensorflow/commit/8d46773e6aa6b2eb44b62e422b3c7ccc4adb79b6
"
55250,1,0,0,0,0,ranvirdesai,0,"title:multi worker training in tensorflow gives error - ""terminate called without an active exception aborted"" description:I am running a tensorflow distributed training model on multiple workers using vertex AI/local and multiworkermirroredstrategyAfter completion of all epochs it gives the error of ""terminate called without an active exception aborted""python=3.8tensorflow=2.7Can anyone explain why is this happening.
"
55234,0,184,39,0,0,kanghj,0,"title:MutexLock should not segfault given a non-mutex description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below):2.8.0- Python version:3.7.12- Bazel version (if compiling from source):N/A- GCC/Compiler version (if compiling from source):N/A- CUDA/cuDNN version:N/A- GPU model and memory:N/A**Describe the current behavior**Given the following code:```import numpy as npimport tensorflow as tftf.raw_ops.MutexLock(mutex=np.shape(a=4))```This currently crashes the colab notebook. On a different machine, it leads to a segfault.**Describe the expected behavior**Tensorflow should throw an exception rejecting the invalid argument.  Tensorflow should reject the `mutex` argument since it's not a mutex resource (`A Tensor of type resource`, based on https://www.tensorflow.org/api_docs/python/tf/raw_ops/MutexLock)**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.https://colab.research.google.com/drive/1cwMDIYX9pruA1eG22eYykAxlbhDayjT8?usp=sharing```import numpy as npimport tensorflow as tftf.raw_ops.MutexLock(mutex=np.shape(a=4))```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
55230,0,0,84,0,1,coreyjadams,0,"title:Incorrect calculation of 2nd derivative of a determinant of a matrix description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Monterey.  Also seen on Linux.- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): v2.8.0-rc1-32-g3f878cff5b6 2.8.0 (also seen in 2.6.X)- Python version: 3.8.6- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source): N/A- CUDA/cuDNN version: N/A, but also seen in cuda11.4- GPU model and memory: CPU, but also seen in A100 40GB/80GB**Describe the current behavior**The calculation of the second derivative of a determinant is incorrect. It is a little challenging to describe the full issue in markdown.  I've created a standalone notebook to reproduce the issue.  I compare the TF gradients through a determinant against both finite differences and a custom (albeit slow) determinant implementation.  The TF gradients for the determinant of a matrix, let's call them G, agree with both finite differences and the custom op.  The second derivative, dG/dx, aka the Hessian of the determinant of a matrix, is badly incorrect.  Using autodifferentiation on the custom determinant operation twice, however, agrees well with finite differences methods.Full reproducer available in this notebook (standalone) here: https://github.com/Nuclear-Physics-with-Machine-Learning/MLQM/blob/spin/examples/2nd%20Derivative%20of%20Determinant%20of%20a%20matrix.ipynb**Describe the expected behavior**The calculation of the second derivative of a determinant should be correct.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):  If we can track down the bug, and it's something i can fix, I could.  I haven't modified yet.- Briefly describe your candidate solution(if contributing):  I believe the operations registered for the backprop through a determinant of a matrix must be missing something, and this could be fixed.**Standalone code to reproduce the issue**https://github.com/Nuclear-Physics-with-Machine-Learning/MLQM/blob/spin/examples/2nd%20Derivative%20of%20Determinant%20of%20a%20matrix.ipynb
"
55216,0,1153,57,0,0,snarb,0,"title:Model.compute_loss documentation section is not correct. compute_loss function is not called description:Hello,[Model.compute_loss documentation section is not correct]( https://www.tensorflow.org/api_docs/python/tf/keras/Model)compute_loss function is not called for some reason.```import tensorflow as tfclass MyModel(tf.keras.Model):  def __init__(self, *args, **kwargs):    super(MyModel, self).__init__(*args, **kwargs)    self.loss_tracker = tf.keras.metrics.Mean(name='loss')  def compute_loss(self, x, y, y_pred, sample_weight):    tf.print(""compute_loss is called:"")    loss = tf.reduce_mean(tf.math.squared_difference(y_pred, y))    loss += tf.add_n(self.losses)    self.loss_tracker.update_state(loss)    return loss  def reset_metrics(self):    self.loss_tracker.reset_states()  @property  def metrics(self):    return [self.loss_tracker]tensors = tf.random.uniform((10, 10)), tf.random.uniform((10,))dataset = tf.data.Dataset.from_tensor_slices(tensors).repeat().batch(1)inputs = tf.keras.layers.Input(shape=(10,), name='my_input')outputs = tf.keras.layers.Dense(10)(inputs)model = MyModel(inputs, outputs)model.add_loss(tf.reduce_sum(outputs))optimizer = tf.keras.optimizers.SGD()model.compile(optimizer, loss='mse', steps_per_execution=10)model.fit(dataset, epochs=2, steps_per_epoch=10)print('My custom loss: ', model.loss_tracker.result().numpy())```'compute_loss is called:' is not printed.  Checked on tensorflow 2.6.2
"
55215,1,5561,3,0,0,DLFrameworkBug,0,"title:Crashed if using negative num_streams in QuantileOpsTest description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.5.0- Python version: 3.7.11- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version: None- GPU model and memory:**Describe the current behavior**QuantileOpsTest got crashed if using negative parameters in _create_quantile_stream_resource_() in tf 2.5.0. When running the same test case in tf 2.6.0, it would not crash and printed error info.**Describe the expected behavior**The test case sould fail and give relative error information.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**I ran the following code named bug3.py:```from tensorflow.python.framework import test_utilfrom tensorflow.python.ops import boosted_trees_opsfrom tensorflow.python.ops import resourcesfrom tensorflow.python.ops.gen_boosted_trees_ops import boosted_trees_quantile_stream_resource_handle_op as resource_handle_opfrom tensorflow.python.ops.gen_boosted_trees_ops import is_boosted_trees_quantile_stream_resource_initialized as resource_initializedfrom tensorflow.python.platform import googletest@test_util.run_deprecated_v1class QuantileOpsTest(test_util.TensorFlowTestCase):    def setUp(self):        self.eps = 0.01        self.max_elements = (1 << 16)    def testBasicQuantileBucketsSingleResourcesAddFlushed(self):        with self.cached_session():            quantile_accumulator_handle = resource_handle_op(container='', shared_name='floats_0', name='floats_0')            create_op = boosted_trees_ops.create_quantile_stream_resource(quantile_accumulator_handle, epsilon=self.eps,                                                                              max_elements=self.max_elements,                                                                              num_streams= -2 )                        is_initialized_op = resource_initialized(quantile_accumulator_handle)            resources.register_resource(quantile_accumulator_handle, create_op, is_initialized_op)            resources.initialize_resources(resources.shared_resources()).run()if (__name__ == '__main__'):    googletest.main()```**Other info / logs** Include any logs or source code that would be helpful toHere is the log in tf 2.5.0:```[ RUN      ] QuantileOpsTest.testBasicQuantileBucketsSingleResourcesAddFlushedTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2022-03-14 05:21:01.193346: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2097635000 Hzterminate called after throwing an instance of 'std::length_error'  what():  vector::reserveFatal Python error: AbortedThread 0x00007efcd9c1b180 (most recent call first):  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1453 in _call_tf_sessionrun  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1360 in _run_fn  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1375 in _do_call  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1369 in _do_run  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1191 in _run  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 968 in run  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/tensorflow/python/framework/test_util.py"", line 1729 in run  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 5578 in _run_using_default_session  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 2625 in run  File ""test_bug/bug3.py"", line 24 in testBasicQuantileBucketsSingleResourcesAddFlushed  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/tensorflow/python/framework/test_util.py"", line 1345 in decorated  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/unittest/case.py"", line 628 in run  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/unittest/case.py"", line 676 in __call__  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/unittest/suite.py"", line 122 in run  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/unittest/suite.py"", line 84 in __call__  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/unittest/suite.py"", line 122 in run  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/unittest/suite.py"", line 84 in __call__  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/unittest/runner.py"", line 176 in run  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/absl/testing/_pretty_print_reporter.py"", line 87 in run  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/unittest/main.py"", line 271 in runTests  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/unittest/main.py"", line 101 in __init__  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/absl/testing/absltest.py"", line 2553 in _run_and_get_tests_result  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/absl/testing/absltest.py"", line 2585 in run_tests  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/absl/testing/absltest.py"", line 2172 in _run_in_app  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/absl/testing/absltest.py"", line 2065 in main  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/tensorflow/python/platform/googletest.py"", line 55 in g_main  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/absl/app.py"", line 258 in _run_main  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/absl/app.py"", line 312 in run  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/tensorflow/python/platform/googletest.py"", line 64 in main_wrapper  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/tensorflow/python/platform/benchmark.py"", line 518 in benchmarks_main  File ""/root/anaconda3/envs/tf2.5.0/lib/python3.7/site-packages/tensorflow/python/platform/googletest.py"", line 66 in main  File ""test_bug/bug3.py"", line 28 in <module>Aborted (core dumped)```
"
55209,0,2264,39,1,0,ageron,0,"title:tf.matmul() fails with ragged inputs of shape [batch_size, None, dims] and transpose_b=True description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 11.2 ""bullseye"" (keras-dev docker image)- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): v1.12.1-72504-gfeb9095a53f 2.9.0-dev20220311- Python version: 3.9.10- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source): N/A- CUDA/cuDNN version: N/A- GPU model and memory: N/A**Describe the current behavior**When A and B are two ragged tensors, both of shape [batch_size, None, dims], then `tf.matmul(A, B, transpose_b=True)` fails. However, when A and B are ragged tensors of shape [batch_size, None, None], with the exact same values, then it works fine.**Describe the expected behavior**I expect the first scenario to work, and it should return the same result as the second scenario.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): not sure I'll have time, but if it's not too long ok- Briefly describe your candidate solution(if contributing): I'm guessing there's a bug in the shape checks in `tf.matmul()` since I see no reason why [batch_size, None, dims] would fail while [batch_size, None, None] would succeed. If anything, I would have expected the opposite.**Standalone code to reproduce the issue**See this [gist](https://colab.research.google.com/gist/ageron/1226921dbd2be65dc69f12847f2c229b/tensorflow-issue-matmul-ragged.ipynb).This code fails:```pythonimport tensorflow as tfA_ids = tf.ragged.constant([[1, 2], [1, 0, 5]])B_ids = tf.ragged.constant([[2, 4, 6, 9], [3]])embedding_layer = tf.keras.layers.Embedding(10, 3)A = embedding_layer(A_ids)  # A has shape [2, None, 3]B = embedding_layer(B_ids)  # B has shape [2, None, 3]C = tf.matmul(A, B, transpose_b=True)  # RAISES AN InvalidArgumentError!!!```But this one succeeds:```pythonimport tensorflow as tfA = tf.ragged.constant([  # A has shape [2, None, None]    [[1., 2., 3.], [3., 4., 5.]],    [[1., 3., 5.], [5., 7., 9.], [9., 11., 13.]]])B = tf.ragged.constant([  # B has shape [2, None, None]    [[10., 20., 30.], [30., 40., 50.], [50., 60., 70.], [70., 80., 90.]],    [[11., 21., 31.]]])C = tf.matmul(A, B, transpose_b=True)  # WORKS FINE NOW!```The only difference I can see is that the shape of `A` and `B` is `[2, None, 3]` in the first case, but it's `[2, None, None]` in the second case. It doesn't make sense.**Other info / logs**Below is the full stacktrace:```stacktrace---------------------------------------------------------------------------InvalidArgumentError                      Traceback (most recent call last)[<ipython-input-8-3e4e536edcee>](https://localhost:8080/#) in <module>()      6 A = embedding_layer(A_ids)  # A has shape [2, None, 3]      7 B = embedding_layer(B_ids)  # B has shape [2, None, 3]----> 8 C = tf.matmul(A, B, transpose_b=True)  # ERROR!1 frames[/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py](https://localhost:8080/#) in error_handler(*args, **kwargs)    151     except Exception as e:    152       filtered_tb = _process_traceback_frames(e.__traceback__)--> 153       raise e.with_traceback(filtered_tb) from None    154     finally:    155       del filtered_tb[/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py](https://localhost:8080/#) in raise_from_not_ok_status(e, name)   7184 def raise_from_not_ok_status(e, name):   7185   e.message += ("" name: "" + name if name is not None else """")-> 7186   raise core._status_to_exception(e) from None  # pylint: disable=protected-access   7187    7188 InvalidArgumentError: All flat_values must have compatible shapes.  Shape at index 0: [4].  Shape at index 1: [1].  If you are using tf.map_fn, then you may need to specify an explicit fn_output_signature with appropriate ragged_rank, and/or convert output tensors to RaggedTensors. [Op:RaggedTensorFromVariant]```</details>**Related issues*** This TF issue is probably the cause of this Keras issue: https://github.com/keras-team/keras/issues/16226* There's a closed issue about TF matmul + ragged tensors, but I think it's different: https://github.com/tensorflow/tensorflow/issues/28109
"
55199,1,90,39,0,0,kanghj,0,"title:Missing input validation on `tf.ragged.constant` description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): - Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary): - TensorFlow version (use command below):2.8.0- Python version: 3.7.12- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version: using a colab notebook- GPU model and memory: using a colab notebookYou can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**If I pass an empty list with a large ragged_rank to `tf.ragged.constant`,all RAM is consumed, causing the notebook to crash.The docs indicate that ragged_rank should be between 0 and the rank of pylist, so the large value of ragged_rank should be rejected**Describe the expected behavior**Some input validation should be done and an exception thrown.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.The colab notebook:https://colab.research.google.com/drive/1OyQNTCiqHKjmHKfYbSOmVt4EfkLEgsNA?usp=sharing```import tensorflow as tftf.ragged.constant(pylist=[],ragged_rank=8968073515812833920)```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
55178,1,0,0,0,0,ranvirdesai,0,"title:tf.distribute.MultiWorkerMirroredStrategy() not getting initialized(RuntimeError: Collective ops must be configured at program startup) description:I am running a training job on vertex AI. Versions:TF = 2.7python = 3.8The model runs fine for mirrored strategy where we have only one node with multiple gpu attached to it.For MultiWorkerMirroredStrategy I am usingchief - n1 32 with 4V100 gpu - count 1(default)worker n1 32 with 4V100 gpu - count 1When I am trying to run the code using multiple nodes using MultiWorkerMirroredStrategy, It is giving following errorRuntimeError: Collective ops must be configured at program startupI found some suggestions to put the strategy at the program beginning  but that also didn't help.Vertex AI is setting the TF_CONFIG correctly.But it is not able to instantiate the MultiWorkerMirroredStrategyThe stack trace mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()"""" File ""/opt/conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py"", line 348, in new_func""workerpool0-0"" File ""/opt/conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 253, in __init__"""" super(_CollectiveAllReduceStrategyExperimental,""workerpool0-0"" File ""/opt/conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 186, in __init__""workerpool0-0"" CollectiveAllReduceExtended(""workerpool0-0"" File ""/opt/conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 330, in __init__""workerpool0-0"" self._initialize_strategy(self._cluster_resolver)""Error2022-03-09 09:57:09.622 ISTworkerpool0-0"" File ""/opt/conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 342, in _initialize_strategy""Error2022-03-09 09:57:09.622 ISTworkerpool0-0"" self._initialize_multi_worker(cluster_resolver)""Error2022-03-09 09:57:09.622 ISTworkerpool0-0"" File ""/opt/conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 463, in _initialize_multi_worker""Error2022-03-09 09:57:09.622 ISTworkerpool0-0"" context.context().configure_collective_ops(""Error2022-03-09 09:57:09.622 ISTworkerpool0-0"" File ""/opt/conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/eager/context.py"", line 817, in configure_collective_ops""Error2022-03-09 09:57:09.622 ISTworkerpool0-0"" raise RuntimeError(""Collective ops must be configured at program startup"")""Error2022-03-09 09:57:09.622 ISTworkerpool0-0""RuntimeError: Collective ops must be configured at program startup""
"
55139,1,10436,7,0,0,h1t35h,0,"title:Bug in feature_column.embedding_column based on vocabulary size description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux  AL2- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): v2.3.3-137-gea90cf44f73 2.3.4- Python version: 3.6- CUDA/cuDNN version: replicable on CPU- GPU model and memory: replicable on CPU **Describe the current behavior**While trying to use `tf.feature_column.embedding_column` API. While I don't think is relevant I'm generating the input data via `tf.data.Dataset.from_generator`.Code   ```python for colmn_name in indicator_colms:        feature_col = feature_column.categorical_column_with_vocabulary_list(            colmn_name, unique_values(colmn_name))        indicator_column = feature_column.indicator_column(feature_col)        feature_columns.append(indicator_column)```**Describe the expected behavior**For small vocabulary sizes everything works fine but the moment I try to add more elements to my vocabulary I weirdly get the following exception:```---------------------------------------------------------------------------TypeError                                 Traceback (most recent call last)~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/keras/feature_column/dense_features.py in call(self, features, cols_to_output_tensors, training)    165           tensor = column.get_dense_tensor(--> 166               transformation_cache, self._state_manager, training=training)    167         except TypeError:TypeError: get_dense_tensor() got an unexpected keyword argument 'training'During handling of the above exception, another exception occurred:TypeError                                 Traceback (most recent call last)~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in get(self, key, state_manager, training)   2457       transformed = column.transform_feature(-> 2458           self, state_manager, training=training)   2459     except TypeError:TypeError: transform_feature() got an unexpected keyword argument 'training'During handling of the above exception, another exception occurred:TypeError                                 Traceback (most recent call last)~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in get(self, key, state_manager, training)   2457       transformed = column.transform_feature(-> 2458           self, state_manager, training=training)   2459     except TypeError:TypeError: transform_feature() got an unexpected keyword argument 'training'During handling of the above exception, another exception occurred:ValueError                                Traceback (most recent call last)<ipython-input-69-8136fbe34af6> in <module>      2     feature_columns = _create_feature_columns()      3     feature_layer = tf.keras.layers.DenseFeatures(feature_columns)----> 4     demo(feature_columns)<ipython-input-10-65d9d3c8db7a> in demo(feature_column)      1 def demo(feature_column):      2     demo_feature_layer = layers.DenseFeatures(feature_column)----> 3     print(demo_feature_layer(example_batch).numpy())~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/keras/engine/base_layer.py in __call__(self, *args, **kwargs)   1035         with autocast_variable.enable_auto_cast_variables(   1036             self._compute_dtype_object):-> 1037           outputs = call_fn(inputs, *args, **kwargs)   1038    1039         if self._activity_regularizer:~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/keras/feature_column/dense_features.py in call(self, features, cols_to_output_tensors, training)    167         except TypeError:    168           tensor = column.get_dense_tensor(transformation_cache,--> 169                                            self._state_manager)    170         processed_tensors = self._process_dense_tensor(column, tensor)    171         if cols_to_output_tensors is not None:~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in get_dense_tensor(self, transformation_cache, state_manager)   4301     # Feature has been already transformed. Return the intermediate   4302     # representation created by transform_feature.-> 4303     return transformation_cache.get(self, state_manager)   4304    4305   @deprecation.deprecated(_FEATURE_COLUMN_DEPRECATION_DATE,~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in get(self, key, state_manager, training)   2458           self, state_manager, training=training)   2459     except TypeError:-> 2460       transformed = column.transform_feature(self, state_manager)   2461     if transformed is None:   2462       raise ValueError('Column {} is not supported.'.format(column.name))~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in transform_feature(self, transformation_cache, state_manager)   4238     """"""   4239     id_weight_pair = self.categorical_column.get_sparse_tensors(-> 4240         transformation_cache, state_manager)   4241     return self._transform_id_weight_pair(id_weight_pair,   4242                                           self.variable_shape[-1])~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in get_sparse_tensors(self, transformation_cache, state_manager)   3725     """"""See `CategoricalColumn` base class.""""""   3726     return CategoricalColumn.IdWeightPair(-> 3727         transformation_cache.get(self, state_manager), None)   3728    3729   @deprecation.deprecated(_FEATURE_COLUMN_DEPRECATION_DATE,~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in get(self, key, state_manager, training)   2458           self, state_manager, training=training)   2459     except TypeError:-> 2460       transformed = column.transform_feature(self, state_manager)   2461     if transformed is None:   2462       raise ValueError('Column {} is not supported.'.format(column.name))~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in transform_feature(self, transformation_cache, state_manager)   3703     input_tensor = _to_sparse_input_and_drop_ignore_values(   3704         transformation_cache.get(self.key, state_manager))-> 3705     return self._transform_input_tensor(input_tensor, state_manager)   3706    3707   @deprecation.deprecated(_FEATURE_COLUMN_DEPRECATION_DATE,~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in _transform_input_tensor(self, input_tensor, state_manager)   3691             num_oov_buckets=self.num_oov_buckets,   3692             dtype=key_dtype,-> 3693             name=name)   3694       if state_manager is not None:   3695         state_manager.add_resource(self, name, table)~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py in index_table_from_tensor(vocabulary_list, num_oov_buckets, default_value, hasher_spec, dtype, name)   1507    1508   with ops.name_scope(name, ""string_to_index""):-> 1509     keys = ops.convert_to_tensor(vocabulary_list)   1510     if keys.dtype.is_integer != dtype.is_integer:   1511       raise ValueError(~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/profiler/trace.py in wrapped(*args, **kwargs)    161         with Trace(trace_name, **trace_kwargs):    162           return func(*args, **kwargs)--> 163       return func(*args, **kwargs)    164     165     return wrapped~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)   1564    1565     if ret is None:-> 1566       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)   1567    1568     if ret is NotImplemented:~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)    344                                          as_ref=False):    345   _ = as_ref--> 346   return constant(v, dtype=dtype, name=name)    347     348 ~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)    270   """"""    271   return _constant_impl(value, dtype, shape, name, verify_shape=False,--> 272                         allow_broadcast=True)    273     274 ~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)    281       with trace.Trace(""tf.constant""):    282         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)--> 283     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)    284     285   g = ops.get_default_graph()~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)    306 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):    307   """"""Creates a constant on the current device.""""""--> 308   t = convert_to_eager_tensor(value, ctx, dtype)    309   if shape is None:    310     return t~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)    104       dtype = dtypes.as_dtype(dtype).as_datatype_enum    105   ctx.ensure_initialized()--> 106   return ops.EagerTensor(value, ctx.device_name, dtype)    107     108 ValueError: Can't convert Python sequence with mixed types to Tensor.```**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.```pythondef _create_feature_columns():     indicator_columns = [...] # list of columns    for colmn_name in indicator_colms:        feature_col = feature_column.categorical_column_with_vocabulary_list(            colmn_name, unique_values(colmn_name))        indicator_column = feature_column.indicator_column(feature_col)        feature_columns.append(indicator_column)def demo(feature_column):    demo_feature_layer = layers.DenseFeatures(feature_column)    print(demo_feature_layer(example_batch).numpy())with tf.device(device):    feature_columns = _create_feature_columns()    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)    demo(feature_columns)```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
55135,1,9046,262,0,0,fengyuentau,0,"title:Setting `converter.inference_type=uint8` does not produce quantized TFLite model of uint8 weight data type description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.1 LTS- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): v2.8.0-rc1-32-g3f878cff5b6 2.8.0- Python version: 3.8.8- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source): N/A- CUDA/cuDNN version: N/A- GPU model and memory: N/AYou can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**I was trying to convert [a TF model](http://download.tensorflow.org/models/official/20181001_resnet/savedmodels/resnet_v1_fp32_savedmodel_NHWC.tar.gz) to TFLite model and quantize by the way. With `converter.optimizations = [tf.lite.Optimize.DEFAULT]` set, I could get a quantized model of int8 weight data type. I would also want a quantized model of uint8 type, so I tried `converter.inference_type=uint8`, but the produced model is still in int8 data type.**Describe the expected behavior**Set `converter.inference_type=uint8` to produce a quantized TFLite model of **uint8 weight data type**.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): No- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.```python import tensorflow as tf tf_path = ""./resnet_v1_fp32_savedmodel_NHWC/1538686669"" tflite_path = ""{}.tflite"".format(""resnet50.uint8.nhwc"") converter = tf.lite.TFLiteConverter.from_saved_model(tf_path) converter.optimizations = [tf.lite.Optimize.DEFAULT] converter.inference_type = tf.uint8 tf_lite_model = converter.convert() with open(tflite_path, 'wb') as f:     f.write(tf_lite_model)```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.```$ python tf2tflite.py2022-03-08 19:46:26.965733: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/conv2d/kernel:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/conv2d/kernel:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/conv2d/kernel:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/conv2d/kernel:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().2022-03-08 19:46:28.726330: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.2022-03-08 19:46:28.726362: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.2022-03-08 19:46:28.727314: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: ./resnet_v1_fp32_savedmodel_NHWC/15386866692022-03-08 19:46:28.732979: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }2022-03-08 19:46:28.733019: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: ./resnet_v1_fp32_savedmodel_NHWC/15386866692022-03-08 19:46:28.765293: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.2022-03-08 19:46:28.997319: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: ./resnet_v1_fp32_savedmodel_NHWC/15386866692022-03-08 19:46:29.010577: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 283266 microseconds.2022-03-08 19:46:29.097206: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.2022-03-08 19:46:30.323709: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 1049.200 G  ops, equivalently 524.600 G  MACsEstimated count of arithmetic ops: 1049.200 G  ops, equivalently 524.600 G  MACs```
"
54984,1,1561,2,0,0,mikegashler,0,"title:gradient tape with rank 0 tensors causes integer division or modulo by zero description:**System information**- Have I written custom code? Yes- OS Platform and Distribution: Linux Ubuntu 20.04- TensorFlow installed from: binary (via apt-get)- TensorFlow version: 2.3.1- Python version: 3.8.10- GPU model: GeForce GTX 1050 Ti Mobile**Describe the current behavior**The simple repro below throws the following exception:```Traceback (most recent call last):  File ""bug.py"", line 16, in <module>    grads = tape.gradient(loss, model.trainable_variables)  File ""/home/mike/.local/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py"", line 1067, in gradient    flat_grad = imperative_grad.imperative_grad(  File ""/home/mike/.local/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py"", line 71, in imperative_grad    return pywrap_tfe.TFE_Py_TapeGradient(  File ""/home/mike/.local/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py"", line 162, in _gradient_function    return grad_fn(mock_op, *out_grads)  File ""/home/mike/.local/lib/python3.8/site-packages/tensorflow/python/ops/array_grad.py"", line 228, in _ConcatGradV2    return _ConcatGradHelper(  File ""/home/mike/.local/lib/python3.8/site-packages/tensorflow/python/ops/array_grad.py"", line 118, in _ConcatGradHelper    concat_dim._numpy().item(0) % input_values[0]._rank())  # pylint: disable=protected-accessZeroDivisionError: integer division or modulo by zero```**Describe the expected behavior**It should either compute a gradient (or throw a meaningful exception if that is not possible).**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? No**Standalone code to reproduce the issue**```import tensorflow as tfclass MyModel(tf.keras.Model):    def __init__(self) -> None:        super().__init__()        self.layer = tf.keras.layers.Dense(1)    def call(self, inputs: tf.Tensor) -> tf.Tensor:        return self.layer(inputs)model = MyModel()x = tf.zeros((1, 1))with tf.GradientTape() as tape:    theta = model(x)[0,0]    loss = tf.concat([tf.math.cos(theta), tf.math.sin(theta)], axis=0)grads = tape.gradient(loss, model.trainable_variables)```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
54958,0,0,12,0,0,rubber815,0,"title:Suspicious usage of clGetProgramInfo parameters in CLProgram::GetBinary description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Android 12- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S22- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below):  tensorflow lite tag 2.8.0- Python version: 3.7- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version: N/A- GPU model and memory: Exynos Samsung GPU**benchmark build**- bazel build -c opt --config=android_arm64 tensorflow/lite/tools/benchmark:benchmark_model)**run**/data/local/tmp/benchmark_model --graph=/data/local/tmp/--mini_cnn.tflite --use_gpu=true --num_runs=10 --num_threads=1 --gpu_backend=cl --delegate_serialize_dir=/data/local/tmp/cache --delegate_serialize_token=mini**Describe the current behavior**clGetProgramInfo in CLProgram::GetBinary **parameters**CL_PROGRAM_BINARIES: 0x1166**binary_size: 5592** (the current codes are intended for kernel size not address size)**Describe the expected behavior**clGetProgramInfo in CLProgram::GetBinary **parameters**CL_PROGRAM_BINARIES: 0x1166**binary_size: sizof(binary_ptr) maybe**In OpenCL Spec(https://www.khronos.org/registry/OpenCL/sdk/2.0/docs/man/xhtml/), it says - **param_value_size** (third param in clGetProgramInfo)Used to specify the size in bytes of memory pointed to by param_value- **param_value** (forth param in clGetProgramInfo)Return type: unsigned char *[] (double pointers) in case of clGetProgramInfo's CL_PROGRAM_BINARIESTherefore, param_value_size should be size of  unsigned char * (maybe address)size of binary address, not binary itself.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): yes- Briefly describe your candidate solution(if contributing):`  cl_int error_code = clGetProgramInfo(program_, CL_PROGRAM_BINARIES,                                       sizeof(binary_ptr), &binary_ptr, nullptr);`**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
54925,1,196,13,0,0,hshazly,0,"title:TF_CPP_MIN_VLOG_LEVEL and TF_CPP_MIN_LOG_LEVEL do not output log info in TF 2.8.0/2.9 description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20- TensorFlow installed from (source or binary): Yes- TensorFlow version (use command below): v2.8.0-2-ge994fb9c3ad and 2.9 (master branch)- Python version: 3.8.10- Bazel version (if compiling from source): 4.2.1- GCC/Compiler version (if compiling from source): 9.3.0- CUDA/cuDNN version: NA- GPU model and memory: NA**Describe the current behavior**Setting TF_CPP_MIN_LOG_LEVEL and TF_CPP_MIN_VLOG_LEVEL does not work with the latest release 2.8.0 and 2.9 of the master branch (both built from source) -- Nor exporting an envvar before launching the TF script, nor using the os module in the code (tried setting it before and after the tensorflow module import).For source build, I followed the steps mentioned in the [official guide](https://www.tensorflow.org/install/source), using the build command: `bazel build //tensorflow/tools/pip_package:build_pip_package` Both builds are CPU only without CUDA nor ROCm support. **Describe the expected behavior**As discussed in #31870, setting TF_CPP_MIN_LOG_LEVEL=0 and/or TF_CPP_MIN_VLOG_LEVEL=2 should show logging/debugging information regarding the C++ implementation inner operations, memory allocations, etc. **Standalone code to reproduce the issue**```import os #os.environ['TF_CPP_MIN_VLOG_LEVEL'] = ""2""import tensorflow as tfos.environ['TF_CPP_MIN_VLOG_LEVEL'] = ""2""a = tf.Variable(tf.zeros(shape=(2)), name=""a"")print(a)```
"
54900,1,2854,19,0,0,Bidski,0,"title:MirroredStrategy with custom model throws AttributeError: 'NoneTensorSpec' object has no attribute 'rank' description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary): Binary- TensorFlow version (use command below): v2.8.0-rc1-32-g3f878cff5b6 2.8.0- Python version: 3.10.2- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version: 11.6/8.3- GPU model and memory: GTX1080Ti 11GB**Describe the current behavior**When training a custom model with `keras.fit` and `MirroredStrategy` an `AttributeError` is thrown. Without the `MirroredStrategy` training proceeds without error. I also notice that the console output does not mention something like `INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)` even though the strategy reports having 3 replicas in syncCode snippet```    ### ----- create datasets        strategy = tf.distribute.MirroredStrategy()    print(""Number of devices: {}"".format(strategy.num_replicas_in_sync))    # with strategy.scope():        model = MyModel()        # Setup the optimiser with the learning rate schedule        lr_schedule = CosineDecayWithWarmup(            initial_learning_rate=args.lr,            decay_steps=args.num_epochs,            warmup_steps=args.warmup_steps,            warmup_learning_rate=args.warmup_lr,            hold_base_rate_steps=args.hold_base_rate_steps,        )        optimiser = tf.keras.optimizers.Adam(learning_rate=lr_schedule)        # Compile the model        model.compile(optimizer=optimiser, loss=MyLoss())    # Build the model and print a summary    model.build(input_shape=(args.batch_size, image_shape[0], image_shape[1], 3))    model.summary()    history = model.fit(        training_dataset,        epochs=args.num_epochs,        validation_data=validation_dataset,        callbacks=callbacks,    )```Console output```2022-03-03 14:30:59.481815: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2022-03-03 14:31:00.853869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8972 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.12022-03-03 14:31:00.854710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10405 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.12022-03-03 14:31:00.855231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 10405 MB memory:  -> device: 2, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:43:00.0, compute capability: 6.1Number of devices: 3#### ----- MODEL SUMMARY ----Traceback (most recent call last):  File ""$HOME/network/train.py"", line 279, in <module>    main()  File ""$HOME/network/train.py"", line 242, in main    history = model.fit(  File ""$HOME/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler    raise e.with_traceback(filtered_tb) from None  File ""$HOME/.local/lib/python3.10/site-packages/tensorflow/python/data/experimental/ops/distribute.py"", line 513, in get_static_batch_dim    if output_shape.rank is None:AttributeError: 'NoneTensorSpec' object has no attribute 'rank'```**Describe the expected behavior**Network trains with the `MirroredStrategy` with no error.
"
54855,0,406,0,0,0,ArrowIntoTheSky,0,"title:`tf.raw_ops.RGBToHSV` lack support for bfloat16 description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.7.0- Python version: 3.8- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source):N/A- CUDA/cuDNN version: N/A- GPU model and memory: N/A**Standalone code to reproduce the issue**```import tensorflow as tfimages = tf.random.uniform([1, 1, 3], dtype=tf.bfloat16)tf.raw_ops.RGBToHSV(images=images)```throws error:```NotFoundError: Could not find device for node: {{node RGBToHSV}} = RGBToHSV[T=DT_BFLOAT16]All kernels registered for op RGBToHSV:  device='CPU'; T in [DT_DOUBLE]  device='CPU'; T in [DT_FLOAT]  device='GPU'; T in [DT_DOUBLE]  device='GPU'; T in [DT_FLOAT] [Op:RGBToHSV]```**Describe the current behavior**[`tf.raw_ops.RGBToHSV`](https://www.tensorflow.org/api_docs/python/tf/raw_ops/RGBToHSV) should support half, bfloat16, float32, float64 according to the document.
"
54851,0,147,0,0,0,ArrowIntoTheSky,0,"title:`tf.ragged.segment_ids_to_row_splits` lack check for `out_type` description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.7.0- Python version: 3.8- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source):N/A- CUDA/cuDNN version: N/A- GPU model and memory: N/A**Standalone code to reproduce the issue**```import tensorflow as tfsegment_ids = [0,0,0,2,2,3,4,4,4]res=tf.ragged.segment_ids_to_row_splits(segment_ids, out_type=1) # passprint(res)```**Describe the current behavior**[`tf.ragged.segment_ids_to_row_splits`](https://www.tensorflow.org/api_docs/python/tf/ragged/segment_ids_to_row_splits?hl=en) should check `out_type` is a valid DType. In the example code, `out_typt` is `1`, so it should raise an error instead of silently pass.
"
54745,1,0,0,0,0,TiNNTRAVELL,0,"title:Drive  description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary):- TensorFlow version (use command below):- Python version:- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version:- GPU model and memory:You can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior****Describe the expected behavior****[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
54582,1,21217,7,0,1,alanpurple,0,"title:huggingface pre_trained model loading takes full gpu memory description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  N/A- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11 x64- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below):  2.8.0- Python version: 3.9.7- Bazel version (if compiling from source): 4.2.1- GCC/Compiler version (if compiling from source): visual studio 2019- CUDA/cuDNN version: 11.6/8.3.2- GPU model and memory: RTX3090 24GBYou can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**load from pre-trained transformer takes full gpu memory, so oom happens after training started**Describe the expected behavior**appropriate memory allocation**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): yes- Briefly describe your candidate solution(if contributing): add options for loading pre-trained model**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.with kaggle feedback-prize datahttps://www.kaggle.com/c/feedback-prize-2021/datathis code```import osfrom tqdm import tqdm,trangeimport tensorflow as tffrom tensorflow.keras import layers,Model,Input,Sequential,metrics,regularizers,optimizers,losses,activations,callbacksfrom transformers import AutoTokenizer,AutoConfig,TFAutoModelimport pandas as pdimport numpy as npfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFoldfrom copy import copy,deepcopyVER=12EPOCHS=5N_SPLITS=5MAX_LEN=1024NUM_ATTENTION_HEADS=16ROBERTA_LARGE_PATH='../../pretrained/transformers/roberta-large/'LRS = [1e-4, 1e-4, 1e-4, 1e-5, 1e-5,1e-5]RUN_TRAINING=Falsetokenizer=AutoTokenizer.from_pretrained(ROBERTA_LARGE_PATH+'auto_tokenizer')config=AutoConfig.from_pretrained(ROBERTA_LARGE_PATH+'auto_config')backbone=TFAutoModel.from_pretrained('roberta-large',config=config)FEEDBACK_DATA_PATH='../../mldata/Feedback_Prize/'train_df=pd.read_csv(FEEDBACK_DATA_PATH+'train.csv')N_ID=train_df.id.nunique()IDS=train_df.id.unique()discourse_types=['Lead','Position','Evidence','Claim','Concluding Statement','Counterclaim','Rebuttal']target_map = {k:i for i,k in enumerate(discourse_types)}targets=np.load(f'../kaggle/encoded/targets_{MAX_LEN}.npy',allow_pickle=False)train_tokens=np.load(f'../kaggle/encoded/tokens_{MAX_LEN}.npy',allow_pickle=False)train_attention=np.load(f'../kaggle/encoded/attention_{MAX_LEN}.npy',allow_pickle=False)test_files=os.listdir('../../mldata/Feedback_Prize/test/')TEST_IDS=[f.replace('.txt','') for f in test_files if 'txt' in f]print('There are',len(TEST_IDS),'test texts.')test_tokens=np.zeros((len(TEST_IDS),MAX_LEN),dtype=np.int32)test_attention=np.zeros((len(TEST_IDS),MAX_LEN),dtype=np.int32)for i,id in enumerate(TEST_IDS):    name=f'../../mldata/Feedback_Prize/test/{id}.txt'    txt=open(name,'r').read()    tokens=tokenizer.encode_plus(txt,max_length=MAX_LEN,padding='max_length',truncation=True,return_offsets_mapping=True)    test_tokens[i]=tokens['input_ids']    test_attention[i]=tokens['attention_mask']def build_model():    tokens=Input((MAX_LEN,),name='tokens',dtype=tf.int32)    attention=Input((MAX_LEN,),name='attention',dtype=tf.int32)    x=backbone(tokens,attention_mask=attention)    x1=layers.Dropout(0.1)(x[0])    x=layers.Dense(15,activation='softmax',dtype=tf.float32)(x1)    model=Model(inputs=[tokens,attention],outputs=x)    model.compile(optimizers.Adam(1e-4),losses.CategoricalCrossentropy(),[metrics.CategoricalAccuracy()])    return modeldiscourse_types_ext=copy(discourse_types)discourse_types_ext.append('blank')def get_preds(dataset = 'train', verbose = True, text_ids = None, preds = None):    all_predictions = []    for id_num in range(len(preds)):        if (id_num % 100 == 0) & (verbose): print(id_num, ', ', end = '')        n = text_ids[id_num]        name = f'../input/feedback-prize-2021/{dataset}/{n}.txt'        txt = open(name, 'r').read()        tokens = tokenizer.encode_plus(txt, max_length = MAX_LEN, padding = 'max_length', truncation = True, return_offsets_mapping = True)        off = tokens['offset_mapping']        w = []        blank = True        for i in range(len(txt)):            if (txt[i] != ' ') & (txt[i] != '\n') & (blank == True):                w.append(i)                blank = False            elif (txt[i] == ' ') | (txt[i] == '\n'):                blank = True        w.append(1e6)        word_map = -1 * np.ones(MAX_LEN, dtype = 'int32')        w_i = 0        for i in range(len(off)):            if off[i][1] == 0: continue            while off[i][0] >= w[w_i + 1]: w_i += 1            word_map[i] = int(w_i)        pred = preds[id_num,] / 2.0        i = 0        while i < MAX_LEN:            prediction = []            start = pred[i]            if start in [0, 1, 2, 3, 4, 5, 6, 7]:                prediction.append(word_map[i])                i += 1                if i >= MAX_LEN: break                while pred[i] == start + 0.5:                    if not word_map[i] in prediction: prediction.append(word_map[i])                    i += 1                    if i >= MAX_LEN: break            else: i += 1            prediction = [x for x in prediction if x != -1]            if len(prediction) > 4: all_predictions.append((n, discourse_types_ext[int(start)], ' '.join([str(x) for x in prediction])))    # MAKE DATAFRAME    df = pd.DataFrame(all_predictions)    df.columns = ['id', 'class', 'predictionstring']    return dfdef calc_overlap(row):    set_pred = set(row.predictionstring_pred.split(' '))    set_gt = set(row.predictionstring_gt.split(' '))    len_gt = len(set_gt)    len_pred = len(set_pred)    inter = len(set_gt.intersection(set_pred))    overlap_1 = inter / len_gt    overlap_2 = inter / len_pred    return [overlap_1, overlap_2]def score_feedback_comp(pred_df, gt_df):    gt_df = gt_df[['id', 'discourse_type', 'predictionstring']].reset_index(drop = True).copy()    pred_df = pred_df[['id', 'class', 'predictionstring']].reset_index(drop = True).copy()    pred_df['pred_id'] = pred_df.index    gt_df['gt_id'] = gt_df.index    joined = pred_df.merge(gt_df, left_on = ['id', 'class'], right_on = ['id', 'discourse_type'], how = 'outer', suffixes = ('_pred', '_gt'))    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')    joined['overlaps'] = joined.apply(calc_overlap, axis=1)    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)    tp_pred_ids = joined.query('potential_TP').sort_values('max_overlap', ascending=False).groupby(['id','predictionstring_gt']).first()['pred_id'].values    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]    TP = len(tp_pred_ids)    FP = len(fp_pred_ids)    FN = len(unmatched_gt_ids)    my_f1_score = TP / (TP + 0.5*(FP+FN))    return my_f1_scoreEPOCHS=6all_scores=[]oof_preds=np.zeros((N_ID,15))test_preds=np.zeros((len(TEST_IDS),MAX_LEN,15))for fold,(train_idx,valid_idx) in enumerate(MultilabelStratifiedKFold(n_splits=N_SPLITS,shuffle=True,random_state=777).split(train_tokens,targets[:,0,:])):    print('#'*25)    print('### FOLD %i' % (fold + 1))    print('#'*25)        model=build_model()        def lrfn(epoch):        return LRS[epoch]        model.fit([train_tokens[train_idx],train_attention[train_idx]],targets[train_idx],validation_data=([train_tokens[valid_idx],train_attention[valid_idx]],targets[valid_idx]),                callbacks=[callbacks.LearningRateScheduler(lrfn,verbose=True),callbacks.ModelCheckpoint('models/%s-roberta-%i.tf'%(VER,fold),monitor='val_loss',                                                                                                        save_best_only=True,save_weights_only=True)],                epochs=EPOCHS,batch_size=8,verbose=1)    pred=model.predict([train_tokens[valid_idx],train_attention[valid_idx]],batch_size=4,verbose=1)    print(pred.shape)    print('predicting OOF...')    oof=get_preds(dataset='train',verbose=True,text_ids=IDS[valid_idx],preds=np.argmax(pred,axis=-1))    f1s=[]    CLASSES=oof['class'].unique()    for c in CLASSES:        pred_df = oof.loc[oof['class'] == c].copy()        valid = train_df.loc[train_df.id.isin(IDS[valid_idx])]        gt_df = valid.loc[valid['discourse_type'] == c].copy()        f1 = score_feedback_comp(pred_df, gt_df)        print(c, f1)        f1s.append(f1)    print()    print('Fold score: ', np.mean(f1s))        test_preds+=model.predict([test_tokens,test_attention],batch_size=4,verbose=1)/N_SPLITS```error```2022-02-25 20:42:31.859917: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size:2022-02-25 20:42:31.859950: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 131 Chunks of size 256 totalling 32.8KiB2022-02-25 20:42:31.859978: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB2022-02-25 20:42:31.860006: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 3072 totalling 3.0KiB2022-02-25 20:42:31.860033: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 3840 totalling 3.8KiB2022-02-25 20:42:31.860061: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 655 Chunks of size 4096 totalling 2.56MiB2022-02-25 20:42:31.860089: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 4352 totalling 4.2KiB2022-02-25 20:42:31.860117: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 4864 totalling 4.8KiB2022-02-25 20:42:31.860145: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 7168 totalling 7.0KiB2022-02-25 20:42:31.860173: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 66 Chunks of size 16384 totalling 1.03MiB2022-02-25 20:42:31.860201: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 19200 totalling 18.8KiB2022-02-25 20:42:31.860228: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 20480 totalling 40.0KiB2022-02-25 20:42:31.860256: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 24576 totalling 24.0KiB2022-02-25 20:42:31.860284: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 28672 totalling 28.0KiB2022-02-25 20:42:31.860311: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 32256 totalling 31.5KiB2022-02-25 20:42:31.860339: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 32 Chunks of size 32768 totalling 1.00MiB2022-02-25 20:42:31.860367: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 40960 totalling 40.0KiB2022-02-25 20:42:31.860394: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 48896 totalling 47.8KiB2022-02-25 20:42:31.860422: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 61440 totalling 180.0KiB2022-02-25 20:42:31.860450: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 65536 totalling 64.0KiB2022-02-25 20:42:31.860478: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 491520 totalling 480.0KiB2022-02-25 20:42:31.860506: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 2105344 totalling 4.02MiB2022-02-25 20:42:31.860534: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 289 Chunks of size 4194304 totalling 1.13GiB2022-02-25 20:42:31.860562: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 4578560 totalling 4.37MiB2022-02-25 20:42:31.860590: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 139 Chunks of size 16777216 totalling 2.17GiB2022-02-25 20:42:31.860618: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 20971520 totalling 20.00MiB2022-02-25 20:42:31.860646: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 21258240 totalling 20.27MiB2022-02-25 20:42:31.860674: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 25530368 totalling 24.35MiB2022-02-25 20:42:31.860702: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 27955200 totalling 26.66MiB2022-02-25 20:42:31.860730: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 29360128 totalling 28.00MiB2022-02-25 20:42:31.860758: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 89 Chunks of size 33554432 totalling 2.78GiB2022-02-25 20:42:31.860787: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 39534592 totalling 37.70MiB2022-02-25 20:42:31.860815: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 40222720 totalling 38.36MiB2022-02-25 20:42:31.860843: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 28 Chunks of size 134217728 totalling 3.50GiB2022-02-25 20:42:31.860872: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 205885440 totalling 589.04MiB2022-02-25 20:42:31.860901: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 21 Chunks of size 536870912 totalling 10.50GiB2022-02-25 20:42:31.860928: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 20.86GiB2022-02-25 20:42:31.860953: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 22729785344 memory_limit_: 22729785344 available bytes: 0 curr_region_allocation_bytes_: 454595706882022-02-25 20:42:31.860988: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats:Limit:                     22729785344InUse:                     22400043008MaxInUse:                  22433597440NumAllocs:                        5211MaxAllocSize:                536870912Reserved:                            0PeakReserved:                        0LargestFreeBlock:                    02022-02-25 20:42:31.861075: W tensorflow/core/common_runtime/bfc_allocator.cc:474] ***************************************************************************************************_2022-02-25 20:42:31.861123: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at matmul_op_impl.h:681 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8,16,1024,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfcTraceback (most recent call last):  File ""D:\repo\robertatraining\robertatraining.py"", line 157, in <module>    model.fit([train_tokens[train_idx],train_attention[train_idx]],targets[train_idx],validation_data=([train_tokens[valid_idx],train_attention[valid_idx]],targets[valid_idx]),  File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler    raise e.with_traceback(filtered_tb) from None  File ""C:\Users\alanp\anaconda3\lib\site-packages\tensorflow\python\eager\execute.py"", line 54, in quick_execute    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:Detected at node 'model/tf_roberta_model/roberta/encoder/layer_._7/attention/self/MatMul' defined at (most recent call last):    File ""D:\repo\robertatraining\robertatraining.py"", line 157, in <module>      model.fit([train_tokens[train_idx],train_attention[train_idx]],targets[train_idx],validation_data=([train_tokens[valid_idx],train_attention[valid_idx]],targets[valid_idx]),    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler      return fn(*args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\training.py"", line 1384, in fit      tmp_logs = self.train_function(iterator)    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\training.py"", line 1021, in train_function      return step_function(self, iterator)    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\training.py"", line 1010, in step_function      outputs = model.distribute_strategy.run(run_step, args=(data,))    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\training.py"", line 1000, in run_step      outputs = model.train_step(data)    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\training.py"", line 859, in train_step      y_pred = self(x, training=True)    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler      return fn(*args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\base_layer.py"", line 1096, in __call__      outputs = call_fn(inputs, *args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 92, in error_handler      return fn(*args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\functional.py"", line 451, in call      return self._run_internal_graph(    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\functional.py"", line 589, in _run_internal_graph      outputs = node.layer(*args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler      return fn(*args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\base_layer.py"", line 1096, in __call__      outputs = call_fn(inputs, *args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 92, in error_handler      return fn(*args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\transformers\models\roberta\modeling_tf_roberta.py"", line 996, in call      outputs = self.roberta(    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler      return fn(*args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\base_layer.py"", line 1096, in __call__      outputs = call_fn(inputs, *args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 92, in error_handler      return fn(*args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\transformers\models\roberta\modeling_tf_roberta.py"", line 755, in call      encoder_outputs = self.encoder(    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler      return fn(*args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\base_layer.py"", line 1096, in __call__      outputs = call_fn(inputs, *args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 92, in error_handler      return fn(*args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\transformers\models\roberta\modeling_tf_roberta.py"", line 528, in call      for i, layer_module in enumerate(self.layer):    File ""C:\Users\alanp\anaconda3\lib\site-packages\transformers\models\roberta\modeling_tf_roberta.py"", line 534, in call      layer_outputs = layer_module(    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler      return fn(*args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\base_layer.py"", line 1096, in __call__      outputs = call_fn(inputs, *args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 92, in error_handler      return fn(*args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\transformers\models\roberta\modeling_tf_roberta.py"", line 443, in call      self_attention_outputs = self.attention(    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler      return fn(*args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\base_layer.py"", line 1096, in __call__      outputs = call_fn(inputs, *args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 92, in error_handler      return fn(*args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\transformers\models\roberta\modeling_tf_roberta.py"", line 356, in call      self_outputs = self.self_attention(    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler      return fn(*args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\base_layer.py"", line 1096, in __call__      outputs = call_fn(inputs, *args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 92, in error_handler      return fn(*args, **kwargs)    File ""C:\Users\alanp\anaconda3\lib\site-packages\transformers\models\roberta\modeling_tf_roberta.py"", line 284, in call      attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)Node: 'model/tf_roberta_model/roberta/encoder/layer_._7/attention/self/MatMul'OOM when allocating tensor with shape[8,16,1024,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc         [[{{node model/tf_roberta_model/roberta/encoder/layer_._7/attention/self/MatMul}}]]Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode. [Op:__inference_train_function_53102]```[8,16,1024,1024] with tf.fload32 is just 512MB total, it doesn't make sensejust with ""from_pretrained"", gpu allocate almost all of its memory![image](https://user-images.githubusercontent.com/4515120/155709476-168583f7-e1cc-495b-8ab3-2a218934d7ac.png)**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
54540,1,8038,10,0,0,ae20cg,0,"title:InaccessibleTensorError when running model as a .expand_dims call in the main model code description:**System information** I have written a custom model in TensorFlow as a modification of the transformer model.M1 Pro 32 GB Macbook Pro 12.1 Monterey(also ran on colab T4 GPU 16 GB)- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- TensorFlow installed from (source or binary): Binary- TensorFlow version (use command below): 2.7.1 also tried 2.8- Python version: 3.7.1.2- CUDA/cuDNN version: N/A- GPU model and memory: 32 GB M1 PRO,  Tesla T4 16 GB**Describe the current behavior**Current when I run my model in eager execution model it is fine, but when I run it with @tf.function wrapper I get an inaccessible tensor error when building the AutoGraph. This is coming in the model call method, when I .expand_dims on the call of another layer class. The error is:```TypeError: <tf.Tensor 'hi_d_transformer/ExpandDims:0' shape=(64, 1, 64) dtype=float32> is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it.Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.<tf.Tensor 'hi_d_transformer/ExpandDims:0' shape=(64, 1, 64) dtype=float32> was defined here:    File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main      ""__main__"", mod_spec)    File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code      exec(code, run_globals)    File ""/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py"", line 16, in <module>      app.launch_new_instance()    File ""/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py"", line 846, in launch_instance      app.start()    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py"", line 499, in start      self.io_loop.start()    File ""/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py"", line 132, in start      self.asyncio_loop.run_forever()    File ""/usr/lib/python3.7/asyncio/base_events.py"", line 541, in run_forever      self._run_once()    File ""/usr/lib/python3.7/asyncio/base_events.py"", line 1786, in _run_once      handle._run()    File ""/usr/lib/python3.7/asyncio/events.py"", line 88, in _run      self._context.run(self._callback, *self._args)    File ""/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py"", line 122, in _handle_events      handler_func(fileobj, events)    File ""/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py"", line 300, in null_wrapper      return fn(*args, **kwargs)    File ""/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py"", line 452, in _handle_events      self._handle_recv()    File ""/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py"", line 481, in _handle_recv      self._run_callback(callback, msg)    File ""/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py"", line 431, in _run_callback      callback(*args, **kwargs)    File ""/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py"", line 300, in null_wrapper      return fn(*args, **kwargs)    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py"", line 283, in dispatcher      return self.dispatch_shell(stream, msg)    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py"", line 233, in dispatch_shell      handler(stream, idents, msg)    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py"", line 399, in execute_request      user_expressions, allow_stdin)    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py"", line 208, in do_execute      res = shell.run_cell(code, store_history=store_history, silent=silent)    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py"", line 537, in run_cell      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)    File ""/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py"", line 2718, in run_cell      interactivity=interactivity, compiler=compiler, result=result)    File ""/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py"", line 2822, in run_ast_nodes      if self.run_code(code, result):    File ""/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py"", line 2882, in run_code      exec(code_obj, self.user_global_ns, self.user_ns)    File ""<ipython-input-32-1bb5ee661811>"", line 13, in <module>      train_step(inp, tar)    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py"", line 150, in error_handler      return fn(*args, **kwargs)    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py"", line 915, in __call__      result = self._call(*args, **kwds)    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py"", line 963, in _call      self._initialize(args, kwds, add_initializers_to=initializers)    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py"", line 786, in _initialize      *args, **kwds))    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 2983, in _get_concrete_function_internal_garbage_collected      graph_function, _ = self._maybe_define_function(args, kwargs)    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 3292, in _maybe_define_function      graph_function = self._create_graph_function(args, kwargs)    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 3140, in _create_graph_function      capture_by_value=self._capture_by_value),    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py"", line 1161, in func_graph_from_py_func      func_outputs = python_func(*func_args, **func_kwargs)    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py"", line 677, in wrapped_fn      out = weak_wrapped_fn().__wrapped__(*args, **kwds)    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py"", line 1143, in autograph_handler      user_requested=True,    File ""<ipython-input-20-949802b44a4a>"", line 5, in train_step      predictions,_ = transformer([inp, tar],    File ""/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler      return fn(*args, **kwargs)    File ""/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py"", line 1096, in __call__      outputs = call_fn(inputs, *args, **kwargs)    File ""/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler      return fn(*args, **kwargs)    File ""<ipython-input-14-ac60aac10b0e>"", line 131, in call      if self.scvs == None:    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1321, in if_stmt      _py_if_stmt(cond, body, orelse)    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1374, in _py_if_stmt      return body() if cond else orelse()    File ""<ipython-input-14-ac60aac10b0e>"", line 133, in call      scvs = tf.expand_dims(scvs, axis=1)    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py"", line 150, in error_handler      return fn(*args, **kwargs)    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py"", line 1082, in op_dispatch_handler      return dispatch_target(*args, **kwargs)    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 435, in expand_dims_v2      return gen_array_ops.expand_dims(input, axis, name)    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 2354, in expand_dims      ""ExpandDims"", input=input, dim=axis, name=name)    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 742, in _apply_op_helper      attrs=attr_protos, op_def=op_def)    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py"", line 695, in _create_op_internal      compute_device)    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 3784, in _create_op_internal      op_def=op_def)    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 2175, in __init__      self._traceback = tf_stack.extract_stack_for_node(self._c_op)The tensor <tf.Tensor 'hi_d_transformer/ExpandDims:0' shape=(64, 1, 64) dtype=float32> cannot be accessed from here, because it was defined in FuncGraph(name=train_step, id=140104354578128), which is out of scope.```I expect the model to compile, I have tried to move the tf.expand_dims to other layers/subclasses and still get the same bug no matter where. I guess this could be rooted from somewhere else as well but I cannot understand the error.**Standalone code to reproduce the issue**Reproducible test case can be found here:  https://colab.research.google.com/gist/ae20cg/fe1577f53f6e68db1e3429643f1e957a/tft2tf2-graph.ipynb
"
54509,1,0,4,0,1,Christinele14,0,"title:AttributeError: module 'tensorflow.python.pywrap_tensorflow' has no attribute 'EqualGraphDefWrapper' description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): The code is from a conda module which is not continually developed.- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): WSL Ubuntu 20.04.3 LTS- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No- TensorFlow installed from: conda package - TensorFlow version (use command below): 2.4.1- TensorFlow-GPU: 2.4.1- Python version: 3.7.11- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version: 7.6.5- GPU model and memory: NVIDIA-SMI 510.00 (4GB)** Description:This code is developed in Python 3.6 and TF1. Now I want to upgrade it to run in Python 3.7 and TF2. I followed the instructions to migrate TF1 to TF2 (https://www.tensorflow.org/guide/migrate/migrate_tf2) and fix almost all errors except this AttributeError. I am wondering how to fix it or do we have similar functions to compare graphs in TF2. Thank you. **Code:@pytest.mark.parametrize('conv_patch', (2, 5, 10), ids=lambda x: 'c=%s' % x)@pytest.mark.parametrize('pool_patch', (2, 3), ids=lambda x: 'p=%s' % x)def test_convolve3D(conv_patch, pool_patch):    from tfbio.net import hidden_conv3D, convolve3D    out_chnls = [8, 16]    g1 = tf.Graph()    with g1.as_default():        x = tf.compat.v1.placeholder(tf.float32, shape=(None, 21, 21, 21, 19))        h11 = hidden_conv3D(x, out_chnls[0], conv_patch=conv_patch,                            pool_patch=pool_patch, name='conv0')        h12 = hidden_conv3D(h11, out_chnls[1], conv_patch=conv_patch,                            pool_patch=pool_patch, name='conv1')    def1 = g1.as_graph_def().SerializeToString()    g2 = tf.Graph()    with g2.as_default():        x = tf.compat.v1.placeholder(tf.float32, shape=(None, 21, 21, 21, 19))        h2 = convolve3D(x, out_chnls, conv_patch=conv_patch,                        pool_patch=pool_patch)    def2 = g2.as_graph_def().SerializeToString()    # graphs should be identical    assert not pywrap_tensorflow.EqualGraphDefWrapper(def1, def2)**Describe the current behavior**FAILED net_test.py::test_convolve3D[p=2-c=2] - AttributeError: module 'tensorflow.python.pywrap_tensorflow' has no attribute 'EqualGraphDefWrapper'**Describe the expected behavior**Pass the pytest
"
54500,1,12265,1,0,0,ahlzouao,0,"title:Libtensorflowlite.so build failed for aarch64 with Flex Delegate included in dependencies description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux 0be7898f8b6f 5.4.144+ from Google Colab**- Mobile device: **Raspberry Pi Aarch64 target architecture**- TensorFlow installed from (source or binary): **Source**- TensorFlow version (use command below): **2.7.0**- Python version: **3.7.12**- Bazel version (if compiling from source): **3.7.2****Describe the current behavior**I'm trying to build a shared library for tensorflow-lite including the Flex Delegate by Modifying the BUILD to add following dependency:```""//tensorflow/lite/delegates/flex:delegate"",```Since I'm targeting a Linux system running on my aarch64 board, the command I ran is the below: ```bazel build --config=monolithic --config=elinux_aarch64 --define=with_select_tf_ops=true -c opt //tensorflow/lite:libtensorflowlite.so```**Describe the expected behavior**The expected behavior is a build completing successfully. **Standalone code to reproduce the issue**I provide a gist from google Colab to make you able to reproduce the issue. **Other info / logs**```ERROR: /root/.cache/bazel/_bazel_root/889612a75a81b3d8b4ed860522ba4e34/external/com_github_grpc_grpc/BUILD:1897:16: C++ compilation of rule '@com_github_grpc_grpc//:grpc_transport_chttp2_server_secure' failed (Exit 1): aarch64-linux-gnu-gcc failed: error executing command /root/.cache/bazel/_bazel_root/889612a75a81b3d8b4ed860522ba4e34/external/aarch64_linux_toolchain/bin/aarch64-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections ... (remaining 67 argument(s) skipped)In file included from /usr/include/openssl/evp.h:16,                 from /usr/include/openssl/x509.h:18,                 from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,                 from external/com_github_grpc_grpc/src/core/lib/security/context/security_context.h:28,                 from external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/secure/server_secure_chttp2.cc:35:/usr/include/openssl/bio.h:690:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_1_0' DEPRECATEDIN_1_1_0(int BIO_get_port(const char *str, unsigned short *port_ptr)) ^~~~~~~~~~~~~~~~~~In file included from /usr/include/openssl/asn1.h:23,                 from /usr/include/openssl/objects.h:15,                 from /usr/include/openssl/evp.h:28,                 from /usr/include/openssl/x509.h:18,                 from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,                 from external/com_github_grpc_grpc/src/core/lib/security/context/security_context.h:28,                 from external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/secure/server_secure_chttp2.cc:35:/usr/include/openssl/bn.h:183:43: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'? int BN_abs_is_word(const BIGNUM *a, const BN_ULONG w);                                           ^~~~~~~~                                           SHA_LONG/usr/include/openssl/bn.h:186:39: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'? int BN_is_word(const BIGNUM *a, const BN_ULONG w);                                       ^~~~~~~~                                       SHA_LONG/usr/include/openssl/bn.h:214:22: error: 'BN_ULONG' was not declared in this scope int BN_num_bits_word(BN_ULONG l);                      ^~~~~~~~/usr/include/openssl/bn.h:214:22: note: suggested alternative: 'SHA_LONG' int BN_num_bits_word(BN_ULONG l);                      ^~~~~~~~                      SHA_LONG/usr/include/openssl/bn.h:266:1: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'? BN_ULONG BN_mod_word(const BIGNUM *a, BN_ULONG w); ^~~~~~~~ SHA_LONG/usr/include/openssl/bn.h:267:1: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'? BN_ULONG BN_div_word(BIGNUM *a, BN_ULONG w); ^~~~~~~~ SHA_LONG/usr/include/openssl/bn.h:268:28: error: 'BN_ULONG' has not been declared int BN_mul_word(BIGNUM *a, BN_ULONG w);                            ^~~~~~~~/usr/include/openssl/bn.h:269:28: error: 'BN_ULONG' has not been declared int BN_add_word(BIGNUM *a, BN_ULONG w);                            ^~~~~~~~/usr/include/openssl/bn.h:270:28: error: 'BN_ULONG' has not been declared int BN_sub_word(BIGNUM *a, BN_ULONG w);                            ^~~~~~~~/usr/include/openssl/bn.h:271:28: error: 'BN_ULONG' has not been declared int BN_set_word(BIGNUM *a, BN_ULONG w);                            ^~~~~~~~/usr/include/openssl/bn.h:272:1: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'? BN_ULONG BN_get_word(const BIGNUM *a); ^~~~~~~~ SHA_LONG/usr/include/openssl/bn.h:288:37: error: 'BN_ULONG' has not been declared int BN_mod_exp_mont_word(BIGNUM *r, BN_ULONG a, const BIGNUM *p,                                     ^~~~~~~~/usr/include/openssl/bn.h:323:24: error: variable or field 'BN_consttime_swap' declared void void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);                        ^~~~~~~~/usr/include/openssl/bn.h:323:24: error: 'BN_ULONG' was not declared in this scope/usr/include/openssl/bn.h:323:24: note: suggested alternative: 'SHA_LONG' void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);                        ^~~~~~~~                        SHA_LONG/usr/include/openssl/bn.h:323:46: error: expected primary-expression before '*' token void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);                                              ^/usr/include/openssl/bn.h:323:47: error: 'a' was not declared in this scope void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);                                               ^/usr/include/openssl/bn.h:323:57: error: expected primary-expression before '*' token void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);                                                         ^/usr/include/openssl/bn.h:323:58: error: 'b' was not declared in this scope void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);                                                          ^/usr/include/openssl/bn.h:323:61: error: expected primary-expression before 'int' void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);                                                             ^~~/usr/include/openssl/bn.h:332:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_0_9_8' DEPRECATEDIN_0_9_8(int ^~~~~~~~~~~~~~~~~~/usr/include/openssl/bn.h:403:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_0_9_8' DEPRECATEDIN_0_9_8(int BN_get_params(int which)) /* 0, mul, 1 high, 2 low, 3 ^~~~~~~~~~~~~~~~~~In file included from /usr/include/openssl/objects.h:15,                 from /usr/include/openssl/evp.h:28,                 from /usr/include/openssl/x509.h:18,                 from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,                 from external/com_github_grpc_grpc/src/core/lib/security/context/security_context.h:28,                 from external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/secure/server_secure_chttp2.cc:35:/usr/include/openssl/asn1.h:555:7: error: expected constructor, destructor, or type conversion before 'unsigned' const unsigned char *ASN1_STRING_get0_data(const ASN1_STRING *x);       ^~~~~~~~In file included from /usr/include/openssl/x509.h:22,                 from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,                 from external/com_github_grpc_grpc/src/core/lib/security/context/security_context.h:28,                 from external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/secure/server_secure_chttp2.cc:35:/usr/include/openssl/ec.h:274:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_2_0' DEPRECATEDIN_1_2_0(int EC_GROUP_get_curve_GFp(const EC_GROUP *group, BIGNUM *p, ^~~~~~~~~~~~~~~~~~/usr/include/openssl/ec.h:543:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_2_0' DEPRECATEDIN_1_2_0(int EC_POINT_get_affine_coordinates_GFp(const EC_GROUP *group, ^~~~~~~~~~~~~~~~~~/usr/include/openssl/ec.h:631:1: error: expected constructor, destructor, or type conversion before 'size_t' size_t EC_POINT_point2oct(const EC_GROUP *group, const EC_POINT *p, ^~~~~~In file included from /usr/include/openssl/x509.h:25,                 from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,                 from external/com_github_grpc_grpc/src/core/lib/security/context/security_context.h:28,                 from external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/secure/server_secure_chttp2.cc:35:/usr/include/openssl/rsa.h:239:1: error: expected constructor, destructor, or type conversion before 'int' int RSA_generate_key_ex(RSA *rsa, int bits, BIGNUM *e, BN_GENCB *cb); ^~~In file included from /usr/include/openssl/dsa.h:25,                 from /usr/include/openssl/x509.h:26,                 from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,                 from external/com_github_grpc_grpc/src/core/lib/security/context/security_context.h:28,                 from external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/secure/server_secure_chttp2.cc:35:/usr/include/openssl/dh.h:142:1: error: expected constructor, destructor, or type conversion before 'int' int DH_generate_parameters_ex(DH *dh, int prime_len, int generator, ^~~In file included from /usr/include/openssl/x509.h:26,                 from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,                 from external/com_github_grpc_grpc/src/core/lib/security/context/security_context.h:28,                 from external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/secure/server_secure_chttp2.cc:35:/usr/include/openssl/dsa.h:103:1: error: expected constructor, destructor, or type conversion before 'int' int DSA_sign(int type, const unsigned char *dgst, int dlen, ^~~/usr/include/openssl/dsa.h:127:1: error: expected constructor, destructor, or type conversion before 'int' int DSA_generate_parameters_ex(DSA *dsa, int bits, ^~~In file included from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,                 from external/com_github_grpc_grpc/src/core/lib/security/context/security_context.h:28,                 from external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/secure/server_secure_chttp2.cc:35:/usr/include/openssl/x509.h:728:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_1_0' DEPRECATEDIN_1_1_0(ASN1_TIME *X509_CRL_get_nextUpdate(X509_CRL *crl)) ^~~~~~~~~~~~~~~~~~Target //tensorflow/lite:libtensorflowlite.so failed to buildUse --verbose_failures to see the command lines of failed build steps.INFO: Elapsed time: 916.628s, Critical Path: 16.49sINFO: 2033 processes: 1075 internal, 958 local.FAILED: Build did NOT complete successfully```Thank you in advance for your support.
"
54487,0,108,0,1,0,nikitamaia,0,"title:ConnectionError when downloading deep_weeds dataset description:I'm unable to use the [deep_weeds dataset](https://www.tensorflow.org/datasets/catalog/deep_weeds) from TensorFlow Datasets. Seems like this has happened in the past [#44053](https://github.com/tensorflow/tensorflow/issues/44053)```import tensorflow as tfimport tensorflow_datasets as tfdsdata, info = tfds.load(name='deep_weeds')```Results in `ConnectionError`[See gist here](https://colab.research.google.com/gist/nikitamaia/fdd8654acd6d6d889585f60895f158f5/untitled4.ipynb)
"
54480,0,0,0,0,0,reemaljaber,0,"title:cant open pages description:the website downloads  pages instead of opening them 
"
54478,0,0,9,0,0,PeraltaFede,0,"title:Web Page problem description:When trying to access some pages of tensorflow it downloads an xml instead of showing the webpage![image](https://user-images.githubusercontent.com/19721178/155134500-416d0005-29e5-404f-bc8b-0bfb6358a2ab.png)
"
54475,0,698,0,0,0,ArrowIntoTheSky,0,"title:Error message of `tf.nn.gelu` with `uint16` input is misleading  description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.7.0- Python version: 3.8- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source):N/A- CUDA/cuDNN version: N/A- GPU model and memory: N/A**Standalone code to reproduce the issue**```import tensorflow as tffeatures = tf.zeros([3, 4], dtype=tf.uint16)tf.nn.gelu(features)```Throws `TypeError````TypeError: Cannot convert 0.5 to EagerTensor of dtype uint16```**Describe the current behavior**The current message is misleading, as it seems to be some computation error. If `tf.nn.gelu` does not accept `uint16` inputs, the message should be a standard message like```Value for attr 'T' of uint16 is not in the list of allowed values:```Similar to `tf.nn.crelu`:```import tensorflow as tffeatures = tf.zeros([3, 4], dtype=tf.uint16)tf.nn.crelu(features)# InvalidArgumentError: Value for attr 'T' of uint16 is not in the list of allowed values: bfloat16, half, float, double, int8, int16, int32, int64, complex64, complex128; NodeDef: {{node Neg}}; Op<name=Neg; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]> [Op:Neg]```**Describe the expected behavior**`tf.nn.gelu` should have better error message in this case.
"
54445,1,2171,70,0,0,trungnt13,0,"title:Save and load a model with PReLU activation results in error description:There is very specific scenarios when PReLU activation failed during `tf.keras.models.load`- Create and save the model via keras.Model.save- Load the model, make modifications to the Layer that including the PReLU activation- Save the modified Model and load it again, PReLU layer failed during `build` when initializing the `alpha` parametersNote: this issue disappear when I subclassing the `PReLU` that is replacing all `PReLU` with `ParamReLU`or specify the `share_axes` argument also solve the problem```pythonclass ParamReLU(PReLU):    ...```---**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 20.04**, **Windows 10**- TensorFlow installed from (source or binary): **binary**- TensorFlow version (use command below): **2.6.0, 2.6.1, 2.7.0, 2.8.0**; **v2.8.0-rc1-32-g3f878cff5b6 2.8.0**- Python version: **3.7.10**- CUDA/cuDNN version: **11.2/8.10**- GPU model and memory: **RTX 3080 / 10GB****[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): **no**- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**```pythonimport tensorflow as tffrom tensorflow.keras.models import Modelfrom tensorflow.keras.layers import Conv2D, Input, PReLUx = Input((1, 1, 1))y = Conv2D(8, 1)(x)y = PReLU()(y)y = Conv2D(40, 1)(y)model = Model(x, y)model.save('/tmp/model_abc')model_new = tf.keras.models.load_model('/tmp/model_abc')layer1 = model_new.get_layer('conv2d')layer2 = model_new.get_layer('p_re_lu')layer3 = model_new.get_layer('conv2d_1')# === prune the last filter of layer1cfg = layer1.get_config()cfg['filters'] = 7layer1_copy = layer1.__class__.from_config(cfg)layer1_copy.build((None, 1, 1, 1))layer1_copy.set_weights([layer1.kernel.numpy()[..., :7],                         layer1.bias.numpy()[:7]])y = layer1_copy(model_new.inputs[0])# remove the last alpha of PReLUlayer2_copy = layer2.__class__.from_config(layer2.get_config())layer2_copy.build(y.shape)layer2_copy.set_weights([layer2.alpha.numpy()[..., :7]])y = layer2_copy(y)# remove 1 input filter layer3cfg = layer3.get_config()layer3_copy = layer3.__class__.from_config(cfg)layer3_copy.build(y.shape)layer3_copy.set_weights([layer3.kernel.numpy()[..., :7, :],                         layer3.bias.numpy()])y = layer3_copy(y)model_prune = Model(model_new.inputs, y)# everything OK hereprint(model_prune.summary())# === error happening here when we save and load the model againmodel_prune.save('/tmp/model_abc_prune')model_prune_new = tf.keras.models.load_model('/tmp/model_abc_prune')```**Other info / logs** ```Traceback (most recent call last):  File ""..."", line 44, in <module>    model_prune_new = tf.keras.models.load_model('/tmp/model_abc_prune')  File ""/home/trung/miniconda3/envs/denoise/lib/python3.7/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler    raise e.with_traceback(filtered_tb) from None  File ""/home/trung/miniconda3/envs/denoise/lib/python3.7/site-packages/keras/initializers/initializers_v2.py"", line 145, in __call__    return tf.zeros(shape, dtype)ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.```
"
54436,1,3564,0,0,1,jacoblubecki,0,"title:Using `+` in custom residual Keras `Layer` does not create correct model graph description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.6- TensorFlow installed from (source or binary): Binary- TensorFlow version (use command below): v2.8.0-rc1-32-g3f878cff5b6 2.8.0- Python version: 3.7.12Also reproducible on various hosted Jupyter environments (Kaggle, Colab) with and without GPU.**Describe the current behavior**In custom residual block implementation with keras APIs, `+` yields broken graph.```pythondef call(self, x, training=False):    for block in self.blocks:        h = x        for conv in block:            h = conv(h, training=training)        x = x + h    return x```This is the resulting graph:![res-block-broken](https://user-images.githubusercontent.com/7884451/154602227-346a939a-275f-4508-84ff-8b0163e5ac13.png)**Describe the expected behavior**This code produces the correct graph, where each `add` is a separate instance of `keras.layers.Add`.The `+` operator should produce the same graph.```pythondef call(self, x, training=False):    for block, add in zip(self.blocks, self.adds):        h = x        for conv in block:            h = conv(h, training=training)        x = add([x, h])    return x```![res-block-working](https://user-images.githubusercontent.com/7884451/154602398-cd61f319-cfee-4bcc-89a3-b1b50e49109d.png)I will add that this isn't just a visualization issue. My model would not train until after I identified this problem and applied the fixed implementation described above. This was causing serious issues with my gradients and the model could not learn because it was too deep without the skip connections.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): I would consider it, but leaning towards no.- Briefly describe your candidate solution(if contributing): N/A**Standalone code to reproduce the issue**```pythonfrom typing import Optional, Tuple, Unionimport tensorflow as tfimport tensorflow.nnimport tensorflow.keras.backend as Kimport tensorflow.keras.layers as Lfrom tensorflow import kerasdef plot_model(model, shape):    inputs = keras.Input(shape[1:])    ones = tf.ones(shape)    model(ones)  # I think needed to properly init graph for plotting    outputs = model.call(inputs)    wrapped_model = keras.Model(inputs, outputs)    return tensorflow.keras.utils.plot_model(        wrapped_model, expand_nested=True, show_shapes=True)class ConvBnAct(L.Layer):    def __init__(        self,        out_channels: int,        kernel_size: Union[int, Tuple[int]],        stride: Union[int, Tuple[int]],        activation: Optional[str] = 'swish',        use_bias=False,        use_batch_norm=True,        data_format='channels_last'            ):        super().__init__()        self.out_channels = out_channels        self.kernel_size = kernel_size        self.stride = stride        self.use_bias = use_bias        self.data_format = data_format        self.activation = L.Activation(activation)        self.act_type = activation        bn_axis = 1 if data_format == 'channels_first' else -1        self.batch_norm = L.BatchNormalization(            axis=bn_axis) if use_batch_norm else None    def build(self, input_shape):        self.conv = L.Conv2D(            self.out_channels,            self.kernel_size,            input_shape=input_shape[1:],            padding='same',            strides=self.stride,            activation=None,            use_bias=self.use_bias,            data_format=self.data_format,            )    def call(self, inputs, training=False):        x = self.conv(inputs)        if self.batch_norm:            x = self.batch_norm(x, training=training)        if self.activation:            x = self.activation(x)        return xclass ResBlock(L.Layer):    def __init__(        self,        blocks: int,        shortcut=True,        data_format='channels_last'    ):        super().__init__()        self.n_blocks = blocks        self.shortcut = shortcut        self.data_format = data_format    def build(self, input_shape):        channel_axis = 1 if self.data_format == 'channels_first' else -1        channels = input_shape[channel_axis]        self.blocks = []        for i in range(self.n_blocks):            block = [                ConvBnAct(channels, kernel_size=1, stride=1, data_format=self.data_format),                ConvBnAct(channels, kernel_size=3, stride=1, data_format=self.data_format)                ]            self.blocks.append(block)    def call(self, x, training=False):        for block in self.blocks:            h = x            for conv in block:                h = conv(h, training=training)            x = x + h if self.shortcut else h        return xif __name__ == '__main__':    i = keras.Input((24, 24, 3))    r = ResBlock(2, True)    plot_model(r, (1, 24, 24, 3))```
"
54435,0,0,31,0,1,bmorledge-hampton19,0,"title:Module structure not recognized by Visual Studio Code Linter description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11 10.0.22000- TensorFlow installed from (source or binary): through pip?- TensorFlow version (use command below): 2.8.0- Python version: 3.10.2I am very perplexed by tensorflow's module structure.  I recently started using tensorflow in Visual Studio Code and immediately ran into a problem where imports from tensorflow.keras cannot be resolved by Pylance.  For example, the ""layers"" module is not recognized from the line `from tensorflow.keras import layers`.  Interestingly enough, the code runs fine despite this error, but the lack of support from the linter makes writing code very difficult.  (For reference, I am using the ""Probabilistic Bayesian Neural Networks"" example script.)My attempts to fix this issue led to a number of other discoveries which still have me confused:- Using `from tensorflow import keras` is recognized by the linter, but the linter still can't offer any helpful predictions off of the keras module.  In this instance, replacing references to `layers` with `keras.layers` still work, despite no indication from the linter that they should.  - Importing keras directly (2.8.0) instead of through tensorflow is recognized, and `keras.layers` is still valid, but hints from linting are still not present, and other parts of the code will break unexpectedly.  For example, a call to `keras.optimizers.RMSprop`, is invalid even though `keras.optimizers` is recognized and both are recognized if keras is imported through tensor flow.- Importing keras through tensorflow.python behaves similarly to directly importing keras.- Even though `keras.layers` is not recognized by the linter after importing just keras, the linter does recognize `from keras import layers` and offers completions for layers after the fact.- `from tensorflow import keras` was recognized in tensorflow 2.7.0.  This issue only started when I updated.Admittedly, my understanding of packaging and linting in Python is somewhat limited, but I've never had issues like this with any other package I've worked with.  Am I missing something obvious here?  Is this a known issue?  If this kind of structure is intentional, what is the rationale behind it?  Are there known workarounds/resources that I could use to better understand this issue?Thanks in advance for the help.
"
54415,0,550,0,0,1,ArrowIntoTheSky,0,"title:`tf.histogram_fixed_width_bins` lack checking for `nbins` description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.7.0- Python version: 3.8- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source):N/A- CUDA/cuDNN version: N/A- GPU model and memory: N/A**Standalone code to reproduce the issue**```import tensorflow as tfnbins = -16value_range = [0.0, 5.0]new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]indices = tf.histogram_fixed_width_bins(new_values, value_range, nbins=nbins)indices.numpy()```Outputs:```array([0, 0, 0, 0, 0, 0], dtype=int32)```**Describe the current behavior**`tf.histogram_fixed_width_bins` has an argument `nbins` which should be a **positive** integer. However, it does not perform any validity checking and can accept a **negative** value like `-16`.  `tf.histogram_fixed_width` (another API with similar functionality) can detect this error and raise an `InvalidArgumentError`:```import tensorflow as tfnbins = -16value_range = [0.0, 5.0]new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]indices = tf.histogram_fixed_width(new_values, value_range, nbins=nbins)indices.numpy()# InvalidArgumentError: nbins should be a positive number, but got '-16' [Op:HistogramFixedWidth]```**Describe the expected behavior**`tf.histogram_fixed_width_bins` should have better input checking.
"
54413,0,247,0,0,0,ArrowIntoTheSky,0,"title:`tf.compat.as_bytes` does not check the encoding string description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.7.0- Python version: 3.8- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source):N/A- CUDA/cuDNN version: N/A- GPU model and memory: N/A**Standalone code to reproduce the issue**```import tensorflow as tfbytes_or_text = ""hello""encoding = ""valid""t1 = tf.compat.as_text(bytes_or_text, encoding=encoding)print(t1) # hellot2 = tf.compat.as_bytes(bytes_or_text,encoding=encoding)# LookupError: unknown encoding: valid```**Describe the current behavior**`""valid""` is not valid value for `encoding`, as we can see that `tf.compat.as_bytes` would throw an `LoopupError`. However, `tf.compat.as_text` does not perform any validity checking and can accept it and even give an output.**Describe the expected behavior**`tf.compat.as_text` should check the validity of `encoding`.
"
54412,0,467,0,0,0,ArrowIntoTheSky,0,"title:`tf.boolean_mask` lack checking for bool arguments description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.7.0- Python version: 3.8- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source):N/A- CUDA/cuDNN version: N/A- GPU model and memory: N/A**Standalone code to reproduce the issue**```import tensorflow as tftensor = [0,1,2,3]mask = tf.random.uniform([4], dtype=tf.float64)tf.boolean_mask(tensor, mask) # Outputs: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 1, 2, 3], dtype=int32)>```**Describe the current behavior**`tf.boolean_mask` has an argument `mask` which should be a `bool` tensor. However, it does not perform any validity checking and can accept a `float64` value. **Describe the expected behavior**`tf.boolean_mask` should check the dtype of input tensor `mask`.For example, `tf.math.reduce_any` would check the first argument and throw an `InvalidArgumentError` for non-boolean inputs.```import tensorflow as tfinput_tensor = tf.random.uniform([4], dtype=tf.float64)tf.math.reduce_any(input_tensor) # InvalidArgumentError: cannot compute Any as input #0(zero-based) was expected to be a bool tensor but is a double tensor [Op:Any]```
"
54411,0,594,0,0,0,ArrowIntoTheSky,0,"title:`tf.math.atan` lack support for `complex64` description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.7.0- Python version: 3.8- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source):N/A- CUDA/cuDNN version: N/A- GPU model and memory: N/A**Standalone code to reproduce the issue**```import tensorflow as tfx = tf.complex(tf.random.uniform([8, 8], dtype=tf.float32),tf.random.uniform([8, 8], dtype=tf.float32))print(x.dtype) # <dtype: 'complex64'>tf.math.atan(x)```**Describe the current behavior**`tf.math.atan` cannot accept a tensor of type `complex64`. However, according to the [document](https://www.tensorflow.org/api_docs/python/tf/math/atan?hl=en) it should support `complex64` and `complex128`.For the above code snippet, the error message is:```NotFoundError: Could not find device for node: {{node Atan}} = Atan[T=DT_COMPLEX64]All kernels registered for op Atan:  device='GPU'; T in [DT_DOUBLE]  device='GPU'; T in [DT_FLOAT]  device='GPU'; T in [DT_HALF]  device='GPU'; T in [DT_BFLOAT16]  device='CPU'; T in [DT_DOUBLE]  device='CPU'; T in [DT_FLOAT]  device='CPU'; T in [DT_BFLOAT16]  device='CPU'; T in [DT_HALF] [Op:Atan]```
"
54405,1,895,0,0,0,sonnykurniawan,0,"title:The model size after Lite conversion is much larger than the original Tensorflow model description:### 1. System information- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10- TensorFlow installation (pip package or built from source): pip install- TensorFlow library (version, if pip package or github SHA, if built from source): 2.8.0### 2. Code```import tensorflow as tfimport tensorflow_text as textconverter = tf.lite.TFLiteConverter.from_saved_model(    saved_model_dir='chatter_engine',    signature_keys=['serving_default'])  # path to the SavedModel directoryprint('after load')converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]converter.target_spec.supported_types = [tf.float16]print('before conversion')tflite_model = converter.convert()print('after conversion')# Save the model.with open('model.tflite', 'wb') as f:    f.write(tflite_model)print('after write')``````- Tensorflow model : https://drive.google.com/drive/folders/1WrNca1DQ9xpbH00JAVQKE8PsxFzB60qj?usp=sharing- Tensorflow lite model: https://drive.google.com/file/d/1nqpkM3ShOTit37BHZ1A5DYEDCjaaac6J/view?usp=sharing```### 3. Failure after conversionIf the conversion is successful, but the generated model is wrong, then state what is wrong: the converted model size is more than 400 MB where the original model is only 4 MB
"
54392,0,0,0,0,0,code-review-doctor,0,"title:Some tests misusing assertTrue for comparisons description:`assertTrue` is not for comparing arguments, should use `assertEqual` for that.The developer's intent of the test was to compare argument 1 with argument 2, which is not happening. Really what is happening is the test is passing because first argument is truthy. The correct method to use is assertEqual. [more details](https://codereview.doctor/features/python/best-practice/avoid-misusing-unittest-assert-true)https://github.com/tensorflow/tensorflow/blob/d9e5d3c634c2ab1221e7d30b562c48b0b0e110b7/tensorflow/python/autograph/pyct/cfg_test.py#L85https://github.com/tensorflow/tensorflow/blob/d9e5d3c634c2ab1221e7d30b562c48b0b0e110b7/tensorflow/python/distribute/distributed_variable_test.py#L383https://github.com/tensorflow/tensorflow/blob/d9e5d3c634c2ab1221e7d30b562c48b0b0e110b7/tensorflow/python/eager/def_function_xla_jit_test.py#L1182https://github.com/tensorflow/tensorflow/blob/d9e5d3c634c2ab1221e7d30b562c48b0b0e110b7/tensorflow/python/keras/mixed_precision/loss_scale_optimizer_test.py#L1007https://github.com/tensorflow/tensorflow/blob/d9e5d3c634c2ab1221e7d30b562c48b0b0e110b7/tensorflow/python/ops/parallel_for/control_flow_ops_test.py#L2549I found this issue automatically, see other issues [here](https://codereview.doctor/tensorflow/tensorflow)
"
54361,1,852,5,0,0,shishaochen,0,"title:[BUG] Random initializer produces different values inside/outside a gradient tape. description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04- TensorFlow installed from (source or binary):  Binary- TensorFlow version (use command below): 2.8- Python version: 3.8**Describe the current behavior**As the script below shows, no matter API version of random intializer is v1 or v2, values produced are different between inside a gradient tape or not. ```pythonimport numpy as npimport tensorflow as tfw1_init = tf.compat.v1.random_normal_initializer(mean=0., stddev=1., seed=3)w1_val = w1_init(shape=[4,5], dtype=tf.float64)w2_init = tf.initializers.RandomNormal(mean=0., stddev=1., seed=3)w2_val = w2_init(shape=[4,5], dtype=tf.float64)# assert np.allclose(w1_val, w2_val), 'W1 == W2?'with tf.GradientTape() as tape:    w3_init = tf.compat.v1.random_normal_initializer(mean=0., stddev=1., seed=3)    w3_val = w3_init(shape=[4,5], dtype=tf.float64)    w3_plus_one = w3_val + 1.# assert np.allclose(w1_val, w3_val), 'W1 == W3?'with tf.GradientTape() as tape:    w4_init = tf.initializers.RandomNormal(mean=0., stddev=1., seed=3)    w4_val = w4_init(shape=[4,5], dtype=tf.float64)    w4_plus_one = w4_val + 1.# assert np.allclose(w2_val, w4_val), 'W2 == W4?'```**Describe the expected behavior**We want `w1_val == w3_val` and `w2_val == w4_val`, then  reproducity can be ensured.Due to API version change, `w1_val != w2_val` can be accepted.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no
"
54349,1,21195,33,0,0,FalsoMoralista,0,"title:Error with ""tf.keras.layers.Normalization"" when using multiple GPU devices description:**System information**Tensorflow 2.7.0 (gpu)Python 3.8Linux ubuntu.### Context:I am trying to use a normalization layer as demonstrated in load model code below:The code works normally as expected when removing ```the tf.keras.layers.Normalization``` layer and keeping the Rescaling layer. Therefore i would discard any problem related to input pipeline, tfrecord parsing, incorrect label format, etc...  ***The error occurs at the end of the epoch at validation inference.***# Edit: I found out that the problem is related to running in multiple gpu devices. I created this [gist](https://colab.research.google.com/drive/1-6POGaRSpMhTu5gPO2XC_mtLil7eYXa6?usp=sharing) in order to test tensorflow/python versions and everything ran normally. I have even installed the exact same python version (3.8.10) to verify whether it would be the case but worked with no errors. Then, back in the original enviroment I've limited the number of gpu devices to 1 and the code ran normally.[tfrecords](https://drive.google.com/drive/folders/1iwmkIV93KAwTh3ML0ghcy0tn8u3Ny1wH?usp=sharing) for reproducing.### Code: ```Pythondef load_and_configure_model(optimizer, loss, metrics, path):  model = ResNet50V2(include_top=True, weights='imagenet')  transfer_layer = model.get_layer('avg_pool')  resnet_submodel = Model(inputs=model.input,outputs=transfer_layer.output)  model_config = resnet_submodel.get_config()    submodel = model_config['layers']  submodel.remove(submodel[0]) # Remove the previous input layer    input_layer = keras.Input(shape=(224, 224, 3), dtype='float32',name=""input"") # Create a new input layer  normalization = tf.keras.layers.Normalization(mean=[118.662, 119.194, 96.877], variance=[2769.232, 2633.742, 2702.492], axis=-1, dtype='float32')(input_layer)  rescaling = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset=-1, dtype='float32')(normalization)      new_model = Model(inputs=input_layer,outputs=rescaling) # Declare pre-processing model to be merged with the ResNet model.  new_model_cfg = new_model.get_config()         new_model_cfg['layers'].extend(submodel) # Merge two models.  # Replace the previous input layer with the output from the preprocessing model  # (Connect the preprocessing model to the resnet)   output_name = new_model_cfg['layers'][2]['name'] # Get the output layer name (rescaling).    new_model_cfg['layers'][3]['inbound_nodes'] = [[[output_name, 0, 0, {}]]] # Replace last inbound node name with the preprocessing model layer name.      new_model = new_model.__class__.from_config(new_model_cfg, custom_objects={})  # change custom objects if necessary  # Set back pre-trained weights on new model  weights = [layer.get_weights() for layer in resnet_submodel.layers[1:]] # For each layer (after the input_layer) in the original resnet50:  for layer, weight in zip(new_model.layers[3:], weights): # Set imagenet weights on each new model layer.      layer.set_weights(weight)  for layer in new_model.layers[:]:    layer.trainable = False  for layer in new_model.layers[:]:          trainable = True    layer.trainable = trainable # Train everything.   transfer_layer = new_model.get_layer('avg_pool')  #dropout = tf.keras.layers.Dropout(rate=0.3)(transfer_layer.output)  species = Dense(1000, activation='softmax', dtype='float32',name='species')(transfer_layer.output) # Specify dtype for handling mixed precision specifications.  model = keras.Model(      inputs=[new_model.inputs],      outputs=[species],  )  if not path == None :    model.load_weights(path)  model.compile(optimizer=optimizer, loss=loss, metrics=metrics)  return model```obs: the same normalization/scaling pipeline works just fine and as expected when creating a model cosisting of these two layers only as exemplified below:```Pythonnormalization = tf.keras.layers.Normalization(mean=[118.662, 119.194, 96.877], variance=[2769.232, 2633.742, 2702.492], axis=-1)(input_layer)rescaling = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset=-1)(normalization)model_2 = Model(inputs=input_layer,outputs=rescaling)pred = model_2.predict(img)pred = tf.cast(pred, tf.uint8)pred = tf.squeeze(pred,axis=0)pred = tf.io.encode_jpeg(pred)fname = tf.constant('norm_then_scale.jpg')fwrite = tf.io.write_file(fname, pred)```### Traceback:```Epoch 1/702022-02-11 17:12:35.280711: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 83022022-02-11 17:12:35.671694: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 83022022-02-11 17:12:37.630704: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once. 95/100 [===========================>..] - ETA: 2s - loss: 6.9175 - categori2022-02-11 17:13:46.316950: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: required broadcastable shapesTraceback (most recent call last):  File ""flat_resnet50.py"", line 263, in <module>    history = train_model(train_path, validation_path, epochs, steps_per_epoch, resnet_50V2)  File ""flat_resnet50.py"", line 82, in train_model    history = model.fit(x=train_dataset,  File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 67, in error_handler    raise e.with_traceback(filtered_tb) from None  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 58, in quick_execute    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,tensorflow.python.framework.errors_impl.InvalidArgumentError: 3 root error(s) found.  (0) INVALID_ARGUMENT:  required broadcastable shapes	 [[node replica_1/model_2/normalization/sub (defined at /usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py:257)]]	 [[div_no_nan/ReadVariableOp_1/_62]]  (1) INVALID_ARGUMENT:  required broadcastable shapes	 [[node replica_1/model_2/normalization/sub (defined at /usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py:257)]]	 [[div_no_nan/AddN/_76]]  (2) INVALID_ARGUMENT:  required broadcastable shapes	 [[node replica_1/model_2/normalization/sub (defined at /usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py:257)]]0 successful operations.0 derived errors ignored. [Op:__inference_test_function_65650]Errors may have originated from an input operation.Input Source operations connected to node replica_1/model_2/normalization/sub:In[0] cond/Identity_1 (defined at /usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1355)	In[1] model_2/normalization/sub/y:Operation defined at: (most recent call last)>>>   File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap>>>     self._bootstrap_inner()>>> >>>   File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner>>>     self.run()>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1349, in run_step>>>     outputs = model.test_step(data)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1303, in test_step>>>     y_pred = self(x, training=False)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__>>>     outputs = call_fn(inputs, *args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 451, in call>>>     return self._run_internal_graph(>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 589, in _run_internal_graph>>>     outputs = node.layer(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__>>>     outputs = call_fn(inputs, *args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py"", line 257, in call>>>     return ((inputs - self.mean) />>> Input Source operations connected to node replica_1/model_2/normalization/sub:In[0] cond/Identity_1 (defined at /usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1355)	In[1] model_2/normalization/sub/y:Operation defined at: (most recent call last)>>>   File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap>>>     self._bootstrap_inner()>>> >>>   File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner>>>     self.run()>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1349, in run_step>>>     outputs = model.test_step(data)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1303, in test_step>>>     y_pred = self(x, training=False)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__>>>     outputs = call_fn(inputs, *args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 451, in call>>>     return self._run_internal_graph(>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 589, in _run_internal_graph>>>     outputs = node.layer(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__>>>     outputs = call_fn(inputs, *args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py"", line 257, in call>>>     return ((inputs - self.mean) />>> Input Source operations connected to node replica_1/model_2/normalization/sub:In[0] cond/Identity_1 (defined at /usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1355)	In[1] model_2/normalization/sub/y:Operation defined at: (most recent call last)>>>   File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap>>>     self._bootstrap_inner()>>> >>>   File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner>>>     self.run()>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1349, in run_step>>>     outputs = model.test_step(data)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1303, in test_step>>>     y_pred = self(x, training=False)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__>>>     outputs = call_fn(inputs, *args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 451, in call>>>     return self._run_internal_graph(>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 589, in _run_internal_graph>>>     outputs = node.layer(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__>>>     outputs = call_fn(inputs, *args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py"", line 257, in call>>>     return ((inputs - self.mean) />>> Function call stack:test_function -> test_function -> test_functionError in sys.excepthook:Traceback (most recent call last):  File ""/usr/lib/python3/dist-packages/apport_python_hook.py"", line 72, in apport_excepthook    from apport.fileutils import likely_packaged, get_recent_crashes  File ""/usr/lib/python3/dist-packages/apport/__init__.py"", line 5, in <module>    from apport.report import Report  File ""/usr/lib/python3/dist-packages/apport/report.py"", line 32, in <module>    import apport.fileutils  File ""/usr/lib/python3/dist-packages/apport/fileutils.py"", line 27, in <module>    from apport.packaging_impl import impl as packaging  File ""/usr/lib/python3/dist-packages/apport/packaging_impl.py"", line 23, in <module>    import apt  File ""/usr/lib/python3/dist-packages/apt/__init__.py"", line 35, in <module>    apt_pkg.init_config()apt_pkg.Error: E:Syntax error /etc/apt/apt.conf.d/20auto-upgrades:6: Extra junk at end of fileOriginal exception was:Traceback (most recent call last):  File ""flat_resnet50.py"", line 263, in <module>    history = train_model(train_path, validation_path, epochs, steps_per_epoch, resnet_50V2)  File ""flat_resnet50.py"", line 82, in train_model    history = model.fit(x=train_dataset,  File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 67, in error_handler    raise e.with_traceback(filtered_tb) from None  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 58, in quick_execute    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,tensorflow.python.framework.errors_impl.InvalidArgumentError: 3 root error(s) found.  (0) INVALID_ARGUMENT:  required broadcastable shapes	 [[node replica_1/model_2/normalization/sub (defined at /usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py:257)]]	 [[div_no_nan/ReadVariableOp_1/_62]]  (1) INVALID_ARGUMENT:  required broadcastable shapes	 [[node replica_1/model_2/normalization/sub (defined at /usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py:257)]]	 [[div_no_nan/AddN/_76]]  (2) INVALID_ARGUMENT:  required broadcastable shapes	 [[node replica_1/model_2/normalization/sub (defined at /usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py:257)]]0 successful operations.0 derived errors ignored. [Op:__inference_test_function_65650]Errors may have originated from an input operation.Input Source operations connected to node replica_1/model_2/normalization/sub:In[0] cond/Identity_1 (defined at /usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1355)	In[1] model_2/normalization/sub/y:Operation defined at: (most recent call last)>>>   File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap>>>     self._bootstrap_inner()>>> >>>   File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner>>>     self.run()>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1349, in run_step>>>     outputs = model.test_step(data)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1303, in test_step>>>     y_pred = self(x, training=False)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__>>>     outputs = call_fn(inputs, *args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 451, in call>>>     return self._run_internal_graph(>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 589, in _run_internal_graph>>>     outputs = node.layer(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__>>>     outputs = call_fn(inputs, *args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py"", line 257, in call>>>     return ((inputs - self.mean) />>> Input Source operations connected to node replica_1/model_2/normalization/sub:In[0] cond/Identity_1 (defined at /usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1355)	In[1] model_2/normalization/sub/y:Operation defined at: (most recent call last)>>>   File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap>>>     self._bootstrap_inner()>>> >>>   File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner>>>     self.run()>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1349, in run_step>>>     outputs = model.test_step(data)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1303, in test_step>>>     y_pred = self(x, training=False)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__>>>     outputs = call_fn(inputs, *args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 451, in call>>>     return self._run_internal_graph(>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 589, in _run_internal_graph>>>     outputs = node.layer(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__>>>     outputs = call_fn(inputs, *args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py"", line 257, in call>>>     return ((inputs - self.mean) />>> Input Source operations connected to node replica_1/model_2/normalization/sub:In[0] cond/Identity_1 (defined at /usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1355)	In[1] model_2/normalization/sub/y:Operation defined at: (most recent call last)>>>   File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap>>>     self._bootstrap_inner()>>> >>>   File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner>>>     self.run()>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1349, in run_step>>>     outputs = model.test_step(data)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1303, in test_step>>>     y_pred = self(x, training=False)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__>>>     outputs = call_fn(inputs, *args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 451, in call>>>     return self._run_internal_graph(>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 589, in _run_internal_graph>>>     outputs = node.layer(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__>>>     outputs = call_fn(inputs, *args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler>>>     return fn(*args, **kwargs)>>> >>>   File ""/usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py"", line 257, in call>>>     return ((inputs - self.mean) />>> Function call stack:test_function -> test_function -> test_function100/100 [==============================] - ETA: 0s - loss: 6.9095 - categorical_accuracy: 0.0055```
"
54341,1,2175,23,0,0,arjun-majumdar,0,"title:ValueError: No gradients provided for any variable: description:I am using TensorFlow 2.6.0 and Python 3.9. I am attempting to implement a Variational Autoencoder toy example using MNIST dataset with Convolutional Neural Network as encoder and decoder. You can refer to the complete Jupyter notebook [here](https://github.com/arjun-majumdar/Autoencoders_Experiments/blob/master/Testing-VAE_TF2.ipynb).For some reason, on using GradientTape for training - in this case, computing the gradients with respect to the trainable parameters of the defined model, it keeps giving **ValueError: No gradients provided for any variable:** error message.The exact lines of code are:```with tf.GradientTape() as tape:    total_loss = compute_total_loss(        data = X, reconstruction = X_recon,        mu = mu, log_var = log_var,        alpha = 1    )    grads = tape.gradient(total_loss, model.trainable_weights)type(grads), len(grads)# (list, 16)# No gradients are computed!for x in grads:    print(x)'''NoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNone'''optimizer.apply_gradients(zip(grads, model.trainable_weights))""""""ValueError                                Traceback (most recent call last)~\AppData\Local\Temp/ipykernel_232/111942921.py in <module>----> 1 optimizer.apply_gradients(zip(grads, model.trainable_weights))~\anaconda3\envs\tf-cpu\lib\site-packages\tensorflow\python\keras\optimizer_v2\optimizer_v2.py in apply_gradients(self, grads_and_vars, name, experimental_aggregate_gradients)    639       RuntimeError: If called in a cross-replica context.    640     """"""--> 641     grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)    642     var_list = [v for (_, v) in grads_and_vars]    643 ~\anaconda3\envs\tf-cpu\lib\site-packages\tensorflow\python\keras\optimizer_v2\utils.py in filter_empty_gradients(grads_and_vars)     73      74   if not filtered:---> 75     raise ValueError(""No gradients provided for any variable: %s."" %     76                      ([v.name for _, v in grads_and_vars],))     77   if vars_with_empty_grads:ValueError: No gradients provided for any variable: ['vae_1/encoder_4/conv2d_8/kernel:0', 'vae_1/encoder_4/conv2d_8/bias:0', 'vae_1/encoder_4/conv2d_9/kernel:0', 'vae_1/encoder_4/conv2d_9/bias:0', 'vae_1/decoder_3/dense_9/kernel:0', 'vae_1/decoder_3/dense_9/bias:0', 'vae_1/decoder_3/conv2d_transpose_9/kernel:0', 'vae_1/decoder_3/conv2d_transpose_9/bias:0', 'vae_1/decoder_3/conv2d_transpose_10/kernel:0', 'vae_1/decoder_3/conv2d_transpose_10/bias:0', 'vae_1/decoder_3/conv2d_transpose_11/kernel:0', 'vae_1/decoder_3/conv2d_transpose_11/bias:0', 'vae_1/dense_10/kernel:0', 'vae_1/dense_10/bias:0', 'vae_1/dense_11/kernel:0', 'vae_1/dense_11/bias:0'].""""""```Is this a bug? Is it the case that tf.GradientTape() API is somehow not computing the gradients?
"
54336,1,2039,284,0,0,markub3327,0,"title:tf.TensorArray as a FIFO ??? description:Hello,  [here](https://github.com/keras-team/keras/issues/16015) I was pointed to use `tf.TensorArray` instead of `tf.Variable` or `tf.queue.FIFOQueue` for making FIFO contained in custom layer. Is it an effective way? Exist any alternative here?If it's the most effective method how can I replace `self.queue.assign(tf.concat([self.queue[timesteps:, :], inputs], axis=0))` with methods of `tf.TensorArray`?## Code```pythonclass FIFOLayer(Layer):    def __init__(self, window_size, **kwargs):        super(FIFOLayer, self).__init__(**kwargs)        self.window_size = window_size        self.count = 0    def build(self, input_shape):        super(FIFOLayer, self).build(input_shape)        self.queue = self.add_weight(            name=""queue"",            shape=(self.window_size, input_shape[-1]),            initializer=tf.initializers.Constant(value=np.nan),            trainable=False,        )    def call(self, inputs, training):        timesteps = tf.shape(inputs)[0]        # check if batch_size is more than queue capacity        if timesteps > self.window_size:            raise ValueError()        # 1. append new state to queue        self.queue.assign(tf.concat([self.queue[timesteps:, :], inputs], axis=0))        self.count += timesteps        # 2. feed-forward        if self.count < self.window_size:            # generate mask            attention_mask = tf.cast(                tf.math.reduce_all(                    tf.math.logical_not(tf.math.is_nan(self.queue)), axis=-1                ),                dtype=tf.float32,            )            attention_mask = tf.matmul(                attention_mask[..., tf.newaxis],                attention_mask[..., tf.newaxis],                transpose_b=True,            )            return self.queue[tf.newaxis, ...], attention_mask        # !!! check overflow        elif self.count > self.window_size:            self.count = self.window_size        return self.queue[tf.newaxis, ...], None    @property    def is_full(self):        return self.count == self.window_size    def clear(self):        self.count = 0        self.queue.assign(tf.fill(self.queue.shape, np.nan))l = FIFOLayer(window_size=10)for i in range(6):    x = tf.random.normal((2, 12))    y = l(x)    print(y)print(l.is_full, ""\n\n"")l.clear()print(l(x))print(l.is_full, ""\n\n"")```Thanks a lot for your time.Have a nice day.
"
54317,0,225,0,0,0,ArrowIntoTheSky,0,"title:`tf.math.asin` lack support for complex description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.7.0- Python version: 3.8- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source):N/A- CUDA/cuDNN version: N/A- GPU model and memory: N/A**Standalone code to reproduce the issue**```import tensorflow as tfx = tf.complex(tf.random.uniform([4], dtype=tf.float64),tf.random.uniform([4], dtype=tf.float64))print(tf.math.asin(x))# Could not find device for node: {{node Asin}} = Asin[T=DT_COMPLEX128]```**Expected output**According to the document [tf.math.asin](https://www.tensorflow.org/api_docs/python/tf/math/asin), it should be able to accept a complex input.
"
54296,1,11985,0,0,0,amirjamez,0,"title:saved_model_aot_compile.py removes unwanted tensors from signature description:**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04- TensorFlow installed from (source or binary): Binary- TensorFlow version (use command below): 2.4.1- Python version: 3.6.9- CUDA/cuDNN version:  11.1- GPU model and memory: 12 GB**Describe the current behavior**When I load the frozen graph using `tf.saved_model.load(""saved_model.pb"")`, the signature shows all the tensor names, but when `.local/lib/python3.6/site-packages/tensorflow/python/tools/saved_model_aot_compile.py` does the `_prune_removed_feed_nodes(signature_def, graph_def)`, it prunes multiple tensors that it does not find in `graph_dev.node` and as a result my final deployed model is incorrect:```def _prune_removed_feed_nodes(signature_def, graph_def):  """"""Identify the inputs in the signature no longer in graph_def, prune them.  Args:    signature_def: A `SignatureDef` instance.    graph_def: A `GraphDef` instance.  Returns:    A new pruned `SignatureDef`.  """"""  node_names = set([n.name for n in graph_def.node])  new_signature_def = meta_graph_pb2.SignatureDef()  new_signature_def.CopyFrom(signature_def)  for (k, v) in signature_def.inputs.items():    tensor_name, _ = _parse_tensor_name(v.name)    if tensor_name not in node_names:    濠?logging.warn(    濠?  濠?'Signature input key \'{}\', tensor name \'{}\', has been pruned '    濠?  濠?'while freezing the graph.  Removing it from the compiled signatures.'    濠?  濠?.format(k, tensor_name))    濠?del new_signature_def.inputs[k]  return new_signature_def``` Here are my other TF related packages:```tensorboard             2.6.0tensorboard-data-server 0.6.1tensorboard-plugin-wit  1.7.0tensorflow              2.4.1tensorflow-addons       0.11.2tensorflow-estimator    2.4.0tensorflow-probability  0.12.2tf-agents               0.7.1tf-estimator-nightly    2.4.0.dev2020102201```Also, `2.4.1` was used to create the model: ```>>> imported<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7f6bcc4f7a58>>>> imported.tensorflow_version'2.4.1'```Here is the debug output:```coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 511.41GiB/s2022-02-06 21:56:34.286796: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.02022-02-06 21:56:34.290840: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.112022-02-06 21:56:34.290891: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.112022-02-06 21:56:34.293434: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.102022-02-06 21:56:34.293855: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.102022-02-06 21:56:34.296799: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.102022-02-06 21:56:34.297732: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.112022-02-06 21:56:34.297943: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.82022-02-06 21:56:34.299759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 02022-02-06 21:56:34.299802: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.02022-02-06 21:56:35.075324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:2022-02-06 21:56:35.075387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      02022-02-06 21:56:35.075400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N2022-02-06 21:56:35.078385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11119 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-12GB, pci bus id: $000:82:00.0, compute capability: 6.0)2022-02-06 21:56:35.096700: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2194810000 Hz2022-02-06 21:56:35.236522: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:592] model_pruner failed: Invalid argument: Graph does not contain terminal node StatefulPartitionedCall_2.2022-02-06 21:56:35.247615: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize  model_pruner: Graph size after: 38 nodes (-2), 48 edges (0), time = 0.987ms.  implementation_selector: Graph size after: 38 nodes (0), 48 edges (0), time = 0.562ms.  function_optimizer: Graph size after: 343 nodes (305), 581 edges (533), time = 22.39ms.  common_subgraph_elimination: Graph size after: 303 nodes (-40), 541 edges (-40), time = 3.508ms.  constant_folding: Graph size after: 227 nodes (-76), 387 edges (-154), time = 55.232ms.  shape_optimizer: shape_optimizer did nothing. time = 0.41ms.  arithmetic_optimizer: Graph size after: 238 nodes (11), 398 edges (11), time = 4.174ms.  layout: Graph size after: 238 nodes (0), 398 edges (0), time = 5.838ms.  remapper: Graph size after: 238 nodes (0), 398 edges (0), time = 1.459ms.  loop_optimizer: Graph size after: 238 nodes (0), 397 edges (-1), time = 1.714ms.  dependency_optimizer: Graph size after: 156 nodes (-82), 221 edges (-176), time = 3.391ms.  memory_optimizer: Graph size after: 156 nodes (0), 221 edges (0), time = 6.835ms.  model_pruner: Invalid argument: Graph does not contain terminal node StatefulPartitionedCall_2.  implementation_selector: Graph size after: 156 nodes (0), 221 edges (0), time = 0.468ms.  function_optimizer: function_optimizer did nothing. time = 0.127ms.  common_subgraph_elimination: Graph size after: 146 nodes (-10), 211 edges (-10), time = 1.021ms.  constant_folding: Graph size after: 146 nodes (0), 211 edges (0), time = 3.151ms.  shape_optimizer: shape_optimizer did nothing. time = 0.133ms.  arithmetic_optimizer: Graph size after: 146 nodes (0), 211 edges (0), time = 2.64ms.  remapper: Graph size after: 146 nodes (0), 211 edges (0), time = 0.8ms.  dependency_optimizer: Graph size after: 146 nodes (0), 211 edges (0), time = 1.752ms.2022-02-06 21:56:35.281080: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set2022-02-06 21:56:35.282047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:pciBusID: 0000:82:00.0 name: Tesla P100-PCIE-12GB computeCapability: 6.0coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 511.41GiB/s2022-02-06 21:56:35.282083: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.02022-02-06 21:56:35.282137: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.112022-02-06 21:56:35.282155: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.112022-02-06 21:56:35.282172: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.102022-02-06 21:56:35.282190: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.102022-02-06 21:56:35.282208: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.102022-02-06 21:56:35.282226: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.112022-02-06 21:56:35.282243: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.82022-02-06 21:56:35.283971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 02022-02-06 21:56:35.284299: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set2022-02-06 21:56:35.285214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:pciBusID: 0000:82:00.0 name: Tesla P100-PCIE-12GB computeCapability: 6.0coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 511.41GiB/s2022-02-06 21:56:35.285237: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.02022-02-06 21:56:35.285258: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.112022-02-06 21:56:35.285277: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.112022-02-06 21:56:35.285295: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.102022-02-06 21:56:35.285311: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.102022-02-06 21:56:35.285328: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.102022-02-06 21:56:35.285345: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.112022-02-06 21:56:35.285363: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.82022-02-06 21:56:35.287113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 02022-02-06 21:56:35.287143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:2022-02-06 21:56:35.287153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      02022-02-06 21:56:35.287161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N2022-02-06 21:56:35.288959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11119 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:82:00.0, compute capability: 6.0)INFO:tensorflow:Restoring parameters from /home/llvm-project/llvm/lib/Analysis/models/inliner/variables/variables2022-02-06 21:56:35.354948: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)WARNING:tensorflow:From /home/.local/lib/python3.6/site-packages/tensorflow/python/tools/saved_model_aot_compile.py:332: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.Instructions for updating:Use `tf.compat.v1.graph_util.convert_variables_to_constants`WARNING:tensorflow:From /home/.local/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py:856: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.Instructions for updating:Use `tf.compat.v1.graph_util.extract_sub_graph`WARNING:tensorflow:Signature input key 'XXX', tensor name 'action_XXX', has been pruned while freezing the graph.  Removing it from the compiled signatures.WARNING:tensorflow:Signature input key 'discount', tensor name 'action_discount', has been pruned while freezing the graph.  Removing it from the compiled signatures.WARNING:tensorflow:Signature input key 'XXX', tensor name 'action_XXX', has been pruned while freezing the graph.  Removing it from the compiled signatures.WARNING:tensorflow:Signature input key 'reward', tensor name 'action_reward', has been pruned while freezing the graph.  Removing it from the compiled signatures.WARNING:tensorflow:Signature input key 'step_type', tensor name 'action_step_type', has been pruned while freezing the graph.  Removing it from the compiled signatures.WARNING:tensorflow:Signature input key 'inlining_default', tensor name 'action_inlining_default', has been pruned while freezing the graph.  Removing it from the compiled signatures.INFO:tensorflow:Writing graph def to: /tmp/saved_model_clilxj7nh6h/frozen_graph.pbINFO:tensorflow:Writing config_pbtxt to: /tmp/saved_model_clilxj7nh6h/config.pbtxtINFO:tensorflow:Generating XLA AOT artifacts in: /home/llvm-project/build/lib/Analysis```
"
54281,1,1816,107,0,0,tranvansang,0,"title:train_step method of custom model is not called in graph execution (i.e., non eager) mode description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, but I modified only a minor portion from the stock example.- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ArchLinux.- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA.- TensorFlow installed from (source or binary): via `pip install tensorflow tensorflow-gpu keras`, probably from binary.- TensorFlow version (use command below): `v2.8.0-rc1-32-g3f878cff5b6 2.8.0`- Python version: `Python 3.9.7`- Bazel version (if compiling from source): NA- GCC/Compiler version (if compiling from source): NA- CUDA/cuDNN version: CUDA 11.5- GPU model and memory: NVIDIA GeForce GTX 1060 6GB.**Describe the current behavior**Custom model's `train_step` is not being used in non-eager execution mode.**Describe the expected behavior**Custom model's `train_step` is used regardless of whether eager execution is enabled or not.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**```pythonimport kerasimport numpy as npimport tensorflow as tfclass CustomModel(keras.Model):	def train_step(self, data):		return {m.name: m.result() for m in self.metrics}if __name__ == '__main__':	# config	# tf.compat.v1.enable_eager_execution()	tf.compat.v1.disable_eager_execution()	print(""TensorFlow version: {}"".format(tf.__version__))	print(""Eager execution: {}"".format(tf.executing_eagerly()))	# Construct and compile an instance of CustomModel	inputs = keras.Input(shape=(32,))	outputs = keras.layers.Dense(1)(inputs)	model = CustomModel(inputs, outputs)	model.compile(optimizer=""adam"", loss=""mse"", metrics=[""mae""])	# Just use `fit` as usual	x = np.random.random((1000, 32))	y = np.random.random((1000, 1))	print(model.evaluate(x, y))	model.fit(x, y, epochs=3)	print(model.evaluate(x, y))```**Other info / logs**When eager execution is enabled, `train_step` gets called, which means the model **isn't** trained as **expected**.```32/32 [==============================] - 0s 1ms/step - loss: 0.3040 - mae: 0.4428[0.3039644658565521, 0.442813515663147]Epoch 1/332/32 [==============================] - 0s 874us/step - loss: 0.0000e+00 - mae: 0.0000e+00Epoch 2/332/32 [==============================] - 0s 810us/step - loss: 0.0000e+00 - mae: 0.0000e+00Epoch 3/332/32 [==============================] - 0s 762us/step - loss: 0.0000e+00 - mae: 0.0000e+0032/32 [==============================] - 0s 1ms/step - loss: 0.3040 - mae: 0.4428[0.3039644658565521, 0.442813515663147]```When eager execution is disabled, `train_step` is ignored, and the model is trained normally and `train_step` is ignored.This is **not expected**.```[0.26861959040164946, 0.41127136]Train on 1000 samplesEpoch 1/31000/1000 [==============================] - 0s 71us/sample - loss: 0.2598 - mae: 0.4037Epoch 2/31000/1000 [==============================] - 0s 40us/sample - loss: 0.2432 - mae: 0.3912Epoch 3/31000/1000 [==============================] - 0s 37us/sample - loss: 0.2296 - mae: 0.3805[0.2220638926625252, 0.3743403]```related issues: #45922 #40880the snippet is modified from stock example [here](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit/#a_first_simple_example).
"
54276,0,320,194,1,0,bersbersbers,0,"title:Deterministic GPU implementation of unsorted segment reduction op not available on Windows description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): see below- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 21H2- TensorFlow installed from (source or binary): from PyPI- TensorFlow version (use command below): v2.8.0-rc1-32-g3f878cff5b6 2.8.0- Python version: 3.10.2- CUDA/cuDNN version: 11.2, 8.1.1- GPU model and memory: GeForce RTX 2060**Describe the current behavior**The code below works on Linux, but not on Windows where I am seeing> tensorflow.python.framework.errors_impl.UnimplementedError: Graph execution error:> Detected at node 'UnsortedSegmentSum_1' defined at (most recent call last):> Node: 'UnsortedSegmentSum_1'> Deterministic GPU implementation of unsorted segment reduction op not available.>          [[{{node UnsortedSegmentSum_1}}]] [Op:__inference_train_function_517]**Describe the expected behavior**It works on both OSs.**Standalone code to reproduce the issue**```pythonimport tensorflow as tftf.random.set_seed(0)tf.config.experimental.enable_op_determinism()data = tf.ones((1, 1))layer = tf.keras.layers.Input(shape=[1])model = tf.keras.models.Model(inputs=layer, outputs=layer)model.compile(loss=""categorical_crossentropy"", metrics=""AUC"")model.fit(x=data, y=data)```This is due to the `AUC` metric as discussed in https://github.com/tensorflow/tensorflow/issues/51978. It was resolved for Linux, but not Windows in https://github.com/tensorflow/tensorflow/pull/51861. A workaround is given by `set TF_DISABLE_SEGMENT_REDUCTION_OP_DETERMINISM_EXCEPTIONS=True`. I am posting a new issue here as recommended in https://github.com/tensorflow/tensorflow/issues/39751#issuecomment-982919265.
"
54274,0,6284,25,0,0,andmis,0,"title:AutoGraph could not transform <function ...> description:```INFO:tensorflow:<function train_main.<locals>.train at 0x2976dd700> is not cached for subkey ConversionOptions[{}]INFO:tensorflow:Source code of <function train_main.<locals>.train at 0x2976dd700>:@tf.functiondef train(batch):    s    = batch[:, start_s:    end_s]    a    = batch[:, start_a:    end_a]    s_   = batch[:, start_s_:   end_s_]    r    = batch[:, start_r:    end_r]    done = batch[:, start_done: end_done]    noise = tf.random.normal([args.batch_size, env.action_dim])    a_, log_闂傚倸鍊烽悞锔锯偓绗涘懎鍨濋幖鎼厛閺?= actor([s, noise])    y = r + args.gamma * (1-done) * (    tf.minimum(critic_1_target([s_, a_]),    critic_2_target([s_, a_]))    - args.alpha * log_闂傚倸鍊烽悞锔锯偓绗涘懎鍨濋幖鎼厛閺?    with tf.GradientTape() as tape:        MSBE_1 = (1/args.batch_size) * tf.reduce_sum((critic_1([s, a]) - y)**2)    MSBE_1_grads = tape.gradient(MSBE_1, critic_1.trainable_weights)    critic_1_optimizer.apply_gradients(zip(MSBE_1_grads, critic_1.trainable_weights))    with tf.GradientTape() as tape:        MSBE_2 = (1/args.batch_size) * tf.reduce_sum((critic_2([s, a]) - y)**2)    MSBE_2_grads = tape.gradient(MSBE_2, critic_2.trainable_weights)    critic_2_optimizer.apply_gradients(zip(MSBE_2_grads, critic_2.trainable_weights))    noise = tf.random.normal([args.batch_size, env.action_dim])    with tf.GradientTape() as tape:        log_闂傚倸鍊烽悞锔锯偓绗涘懎鍨濋幖鎼厛閺佸鎲搁弮鍫濊摕? tf.Tensor        a_闂?, log_闂傚倸鍊烽悞锔锯偓绗涘懎鍨濋幖鎼厛閺佸鎲搁弮鍫濊摕?= actor([s, noise])        expected_reward = (1/args.batch_size) *                 tf.reduce_sum(critic_1([s, a_闂傚倸鍊烽懗鍫曞储瑜斿畷顖炲锤濡も偓绾? - args.alpha * log_闂傚倸鍊烽悞锔锯偓绗涘懎鍨濋幖鎼厛閺佸鎲搁弮鍫濊摕?        neg_expected_reward = -expected_reward    expected_reward_grad = tape.gradient(neg_expected_reward, actor.trainable_weights)    actor_optimizer.apply_gradients(zip(expected_reward_grad, actor.trainable_weights))    polyak_average(critic_1_target.variables, critic_1.variables)    polyak_average(critic_2_target.variables, critic_2.variables)    if args.model_name:        MSBE_1_log(MSBE_1)        MSBE_2_log(MSBE_2)        expected_reward_log(expected_reward)INFO:tensorflow:Error transforming entity <function train_main.<locals>.train at 0x2976dd700>Traceback (most recent call last):  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 433, in converted_call    converted_f = _convert_actual(target_entity, program_ctx)  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 275, in _convert_actual    transformed, module, source_map = _TRANSPILER.transform(entity, program_ctx)  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 286, in transform    return self.transform_function(obj, user_context)  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 470, in transform_function    nodes, ctx = super(PyToPy, self).transform_function(fn, user_context)  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 363, in transform_function    result = self.transform_ast(node, context)  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 243, in transform_ast    node = self.initial_analysis(node, ctx)  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 231, in initial_analysis    node = activity.resolve(node, ctx, None)  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py"", line 709, in resolve    return ActivityAnalyzer(context, parent_scope).visit(node)  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transformer.py"", line 445, in visit    result = super(Base, self).visit(node)  File ""/opt/homebrew/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ast.py"", line 407, in visit    return visitor(node)  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py"", line 601, in visit_FunctionDef    node.body = self.visit_block(node.body)  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transformer.py"", line 340, in visit_block    replacement = self.visit(node)  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transformer.py"", line 445, in visit    result = super(Base, self).visit(node)  File ""/opt/homebrew/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ast.py"", line 407, in visit    return visitor(node)  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py"", line 651, in visit_With    node = self.generic_visit(node)  File ""/opt/homebrew/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ast.py"", line 483, in generic_visit    value = self.visit(value)  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transformer.py"", line 445, in visit    result = super(Base, self).visit(node)  File ""/opt/homebrew/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ast.py"", line 407, in visit    return visitor(node)  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py"", line 394, in visit_AnnAssign    node.value = self.visit(node.value)  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transformer.py"", line 431, in visit    raise ValueError(msg)ValueError: invalid value for ""node"": expected ""ast.AST"", got ""<class 'NoneType'>""; to visit lists of nodes, use ""visit_block"" insteadWARNING:tensorflow:AutoGraph could not transform <function train_main.<locals>.train at 0x2976dd700> and will run it as-is.Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.Cause: invalid value for ""node"": expected ""ast.AST"", got ""<class 'NoneType'>""; to visit lists of nodes, use ""visit_block"" insteadTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert```**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 12- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow installed from (source or binary): binary (`pip3 install tensorflow-macos`)- TensorFlow version (use command below): `unknown 2.7.0`- Python version: 3.9.9- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source): - CUDA/cuDNN version:- GPU model and memory:You can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior****Describe the expected behavior****[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.Sorry, but I can't provide the full source code.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
54271,1,7582,12,0,0,ghylander,0,"title:model.fit() bug when using a zipped Dataset as input for a multiple-input model description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- TensorFlow installed from (source or binary): pip- TensorFlow version (use command below): 2.9.0.dev20220202- Python version: 3.10.2**Describe the current behavior**I have a custom model which takes 3 images as inputI have 3 separate (currently unbatched as I debug this error) datasets, classes encoded as categorical, meaning each input tensor has shape ((x, y, z), (c,))Trying to input the 3 datasets separately fails, either by inputting them as a dict mapping each ds to a named input `{""Input1"": ds1, {""Input2"": ds2, {""Input3"": ds3}`, or using a list `[ds, ds2, ds3]`I zip the three datasets. Testing the resulting dataset with [(using the docs as guidance)](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#zip):```for element in zipped_ds.as_numpy_iterator():print(""element"", element)```Outputs:```element [[[x1, y1, z1], [c1,]], [[x2, y2, z2], [c2,]], [[x3, y3, z3], [c3,]]] element [[[x1, y1, z1], [c1,]], [[x2, y2, z2], [c2,]], [[x3, y3, z3], [c3,]]] ...```Seems to work, right? Every call to the iterator returns 3 elements.Well, when I use the zipped dataset as input of model_fit(), the first element in the tuple returned by the dataset object is treated as the input for the whole model, meaning that instead of using [[[x1, y1, z1], [c1,]], [[x2, y2, z2], [c2,]], [[x3, y3, z3], [c3,]]] as the input to the model, it uses [[x1, y1, z1], [c1,]], and the training fails.I've tried many approaches, like using `zipped_ds.as_numpy_iterator()` or `([ds1, ds2, ds3] for idx, (ds1, ds2, ds3) in enumerate(zipped_ds))`, but both fail as the returned item is empty**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.```# %%import osimport tensorflow as tf # tensorflow nightly, version>=2.5from tensorflow import kerasfrom tensorflow.image import crop_to_bounding_box as tfimgcropfrom tensorflow.keras.preprocessing import image_dataset_from_directoryBATCH_SIZE=32 # Adjust?IMG_SIZE=(224, 224)IMG_SHAPE = IMG_SIZE + (3,)# %%_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')train_dir = os.path.join(PATH, 'train')validation_dir = os.path.join(PATH, 'validation')train_dataset = tf.keras.preprocessing.image_dataset_from_directory(train_dir,                                             shuffle=False,                                             label_mode='categorical',                                             batch_size=32,                                             image_size=IMG_SIZE)validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(validation_dir,                                             shuffle=False,                                             label_mode='categorical',                                             batch_size=32,                                             image_size=IMG_SIZE)# %%base_model1 = tf.keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,                                               include_top=False,                                               weights='imagenet',                                               minimalistic=False,                                               pooling=max,                                               dropout_rate=0.2)base_model2 = tf.keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,                                               include_top=False,                                               weights='imagenet',                                               minimalistic=False,                                               pooling=max,                                               dropout_rate=0.2)base_model3 = tf.keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,                                               include_top=False,                                               weights='imagenet',                                               minimalistic=False,                                               pooling=max,                                               dropout_rate=0.2)# %%pre_concat_layer1 = tf.keras.layers.Dense(64,                                         activation='relu',                                         kernel_initializer='random_uniform',                                         bias_initializer='zeros')pre_concat_layer2 = tf.keras.layers.Dense(64,                                         activation='relu',                                         kernel_initializer='random_uniform',                                         bias_initializer='zeros')pre_concat_layer3 = tf.keras.layers.Dense(64,                                         activation='relu',                                         kernel_initializer='random_uniform',                                         bias_initializer='zeros')post_concat_layer = tf.keras.layers.Dense(128,                                         activation='relu',                                         kernel_initializer='random_uniform',                                         bias_initializer='zeros')prediction_layer = tf.keras.layers.Dense(2,                                         activation='softmax',                                         kernel_initializer='random_uniform',                                         bias_initializer='zeros')# %%input1 = tf.keras.Input(shape=(64, 64, 3), name=""First"")input2 = tf.keras.Input(shape=(64, 64, 3), name=""Second"")input3 = tf.keras.Input(shape=(64, 64, 3), name=""Third"")x = base_model1(input1, training=False)x = tf.keras.layers.GlobalAveragePooling2D()(x)x = tf.keras.layers.Dropout(0.2)(x)x = tf.keras.layers.BatchNormalization()(x)x = pre_concat_layer1(x)x = tf.keras.layers.Dropout(0.2)(x)outputs = tf.keras.layers.BatchNormalization()(x)body1 = tf.keras.Model(input1, outputs)x = base_model2(input2, training=False)x = tf.keras.layers.GlobalAveragePooling2D()(x)x = tf.keras.layers.Dropout(0.2)(x)x = tf.keras.layers.BatchNormalization()(x)x = pre_concat_layer2(x)x = tf.keras.layers.Dropout(0.2)(x)outputs = tf.keras.layers.BatchNormalization()(x)body2 = tf.keras.Model(input2, outputs)x = base_model3(input3, training=False)x = tf.keras.layers.GlobalAveragePooling2D()(x)x = tf.keras.layers.Dropout(0.2)(x)x = tf.keras.layers.BatchNormalization()(x)x = pre_concat_layer3(x)x = tf.keras.layers.Dropout(0.2)(x)outputs = tf.keras.layers.BatchNormalization()(x)body3 = tf.keras.Model(input3, outputs)# %%body1.get_layer(""MobilenetV3large"")._name = ""MobilenetV3large1""body2.get_layer(""MobilenetV3large"")._name = ""MobilenetV3large2""body3.get_layer(""MobilenetV3large"")._name = ""MobilenetV3large3""# %%combinedInput = tf.keras.layers.concatenate([body1.output, body2.output, body3.output])x = post_concat_layer(combinedInput)x = tf.keras.layers.Dropout(0.2)(x)x = tf.keras.layers.BatchNormalization()(x)foutput = prediction_layer(x)final_model = tf.keras.Model(inputs=[body1.input, body2.input, body3.input], outputs=foutput)# %%def resize_data1(images, classes):    return (tfimgcrop(images,                        offset_height=0,                        offset_width=0,                        target_height=64,                        target_width=64),                    classes)def resize_data2(images, classes):    return (tfimgcrop(images,                        offset_height=0,                        offset_width=64,                        target_height=64,                        target_width=64),                    classes)def resize_data3(images, classes):    return (tfimgcrop(images,                        offset_height=0,                        offset_width=128,                        target_height=64,                        target_width=64),                    classes)# %%train_dataset_unb = train_dataset.unbatch()train_dataset1 = train_dataset_unb.map(resize_data1)train_dataset2 = train_dataset_unb.map(resize_data2)train_dataset3 = train_dataset_unb.map(resize_data3)train_dataset_zip = tf.data.Dataset.zip((train_dataset1, train_dataset2, train_dataset3))validation_dataset_unb = validation_dataset.unbatch()validation_dataset1 = validation_dataset_unb.map(resize_data1)validation_dataset2 = validation_dataset_unb.map(resize_data2)validation_dataset3 = validation_dataset_unb.map(resize_data3)validation_dataset_zip = tf.data.Dataset.zip((validation_dataset1, validation_dataset2, validation_dataset3))# %%final_model.compile()# %%history = final_model.fit(train_dataset_zip,                        epochs=999,                         validation_data=validation_dataset_zip,                        validation_steps=32                        )```
"
54267,1,0,0,0,0,Curry-yilongxu,0,"title:asd description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary):- TensorFlow version (use command below):- Python version:- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version:- GPU model and memory:You can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior****Describe the expected behavior****[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
54266,1,503,7,0,0,jiannanWang,0,"title:The gif encoding and decoding is not lossless in tf and tfio description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): - Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.7.0- Python version: 3.7.11- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version: running on CPU- GPU model and memory: running on CPU**Describe the current behavior**The gif encoding and decoding is not lossless. The input after gif encoding and decoding is not equivalent to the original input.**Describe the expected behavior**The image encoded by gif and then decoded should be lossless.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**need to install tensorflow=2.7.0 and tensorflow_io=0.22.0```import tensorflow as tfimport tensorflow_io as tfioimport numpy as npinput = np.array(    [[[[13, 54, 87,],    [56, 210, 195,],    [230, 135, 61,],],    [[13, 54, 87,],    [56, 210, 195,],    [230, 135, 61,],],    [[13, 54, 87,],    [56, 210, 195,],    [230, 135, 61,]]]])input_uint8 = tf.cast(input, tf.uint8)encoded_file = tfio.image.encode_gif(input_uint8)input_decoded = tf.io.decode_gif(encoded_file) print(np.allclose(input_uint8, input_decoded))  # False```The input after gif encoding and decoding is not equivalent to the original input. But gif itself should be lossless.
"
54248,1,3832,66,0,0,krafczyk,0,"title:Tensorflow accesses .hdf5 weights file after it's been loaded description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux - Redhat 7.9- TensorFlow installed from (source or binary): binary (conda)- TensorFlow version (use command below): On all versions from 2.2 to 2.7 (2.7.0 for the latest)- Python version: 3.8- Bazel version (if compiling from source): 3.7.2- GCC/Compiler version (if compiling from source): 10.2.0**Describe the current behavior**When loading a keras model from a temporary `.hdf5` weights file, the weights are read successfully read into the model, but Tensorflow outputs a DATA_LOSS warning after the `load_weights` method has finished executing indicating that Tensorflow is still trying to read the file.**Describe the expected behavior**I expect Tensorflow to not touch the weights file once the `load_weights` function as returned.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no**Standalone code to reproduce the issue**```import tensorflow as tfimport tensorflow_datasets as tfdsimport tempfileimport osimport shutil# Load MNIST Datasetsprint(""Load MNIST Dataset"")mnist_datasets, ds_info = tfds.load('mnist', as_supervised=True, with_info=True)train_ds = mnist_datasets['train']test_ds = mnist_datasets['test']batch_size = 32def normalize_img(image, label):  """"""Normalizes images: `uint8` -> `float32`.""""""  return tf.cast(image, tf.float32) / 255., labeltry:    autotune_opt = tf.data.AUTOTUNEexcept:    autotune_opt = tf.data.experimental.AUTOTUNEtrain_ds = train_ds.map(    normalize_img, num_parallel_calls=autotune_opt)train_ds = train_ds.cache()train_ds = train_ds.shuffle(ds_info.splits['train'].num_examples)train_ds = train_ds.batch(batch_size)test_ds = test_ds.map(    normalize_img, num_parallel_calls=autotune_opt)test_ds = test_ds.batch(batch_size)test_ds = test_ds.cache()test_ds = test_ds.prefetch(autotune_opt)# Define function to create functional keras modeldef create_keras_model(layer_dims):    inp = tf.keras.layers.Input((28,28,1), name=""Input_Name"")    last_layer = inp    last_layer = tf.keras.layers.Reshape((28*28*1,), name=""Input_Reshape"")(last_layer)    for i in range(len(layer_dims)):        dim = layer_dims[i]        last_layer = tf.keras.layers.Dense(dim, name=f""Layer_{i}"")(last_layer)    outp = tf.keras.layers.Dense(10, name=""Output_Layer"", activation='sigmoid')(last_layer)    return tf.keras.Model(inputs=inp, outputs=outp)# Create and train initial model:print(""Create and train initial keras model"")mdl_1 = create_keras_model([32,32,32])mdl_1.compile(    loss=tf.keras.losses.SparseCategoricalCrossentropy(),    metrics=tf.keras.metrics.SparseCategoricalAccuracy())mdl_1.fit(train_ds, epochs=1)# Create function to test modelsdef eval_on_test(mdl):    num_matches = 0    total_examples = 0    for X, Y in test_ds:        # Eval model        Y_eval = tf.argmax(mdl(X),axis=1).numpy()        # Count        total_examples += len(Y_eval)        num_matches += (Y.numpy() == Y_eval).sum()    return num_matches/total_examplesprint(""Initial model performance"")print(eval_on_test(mdl_1))# Save weights to diskmdl_weight_file = 'weights.hdf5'mdl_1.save_weights(mdl_weight_file)# Create new model and load from a temporary filemdl_2 = create_keras_model([32,32,32])with tempfile.NamedTemporaryFile('w+b') as temp_f:    # Copy content to temporary file    print(""Copy weights to temp file"")    with open(mdl_weight_file, 'rb') as f:        temp_f.write(f.read())    # Flush to disk    temp_f.flush()    print(""Load weights from temp file"")    # Load weights from temp file    mdl_2.load_weights(temp_f.name)print(""temp file gone"")print(""Loaded model performance"")print(eval_on_test(mdl_2))```**Other info / logs**This the output I get by running this code locally:```Load MNIST Dataset2022-02-02 15:45:23.299505: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected2022-02-02 15:45:23.299702: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (iforge123): /proc/driver/nvidia/version does not existCreate and train initial keras model1875/1875 [==============================] - 6s 2ms/step - loss: 0.3753 - sparse_categorical_accuracy: 0.8929 Initial model performance0.9172Copy weights to temp fileLoad weights from temp file2022-02-02 15:45:32.765376: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open /tmp/tmpfpsuz_25: DATA_LOSS: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?temp file goneLoaded model performance0.9172```I tried running this code on colab, however I can't see any of the tensorflow logging lines I see on my local machine.
"
54244,1,1272,1,0,0,ahlzouao,0,"title:On-device-training fails using Tflite_runtime: Node number 54 (FlexReluGrad) failed to prepare. description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Custom embedded Linux distribution (Kernel 5.15)**- Mobile device: **Raspberry Pi 4B**- TensorFlow installed from (source or binary): **Source (Building on Tflite_runtime using pip_package scripts)**- TensorFlow version (use command below): **2.7.0**- Python version: **3.8**- Bazel version (if compiling from source): **N/A**- GCC/Compiler version (if compiling from source): **9.3 (OpenEmbedded GNU Toolchain)**- CUDA/cuDNN version: **N/A**- GPU model and memory: **N/A****Describe the current behavior**I am following the tutorial on how to do [on-device-training](https://www.tensorflow.org/lite/examples/on_device_training/overview). The first step was to create and train the Fashion_mnist model on google Colab which was successful since I managed to download as an output the tflite model after converting (I made sure to mention all the necessary signatures while saving the model). The documentation is providing an example only on Java for android Apps, but I'm trying to explore whether this is feasible with tflite_runtime wheel.I sent then the tflite model to the target (raspberry pi) where the Tflite_runtime has been installed using [this article](https://www.tensorflow.org/lite/guide/build_cmake_pip) and I'm trying to run the training function by feeding my neural network with arrays of zeros just to prove that it is working. This is the code snippet I'm running on my target. ```import numpy as npimport struct as stimport tflite_runtime.interpreter as tflrnImg = 10000nR = 28nC =28images_array = np.zeros((1, 28, 28), dtype=""float32"") labels_array = np.zeros((1,10), dtype=""float32"")modelpath=""fashion_mnist_model.tflite""interpreter = tflr.Interpreter(modelpath)signatures = interpreter.get_signature_list()``````print('Signature:', signatures)>>> Signature: {'infer': {'inputs': ['x'], 'outputs': ['logits', 'output']}, 'restore': {'inputs': ['checkpoint_path'], 'outputs': ['dense_1/bias:0', 'dense_1/kernel:0', 'dense_2/bias:0', 'dense_2/kernel:0']}, 'save': {'inputs': ['checkpoint_path'], 'outputs': ['checkpoint_path']}, 'train': {'inputs': ['x', 'y'], 'outputs': ['loss']}}``````train = interpreter.get_signature_runner('train')infer = interpreter.get_signature_runner('infer')train(x=images_array, y=labels_array)```The script fails at the train function call and return the following traces:````RuntimeError: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_selectNode number 54 (FlexReluGrad) failed to prepare.````**Describe the expected behavior**The expected behavior is to get a complete and functional training on target. Any help would be appreciated, thank you in advance.
"
54232,0,788,48,0,1,dniku,0,"title:tf.data.Dataset.from_generator() does not work with tf.data.experimental.enable_debug_mode() description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab- TensorFlow version (use command below): `v2.7.0-0-gc256c071bb2 2.7.0`- Python version: 3.7.12**Describe the current behavior**`tf.data.Dataset.from_generator()` does not work when `tf.data.experimental.enable_debug_mode()` is called in advance.```pythonimport tensorflow as tftf.data.experimental.enable_debug_mode()def gen():    yield from iter(range(10))dataset = tf.data.Dataset.from_generator(gen, output_signature=tf.TensorSpec(shape=(), dtype=tf.int32))for item in dataset:    print(item)```Results in```  ...  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 810, in get_iterator    return self._iterators[iterator_id]TypeError: unhashable type: 'numpy.ndarray'	 [[{{node EagerPyFunc}}]] [Op:IteratorGetNext]```From a small investigation it seems that the ""obvious"" solution 闂傚倸鍊搁崐鐑芥嚄閸洖纾婚柕濞炬櫅绾惧潡鏌＄仦璇插姎闁告垹濞€閺屽秷顧侀柛鎾跺枛瀵鏁愭径濠傚祮闂佺粯顭囬。浠嬪绩閻烆柎ging https://github.com/tensorflow/tensorflow/blob/ffe6f62b7f8e57177c26ca3b38c0929d5f64b43f/tensorflow/python/data/ops/dataset_ops.py#L833 to```pythonreturn np.int64(ret)```does not fix the issue, even though `np.int64(0)` is hashable, while `np.array(0, dtype=np.int64)` is not.Additionally, I tried a workaround and added to `_GeneratorState` the following method:```python@staticmethoddef _fix_iterator_id(iterator_id):    assert isinstance(iterator_id, np.ndarray)    assert iterator_id.ndim == 0    assert iterator_id.dtype == np.int64    return np.int64(iterator_id)```and added calls to it from `get_iterator()` and `iterator_completed()`. However, this triggered yet another issue: in `tf.python.ops.script_ops`, `FuncRegistry` started throwing an error `ValueError('callback pyfunc_63 is not found')`.**Describe the expected behavior**No crash.**Standalone code to reproduce the issue**https://colab.research.google.com/gist/dniku/80456bc9d30fbaaadca4a22469c2c1df/tf_dataset_from_generator_with_debug_mode_crash.ipynb
"
54208,1,0,1,0,0,jpiedrafita,0,"title:partially initialized module 'tensorflow' has no attribute 'Tensor' (most likely due to a circular import) description:**System information**- Running code from: https://github.com/jpiedrafita/ai_number_read/blob/main/numbers.py闂傚倸鍊搁崐鐑芥倿閿曗偓椤洤鈻庨幘宕囷紮闁荤姴娲ゅ顒€鈽夊杈╃厰濡炪們鍊楅弲顪祅3 numbers.py- OS Platform and Distribution: OSX 10.15.7- TensorFlow installed from: pip install tensorflowAs described in other ""has no attribute"" issues I tried other tf versions usingpip install tensorflow==x.x.x --ignore-installed- TensorFlow version (use command below):闂?pip show tensorflowName: tensorflowVersion: 2.7.0Summary: TensorFlow is an open source machine learning framework for everyone.Home-page: https://www.tensorflow.org/Author: Google Inc.Author-email: packages@tensorflow.orgLicense: Apache 2.0Location: /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packagesRequires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, keras-preprocessing, libclang, numpy, opt-einsum, protobuf, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wheel, wraptRequired-by:闂?pip show tensorflow_datasetsName: tensorflow-datasetsVersion: 4.5.0Summary: tensorflow/datasets is a library of datasets ready to use with TensorFlow.Home-page: https://github.com/tensorflow/datasetsAuthor: Google Inc.Author-email: packages@tensorflow.orgLicense: Apache 2.0Location: /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packagesRequires: absl-py, dill, numpy, promise, protobuf, requests, six, tensorflow-metadata, termcolor, tqdmRequired-by:- Python version 3.9.6**Describe the current behavior**Console error: https://github.com/jpiedrafita/ai_number_read/blob/main/traceback.txtFile ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_datasets/core/utils/type_utils.py"", line 49, in <module>    Tensor = Union[tf.Tensor, tf.SparseTensor, tf.RaggedTensor]AttributeError: partially initialized module 'tensorflow' has no attribute 'Tensor' (most likely due to a circular import)In my code ther isn't any Tensor reference that could make a circular import.**Describe the expected behavior**Simple console output similar to:>Resultado: 0.974**Standalone code to reproduce the issue**https://github.com/jpiedrafita/ai_number_read/blob/main/numbers.py)**Other info / logs**
"
54187,0,0,1,0,0,Northerneye,0,"title:AttributeError: 'float' object has no attribute 'dtype',  When executing tfp.optimizer.differential_evolution_one_step description:This error is shown whenever tf.optimizer.differential_evolution_one_step is called.This error is due to line 125 in the tensorflow optimizer file differential_evolution.py, which is,crossover_prob=0.9In this line, the variable crossover_prob is given as a float whereas it is required to be a tensorflow variable later in the file on line 652, which is,dtype=crossover_prob.dtype.base_dtype,changing crossover_prob to a tf.Variable before line 652 will solve this problem.Also, setting the default value of crossover_prob in line 125 fromcrossover_prob=0.9, tocrossover_prob=tf.Variable(0.9)will also solve this problem and allow for this function to run.Thank you
"
54125,0,208,0,0,1,ArrowIntoTheSky,0,"title:tf.histogram_fixed_width_bins miss input check for `nbins` description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.7.0- Python version: 3.8- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source):N/A- CUDA/cuDNN version: N/A- GPU model and memory: N/A**Standalone code to reproduce the issue**```value_range = [0.0, 5.0]new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]indices = tf.histogram_fixed_width_bins(new_values, value_range, nbins=-5)indices.numpy()```Outputs:```array([0, 0, 0, 0, 0, 0], dtype=int32)```**Expected output**Expected nbins to be a positive interger, and raise an error if it's negative.
"
54094,1,10400,16,0,0,EnderWiggin14,0,"title:Model.fit() batch_size breaks for some batch_sizes description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 and (RHEL-like (custom distro ran by organization))- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): Tensorflow 2.5.0 on Windows 10 , Tensorflow 2.5.0 AND 2.7.0 on RHEL- Python version:  Windows 10 -> python 3.7.6    :   RHEL -> python 3.7.2+Tensorflow 2.5.0 AND python 3.8.2+Tensorflow 2.7.0- CUDA/cuDNN version: CUDA:11.2.0 cuDNN:8.1.1- GPU model and memory: Windows -> nVidia Quadro RTX 3000 : RHEL -> nVidia P100**Describe the current behavior**When utilizing a custom loss function that has been tested to work, it will break with certain batch sizes by claiming that a (None) datatype was received, or rather, the `None` could not be converted to tensor. This is despite the fact that a numpy array of dtype `'float32'` was passed to it.This was first noted in a script that trained two separate autoencoders on two separate datasets, 1- 12800x400 and 1-8000x400, with batch sizes of 128 and 80 respectively. The first would train, but the second would break despite the code for the second autoencoder being a duplicate of the first. The first one in that situation utilized the same custom loss function `ExplainedVar().` This does not occur for the built-in loss functions. The `batch_size` in all cases was smaller than the training set.It should be noted that when it fails, `true` and `pred` reach the loss function as `None`s . This is not a result of the math inside the loss function.Below are the two different errors produced by Tensorflow 2.5.0 and 2.7.0 respectively.**Tensorflow 2.5.0**```Traceback (most recent call last):  File ""divided_models.py"", line 218, in <module>    a_sae.fit(rnd_dat,rnd_dat,epochs=10,batch_size=32,verbose=2)  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/keras/engine/training.py"", line 1184, in fit    tmp_logs = self.train_function(iterator)  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 885, in __call__    result = self._call(*args, **kwds)  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 933, in _call    self._initialize(args, kwds, add_initializers_to=initializers)  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 760, in _initialize    *args, **kwds))  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3066, in _get_concrete_function_internal_garbage_collected    graph_function, _ = self._maybe_define_function(args, kwargs)  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3463, in _maybe_define_function    graph_function = self._create_graph_function(args, kwargs)  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3308, in _create_graph_function    capture_by_value=self._capture_by_value),  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 1007, in func_graph_from_py_func    func_outputs = python_func(*func_args, **func_kwargs)  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 668, in wrapped_fn    out = weak_wrapped_fn().__wrapped__(*args, **kwds)  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 994, in wrapper    raise e.ag_error_metadata.to_exception(e)TypeError: in user code:    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/keras/engine/training.py:853 train_function  *        return step_function(self, iterator)    ./custom_loss_funcs.py:117 call  *        pred_mean = tf.reshape(tf.math.reduce_mean(pred,axis=1),(pred.shape[0],1))    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:206 wrapper  **        return target(*args, **kwargs)    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:196 reshape        result = gen_array_ops.reshape(tensor, shape, name)    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:8404 reshape        ""Reshape"", tensor=tensor, shape=shape, name=name)    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:525 _apply_op_helper        raise err    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:515 _apply_op_helper        preferred_dtype=default_dtype)    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/profiler/trace.py:163 wrapped        return func(*args, **kwargs)    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1566 convert_to_tensor        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py:346 _constant_tensor_conversion_function        return constant(v, dtype=dtype, name=name)    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py:272 constant        allow_broadcast=True)    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py:290 _constant_impl        allow_broadcast=allow_broadcast))    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:553 make_tensor_proto        ""supported type."" % (type(values), values))    TypeError: Failed to convert object of type <class 'tuple'> to Tensor. Contents: (None, 1). Consider casting elements to a supported type.```**Tensorflow 2.7.0**```Traceback (most recent call last):  File ""divided_models3.py"", line 111, in <module>    a_ae.fit(scaled_aData,scaled_aData,epochs=n_epochs,  File ""/usr/WS2/mvander/py3_8/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler    raise e.with_traceback(filtered_tb) from None  File ""/usr/WS2/mvander/py3_8/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 1129, in autograph_handler    raise e.ag_error_metadata.to_exception(e)TypeError: in user code:    File ""/usr/WS2/mvander/py3_8/lib/python3.8/site-packages/keras/engine/training.py"", line 878, in train_function  *        return step_function(self, iterator)    File ""divided_models3.py"", line 23, in call  *        pred_mean = tf.reshape(tf.math.reduce_mean(pred,axis=1),(pred.shape[0],1))    TypeError: Failed to convert elements of (None, 1) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.```**Describe the expected behavior**`Model.fit()` should work for any batch_size > 0 and <= dataset size. Alternatively, it should frankly work for any positive dataset size where a batch_size larger than the dataset size would default to the size of the dataset.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): No**Standalone code to reproduce the issue**```import osimport sysimport numpy as npimport matplotlib.pyplot as pltimport tensorflow as tfimport tensorflow.keras as krsclass ExplainedVar(krs.losses.Loss):    def __init__(self, reduction = krs.losses.Reduction.AUTO, **kwargs):        super( ExplainedVar, self).__init__(reduction = reduction, **kwargs)        # self.counter = 0    def call(self,true,pred):        #print(pred.shape)        #print(true.shape)        #if true.shape[0] is None:        #    raise Exception()        # self.counter += 1        # print(self.counter)        pred = tf.convert_to_tensor(pred)        true = tf.cast(true, pred.dtype)        res = tf.math.reduce_sum( tf.math.square( pred - true ), axis=1 )        pred_mean = tf.reshape(tf.math.reduce_mean(pred,axis=1),(pred.shape[0],1))        tot = tf.math.reduce_sum( tf.math.square( pred - pred_mean ),axis=1)        return res/totdef make_auto(in_dim,string):    enc_in = krs.Input((in_dim,))    x = enc_in    x0_0 = krs.layers.Dense(96,activation='elu',kernel_initializer='lecun_normal')(x)    x1_0 = krs.layers.Dense(48,activation='elu',kernel_initializer='lecun_normal')(x0_0)    x2_0 = krs.layers.Dense(25,activation='elu',kernel_initializer='lecun_normal')(x1_0)    x3_0 = krs.layers.Dense(16,activation='elu',kernel_initializer='lecun_normal')(x2_0)    x0_1 = krs.layers.Dense(12,activation='elu',kernel_initializer='lecun_normal')(x)    x1_1 = krs.layers.Dense(10,activation='elu',kernel_initializer='lecun_normal')(x0_0)    x2_1 = krs.layers.Dense(8,activation='elu',kernel_initializer='lecun_normal')(x1_0)    x3_1 = krs.layers.Dense(6,activation='elu',kernel_initializer='lecun_normal')(x2_0)    x4_1 = krs.layers.Dense(4,activation='elu',kernel_initializer='lecun_normal')(x3_0)    enc_out = krs.layers.Concatenate(axis=1)([x0_1,x1_1,x2_1,x3_1,x4_1])    a_enc = krs.Model( inputs = enc_in, outputs = enc_out, name = ""%s_enc""%string)    dec_in = krs.Input((40,))    x = dec_in    x0 = x[:,:4]    x1 = x[:,4:10]    x2 = x[:,10:18]    x3 = x[:,18:28]    x4 = x[:,28:40]    x4 = krs.layers.Dense(16,activation='elu',kernel_initializer='lecun_normal')(x4)    x3 = krs.layers.Concatenate()([x3,x4])    x3 = krs.layers.Dense(25,activation='elu',kernel_initializer='lecun_normal')(x3)    x2 = krs.layers.Concatenate()([x2,x3])    x2 = krs.layers.Dense(48,activation='elu',kernel_initializer='lecun_normal')(x2)    x1 = krs.layers.Concatenate()([x1,x2])    x1 = krs.layers.Dense(96,activation='elu',kernel_initializer='lecun_normal')(x1)    x0 = krs.layers.Concatenate()([x0,x1])    dec_out = krs.layers.Dense(400,activation='elu',kernel_initializer='lecun_normal')(x0)    a_dec = krs.Model( inputs = dec_in, outputs = dec_out, name = ""%s_dec""%string)    a_sae = krs.Model( inputs = enc_in, outputs = a_dec( a_enc(enc_in)), name = ""%s_sae""%string)    return a_sae, a_enc, a_decdef samp_minmax_transform(xmin,xmax,data):    data = ((data.T - xmin)/(xmax-xmin)).T    return datadef samp_minmax_inverse_transform(xmin,xmax,data):    data = (data.T*(xmax-xmin)+xmin).T    return datadef scale(x):    return np.power(x,1./30.)def descale(x):    return np.power(x,30.)aData = np.random.uniform(0,10.,(5000,400))bData = np.random.uniform(0,10.,(5000,400))scaled_aData = scale(aData)scaled_bData = scale(bData)print(""aData DTYPE :"",aData.dtype)print(""bData DTYPE :"",bData.dtype)###############################################a_ae, a_enc, a_dec = make_auto(400,'a')loss = ExplainedVar()# loss = krs.losses.MSLE #### This works with all batch sizesopt = krs.optimizers.Adam(.0001)a_ae.compile(optimizer=opt,loss=loss)n_epochs = 10batch_size = int(scaled_aData.shape[0]*.8) ### This will break everything with custom loss function# batch_size = int(scaled_aData.shape[0]*.05) #### This make it work with custom loss functiona_ae.fit(scaled_aData,scaled_aData,epochs=n_epochs,        batch_size=batch_size,verbose=2)################################################# THIS SECTION IS TO VERIFY THAT IT ISN'T THE ARCHITECTURE ITSELF# enc_in = krs.Input((400,))# x = krs.layers.Dense(50,activation='elu')(enc_in)# enc_out = krs.layers.Dense(10,activation='elu')(x)# enc = krs.Model(inputs=enc_in,outputs=enc_out,name=""enc"")# dec_in = krs.Input((10,))# x = krs.layers.Dense(50,activation='elu')(dec_in)# dec_out = krs.layers.Dense(400,activation='elu')(x)# dec = krs.Model(inputs=dec_in,outputs=dec_out,name=""dec"")# n_epochs = 10# batch_size = int(scaled_aData.shape[0]*.8)# ae = krs.Model(inputs=enc_in, outputs = dec( enc(enc_in)),name='ae')# loss = ExplainedVar()# opt = krs.optimizers.Adam(.0001)# ae.compile(optimizer=opt,loss=loss)# ae.fit(scaled_aData,scaled_aData,epochs=n_epochs,#         batch_size=batch_size,verbose=2)```
"
54090,1,715,6,0,0,StevenHuang2020,0,"title:LSTM model save warning, Tensorflow 2.7.0 description:**System information**- OS Platform and Distribution: Windows10- TensorFlow installed from: binary- TensorFlow version: v2.7.0-rc1-69-gc256c071bb2 2.7.0- Python version: 3.9.10**Describe the current behavior**When model.save called, the below warning message occurred:```WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000021419A6A820> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.```I just upgraded python from 3.6 to 3.9, reinstall Tensorflow under Python 3.9.10. I found the LSTM model hard to train to reduce loss value and the previous warning information occurred.![image](https://user-images.githubusercontent.com/61686583/151123541-210c3046-b23a-4a09-812d-34bb2dde9ad3.png)
"
54067,1,1217,194,0,0,bersbersbers,0,"title:Cannot reproducibly save a model to disk description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, below- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OpenSUSE Leap 15.2- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no- TensorFlow installed from (source or binary): from pip- TensorFlow version (use command below): 2.8.0-rc1- Python version: 3.9.10**Describe the current behavior**When training and saving the same model twice, I get different outputs stored to disk, even without any training: when saving the exact same model twice in the same Python process (same ""run""), `saved_model.pb` files differ while all other files are identical. Interestingly, when saving the model in two separate runes, `saved_model.pb` files are identical.**Describe the expected behavior**Save the exact same files four times (two in each of two runs).**Standalone code to reproduce the issue**```python""""""Demonstrate bug.""""""import filecmpimport sysimport tensorflow as tfRUN = 1 if len(sys.argv) > 1 else 0tf.keras.utils.set_random_seed(0)tf.config.experimental.enable_op_determinism()model = tf.keras.applications.vgg16.VGG16(weights=None, classes=1)model.compile(optimizer=tf.keras.optimizers.Adam(), loss=""categorical_crossentropy"")model.save(f""tmp_save0_run{RUN}.tf"")model.save(f""tmp_save1_run{RUN}.tf"")if not filecmp.cmp(    f""tmp_save0_run{RUN}.tf/saved_model.pb"",    f""tmp_save1_run{RUN}.tf/saved_model.pb"",):    print(""Files are different in the same run!"")if (RUN == 1) and filecmp.cmp(    ""tmp_save0_run0.tf/saved_model.pb"",    ""tmp_save0_run1.tf/saved_model.pb"",):    print(""Files are identical across runs!"")```To compare between runs, run twice like this:```shellpython bug.py && python bug.py X```Then compare:```shell# compare between two saves within the same run:diff -r tmp_save0_run0.tf tmp_save1_run0.tf# compare between two saves of different runs:diff -r tmp_save0_run0.tf tmp_save0_run1.tf```Output:```Files are different in the same run!Files are different in the same run!Files are identical across runs!Binary files tmp_save0_run0.tf/saved_model.pb and tmp_save1_run0.tf/saved_model.pb differ```
"
53951,1,2398,96,0,0,cyrusbehr,0,"title:Tensorflow Lite: Using output index causes segfault.  description:I am using Tensorflow Lite C++ interface for running inference, commit hash `040585c0f25681b399c9087b53c982959bcca44f` (HEAD of master branch at the time of posting this). I am using a model with 2 inputs, and a single output (array). I am trying to run the following code as a sanity check: ```#include <iostream>#include <fstream>#include <vector>#include ""tensorflow/lite/interpreter.h""#include ""tensorflow/lite/kernels/register.h""#include ""tensorflow/lite/model.h""#include <opencv2/opencv.hpp>using namespace tflite;int main() {    std::ifstream modelFile (""../models/liveness.tflite"", std::ios::binary | std::ios::ate);    std::streamsize size = modelFile.tellg();    modelFile.seekg(0, std::ios::beg);    std::vector<char> buffer(size);    if (!modelFile.read(buffer.data(), size)) {        throw std::runtime_error(""There was a issue reading the model file into memory"");    }    StderrReporter errorReporter;    auto model = FlatBufferModel::BuildFromBuffer(buffer.data(), size, &errorReporter);    if (model == nullptr) {        throw std::runtime_error(""Error creating builder from model file"");    }    tflite::ops::builtin::BuiltinOpResolver resolver;    std::unique_ptr<Interpreter> interpreter;    if (InterpreterBuilder(*model, resolver)(&interpreter) != kTfLiteOk) {        throw std::runtime_error(""Unable to create interpreter"");        // Return failure.    }    auto inputs = interpreter->inputs();    for (const auto input: inputs) {        std::cout << ""Input: "" << input << std::endl;        std::cout << ""Input name: "" << interpreter->GetInputName(input) << std::endl;    }    auto outputs = interpreter->outputs();    for (const auto output: outputs) {        std::cout << ""Output: "" << output << std::endl;        std::cout << ""Output name: "" << interpreter->GetOutputName(output) << std::endl;    }    return 0;}```When I run the above, it prints:```INFO: Created TensorFlow Lite XNNPACK delegate for CPU.Input: 199Input name: batch_normalization/batchnorm/add_1Input: 200Input name: batch_normalization/batchnorm/add_1Output: 193Segmentation fault (core dumped)```As can be seen, the call to ` std::cout << ""Output name: "" << interpreter->GetOutputName(output) << std::endl;` causes a segfault. If I instead change it to ` std::cout << ""Output name: "" << interpreter->GetOutputName(0) << std::endl;` it prints `Output name: dense/Softmax` Why is the first version calling a segfault? As a sanity check, I use python to run this snippet of code with the same model:```    output_details = interpreter.get_output_details()    print(output_details)```This prints:```[{'name': 'dense/Softmax', 'index': 193, 'shape': array([1, 2], dtype=int32), 'shape_signature': array([1, 2], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]```The reason I even ask this question is because I expect to be able to obtain the inference results by calling:```float* out = interpreter->typed_output_tensor<float>(interpreter->outputs()[0]);```However, this too causes a segmentation fault. Running `float* out = interpreter->typed_output_tensor<float>(0);` does give me the correct output.
"
53851,1,3965,21,0,0,hoangtnm,0,"title:[Metric] get shape of a tensor for custom metric description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04- TensorFlow installed from (source or binary): conda-forge- TensorFlow version (use command below): 2.6.2- Python version: 3.7- CUDA/cuDNN version: 10.2 / 7.6.5.32- GPU model and memory: GTX 1080Ti**Describe the current behavior**I am implementing a custom metric for `tf.keras` which need to extract the batch dimension of input tensor `x.shape[0]` during `model.fit(...)` but it raises:```(None, None)Traceback (most recent call last):  File ""train.py"", line 81, in <module>    main()  File ""train.py"", line 71, in main    use_multiprocessing=True)  File ""/opt/conda/lib/python3.7/site-packages/keras/engine/training.py"", line 1184, in fit    tmp_logs = self.train_function(iterator)  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 885, in __call__    result = self._call(*args, **kwds)  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 933, in _call    self._initialize(args, kwds, add_initializers_to=initializers)  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 760, in _initialize    *args, **kwds))  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3066, in _get_concrete_function_internal_garbage_collected    graph_function, _ = self._maybe_define_function(args, kwargs)  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3463, in _maybe_define_function    graph_function = self._create_graph_function(args, kwargs)  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3308, in _create_graph_function    capture_by_value=self._capture_by_value),  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 1007, in func_graph_from_py_func    func_outputs = python_func(*func_args, **func_kwargs)  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 668, in wrapped_fn    out = weak_wrapped_fn().__wrapped__(*args, **kwds)  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 994, in wrapper    raise e.ag_error_metadata.to_exception(e)ValueError: in user code:    /opt/conda/lib/python3.7/site-packages/keras/engine/training.py:853 train_function  *        return step_function(self, iterator)    /AgeGenderMask/metrics/age_mae.py:36 update_state  *        self.count.assign_add(y_true.shape[0])    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:857 assign_add  **        ops.convert_to_tensor(delta, dtype=self.dtype),    /opt/conda/lib/python3.7/site-packages/tensorflow/python/profiler/trace.py:163 wrapped        return func(*args, **kwargs)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1566 convert_to_tensor        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py:346 _constant_tensor_conversion_function        return constant(v, dtype=dtype, name=name)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py:272 constant        allow_broadcast=True)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py:290 _constant_impl        allow_broadcast=allow_broadcast))    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:445 make_tensor_proto        raise ValueError(""None values not supported."")```**Describe the expected behavior**As mentioned above.**Standalone code to reproduce the issue**```pythonclass CustomMetric(keras.metrics.Metric):    def __init__(self, name: Optional[str] = None) -> None:        super().__init__(name=name)        self.total = self.add_weight(""total"", initializer=""zero"")        self.count = self.add_weight(""count"", initializer=""zero"")    def update_state(self, y_true, y_pred, sample_weight=None) -> None:        self.total.assign_add(tf.reduce_sum(y_pred))        print(y_true.shape[0]) # Cannot extract tensor's shape        self.count.assign_add(y_true.shape[0])    def result(self) -> tf.Tensor:        return tf.math.divide_no_nan(self.total, self.count)    def reset_state(self) -> None:        self.total.assign(0)        self.count.assign(0)```
"
53846,1,0,0,0,0,yanniskar,0,"title:tf.data.experimental.sample_from_datasets non-deterministic in multi-gpu.  description:See https://github.com/NVIDIA/framework-determinism/issues/39**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Does not apply- TensorFlow installed from (source or binary): pip- TensorFlow version (use command below): 2.4.1- Python version: 3.7- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version: 11.2- GPU model and memory:
"
53842,1,1130,0,0,0,dfossl,0,"title:Shape issues in tf.keras.metrics.SparseTopKCategoricalAccuracy with multiple dimensions (previously #36985) description:Might not be a bug could be my application. But if that is the case I feel the documentation should be edited as from reading those functionality seems to be similar.When I run this:```target = tf.constant([[1,1,2,0,0,0],[1,1,2,2,0,0]])predict = tf.constant([[[0,1,0],                       [0,1,0],                       [0,0,1],                       [0,1,0],                       [1,0,0],                       [1,0,0]],                       [[0,1,0],                        [0,1,0],                        [0,0,1],                        [0,0,1],                        [0,1,0],                        [0,1,1000]]], dtype=tf.float32)print(tf.shape(target))print(tf.shape(predict))mask = tf.math.logical_not(tf.math.equal(target, 0))mask = tf.cast(mask, dtype=tf.int64)loss = tf.keras.losses.SparseCategoricalCrossentropy(        from_logits=True)loss(target, predict, sample_weight=mask)``````tf.Tensor([2 6], shape=(2,), dtype=int32)tf.Tensor([2 6 3], shape=(3,), dtype=int32)<tf.Tensor: shape=(), dtype=float32, numpy=0.3216761>```Works as expected. But when I try it for TopKAccuracy I get this:```m = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)m.update_state(target, predict, sample_weight=mask)m.result.numpy()InvalidArgumentError: Can not squeeze dim[1], expected a dimension of 1, got 6 [Op:Squeeze]```In the documentation, both of them have the same definitions for y_true, y_pred and sample_weight. So not sure why the applications seem different here.This may be my misuse, but not clear regardless. Any help would be appreciated.
"
53836,1,6137,90,0,0,ghost,0,"title:ValueError: Failed to find data adapter that can handle input description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10- TensorFlow installed from (source or binary):pip- TensorFlow version (use command below):2.5.0- Python version:3.7.11- CUDA/cuDNN version:11.2- GPU model and memory: rtx3060 laptop 6gproblem:```Traceback (most recent call last):  File ""E:/python/cloud_remove_keras/train_models/train_resnet.py"", line 42, in <module>    callbacks=[cp_callback])  # Pass callback to training  File ""D:\anaconda3\envs\PyGIS\lib\site-packages\keras\engine\training.py"", line 1122, in fit    steps_per_execution=self._steps_per_execution)  File ""D:\anaconda3\envs\PyGIS\lib\site-packages\keras\engine\data_adapter.py"", line 1348, in get_data_handler    return DataHandler(*args, **kwargs)  File ""D:\anaconda3\envs\PyGIS\lib\site-packages\keras\engine\data_adapter.py"", line 1136, in __init__    adapter_cls = select_data_adapter(x, y)  File ""D:\anaconda3\envs\PyGIS\lib\site-packages\keras\engine\data_adapter.py"", line 978, in select_data_adapter    _type_name(x), _type_name(y)))ValueError: Failed to find data adapter that can handle input: <class 'data_generator.DataGenerator'>, <class 'NoneType'>```networks:```import keras.backend as Kfrom keras.layers import Conv2D, Activation, Lambda, Addfrom keras.models import Model, Inputfrom tensorflow.keras.utils import plot_modelK.set_image_data_format('channels_first')def resBlock(input_l, feature_size, kernel_size, scale=0.1):    """"""Definition of Residual Block to be repeated in body of network.""""""    tmp = Conv2D(feature_size, kernel_size, kernel_initializer='he_uniform', padding='same')(input_l)    tmp = Activation('relu')(tmp)    tmp = Conv2D(feature_size, kernel_size, kernel_initializer='he_uniform', padding='same')(tmp)    tmp = Lambda(lambda x: x * scale)(tmp)    return Add()([input_l, tmp])def res_net(input_shape=(3, 512, 512),                  num_layers=32,                  feature_size=256):    """"""Definition of network structure. """"""    # define dimensions    input = Input(shape=input_shape)    x = input    # Treat the concatenation    x = Conv2D(feature_size, (3, 3), kernel_initializer='he_uniform', padding='same')(x)    x = Activation('relu')(x)    # main body of network as succession of resblocks    for i in range(num_layers):        x = resBlock(x, feature_size, kernel_size=[3, 3])    # One more convolution    x = Conv2D(input_shape[0], (3, 3), kernel_initializer='he_uniform', padding='same')(x)    # Add first layer (long skip connection)    x = Add()([x, input])    model = Model(inputs=input, outputs=x)    return model```data generator:```import numpy as npimport osfrom tensorflow.keras.utils import Sequencefrom tensorflow.keras.preprocessing.image import load_img, img_to_arrayclass DataGenerator(Sequence):    def __init__(self, infile_path, batch_size, ratio, train_available):        super(DataGenerator, self).__init__()        self.infile_path = infile_path        self.batch_size = batch_size        self.ratio = ratio        self.train_available = train_available        self.cloud_path = os.path.join(infile_path, 'cloudy_image')        self.ground_path = os.path.join(infile_path, 'ground_truth')        # 闂傚倸鍊搁崐椋庣矆娴ｉ潻鑰块梺顒€绉查埀顒€鍊圭粋鎺斺偓锝庝簽閿涙盯姊洪悷鏉库挃缂侇噮鍨堕幃鈥斥槈閵忥紕鍘卞銈嗗姧缁茶法绮婚妷锔跨箚闁圭粯甯炵粔娲煛瀹€瀣？闁逞屽墾缂嶅棙绂嶅鍫濇辈闁绘劗鏁哥壕濂告煕鐏炲墽鈽夌紒妞﹀洦鐓欓柣鐔告緲椤忣厾鈧娲﹂崑濠傜暦閻旂⒈鏁囨繝闈涚墛绾箓姊婚崒娆愮グ鐎规洜鏁诲畷鎴︽倷鐎靛摜鎳濆┑掳鍊曢幊蹇涘磻閸屾凹鐔嗛悹铏瑰劋濠€鐗堛亜椤愶絾绀冪紒缁樼箞濡啫鈽夐崡鐐插闂?cloudy 闂?ground truth 闂傚倸鍊搁崐椋庣矆娓氣偓楠炴牠顢曢敃鈧壕鍦磼鐎ｎ偓绱╂繛宸簼閺呮煡鏌涘☉鍙樼凹闁诲骸顭峰娲濞戞氨鐣鹃梺閫炲苯澧痪缁㈠幘缁綁鎮欓悜妯锋嫽婵炴挻鑹惧ú銈嗙濠靛牏纾奸悗锝庡亞婢х數鈧娲忛崹浠嬨€佸Δ鍛妞ゆ帒鍊搁獮鎰版⒒娴ｄ警鐒鹃柡鍫墰閸犲﹤顓兼径瀣壒濠德板€撻懗鍓佺不妤ｅ啯鐓曢柍鈺佸暙婵洭鏌ｈ箛姘毢闁逞屽墲椤煤韫囨稑纾块梺顒€绋侀弫鍥р攽閸屾粠鐒鹃柣鎰躬閺屾盯鍩勯崘顏呭櫑婵炲濮炬禍顒勫焵椤掍緡鍟忛柛鐘虫礋瀹曪繝骞庢慨鎰ㄥ亾娴ｇ硶妲堟俊顖炴敱閻庡姊虹憴鍕剹闁告娅ｅΣ鎰潨閳ь剙顫忕紒妯诲闁告稑锕ラ崕鎾剁磽娴ｅ壊妲哥紓宥咃龚濡垽姊洪崷顓炰壕闁哄銈稿?       self.file_list = os.listdir(self.ground_path)        # 闂傚倸鍊搁崐鐑芥嚄閸洖绠犻柟鍓х帛閸婅埖鎱ㄥΟ鎸庣【缂佺姵婢橀—鍐偓锝庝簻椤掋垽鎮楀顒夌吋闁哄苯绉归崺鈩冩媴閸涘﹥顓惧┑鐐茬摠缁秶鍒掗幘璇茶摕婵炴垶菤閺€浠嬫煕閳╁喚娈㈠ù灏栧亾缂?       np.random.shuffle(self.file_list)        # 闂傚倸鍊搁崐椋庣矆娓氣偓楠炲鏁嶉崟顒佹闂佸湱鍎ら崵锕€鈽夊Ο婊勬⒐閹峰懘鎼归悷浼寸崕婵犵數濮烽弫鍛婃叏鐎电硶鍋撳鐓庡箺闁告帒锕畷妤冪箔鏉炴壆鐩庨梻浣筋潐瀹曟ê鈻斿☉妯绘瘎闂傚倷鐒﹀鍧楀储婵傚憡鏅濇い蹇撶墕閽冪喖鏌嶉埡浣告殲濠殿垱鎸抽弻娑樷攽閸℃浼岄梺鍝勬－閸欏啴骞冨Δ鍐╁枂闁告洦鍓欓惌顔剧磽娴ｈ棄鐓愰柤褰掔畺閹箖鎮滅粵瀣櫆闂佺硶鍓濆ú姗€宕熼崘顔解拺闁告稑锕ョ€垫瑥霉閿濆娅滄繛宀婁邯濮婄粯鎷呴挊澶婃優婵犳鍠楀娆戝弲闂佹寧娲嶉崑鎾绘偂閵堝棙鍙忔俊鐐额嚙娴滈箖鎮楀▓鍨灍濠电偛锕獮鍐閵堝棗浜楅柟鑹版彧缂嶅棝宕?       n_train = int(ratio * len(self.file_list))        self.train_file_list = self.file_list[:n_train]        self.test_file_list = self.file_list[n_train:]        # 闂傚倸鍊峰ù鍥敋瑜忛幑銏ゅ箛椤旇棄搴婇梺褰掑亰閸庨潧鈽夊鍡欏弳闂佸壊鍋掗崑鍛村疾閳哄懏鈷戦柛娑橈攻婢跺嫰鏌涘Ο鍝勨挃濠㈣娲樼缓浠嬪川婵犲嫬骞堥梻浣筋潐婢瑰寮插☉娆庣箚濞寸姴顑嗛崐鍨叏濮楀棗澧紒鐙欏懐纾肩紓浣诡焽濞插鈧鍠栭悥濂哥嵁鐎ｎ噮鏁囬柣鎰絻椤ユ帡姊婚崒娆愵樂缂侀硸鍠氶埀顒勬涧閻倸鐣烽崷顓熷磯濞撴凹鍨板▓銊ヮ渻閵堝棗绗掗悗姘煎墴閹瑦绻濋崶銊у弳闂佸搫鍟崐濠氬箺閸岀偞鐓曢悗锝冨妼婵＄晫绱掔紒妯兼创闁轰焦鍔欏畷銊╊敊绾拌鲸袩缂傚倸鍊峰ù鍥敋瑜斿畷鎰板锤濡も偓缁犳牗绻涘顔荤盎閹喖姊虹€圭姵銆冮柤鍐茬埣钘濇い鎰堕檮閳锋垹绱撴担鐧镐緵婵炲牊妫冮弻锝呂旈埀顒勬晝椤忓牆绠栨俊銈呮媼閺佸﹥绻涢弶鎴︻€楃紓宥咃躬楠炲啴鍩￠崨顔藉劒濡炪倖鍔х徊鍧楀焵椤掑鐏︽慨?       # np.savetxt(os.path.join(infile_path, 'train_file_list.txt'), np.array(self.train_file_list), fmt='%s')        # np.savetxt(os.path.join(infile_path, 'test_file_list.txt'), np.array(self.test_file_list), fmt='%s')    def __len__(self):        if self.train_available:            return int(np.floor(len(self.train_file_list) / self.batch_size))        else:            return int(np.floor(len(self.test_file_list) / self.batch_size))    def __getitem__(self, index):        if self.train_available:            file_list = self.train_file_list[index * self.batch_size:(index + 1) * self.batch_size]            batch_x = [np.transpose(img_to_array(load_img(os.path.join(self.cloud_path, file_name)))/255.0, [2, 0, 1]) for file_name in file_list]            batch_y = [np.transpose(img_to_array(load_img(os.path.join(self.ground_path, file_name)))/255.0, [2, 0, 1]) for file_name in file_list]            return np.array(batch_x), np.array(batch_y)        else:            file_list = self.test_file_list[index * self.batch_size:(index + 1) * self.batch_size]            batch_x = [np.transpose(img_to_array(load_img(os.path.join(self.cloud_path, file_name)))/255.0, [2, 0, 1]) for file_name in file_list]            batch_y = [np.transpose(img_to_array(load_img(os.path.join(self.ground_path, file_name))) / 255.0, [2, 0, 1]) for file_name in file_list]            return np.array(batch_x), np.array(batch_y)```train code:```import tensorflow as tfimport osfrom data_generator import DataGeneratorfrom networks.resnet import res_netparams = {    'infile_path': r""E:\python\cloud_removal\data\RICE_DATASET\RICE2"",    'batch_size': 32,    'ratio': 0.8,    'train_available': True}train_Gen = DataGenerator(**params)params = {    'infile_path': r""E:\python\cloud_removal\data\RICE_DATASET\RICE2"",    'batch_size': 32,    'ratio': 0.8,    'train_available': False}val_Gen = DataGenerator(**params)model = res_net()model.compile(optimizer='adam',              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),              metrics=['accuracy'])checkpoint_path = ""train_models/resnet/cp.ckpt""checkpoint_dir = os.path.dirname(checkpoint_path)# Create a callback that saves the model's weightscp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,                                                 save_weights_only=True,                                                 verbose=1)# Train the model with the new callbackmodel.fit(x=train_Gen,          validation_data=val_Gen,          epochs=10,          callbacks=[cp_callback])  # Pass callback to trainingmodel.save('train_models/resnet')```
"
53833,1,0,111,0,0,castorgit,0,"title:Using learning-rate decay schedule with Keras (transfer learning with RESNET) generates unsupported operand error 'ExponentialDecay' description:** System information **- custom code- Ubuntu 20.4- TF installed from binary- TF / Keras 2.7.0- Python 3.8.12- no GPU** Current Behavior **Training a Network to classify images, The network is based on RESNET152V2 + some new layers. We train last 19 layers of RESNET + the new added layers(data is XRAY dataset from Kaggle)Using a learning-rate decay schedule, the training crashes with an error 'unsupported operand error'TypeError: unsupported operand type(s) for *: 'ExponentialDecay' and 'int'** Standalone code to reproduce the issue **[link to notebook](https://github.com/castorgit/AI-course-2021/blob/main/Group_Project/ResNet152V%20error.ipynb)
"
53818,1,177,210,1,0,ktsitsi,0,"title:File system scheme 's3' not implemented description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA- TensorFlow installed from (source or binary): pip- TensorFlow version (use command below):  tensorflow==2.7.0- TensorFlow-IO version: tensorflow-io==0.23.1- Python version: Python 3.8.12- Bazel version (if compiling from source): NA- GCC/Compiler version (if compiling from source): NA- CUDA/cuDNN version: NA- GPU model and memory: NA**Describe the current behavior**`tensorflow.python.framework.errors_impl.UnimplementedError: File system scheme 's3' not implemented`**Describe the expected behavior**`Should be able to connect the s3 filesystem.`**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): NA- Briefly describe your candidate solution(if contributing): Rollback to `tensorflow-io==0.17` and `tensorflow==2.4.4` seems to resolve issue**Standalone code to reproduce the issue**```from tensorflow.python.lib.io import file_ioprint(file_io.stat('s3://bucketname/path/'))``````from tensorflow.io import gfileprint(gfile.exists(`s3://bucketname/path/`))```**Other info / logs**  Similar older issue [here- 40302](https://github.com/tensorflow/tensorflow/issues/40302)
"
53816,0,110,14,0,0,Janus-Shiau,0,"title:`tf.norm` not accurate in Tensorflow 2.7.0 description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no- TensorFlow installed from (source or binary): pypl- TensorFlow version (use command below): v2.7.0- Python version: 3.8.10- Bazel version (if compiling from source): not required- GCC/Compiler version (if compiling from source): not required- CUDA/cuDNN version: 11.0 (from docker image of TF 2.7.0)- GPU model and memory: 2080ti**Describe the current behavior**We are using `tf.norm` to make a vector be unit vector.This is quite simple and we use the following code in TF 2.2.0.```pythonvec = np.array([4.6463e-03, -4.8086258e+01], dtype=""float32"")amount = tf.norm(vec, ord=2, axis=-1)```The amount is `<tf.Tensor: shape=(), dtype=float32, numpy=48.086258>` in TF 2.2.0.Thus the normalizing is correct and we get an unit vector by `vec / amount`.But in TF 2.7.0, the amount will be `<tf.Tensor: shape=(), dtype=float32, numpy=48.086254>`, which is incorrect.We also found that this situation only happened when the tensor is computed on GPU.In other word, if we set `os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""`, we could get correct value in TF 2.7.**Describe the expected behavior**The norm value should be `<tf.Tensor: shape=(), dtype=float32, numpy=48.086258>` in both CPU or GPU.
"
53806,0,449,48,0,0,dniku,0,"title:Deprecation message for tf.compat.v1.batch_gather suggests invalid migration description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab- TensorFlow version (use command below): `v2.7.0-0-gc256c071bb2 2.7.0`- Python version: `3.7.12`**Describe the current behavior**Using `tf.compat.v1.batch_gather` triggers the following warning:```pythonWARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.Instructions for updating:`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.```This warning was last edited in [this commit](https://github.com/tensorflow/tensorflow/commit/50abb98b7db07adc78df883ffa7b6c6dd4273ffc).However, the suggested migration from `tf.compat.v1.batch_gather(...)` to `tf.gather(..., batch_dims=-1)` is invalid 闂傚倸鍊搁崐鐑芥嚄閸洖纾婚柕濞炬櫅绾惧潡鏌＄仦璇插姎闁告垹濞€閺屽秷顧侀柛鎾跺枛瀵鏁愭径濠傚敤闂侀潧鐗嗛幊宥嗙珶?g., see [this Colab notebook](https://colab.research.google.com/gist/dniku/7510249f61b9b00881af6c0bcfdac7a3/tf_migrate_batch_gather_to_gather.ipynb) for an example. From the [implementation](https://github.com/tensorflow/tensorflow/blob/c256c071bb26e1e13b4666d1b3e229e110bc914a/tensorflow/python/ops/array_ops.py#L5166) of `batch_gather` it seems that the correct migration is from```pythontf.compat.v1.batch_gather(data, indices)```to```pythontf.gather(data, indices, batch_dims=tf.rank(indices) - 1)```**Describe the expected behavior**The warning should be amended.
"
53792,1,0,98,0,1,edwardyehuang,0,"title:Use enable_op_determinism + Fixed seed + same hardware (partial DGX) still get different results in 2.8.  description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS- TensorFlow installed from (source or binary): pip- TensorFlow version (use command below): 2.8rc0- Python version: 3.8.12- GPU model and memory:8*V100After I use enable_op_determinism in TensorFlow 2.8, my model still gets random results (around 0.3 mIOU) after each run. Note that, I set PYTHONHASHEED to a fixed value before start python, and also set a fixed value as the random seed for TensorFlow and numpy etc.I wonder why the result is still random in same hardware, since the UnimplementedError should be thrown if nondeterministic op is used.
"
53779,1,639,13,0,0,mazeltovlee,0,"title:keras.layers.SimpleRNN/LSTM output NaN when setting ""activation"" to ""exponential"" description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.7.0- Python version: 3.7.12- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source): N/A- CUDA/cuDNN version: N/A- GPU model and memory: N/A**Describe the current behavior**`keras.layers.SimpleRNN` and `keras.layers.LSTM` output `NaN` when setting the `activation` parameter to `exponential` if the shape of input is larger than 3.**Describe the expected behavior**`keras.layers.SimpleRNN` and `keras.layer.LSTM` should not output `NaN` under this setting.- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**```from tensorflow import kerasx = keras.layers.Input((6,3))buggy_layer = keras.layers.SimpleRNN(50, activation=""exponential"")y = buggy_layer(x)model = keras.Model(x, y)import numpy as npinput = np.random.rand(10,6,3)res = model.predict(input)model_path = ""simple_rnn_exponential.h5""model.save(model_path)print(res)```You may also access the code here:https://colab.research.google.com/drive/1mVBIPF79kiIYggZUUkvs1IEB1GdxeoJC?usp=sharing**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.To help you identify this issue, I have tried several ways to construct the buggy layer.Trial 1:Use `linear` as the activation while adding an additional exponential activation after the SimpleRNN. In this way, the `res` will not output `NaN````from tensorflow import kerasx = keras.layers.Input((6,3))buggy_layer = keras.layers.SimpleRNN(50, activation=""linear"")y = buggy_layer(x)z = keras.activations.exponential(y)model = keras.Model(x, z)import numpy as npinput = np.random.rand(10,6,3)res = model.predict(input)print(res)```Trial 2:I tried converting the generated model which will output `NaN` to PyTorch using ONNX, I find that: after converting to PyTorch, the output will contain `Inf` and `0` but no `NaN`. Please see this link for more details about this trial:https://colab.research.google.com/drive/1mVBIPF79kiIYggZUUkvs1IEB1GdxeoJC?usp=sharing
"
53766,0,0,72,0,0,lgeiger,0,"title:Constant folding fails when converting int8 transposed convolutions description:When converting a model that used int8 quantization aware training conversion of transposed convolutions fails.The converter isn't able to correctly constant fold the fake quantized weights and keeps an unnecessary `tfl.transpose` operation in the graph which leads to problems when executing the TFLite model. Note that this issue is independent of TensorFlow Model Optimization and can be reproduced using plain TensorFlow as well (see linked notebook).### 1. System information- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS / Ubuntu- TensorFlow installation (pip package or built from source): pip package- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5, 2.6, 2.7, 2.8rc0 and 2.9.0-dev20220114### 2. CodeA minimal reproduction of the issue is available in [this notebook](https://colab.research.google.com/drive/1bfI_Dx3zyoHkabJlHpNFnbDA9r9G5ffE?usp=sharing). Re-run the notebook to show `netron` visualisations showing the conversion problem.
"
53722,1,2590,0,0,0,MathiesW,0,"title:Model subclassing: access layers saved as class variables to self.__dict__ for dynamic model definition description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes- OS Platform and Distribution: Linux Ubuntu 20.04 LTS- TensorFlow installed from binary- TensorFlow version: 2.5.0- Python version: 3.7.9- CUDA/cuDNN version: 11.2- GPU model and memory: 2x RTX3090,, 24GB- Gist with run-able code: https://gist.github.com/MathiesW/2640ac9cf04da4f0f471b0158437c5c6Hi,my goal is to define a dynamically generated FCN model, where I define the amount of filters as a list, and the model is generated with len(filters) Encoder and len(filters) Decoder blocks. I defined my blocks and model according to [this Tensorflow Guide](https://www.tensorflow.org/guide/keras/custom_layers_and_models#the_model_class).Now, to achieve my dynamic model class, I have written all class variables into the `model.__dict__`:```class FullyConvolutionalNetworkDynamic(Model, ABC):    def __init__(self,                 num_filter: List[int],                 kernel_size: List[int],                 strides: List[int],                 name: str = ""FullyConvolutionalNetwork"",                 activation: str = ""relu"",                 data_format: str = ""channels_last"",                 *args, **kwargs):        super(FullyConvolutionalNetworkDynamic, self).__init__(name=name, *args, **kwargs)        # define symmetric encoder and decoder blocks        for i, (f, k, s) in enumerate(zip(num_filter, kernel_size, strides)):            self.__dict__[f""encoder{i}""] = EncoderBlock(num_conv=f, kernel=k, stride=s,                                                        activation=activation,                                                        name=f""enc{i}"", data_format=data_format)            self.__dict__[f""decoder{i}""] = DecoderBlock(num_conv=f, kernel=k, stride=s,                                                        activation=activation,                                                        name=f""dec{i}"", data_format=data_format)        # output section        self.conv1x1 = Conv1D(1, 1, strides=1, padding='same', name=""1x1"", data_format=data_format)        self.flatten = Flatten()```Now that my layers are written to `model.__dict__`, I tried to add them to my call function from here:```    @tf.function    def call(self, x, training=False):        # encoder path        for layer in sorted([key for key in self.__dict__ if ""encoder"" in key]):            x = self.__dict__[layer](x)        # decoder path        for layer in sorted([key for key in self.__dict__ if ""decoder"" in key], reverse=True):            x = self.__dict__[layer](x)        # output section        x = self.conv1x1(x)        return self.flatten(x)```However, when I now create an instance of my model, the model summary states that only the layers defined in the output section are added to the model:```>>> model = FullyConvolutionalNetwork(num_filters=[4, 8, 16], kernel_size=[3, 3, 3], strides=[1, 1, 1])>>> model.build(input_shape=[None, 256, 10])>>> model.summary()Model: ""FullyConvolutionalNetwork""_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================1x1 (Conv1D)                 multiple                  5         _________________________________________________________________flatten (Flatten)            multiple                  0         =================================================================Total params: 5Trainable params: 5Non-trainable params: 0_________________________________________________________________```I already use the model but ""hard-coded"" to three layers depth on each path. When I try to recreate my hard-coded model with the dynamic approach above, the `model.__dict__` indeed contains my EncoderBlock and DecoderBlock layers, they are just not added to the model. The ""hard-coded"" version works flawlessly! Any help to solve my problem is greatly appreciated! Thank you in advance!PS: I have allowed my self to re post my issue here, since the Tensorflow Repository seems much more active than the keras repository.
"
53701,1,0,10,0,0,hayatejp,0,"title:The GRU/LSTM function bugs when using TF2.7 description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary):binary- TensorFlow version (use command below):2.7- Python version:3.8- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version:11.5- GPU model and memory:NVIDIA Geforce 1070, 8GB**Describe the current behavior**Hello闂傚倸鍊搁崐鐑芥倿閿旈敮鍋撶粭娑樻噽閻瑩鏌熸潏鎯х槣闁轰礁妫濋弻锝夘敂閸曨厽娈絥 I used the GRU/LSTM model to accomplish the text classification with IMDB dataset闂傚倸鍊搁崐鐑芥倿閿旈敮鍋撶粭娑樻噽閻瑩鏌熸潏鎯х槣闁轰礁锕弻鐔兼嚒閵堝洠鏀╮e were some problems  in model  fitting process.This is the dataset shape information.![datasets](https://user-images.githubusercontent.com/4904955/148642698-a1fe2d86-ed76-404e-a89b-34a122a21618.png)When I set the GRU layer with  return_sequences=True:![GRU](https://user-images.githubusercontent.com/4904955/148642812-1fd832e2-7d8f-4bc4-94bc-443f0ed4677a.png)![1-True](https://user-images.githubusercontent.com/4904955/148642730-781a87a5-67f1-426f-a3a3-30e70698967c.png)Then I removed the return_sequences=False,the code was seemed to be normal,but the gradient was't decreased:![1-false](https://user-images.githubusercontent.com/4904955/148642765-bffba379-bd36-4d8d-bcc0-1667afff676c.png)The LSTM performance was also similar with the GRU. If I used the two layers GRU with the last GRU's return_sequences=False闂傚倸鍊搁崐鐑芥倿閿旈敮鍋撶粭娑樻噽閻瑩鏌熸潏鎯х槣闁轰礁锕弻鐔兼嚒閵堝洠鏀?performance of model was same as the single GRU/LSTM layer.![M-RNN](https://user-images.githubusercontent.com/4904955/148642917-8adb0da9-b3d3-4a96-8000-ac2764170bdf.png)![M-RNN2](https://user-images.githubusercontent.com/4904955/148642905-e30f9f6b-4483-4bc5-b4a9-5b9dc396da92.png)**Describe the expected behavior**I asked my teammates who used **tf2.6.2**,they said all the problem I mentioned above didn't occur when they ran these codes. But some people who used  **tf2.7** found the same problem. I also checked the different systems like linux in colab or docker,it seems that the bugs could happen when using tf2.7.I has updated my code with zip attachment,**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.The Code is here:[imdb.zip](https://github.com/tensorflow/tensorflow/files/7833147/imdb.zip)
"
53696,1,0,3,0,0,NikitaGordia,0,"title:TensorBoard doesn't work on IOS/IpadOS. Requires apple icons.  description:After starting the server it gives the following result:W0107 19:43:36.780149 139768956442176 application.py:556] path /apple-touch-icon-precomposed.png not found, sending 404W0107 19:43:36.798506 139768956442176 application.py:556] path /apple-touch-icon.png not found, sending 404W0107 19:43:36.832389 139768956442176 application.py:556] path /apple-touch-icon-precomposed.png not found, sending 404W0107 19:43:36.849392 139768956442176 application.py:556] path /apple-touch-icon.png not found, sending 404W0107 19:43:36.900132 139768956442176 application.py:556] path /apple-touch-icon-precomposed.png not found, sending 404W0107 19:43:36.915132 139768956442176 application.py:556] path /apple-touch-icon.png not found, sending 404W0107 19:43:36.948019 139768956442176 application.py:556] path /apple-touch-icon-precomposed.png not found, sending 404W0107 19:43:36.965894 139768956442176 application.py:556] path /apple-touch-icon.png not found, sending 404And in afterwards you can not access tensorboard on apple devices (tested on Iphone 8, iphone 11, ipad 2018). but on android/chome(macos) works great
"
53686,1,635,70,0,0,Lifann,0,"title:API generation makes inheritance relationship confusing after r2.6 description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0- Python version: Python 3.6.8**Describe the current behavior**I'm trying to select different algorithms by different optimizer, and got following unexpected behavior:```pythonimport tensorflow as tffrom tensorflow.python.keras.optimizer_v2 import adamfrom tensorflow.python.keras.optimizer_v2 import optimizer_v2opt = tf.keras.optimizers.Adam(1E-3)cond = isinstance(opt, optimizer_v2.OptimizerV2) # It expects true.```Before r2.6, everything works fine. But after r2.6, it will get false instead. I noticed that there is an API-gen process when exporting the APIs. And I make other tests.```pythonopt = tf.keras.optimizers.Adam(1E-3)cond = isinstance(opt, optimizer_v2.OptimizerV2) # 2.6: false, 2.5: truecond = isinstance(opt, tf.keras.optimizers.Optimizer) # 2.6: true, 2.5: trueopt = adam.Adam(1E-3)cond = isinstance(opt, optimizer_v2.OptimizerV2) # 2.6: true, 2.5: truecond = isinstance(opt, tf.keras.optimizers.Optimizer) # 2.6: false, 2.5: true```It seems that the APIs have an independent type system after r2.6. It makes code hard to manage when we need to extend some functionalities.
"
53660,0,181,0,0,0,ArrowIntoTheSky,0,"title:`tf.sparse.split` crashes when axis is a tuple description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.7.0- Python version: 3.8- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source):N/A- CUDA/cuDNN version: N/A- GPU model and memory: N/A**Standalone code to reproduce the issue**```import tensorflow as tfdata = tf.random.uniform([1, 32, 32], dtype=tf.float32)axis = [1, 2]x = tf.sparse.from_dense(data)result = tf.sparse.split(x,3, axis=axis) # crash```Session crashes. `tf.sparse.split` failed to do proper checking for `axis`.**Describe the expected behavior**`tf.sparse.split` should raise `InvalidArgumentError` when `axis` is not a 0-D tensor, instead of crashing.
"
53658,1,284,0,0,0,ArrowIntoTheSky,0,"title:tf.sparse.softmax randomly outputs wrong results! description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.7.0- Python version: 3.8- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source):N/A- CUDA/cuDNN version: N/A- GPU model and memory: N/A**Standalone code to reproduce the issue**```import tensorflow as tflogits = tf.random.uniform([248, 248, 1, 16], dtype=tf.float32)r1 = tf.nn.softmax(logits,axis=-1)logits_sp = tf.sparse.from_dense(logits)r2 = tf.sparse.softmax(logits_sp)r3 = tf.sparse.to_dense(r2)assert tf.math.reduce_all(tf.math.equal(r1, r3))```Normally the assertion would pass because `tf.sparse.softmax` and `tf.nn.softmax` should have the same output for the same input value. However, if I run the above code for 100 times then sometimes the assertion fails! Please see the [gist](https://colab.research.google.com/gist/ArrowIntoTheSky/fed8a0babf38a699ce40ecf59f70dbb6/untitled0.ipynb?authuser=2#scrollTo=PgfFQVFeCyLF) here for reference.
"
53657,0,524,0,0,0,ArrowIntoTheSky,0,"title:tf.sparse.softmax lack support for float16 description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.7.0- Python version: 3.8- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source):N/A- CUDA/cuDNN version: N/A- GPU model and memory: N/A**Standalone code to reproduce the issue**```import tensorflow as tflogits = tf.random.uniform([16, 1, 10], dtype=tf.float16)r1 = tf.nn.softmax(logits,axis=-1) # passlogits_sp = tf.sparse.from_dense(logits)r2 = tf.sparse.softmax(logits_sp) # InvalidArgumentError```**Describe the current behavior**`tf.sparse.softmax` cannot accept a tensor of type `float16`. However, `tf.nn.softmax` do support `half`. For the above code snippet, the error message is:```InvalidArgumentError: Value for attr 'T' of half is not in the list of allowed values: float, double	; NodeDef: {{node SparseSoftmax}}; Op<name=SparseSoftmax; signature=sp_indices:int64, sp_values:T, sp_shape:int64 -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]> [Op:SparseSoftmax]```**Describe the expected behavior**According to the document for `tf.sparse.softmax`, it is equivalent to `tf.nn.softmax` (but for sparse tensors), so `tf.sparse.softmax` should also support `float16` inputs.
"
53653,0,3483,0,0,0,ArrowIntoTheSky,0,"title:tf.sparse.to_dense don't support complex dtypes description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.7.0- Python version: 3.8- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source):N/A- CUDA/cuDNN version: N/A- GPU model and memory: N/A**Standalone code to reproduce the issue**```import tensorflow as tfx = tf.cast(tf.constant([1.0, 2.0]), tf.complex128)x_sparse = tf.sparse.from_dense(x)print(""from_dense pass:"", x_sparse) # passx_dense = tf.sparse.to_dense(x_sparse) # failprint(""to_dense pass:"", x_dense) ```**Describe the current behavior**`tf.sparse.from_dense` can convert a complex dense tensor to a sparse tensor, however, `tf.sparse.to_dense` fails to convert a complex sparse tensor back to a dense tensor.For the above code snippet, the output is:```from_dense pass: SparseTensor(...)NotFoundError: Could not find device for node: {{node SparseToDense}} = SparseToDense[T=DT_COMPLEX128, Tindices=DT_INT64, validate_indices=true]All kernels registered for op SparseToDense:  device='CPU'; T in [DT_STRING]; Tindices in [DT_INT64]  device='CPU'; T in [DT_STRING]; Tindices in [DT_INT32]  device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT64]  device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT32]  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT64]  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT32]  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT64]  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT32]  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT64]  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT32]  device='CPU'; T in [DT_INT8]; Tindices in [DT_INT64]  device='CPU'; T in [DT_INT8]; Tindices in [DT_INT32]  device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT64]  device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT32]  device='CPU'; T in [DT_INT16]; Tindices in [DT_INT64]  device='CPU'; T in [DT_INT16]; Tindices in [DT_INT32]  device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT64]  device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT32]  device='CPU'; T in [DT_UINT32]; Tindices in [DT_INT64]  device='CPU'; T in [DT_UINT32]; Tindices in [DT_INT32]  device='CPU'; T in [DT_INT64]; Tindices in [DT_INT64]  device='CPU'; T in [DT_INT64]; Tindices in [DT_INT32]  device='CPU'; T in [DT_UINT64]; Tindices in [DT_INT64]  device='CPU'; T in [DT_UINT64]; Tindices in [DT_INT32]  device='GPU'; T in [DT_BOOL]; Tindices in [DT_INT64]  device='GPU'; T in [DT_BOOL]; Tindices in [DT_INT32]  device='GPU'; T in [DT_INT32]; Tindices in [DT_INT64]  device='GPU'; T in [DT_INT32]; Tindices in [DT_INT32]  device='GPU'; T in [DT_INT8]; Tindices in [DT_INT64]  device='GPU'; T in [DT_INT8]; Tindices in [DT_INT32]  device='GPU'; T in [DT_UINT8]; Tindices in [DT_INT64]  device='GPU'; T in [DT_UINT8]; Tindices in [DT_INT32]  device='GPU'; T in [DT_INT16]; Tindices in [DT_INT64]  device='GPU'; T in [DT_INT16]; Tindices in [DT_INT32]  device='GPU'; T in [DT_UINT16]; Tindices in [DT_INT64]  device='GPU'; T in [DT_UINT16]; Tindices in [DT_INT32]  device='GPU'; T in [DT_UINT32]; Tindices in [DT_INT64]  device='GPU'; T in [DT_UINT32]; Tindices in [DT_INT32]  device='GPU'; T in [DT_INT64]; Tindices in [DT_INT64]  device='GPU'; T in [DT_INT64]; Tindices in [DT_INT32]  device='GPU'; T in [DT_UINT64]; Tindices in [DT_INT64]  device='GPU'; T in [DT_UINT64]; Tindices in [DT_INT32]  device='GPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]  device='GPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]  device='GPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]  device='GPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]  device='GPU'; T in [DT_HALF]; Tindices in [DT_INT64]  device='GPU'; T in [DT_HALF]; Tindices in [DT_INT32] [Op:SparseToDense]              ```**Describe the expected behavior**`tf.sparse.to_dense` should also support complex dtypes.
"
53651,0,524,0,0,0,ArrowIntoTheSky,0,"title:tf.bincount outputs wrong results description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.7.0- Python version: 3.8- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source):N/A- CUDA/cuDNN version: N/A- GPU model and memory: N/A**Standalone code to reproduce the issue**```import tensorflow as tfv = tf.constant([[0, 1, 2],       [3, 4, 5]], dtype=tf.int64)res1 = tf.math.bincount(v,axis=-1,maxlength=0, binary_output=True)res2 = tf.sparse.bincount(v,axis=-1,maxlength=0, binary_output=True)print(res1)print(res2)```**Current output**```tf.Tensor([], shape=(2, 0), dtype=int32)SparseTensor(indices=tf.Tensor([[0 0] [0 1] [0 2] [1 3] [1 4] [1 5]], shape=(6, 2), dtype=int64), values=tf.Tensor([1 1 1 1 1 1], shape=(6,), dtype=int64), dense_shape=tf.Tensor([2 6], shape=(2,), dtype=int64))```**Expected output**For `tf.sparse.bincount`, the output should have length at most `maxlength`, in the above code `maxlength` is `0`, so the output should be an empty tensor.(`tf.math.bincount` is just to show that the output for `tf.sparse.bincount` should be have _same length_ as `tf.math.bincount`. )
"
53642,1,0,0,0,0,chensusu11,0,"title:How to build tensorflow 1.15 with docker build闂傚倸鍊搁崐鐑芥倿閿旈敮鍋撶粭娑樻噽閻瑩鏌熼幑鎰靛殭缁?can only compile the latest tensorflow2.9 according to the official tutorial description:![image](https://user-images.githubusercontent.com/44188056/148174661-c22df66d-c624-4d87-8a7c-02ff97837443.png)I want to switch branch r1.15, but it didn't work闂傚倸鍊搁崐鐑芥倿閿旈敮鍋撶粭娑樻噽閻瑩鏌熼柇锕€骞楃紓宥呮处缁绘盯骞橀梽鍫氭殬kyou闂?"
53636,0,0,0,0,0,code-review-doctor,0,"title:Missing comma in test results in unexpected string concatenation description:**System information**.- Have I written custom code (as opposed to using a stock example script provided in Keras): NA- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): NA- TensorFlow installed from (source or binary): NA- TensorFlow version (use command below): NA- Python version: NA- Bazel version (if compiling from source): NA- GPU model and memory: NA- Exact command to reproduce: NA**Describe the problem**.Absent comma results in unwatned string concatenation on line 330:https://github.com/tensorflow/tensorflow/blob/0d8705c82c64dfb39c49e346de1a66182e5eabd1/tensorflow/python/keras/engine/training_generator_test.py#L330-L336So `'matmul'` gets appended with 'Yaks are also quite nice' resulting in `matmulYaks are also quite nice` being used in the test.**Describe the current behavior**.`'matmulYaks are also quite nice'` is used in the test**Describe the expected behavior**.`'matmul', 'Yaks are also quite nice'` is used in the test**[Contributing](https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md)**.- Do you want to contribute a PR? (yes/no): yes- Briefly describe your candidate solution(if contributing):Add the missing comma**Standalone code to reproduce the issue**.NA**Source code / logs**.NA
"
53633,1,3494,0,1,1,marzdeveloper,0,"title:Dataset.batch() change tensor shape and abort LSTM fit description:**System information**- TensorFlow version: v2.4.0-49-g85c8b2a817f 2.4.1- Platform: Kaggle TPUHi, my LSTM code give this error using dataset.batch():```    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *        return step_function(self, iterator)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **        outputs = model.distribute_strategy.run(run_step, args=(data,))    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/tpu_strategy.py:540 run        return self.extended.tpu_run(fn, args, kwargs, options)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/tpu_strategy.py:1296 tpu_run        return func(args, kwargs)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/tpu_strategy.py:1364 tpu_function        xla_options=tpu.XLAOptions(use_spmd_for_xla_partitioning=False))    /opt/conda/lib/python3.7/site-packages/tensorflow/python/tpu/tpu.py:968 replicate        xla_options=xla_options)[1]    /opt/conda/lib/python3.7/site-packages/tensorflow/python/tpu/tpu.py:1439 split_compile_and_replicate        outputs = computation(*computation_inputs)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/tpu_strategy.py:1325 replicated_fn        result[0] = fn(*replica_args, **replica_kwargs)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **        outputs = model.train_step(data)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:754 train_step        y_pred = self(x, training=True)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:223 assert_input_compatibility        str(tuple(shape)))    ValueError: Input 0 of layer model is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, None, 3, 13)```Meanwhile without dataset.batch() the code works correctly.I'm using Tensorflow dataset from tfrecord files:```def read_tfrecord(example_proto):    feature_description = {    'x': tf.io.FixedLenFeature([], tf.string),    'y': tf.io.FixedLenFeature([], tf.string),    }    example = tf.io.parse_single_example(example_proto, feature_description)    vect = tf.io.parse_tensor(example['x'], tf.float16)    label = tf.io.parse_tensor(example['y'], tf.int8)    vect = tf.reshape(vect, [-1, ws, num_features])    label = tf.reshape(label, [-1, 1])    return vect, label```I have to add the reshape because without i get this error:    TypeError: can't multiply sequence by non-int of type 'NoneType'and here suggest to add reshape: [link][1]```def load_dataset(filenames):    ignore_order = tf.data.Options()    ignore_order.experimental_deterministic = False  # disable order, increase speed    dataset = tf.data.TFRecordDataset(filenames)  # automatically interleaves reads from multiple files    dataset = dataset.with_options(ignore_order)  # uses data as soon as it streams in, rather than in its original order    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)    dataset = dataset.batch(bs)    dataset = dataset.prefetch(buffer_size=AUTOTUNE)    return dataset``````    model = Sequential(name = ""model"")    model.add(LSTM(units=units, input_shape=(ws, num_features), return_sequences=False, dropout = dropout))    model.add(Dense(units=num_labels, activation = ""sigmoid""))    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=[BinaryAccuracy(), AUC(), Precision(), Recall()])```Here you can see the dataset with: ```for parsed_record in parsed_dataset.take(10):  print(repr(parsed_record))```![MicrosoftTeams-image](https://user-images.githubusercontent.com/48318112/148097080-fcb05ce4-9769-421f-9815-90ddf1506ad8.png)  [1]: https://github.com/tensorflow/tensorflow/issues/40864
"
53610,1,0,17,0,0,OmriSteiner,0,"title:Loading a Keras model which contains __ne__ or __eq__ operators fails description:I posted this on Keras first, but it didn't get any attention, so posting here as well.https://github.com/keras-team/keras/issues/15832
"
53586,1,0,0,0,0,DamingoNdiwa,0,"title:TypeError: slice indices must be integers or None or have an __index__ method for @tf.function and when I use tfp.mcmc.NoUTurnSampler() description:**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur- TensorFlow installed from (source or binary): using homebrew- TensorFlow version (use command below): tf-nightly==2.9.0.dev20211224- Tensorflow probability version: tensorflow-probability==0.13.0- Python version: Python 3.9.9**Describe the current behavior**I am unable to use @tf.function with indexing, for instance, the code below gives- Auscomp.Qobs is datat_end = len(Auscomp.Qobs)t_obs = np.linspace(start=0.,                    stop= t_end,                    num=int(t_end)).tolist()Auscomp.Prec.values is observed data values.@tf.function(jit_compile=True)def p(t):    pos = tf.raw_ops.Bucketize(input=t,boundaries=t_obs)    precip_t = Auscomp.Prec.values[pos - 1:pos]    return precip_t[0]- call functionp(1.5) throws the error message TypeError: in user code:    File ""<ipython-input-3-f42dda5a54a8>"", line 20, in p  *        precip_t = Auscomp.Prec.values[pos - 1:pos]    TypeError: slice indices must be integers or None or have an __index__ method**Describe the expected behavior**I expect to have scaler value returned. For instance, if I add tf.config.run_functions_eagerly(True)  at the top of the programme the function works.But I also have functions that solve ODEs and do Bayesian inference with TensorFlow probability. So when I run a function tfp.mcmc.NoUTurnSampler() that calls an ODE that calls the above function, I get the  message belowWARNING:tensorflow:It looks like tf.function behavior was disabled, perhaps using tf.config.run_functions_eagerly. Vectorization primitives (e.g. tf.vectorized_map) require tf.function to work. These primitives will override the disable.and later the errorFile ""<ipython-input-6-7453b3323d01>"", line 27, in precip        precip_t = Auscomp.Prec.values[pos - 1:pos]    TypeError: slice indices must be integers or None or have an __index__ method**Standalone code to reproduce the issue**import numpy as npimport tensorflow as tfimport tensorflow_probability as tfpt_end = 21t_obs = np.linspace(0.0, t_end, 20)- data set with zeros and ones precip = np.zeros(t_end + 1)t_precip = np.linspace(0, t_end + 1, num=t_end + 2).tolist()precip[0::7] = 1.0@tf.functiondef p(t):        pos = tf.raw_ops.Bucketize(input=t,boundaries=t_precip)            precip_t = precip[pos - 1:pos]        return precip_t[0]- call function  p(1.5)
"
53581,0,458,0,0,0,ArrowIntoTheSky,0,"title:tf.ragged.stack_dynamic_partitions silently output wrong result with negative `partitions` description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.7.0- Python version: 3.8- Bazel version (if compiling from source): n/a- GCC/Compiler version (if compiling from source): n/a- CUDA/cuDNN version: Cuda 11.4- GPU model and memory: n/a**Standalone code to reproduce the issue**```import tensorflow as tfdata=['a', 'b', 'c', 'd', 'e']partitions=[3, -2, 2, -1, 2]num_partitions=5t1 = tf.ragged.stack_dynamic_partitions(data, partitions, num_partitions) # Succeedprint(t1)t2 = tf.ragged.stack(tf.dynamic_partition(data, partitions, num_partitions)) # Raise InvalidArgumentErrorprint(t2)```Outputs```<tf.RaggedTensor [[], [], [b'b', b'd'], [b'c'], []]>InvalidArgumentError: partitions[1] = -2 is not in [0, 5) [Op:DynamicPartition]```**Describe the current behavior**For `tf.ragged.stack_dynamic_partitions`, values in `partitions` should be greater or equal to zero. (as the document points out)**Describe the expected behavior**In the document it says ""if `num_partitions` is an int (not a Tensor), then `tf.ragged.stack_dynamic_partitions(data, partitions, num_partitions)` is equivalent to `tf.ragged.stack(tf.dynamic_partition(data, partitions, num_partitions))`"". So similar to the latter case, an `InvalidArgumentError` is expected to be raised when calling `tf.ragged.stack_dynamic_partitions` with `partitions` containing negative values.
"
53575,1,10557,0,0,0,popkristina,0,"title:AttributeError: 'NoneType' object has no attribute 'outer_context' when building a token classification model description:Hi, I am training a token-classification model, and I use ELMO embeddings as one layer of my model.I'm using elmo embeddings from tensorflow hub, and I am using them with tensorflow 2.4.1. The same model architecture worked when built in Tensorflow 1.x, but I need to migrate it to TF2. I followed the instructions on how to replace hub.Module() with hub.KerasLayer() or hub.load() in order to use elmo in TF2. The example is with hub.load(). When using hub.KerasLayer() I cannot use the ""token"" signature. Please note that the same model architecture worked in TF1, but I need a solution to make it work in TF2. This is how I'm building the model: ```def ElmoEmbedding(x):    return elmo_model.signatures[""tokens""](tokens = tf.squeeze(tf.cast(x, tf.string)), sequence_len = tf.constant(batch_size * [max_len]))[""elmo""]def build_model(max_len, n_words, n_tags):     word_input_layer = Input(shape=(max_len, 40, ))    elmo_input_layer = Input(shape=(max_len,), dtype=tf.string)    word_output_layer = Dense(n_tags, activation = 'softmax')(word_input_layer)    elmo_output_layer = Lambda(ElmoEmbedding, output_shape=(None, 1024))(elmo_input_layer)    output_layer = Concatenate()([word_output_layer, elmo_output_layer])    output_layer = BatchNormalization()(output_layer)    output_layer = Bidirectional(LSTM(units=512, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(output_layer)    output_layer = TimeDistributed(Dense(n_tags, activation='softmax'))(output_layer)        model = Model([elmo_input_layer, word_input_layer], output_layer)        return model```And therefore want to train the model as follows:```elmo_model = hub.load('https://tfhub.dev/google/elmo/3')model = build_model(max_len, n_words, n_tags)model.compile(optimizer=""adam"", loss=""sparse_categorical_crossentropy"", metrics=[""accuracy""])model.summary()history = model.fit([np.array(X1_train), np.array(X2_train).reshape((len(X2_train), max_len, 40))],                     y_train,                     validation_data=([np.array(X1_valid), np.array(X2_valid).reshape((len(X2_valid), max_len, 40))], y_valid),                    batch_size=32, epochs=2, verbose=1)```To clarify, X1_train is a list of tokenized sentences and X2_train is a list of hand-picked features for every token in every sentence. The error that I'm getting when calling model.fit is the following one:```---------------------------------------------------------------------------AttributeError                            Traceback (most recent call last)<ipython-input-14-04a3722fa4b2> in <module>     51                     y_train,     52                     validation_data=([np.array(X1_valid), np.array(X2_valid).reshape((len(X2_valid), max_len, 40))], y_valid),---> 53                     batch_size=32, epochs=2, verbose=1)     54 hist = pd.DataFrame(history.history)/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)   1098                 _r=1):   1099               callbacks.on_train_batch_begin(step)-> 1100               tmp_logs = self.train_function(iterator)   1101               if data_handler.should_sync:   1102                 context.async_wait()/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)    826     tracing_count = self.experimental_get_tracing_count()    827     with trace.Trace(self._name) as tm:--> 828       result = self._call(*args, **kwds)    829       compiler = ""xla"" if self._experimental_compile else ""nonXla""    830       new_tracing_count = self.experimental_get_tracing_count()/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)    869       # This is the first call of __call__, so we have to initialize.    870       initializers = []--> 871       self._initialize(args, kwds, add_initializers_to=initializers)    872     finally:    873       # At this point we know that the initialization is complete (or less/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)    724     self._concrete_stateful_fn = (    725         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access--> 726             *args, **kwds))    727     728     def invalid_creator_scope(*unused_args, **unused_kwds):/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)   2967       args, kwargs = None, None   2968     with self._lock:-> 2969       graph_function, _ = self._maybe_define_function(args, kwargs)   2970     return graph_function   2971 /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)   3359    3360           self._function_cache.missed.add(call_context_key)-> 3361           graph_function = self._create_graph_function(args, kwargs)   3362           self._function_cache.primary[cache_key] = graph_function   3363 /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)   3204             arg_names=arg_names,   3205             override_flat_arg_shapes=override_flat_arg_shapes,-> 3206             capture_by_value=self._capture_by_value),   3207         self._function_attributes,   3208         function_spec=self.function_spec,/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)    988         _, original_func = tf_decorator.unwrap(python_func)    989 --> 990       func_outputs = python_func(*func_args, **func_kwargs)    991     992       # invariant: `func_outputs` contains only Tensors, CompositeTensors,/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)    632             xla_context.Exit()    633         else:--> 634           out = weak_wrapped_fn().__wrapped__(*args, **kwds)    635         return out    636 /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)    975           except Exception as e:  # pylint:disable=broad-except    976             if hasattr(e, ""ag_error_metadata""):--> 977               raise e.ag_error_metadata.to_exception(e)    978             else:    979               raiseAttributeError: in user code:    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *        return step_function(self, iterator)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **        outputs = model.distribute_strategy.run(run_step, args=(data,))    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica        return self._call_for_each_replica(fn, args, kwargs)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica        return fn(*args, **kwargs)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **        outputs = model.train_step(data)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:754 train_step        y_pred = self(x, training=True)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:1012 __call__        outputs = call_fn(inputs, *args, **kwargs)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:425 call        inputs, training=training, mask=mask)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:560 _run_internal_graph        outputs = node.layer(*args, **kwargs)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:1012 __call__        outputs = call_fn(inputs, *args, **kwargs)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:917 call        result = self.function(inputs, **kwargs)    <ipython-input-7-21ffba0b5a13>:7 ElmoEmbedding        return elmo_model.signatures[""tokens""](tokens = tf.squeeze(tf.cast(x, tf.string)), sequence_len = tf.constant(batch_size * [max_len]))[""elmo""]    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1669 __call__        return self._call_impl(args, kwargs)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py:247 _call_impl        args, kwargs, cancellation_manager)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1687 _call_impl        return self._call_with_flat_signature(args, kwargs, cancellation_manager)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1736 _call_with_flat_signature        return self._call_flat(args, self.captured_inputs, cancellation_manager)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1924 _call_flat        forward_function, args_with_tangents = forward_backward.forward()    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1448 forward        self._inference_args, self._input_tangents)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1207 forward        self._forward_and_backward_functions(inference_args, input_tangents))    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1407 _forward_and_backward_functions        outputs, inference_args, input_tangents)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:910 _build_functions_for_outputs        src_graph=self._func_graph)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:552 _GradientsHelper        to_ops, from_ops, colocate_gradients_with_ops, func_graphs, xs_set)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:125 _PendingCount        between_op_list, between_ops, colocate_gradients_with_ops)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_state.py:780 MaybeCreateControlFlowState        loop_state.AddWhileContext(op, between_op_list, between_ops)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_state.py:577 AddWhileContext        outer_forward_ctxt = forward_ctxt.outer_context    AttributeError: 'NoneType' object has no attribute 'outer_context'```Any tips would help!
"
53571,1,234,39,0,0,maybeLee,0,"title:AveragePooling1D wrongly accept stride parameter to be None description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.7.0- Python version: 3.7- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source): N/A- CUDA/cuDNN version: 11.3/8- GPU model and memory: N/A**Describe the current behavior**`tf.keras.layers.AveragePooling1D` can output normal value when setting `stride` parameter to `None` with no warning/exceptions. In contrast `tf.keras.layers.Conv1D` will raise an exception to forbit such usage.**Describe the expected behavior**This API should throw an exception or raise a warning instead of outputing a normal value.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): yes- Briefly describe your candidate solution(if contributing): I haven't tried a solution yet and I am not familiar with your coding style to deal with such an issue.**Standalone code to reproduce the issue**```import tensorflow as tfimport numpy as npx = tf.keras.layers.Input((32,3))layer = tf.keras.layers.AveragePooling1D(pool_size=3, strides=None)y = layer(x)model = tf.keras.Model(x,y)model.predict(np.random.rand(10,32,3))```You may access the code here:https://colab.research.google.com/drive/1u80B4LFEEctS5GOZj3GUd_G6u_3A9kcu?usp=sharing
"
53564,0,6948,99,0,0,w3sip,0,"title:v.2.7: Failure to build for iOS description:When building our code for iOS against TensorFlow Lite 2.7.0, we're seeing the following errors:```In file included from /Users/alex/.conan/data/ooo/1.0/ci/local/source/integrations/tflite/TFLiteStage.cpp:37:In file included from /Users/alex/.conan/data/tensorflow-lite/2.7.0.11+fb1696d4/ci/branch_release_2.7/package/3b95326f653baf403ae4d70dae74814d9c4fef8a/lib/TensorFlowLiteCMetal.framework/Headers/TensorFlowLiteCMetal.h:1:In file included from /Users/alex/.conan/data/tensorflow-lite/2.7.0.11+fb1696d4/ci/branch_release_2.7/package/3b95326f653baf403ae4d70dae74814d9c4fef8a/lib/TensorFlowLiteCMetal.framework/Headers/metal_delegate.h:19:In file included from /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Metal.framework/Headers/Metal.h:9:In file included from /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Metal.framework/Headers/MTLTypes.h:8:In file included from /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/Foundation.h:8:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:523:1: error: expected unqualified-id@class NSString, Protocol;^/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:525:9: error: unknown type name 'NSString'typedef NSString * NSExceptionName NS_TYPED_EXTENSIBLE_ENUM;        ^/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:526:9: error: unknown type name 'NSString'typedef NSString * NSRunLoopMode NS_TYPED_EXTENSIBLE_ENUM;        ^/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:528:19: error: unknown type name 'NSString'FOUNDATION_EXPORT NSString *NSStringFromSelector(SEL aSelector);                  ^/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:529:44: error: unknown type name 'NSString'FOUNDATION_EXPORT SEL NSSelectorFromString(NSString *aSelectorName);                                           ^/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:531:19: error: unknown type name 'NSString'FOUNDATION_EXPORT NSString *NSStringFromClass(Class aClass);                  ^/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:532:53: error: unknown type name 'NSString'FOUNDATION_EXPORT Class _Nullable NSClassFromString(NSString *aClassName);                                                    ^/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:534:19: error: unknown type name 'NSString'FOUNDATION_EXPORT NSString *NSStringFromProtocol(Protocol *proto) API_AVAILABLE(macos(10.5), ios(2.0), watchos(2.0), tvos(9.0));                  ^/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:534:50: error: unknown type name 'Protocol'FOUNDATION_EXPORT NSString *NSStringFromProtocol(Protocol *proto) API_AVAILABLE(macos(10.5), ios(2.0), watchos(2.0), tvos(9.0));                                                 ^/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:535:19: error: unknown type name 'Protocol'FOUNDATION_EXPORT Protocol * _Nullable NSProtocolFromString(NSString *namestr) API_AVAILABLE(macos(10.5), ios(2.0), watchos(2.0), tvos(9.0));                  ^/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:535:61: error: unknown type name 'NSString'FOUNDATION_EXPORT Protocol * _Nullable NSProtocolFromString(NSString *namestr) API_AVAILABLE(macos(10.5), ios(2.0), watchos(2.0), tvos(9.0));                                                            ^/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:539:30: error: unknown type name 'NSString'FOUNDATION_EXPORT void NSLog(NSString *format, ...) NS_FORMAT_FUNCTION(1,2) NS_NO_TAIL_CALL;                             ^/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:539:53: error: format argument not an NSStringFOUNDATION_EXPORT void NSLog(NSString *format, ...) NS_FORMAT_FUNCTION(1,2) NS_NO_TAIL_CALL;                             ~~~~~~~~~~~~~~~~       ^                  ~/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:94:49: note: expanded from macro 'NS_FORMAT_FUNCTION'        #define NS_FORMAT_FUNCTION(F,A) __attribute__((format(__NSString__, F, A)))                                                       ^                    ~/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:540:31: error: unknown type name 'NSString'FOUNDATION_EXPORT void NSLogv(NSString *format, va_list args) NS_FORMAT_FUNCTION(1,0) NS_NO_TAIL_CALL;                              ^/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:540:63: error: format argument not an NSStringFOUNDATION_EXPORT void NSLogv(NSString *format, va_list args) NS_FORMAT_FUNCTION(1,0) NS_NO_TAIL_CALL;                              ~~~~~~~~~~~~~~~~                ^                  ~/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:94:49: note: expanded from macro 'NS_FORMAT_FUNCTION'        #define NS_FORMAT_FUNCTION(F,A) __attribute__((format(__NSString__, F, A)))```The same exact code building in the same exact environment against TFLite 2.6.2 builds with no errors.
"
53461,0,1196,297,0,0,anhappdev,0,"title:[TFLite] iOS app crashes with EXC_RESOURCE when building with XCode 13 description:**System information**- Have I written custom code: Yes- OS Platform and Distribution: iOS 15- Mobile device if the issue happens on mobile device: iPhone- TensorFlow installed from: source- TensorFlow version: 2.6.0- Python version: Python 3.9.7- Bazel version: 4.2.1-homebrew- GCC/Compiler version: Apple clang version 13.0.0 (clang-1300.0.29.3)- CUDA/cuDNN version: n.a.- GPU model and memory: n.a.**Describe the current behavior**The app crashes with `EXC_RESOURCE` when running TFLite model on iOS device.* This problem occurs only when the app is built with XCode 13.0 / 13.1 / 13.2 and using Core ML backend.* The problem does not occur when the app is built with XCode 12.**Describe the expected behavior**App run without crash when built with XCode 13**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): yes- Briefly describe your candidate solution(if contributing):@freedomtan has identified the issue:> FYR. I upgraded my macOS, iOS, and Xcode to 12.1, 15.2, and Xcode 13.2 yesterday. Then, I spent some time profiling and found that the culprit is [CoreML delegate's invoke function].(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/coreml/coreml_delegate_kernel.mm#L207-L230). Adding `@autoreleasepool {}` to it resolved the EXC_RESOURCE issue. Tested on iPhone 11 Pro and iPhone 13 running iOS 15.2. Supposedly, it worked for earlier Xcode 13.x and iOS.**Standalone code to reproduce the issue**Not possible since the app need to be built and run.**Other info / logs**The app used is located here: https://github.com/mlcommons/mobile_app_openCrash log:```flutter: Running Benchmark:IS_float32 in performance mode......li:cpp/flutter/main.cc:252@run_backend42021-10-02 14:21:30.769776: I cpp/backends/external.cc:135] Using default allocatorEnabling CoreML delegate 0x2836b5a00CoreML delegate: 76 nodes delegated out of 77 nodes, with 1 partitions.INFO: CoreML delegate: 76 nodes delegated out of 77 nodes, with 1 partitions.li:cpp/flutter/main.cc:257@run_backend42021-10-02 14:21:31.456619: E cpp/datasets/ade20k.cc:76] Failed to list all the ground truth files in provided path. Only measuring performance.li:cpp/flutter/main.cc:285@run_backend4li:cpp/flutter/main.cc:289@run_backend4* thread #26, name = 'DartWorker', queue = 'com.apple.CoreMLBatchProcessingQueue', stop reason = EXC_RESOURCE RESOURCE_TYPE_MEMORY (limit=2098 MB, unused=0x0)    frame #0: 0x00000001f37d864c libsystem_platform.dylib`_platform_memmove + 76libsystem_platform.dylib`_platform_memmove:->  0x1f37d864c <+76>: stnp   q0, q1, [x3]    0x1f37d8650 <+80>: add    x3, x3, #0x20             ; =0x20     0x1f37d8654 <+84>: ldnp   q0, q1, [x1]    0x1f37d8658 <+88>: add    x1, x1, #0x20             ; =0x20 Target 0: (Runner) stopped.```![135967965-1e284466-a072-4b15-8895-9bbd6001a70a](https://user-images.githubusercontent.com/85728587/146400916-bc8d5e6d-5053-41fb-b4ba-2df5bca58fc1.png)
"
53455,1,0,0,0,0,csdf-ssm,0,"title:When I use ObjectDetection Demo,it tips Invalid tensor index 1, max index is 0.How can I do? description:**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MacBook Pro (13-inch, 2017, Two Thunderbolt 3 ports) 11.6 (20G165)- TensorFlow installed from (source or binary):TensorFlowLiteSwift','2.6.0'- TensorFlow version (or github SHA if from source):TensorFlowLiteSwift','2.6.0'question:When I use ObjectDetection Demo,it tips Invalid tensor index 1, max index is 0,How can I do?  I do not know where is Error ,I just use my tflitemodel to replace Demo tflitemodel ,it show last tips:Initialized TensorFlow Lite runtime.INFO: Initialized TensorFlow Lite runtime.Failed to invoke the interpreter with error: Invalid tensor index 1, max index is 0.Failed to invoke the interpreter with error: Invalid tensor index 1, max index is 0.Failed to invoke the interpreter with error: Invalid tensor index 1, max index is 0....
"
53438,1,3308,180,0,0,henrysky,0,"title:RNN predict() raised error if shape is not the same as training set for the first time description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 21H2 x64- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow installed from (source or binary): 2.7.0 source- TensorFlow version (use command below): 2.7.0- Python version: 3.9.7- Bazel version (if compiling from source): 4.2.2- GCC/Compiler version (if compiling from source): msvc v1916- CUDA/cuDNN version: 11.5/ 8.2- GPU model and memory: GTX1060 6GB- **Standalone code to reproduce the issue**https://colab.research.google.com/drive/1Baaf74EzBjmuCUyY8MwFOGHAJwK2BUSP?usp=sharing**Describe the current behavior**```pythonimport numpy as npimport tensorflow as tffrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Dense, Dropout, LSTMmnist = tf.keras.datasets.mnist(x_train, y_train), (x_test, y_test) = mnist.load_data()x_train, x_test = x_train / 255.0, x_test / 255.0sample, sample_label = x_train[0], y_train[0]model = Sequential()# Layersmodel.add(LSTM(128, input_shape=(x_train.shape[1:]), return_sequences=True))model.add(Dropout(0.2))model.add(LSTM(128))model.add(Dropout(0.2))model.add(Dense(32, activation='relu'))model.add(Dropout(0.2))model.add(Dense(10, activation='softmax'))# Optimizeropt = tf.keras.optimizers.Adam(lr=1e-3, decay=1e-5)#Compilemodel.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])#Fitmodel.fit(x_train, y_train, epochs=1, validation_data=(x_test, y_test))```and the run```pythonmodel.predict(x_test[0:1, :10, :])```raised error```---------------------------------------------------------------------------ValueError                                Traceback (most recent call last)~\AppData\Local\Temp/ipykernel_5600/1128729238.py in <module>      1 ans = []      2 for i in range(1, 28):----> 3     ans.append(model.predict(x_test[0:1, :i, :])[0])~\miniconda3\lib\site-packages\keras\utils\traceback_utils.py in error_handler(*args, **kwargs)     65     except Exception as e:  # pylint: disable=broad-except     66       filtered_tb = _process_traceback_frames(e.__traceback__)---> 67       raise e.with_traceback(filtered_tb) from None     68     finally:     69       del filtered_tb~\miniconda3\lib\site-packages\tensorflow\python\framework\func_graph.py in autograph_handler(*args, **kwargs)   1127           except Exception as e:  # pylint:disable=broad-except   1128             if hasattr(e, ""ag_error_metadata""):-> 1129               raise e.ag_error_metadata.to_exception(e)   1130             else:   1131               raiseValueError: in user code:    File ""C:\Users\Henry\miniconda3\lib\site-packages\keras\engine\training.py"", line 1621, in predict_function  *        return step_function(self, iterator)    File ""C:\Users\Henry\miniconda3\lib\site-packages\keras\engine\training.py"", line 1611, in step_function  **        outputs = model.distribute_strategy.run(run_step, args=(data,))    File ""C:\Users\Henry\miniconda3\lib\site-packages\keras\engine\training.py"", line 1604, in run_step  **        outputs = model.predict_step(data)    File ""C:\Users\Henry\miniconda3\lib\site-packages\keras\engine\training.py"", line 1572, in predict_step        return self(x, training=False)    File ""C:\Users\Henry\miniconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler        raise e.with_traceback(filtered_tb) from None    File ""C:\Users\Henry\miniconda3\lib\site-packages\keras\engine\input_spec.py"", line 263, in assert_input_compatibility        raise ValueError(f'Input {input_index} of layer ""{layer_name}"" is '    ValueError: Input 0 of layer ""sequential_1"" is incompatible with the layer: expected shape=(None, 28, 28), found shape=(None, 1, 28)```But then if you run this first```pythonmodel.predict(x_test[0:1])```and then run the same code gives expected result```pythonmodel.predict(x_test[0:1, :10, :])```**Describe the expected behavior**```pythonmodel.predict(x_test[0:1, :10, :])```should run fine the first time without running `model.predict(x_test[0:1])` first.**Briefly describe your candidate solution(if contributing):**Something (a shape checking) is probably wrong when building the predict function for the model? thats why after the first predict() ran without error, the code will work fine.
"
53435,1,0,0,0,0,AdrianFuchsData,0,"title:Memory leak in saving and loading a keras model containing CategoricalEncoding and Lookup layers description:### System information-   **Have I written custom code (as opposed to using a stock example script    provided in TensorFlow)**: Yes-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux release 7.9.2009 (Core)-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue    happens on a mobile device**: not applicable-   **TensorFlow installed from (source or binary)**: binary-   **TensorFlow version (use command below)**: 2.6.0 (v2.6.0-rc2-32-g919f693420e), 2.7.0 (v2.7.0-rc1-69-gc256c071bb2)-   **Python version**: 3.7.11, 3.8.12, 3.9.6-   **Bazel version (if compiling from source)**: not applicable-   **GCC/Compiler version (if compiling from source)**: not applicable-   **CUDA/cuDNN version**: no GPU available-   **GPU model and memory**: no GPU available-   **Exact command to reproduce**: See code below### Describe the problemTo solve a binary classification problem, I have a keras model that processes categorical input (as as well as numeric input).  I need to save (`model.save`) and load (`tf.keras.models.load_model`) the model multiple times (performig training of the model inbetween).  I expect that the model consumes constant disk space and constant RAM everytime I load the model since the architecture does not change (only the parameter values change).This does not happen when the model contains an `IntegerLookup` layer followed by a `CategoryEncoding` layer. The issue can be reproduced without training the model at all.  Here is a minimal code example that creates a model and saves it to disk:    import tensorflow as tf    import numpy as np    input_layer = tf.keras.Input(shape=(1,), dtype=""int32"")     index = tf.keras.layers.IntegerLookup(max_values=2)    index.adapt(np.array(range(2)))    encoder = tf.keras.layers.CategoryEncoding(max_tokens=index.vocab_size())    encoded_layer = encoder(index(input_layer))    output_layer = tf.keras.layers.Dense(2, activation=tf.keras.activations.softmax)(encoded_layer)    model = tf.keras.Model(input_layer, output_layer)    model.compile(optimizer=""adam"", loss=""sparse_categorical_crossentropy"")    model.save(""model"")    Every time I execute    model = tf.keras.models.load_model(""model"")    model.save(""model"")the space that the model consumes on disk increases by approx. 8 kB.The even worse: When I load the model, the RAM useage increases by approx. 9 MB in each iteration.So after 100 iterations, the model needs approx. 1 MB on disk and 950 MB RAM (which *is* problematic).This also happens if I start a new python process in each iteration.In my application, the memory consumption grows even faster because the model has several input layers and also several inner layers.This makes the model unusable after some iterations because I cannot load it anymore.Additionaly, of course, the load and save cycles are getting slower with each repetition.So far, I could reproduce this issue on tensorflow versions 2.6 and 2.7 running on python 3.7, 3.8 or 3.9. The behavior is identical.
"
53432,1,1873,53,0,0,mainguyenanhvu,0,"title:Tf Keras Dense layer output nan with kernel_initializer='random_normal' before fitting description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary): pip- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0- Python version: 3.7- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version: 11.2- GPU model and memory:**Describe the current behavior**Before fitting, I used model.predict with my train set, It was ensure that train set was free of nan values. Next, I printed output of each layer. The output is:```0 [[ 1.000000e-03 -1.310000e-01 -9.504200e-02 ...  0.000000e+00   0.000000e+00  1.000000e+00] [ 0.000000e+00 -1.000000e-02  1.556190e+00 ...  0.000000e+00   0.000000e+00  1.000000e+00] [ 1.000000e+00  5.763000e+00  3.027034e+00 ...  0.000000e+00   0.000000e+00  1.000000e+00] ... [ 8.930000e-01 -5.760000e-01  4.266911e+00 ...  0.000000e+00   0.000000e+00  1.000000e+00] [ 1.000000e+00  5.978000e+00  4.650479e+00 ...  0.000000e+00   0.000000e+00  1.000000e+00] [ 8.500000e-01  8.910000e-01  7.704330e-01 ...  0.000000e+00   0.000000e+00  1.000000e+00]]1 [[ 1.000000e-03 -1.310000e-01 -9.504200e-02 ...  0.000000e+00   0.000000e+00  1.000000e+00] [ 0.000000e+00 -1.000000e-02  1.556190e+00 ...  0.000000e+00   0.000000e+00  1.000000e+00] [ 1.000000e+00  5.763000e+00  3.027034e+00 ...  0.000000e+00   0.000000e+00  1.000000e+00] ... [ 8.930000e-01 -5.760000e-01  4.266911e+00 ...  0.000000e+00   0.000000e+00  1.000000e+00] [ 1.000000e+00  5.978000e+00  4.650479e+00 ...  0.000000e+00   0.000000e+00  1.000000e+00] [ 8.500000e-01  8.910000e-01  7.704330e-01 ...  0.000000e+00   0.000000e+00  1.000000e+00]]2 [[nan nan nan nan] [nan nan nan nan] [nan nan nan nan] ... [nan nan nan nan] [nan nan nan nan] [nan nan nan nan]]```**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.```inputs = Input(shape=data.shape)x = Flatten()(inputs)x = Dense(data.classnum, kernel_initializer='random_normal')(x)x = tf.keras.layers.Softmax()(x)model = tf.keras.Model(inputs=inputs, outputs=x)for idx in range(len(model.layers)):        # print(model.layers[idx].name,model.layers[idx].get_weights())        intermediate_layer_model = keras.Model(inputs=model.input,                                    outputs=model.layers[idx].output)        intermediate_output = intermediate_layer_model.predict(data.train_dataset)        print(idx,intermediate_output)```
"
53423,0,0,0,0,0,yakovdan,0,"title:Hexagon delegate fails on a known good model description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Snapdragon 855- TensorFlow installed from (source or binary): source- TensorFlow version (use command below): 2.7- Python version: 3.7- Bazel version (if compiling from source): 3.7.2- GCC/Compiler version (if compiling from source): gcc 7.5.0- CUDA/cuDNN version: N/A- GPU model and memory: N/A**Describe the current behavior**When running inference with TFlite and Hexagon delegate,initialization fails with the following message:09-19 08:24:18.566  4958  4958 I tflite  : Created TensorFlow Lite XNNPACK delegate for CPU.09-19 08:24:18.568  4958  4958 E tflite  : Calling prepare multiple times09-19 08:24:18.568  4958  4958 E tflite  : Node number 90 (TfLiteHexagonDelegate) failed to prepare.09-19 08:24:18.775  4958  4958 E tflite  : Restored original execution plan after delegate application failure.09-19 08:24:18.782  4958  4958 E tflite  : Error in applying the default TensorFlow Lite delegate indexed at 0, and all previously applied delegates are reverted.**Describe the expected behavior**When using the same model with the same code compiled against TensorFlow 2.3, it works fine.See attached the model in question.[model.zip](https://github.com/tensorflow/tensorflow/files/7713641/model.zip)
"
53399,1,578,25,0,0,abhmul,0,"title:tf.reduce_logsumexp throws exception when reducing over RaggedTensor description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  **Ubuntu 18.04.6 LTS**- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow installed from (source or binary): **binary**- TensorFlow version (use command below): **v2.7.0-rc1-69-gc256c071bb2 2.7.0**- Python version: **Python 3.9.0**- Bazel version (if compiling from source): **N/A**- GCC/Compiler version (if compiling from source): **N/A**- CUDA/cuDNN version: **N/A**- GPU model and memory: **N/A**You can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**: When reducing over a ragged dimension, `tf.reduce_logsumexp` throws a `ValueError: TypeError: object of type 'RaggedTensor' has no len()`.**Describe the expected behavior**: When reducing over a ragged dimension, `tf.reduce_logsumexp` should successfully reduce and return a non-ragged tensor. This matches the behavior of other reduce operations like `tf.reduce_sum` or `tf.reduce_max`/**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): **no**- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**:Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.```pythonimport tensorflow as tfa = tf.ragged.constant([[0, 0, 0], [0, 0]], dtype=tf.float32)a.shape  # TensorShape([2, None])tf.reduce_sum(a, axis=1)# >>> <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>tf.reduce_logsumexp(a, axis=1)  # >>> ValueError: TypeError: object of type 'RaggedTensor' has no len()```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.This same error shows up when you try to transpose a ragged tensor.A simple workaround:```pythona_max = tf.reduce_max(a, axis=1, keepdims=True)tf.math.log(tf.math.reduce_sum(tf.math.exp(a - a_max), axis=1)) + a_max# >>> <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.0986123, 0.6931472], dtype=float32)>``````
"
53397,1,0,8,0,0,IchiruTake,0,"title:[BUG] Retrained model on new session cause garbage for saving  description:I still get this issues at Tensorflow 2.5+, Tensorflow-GPU 2.5+, Keras-2.4.3. h5py=3.1.0Even reinstalling Tensorflow, keras, and h5py does not resolve the problem.The model I made is just a stack of Dense layer without anything special. Similar to all people, I can evaluate and predict good in the same training kernel after training. But saving a model by `model.save()` after training, and then reload the trained model for re-training cause weight reinitialization. Despite the arg `compile=True` or `compile=False`, or even `model.reset_states()` or not, The error still occurredNote 1: This error appeared too long, from 2016 till now but it is not resolvedNote 2: Test have been made on TensorFlow 2.5, 2.6, ... but errors still being foundSimilar Issue: # 4875 in Keras (keras-team/keras#4875)The pipeline is similar. (This model is pure of stack of Dense layers)1) Create the model2) Compile2) Train model3) Save the model. -> The model still fine for prediction---------------------------------------------------------------------------------NEW KERNEL:4) Re-load the model (3)5) Save the model (4)-> Still okBUT---------------------------------------------------------------------------------NEW KERNEL:4) Re-load the model (3)5) Retrain the model by `model.fit()` (4) -> Cause new additional weight initializer: I believe this is made by weight += new_initialized_weights6) Save the model --> Model used the weight from beginning at the first step of (5) after retrainingAlthough I have digged into the source code but no sign of this behaviour `weight += new_initialized_weights` was found. But the Keras loss at the first batch during re-training phase is large enough so that I could notify this error. I believe this may come from the global variables state maintained in the source, The callbacks contained the same configuration ModelCheckpoint at two phases
"
53396,0,2000,237,0,0,DocGarbanzo,0,"title:module 'tensorflow.python.compiler.tensorrt.utils' has no attribute 'versionTupleToString' description:# Tensorrt conversion fails on Ubuntu **System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary): 2.7 binary (via pip)- TensorFlow version (use command below): 2.7- Python version: v2.7.0-rc1-69-gc256c071bb2 2.7.0- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version: 11.5- GPU model and memory:  Geforce RTX 2060 Super**Describe the current behavior**The tensorrt conversion fails when the installed version 7.2.3 is higher than the linked version 7.2.2, because the python code in the logger statements is wrong.**Describe the expected behavior**Tensorrt conversion is expected to succeed.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.Not needed here, just look at `trt_convert.py:229ff` - the function `trt_utils.versionTupleToString` is called multiple times but is not defined, there is only `trt_utils._version_tuple_to_string`. The logger only gets called when there is a version mismatch.  A simple ```pythonversionTupleToString = _version_tuple_to_string```in `utils.py` will solve it.File `trt_convert.py:229ff`:```python  def raise_trt_version_deprecated(version_type, trt_version):    assert version_type in [        ""linked"", ""loaded""    ], (""Incorrect value received for version_type: %s. Accepted: ['linked', ""        ""'loaded']"") % version_type    logging.error(        ""The {version_type} version of TensorRT: `{trt_version}` has now ""        ""been removed. Please upgrade to TensorRT 7 or more recent."".format(            version_type=version_type,            trt_version=trt_utils.versionTupleToString(trt_version)))    raise RuntimeError(""Incompatible %s TensorRT versions"" % version_type)  if not trt_utils.is_linked_tensorrt_version_greater_equal(7, 0, 0):    raise_trt_version_deprecated(""linked"", linked_version)  if not trt_utils.is_loaded_tensorrt_version_greater_equal(7, 0, 0):    raise_trt_version_deprecated(""loaded"", loaded_version)  if (loaded_version[0] != linked_version[0] or      not trt_utils.is_loaded_tensorrt_version_greater_equal(*linked_version)):    logging.error(        ""Loaded TensorRT %s but linked TensorFlow against TensorRT %s. A few ""        ""requirements must be met:\n""        ""\t-It is required to use the same major version of TensorRT during ""        ""compilation and runtime.\n""        ""\t-TensorRT does not support forward compatibility. The loaded ""        ""version has to be equal or more recent than the linked version."",        trt_utils.versionTupleToString(loaded_version),        trt_utils.versionTupleToString(linked_version))    raise RuntimeError(""Incompatible TensorRT major version"")  elif loaded_version != linked_version:    logging.info(        ""Loaded TensorRT %s and linked TensorFlow against TensorRT %s. This is ""        ""supported because TensorRT minor/patch upgrades are backward ""        ""compatible."", trt_utils.versionTupleToString(loaded_version),        trt_utils.versionTupleToString(linked_version))```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
53391,1,1035,2,0,0,aritraMandal02,0,"title:AttributeError: 'Sequential' object has no attribute 'predict_classes' description:```seed_text = ""I've got a bad feeling about this""next_words = 100  for _ in range(next_words):	token_list = tokenizer.texts_to_sequences([seed_text])[0]	token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')	predicted = model.predict_classes(token_list, verbose=0)	output_word = """"	for word, index in tokenizer.word_index.items():		if index == predicted:			output_word = word			break	seed_text += "" "" + output_wordprint(seed_text)```This codeblock is giving me the error shown below:```AttributeError                            Traceback (most recent call last)<ipython-input-52-70399a5f93d3> in <module>()      5         token_list = tokenizer.texts_to_sequences([seed_text])[0]      6         token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')----> 7         predicted = model.predict_classes(token_list, verbose=0)      8         output_word = """"      9         for word, index in tokenizer.word_index.items():AttributeError: 'Sequential' object has no attribute 'predict_classes'```How can I solve this error? Please help.Here is the link of the full code:https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbDVSTGdJczNoa2ZBeXpObWxobGJldGd5Q2pLd3xBQ3Jtc0trSVRobkVBM2EtV3lTajlQNzhXbEhGTjN3WXhFTGo3d1U4b3h1Rjl4Rk9ubUVVWG95cUNfaWFoTHpmRlpEM1VmM2NrOXFqc21WYkRKZ0c3WWgtcE5VRV9kZF9pZDZMUDVmWnRtNUtMS3NzUnZodFB3RQ&q=https%3A%2F%2Fgoo.gle%2F3aSTLGx
"
53373,1,0,13,0,0,Didi3333,0,"title:tfjs-v3.12.0 and Wasm 3.12.0 for tfjs-models/pose-detection/  MoveNet  multipose wasm description:When I try to run the demo of the model : tfjs-models/pose-detection/  : https://storage.googleapis.com/tfjs-models/demos/pose-detection/index.html?model=movenet With tfjs-v3.12.0 and Wasm 3.12.0 and configuration model MoveNet type Multipose and backend Wasm  i got the following error :Error: Kernel 'Reciprocal' not registered for backend 'wasm'![image](https://user-images.githubusercontent.com/32233417/145433791-63df6bdb-87a1-4e08-a7cb-b72e41773c18.png)The demo Woks well for the singlepose type (lightning).Regards
"
53370,1,3518,0,0,0,ThanasisGiak,0,"title:Quantized Convolution Layers Operation in TF-lite description:Hello to everyone, for academic and research purposes I am trying to understand the operation behind a quantized convolution layer in Tensorflow Lite. For this purpose, I chose EffiecientNet-lite0 model. So I downloaded pretrained EfficientNet-lite0 float32 and int8 tflite files from the official [repository](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite) and run inference of these models using a sample .jpg image. Firstly, I checked model's architecture and some details using Netron tool and decided to pick first Conv2D layer as my case study.![netron](https://user-images.githubusercontent.com/57605047/145392585-5a4393f8-5ab5-4e36-864a-9927c9782a63.png)I started with fp32 model inference as I thought it will be simplier and used the code below for preprocess the image and for the inference of the model.```import tensorflow as tfimport numpy as npimport PIL.Image as ImageMEAN_RGB = 127.0STDDEV_RGB = 128.0CROP_PADDING = 32IMAGE_SIZE = 224def _decode_and_center_crop(image, image_size, resize_method=Image.BICUBIC):    """"""Crops to center of image with padding then scales image_size.""""""    image_width, image_height = image.size    padded_center_crop_size = int((image_size / (image_size + CROP_PADDING)) * min(image_height, image_width))    offset_height = ((image_height - padded_center_crop_size) + 1) // 2    offset_width = ((image_width - padded_center_crop_size) + 1) // 2    crop_window = [offset_width, offset_height,                    offset_width + padded_center_crop_size,                    offset_height + padded_center_crop_size]    resized_image = image.crop(crop_window)    resized_image = resized_image.resize((image_size, image_size), resize_method)    return resized_imagewith open('image_net_classes.txt') as f:    lines = f.readlines()image = Image.open('beagle.jpg')resized_image = _decode_and_center_crop(image, IMAGE_SIZE)resized_image = np.array(resized_image).astype(np.float32)resized_image -= MEAN_RGBresized_image /= STDDEV_RGBresized_image = np.expand_dims(resized_image, axis=0)# Load TFLite model and allocate tensors.interpreter = tf.lite.Interpreter(model_path=""efficientnet-lite0-fp32.tflite"", experimental_preserve_all_tensors=True)interpreter.allocate_tensors()# Get input and output tensors.input_details = interpreter.get_input_details()output_details = interpreter.get_output_details()input_shape = input_details[0]['shape']interpreter.set_tensor(input_details[0]['index'], resized_image)interpreter.invoke()for t in interpreter.get_tensor_details():  if t['index'] == 102:    test = interpreter.get_tensor(t['index'])print(test.shape)output_data = interpreter.get_tensor(output_details[0]['index'])string = str(np.argmax(output_data))for line in lines:    if string in line:        print('The image is a', line)        break```So after I downloaded Conv2D layer's parameters(kernel) I implented fused Relu6 Conv2D layer using simple Python and later came back to compare results and everything was working pretty good.So the next step was to implement quantized Conv2D layer of EfficientNet-lite0-int8 and used the code below.```MEAN_RGB = 127.0STDDEV_RGB = 128.0IMAGE_SIZE = 224scale = 0.012566016986966133zero_point = 131image = Image.open('beagle.jpg')resized_image = _decode_and_center_crop(image, IMAGE_SIZE)resized_image = np.array(resized_image).astype(np.float32)resized_image -= MEAN_RGBresized_image /= STDDEV_RGBresized_image = np.expand_dims(resized_image, axis=0)resized_image = resized_image / scale + zero_pointresized_image = np.array(resized_image).astype(np.uint8)interpreter = tf.lite.Interpreter(model_path=""efficientnet-lite0-int8.tflite"", experimental_preserve_all_tensors=True)interpreter.allocate_tensors()# Get input and output tensors.input_details = interpreter.get_input_details()output_details = interpreter.get_output_details()input_shape = input_details[0]['shape']interpreter.set_tensor(input_details[0]['index'], resized_image)interpreter.invoke()for t in interpreter.get_tensor_details():  if t['index'] == 102:    test = interpreter.get_tensor(t['index'])print(test.shape)output_data = interpreter.get_tensor(output_details[0]['index'])for line in lines:    if string in line:        print('The image is a', line)        break```I also studied this [paper](https://arxiv.org/pdf/1712.05877.pdf) that provided this Conv layer's implemetation [here](https://github.com/tensorflow/tensorflow/blob/4952f981be07b8bf508f8226f83c10cdafa3f0c4/tensorflow/contrib/lite/kernels/internal/reference/reference_ops.h#L248-L314). So in my understanding, the quantized Convolution Operation is the same as the full precision one but you have also to take into account offsets and scales. I was able to extract input,output,kernel and biases scales and offsets and managed to transform double multipliers to quantized multipliers and right shift using fuctions defined [here](https://github.com/tensorflow/tensorflow/blob/4952f981be07b8bf508f8226f83c10cdafa3f0c4/tensorflow/contrib/lite/kernels/internal/quantization_util.cc) to be able to do the needed operation below.```acc = MultiplyByQuantizedMultiplierSmallerThanOne(              acc, output_multiplier, output_shift);```That I assume is a per-axis operation. **So my question here, is that for this operation the multiplier we use is a quantized multiplier that equals Mo = (Sinput * Skernel) / Soutput or it's something else?**So after I wrote a Python implementation of the above quantized Con2D layer and checked the results using```for t in interpreter.get_tensor_details():  if t['index'] == 102:    test = interpreter.get_tensor(t['index'])```there was a big deviation. Firstly, I thought that the above layer was a fused Relu6 Conv2D layer so I was expecting output tensor's values to be between 0 and 6 but that was not the case.**Could you please provide me a more detailed description of a quantized fused Relu6 Conv2D layer's operation?**### Parameters definedThe sample image I used for inference.![beagle](https://user-images.githubusercontent.com/57605047/145397592-f2d5b484-d91e-4dbd-be53-93f1135a7844.jpg)I am using Google Collaboratory to run the above code snippets.**Tensorflow Version:** 2.7.0**Python version:**  3.7.12**Numpy version:** 1.19.5
"
53365,1,6228,3,0,0,enfild,0,"title:Interpreter->invoke() calls Segmentation Fault description:**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 with USB Coral. - TensorFlow installed from (source or binary): source- TensorFlow version (or github SHA if from source): v2.5.0, v2.6.0**Description**When i calls `initTfLiteInterpreter()`, tfLite work is correct, output has all information (boxes, labels and classes), time of invoke is good (15 ms). But when i calls `processingFrame(cv::Mat)` in other classes with equalent code (i get Seg fault on `Interpreter->invoke()`):```        // From other class for(int i = 0; i< 10; i++)    {        cv::Mat testImage = cv::imread(TestClass->EXAMPLE_FRAME);        TestClass->processingFrame(testImage);    }```I get this error with TF 2.5.0, 2.6.0. **Source of my programm****TestClass.h:**```/// Build EDGE Interpreter for Coral    void BuildEdgeTpuInterpreter(const tflite::FlatBufferModel &model,                                edgetpu::EdgeTpuContext *edgetpu_context);    // Load graph to coral    void initTfLiteInterpreter();    // Processing the received frame    void processingFrame(cv::Mat& frame);    int num_threads = 1;    std::unique_ptr<tflite::Interpreter> interpreter;    std::shared_ptr<edgetpu::EdgeTpuContext> tpu_context;    TfLiteTensor* input_tensor;    TfLiteTensor* output_locations;    TfLiteTensor* output_classes;    TfLiteTensor* output_scores;    TfLiteTensor* num_detections_;    int height;    int width;    int channels;    int row_elems;```**TestClass.cxx:**```void TestClass::BuildEdgeTpuInterpreter(const tflite::FlatBufferModel &model,                                                               edgetpu::EdgeTpuContext *edgetpu_context){    tflite::ops::builtin::BuiltinOpResolver resolver;    resolver.AddCustom(edgetpu::kCustomOp, edgetpu::RegisterCustomOp());    if (tflite::InterpreterBuilder(model, resolver)(&interpreter) != kTfLiteOk) {        std::cerr << ""Failed to build interpreter."" << std::endl;        return;    }    // Allocate tensor buffers.    // Bind given context with interpreter.    interpreter->SetExternalContext(kTfLiteEdgeTpuContext, edgetpu_context);    interpreter->SetNumThreads(1);    if (interpreter->AllocateTensors() != kTfLiteOk)    {      std::cerr << ""Failed to allocate tensors."" << std::endl;    }}void TestClass::initTfLiteInterpreter(void){    auto model = tflite::FlatBufferModel::BuildFromFile(GRAPH.c_str());    tpu_context = edgetpu::EdgeTpuManager::GetSingleton()->OpenDevice();    std::cout << ""Checking readiness of Coral device"" << std::endl;    if(!tpu_context->IsReady())    {        std::cout << ""Coral device is not ready"" << std::endl;        throw -1;    }    std::cout << ""EDGE TPU path: "" << tpu_context->GetDeviceEnumRecord().path << std::endl;    BuildEdgeTpuInterpreter(*model, tpu_context.get());    input_tensor = interpreter->tensor(interpreter->inputs()[0]);    output_locations = interpreter->tensor(interpreter->outputs()[0]);    output_classes = interpreter->tensor(interpreter->outputs()[1]);    output_scores = interpreter->tensor(interpreter->outputs()[2]);    num_detections_ = interpreter->tensor(interpreter->outputs()[3]);    height = input_tensor->dims->data[1];    width = input_tensor->dims->data[2];    channels = input_tensor->dims->data[3];    row_elems = width * channels;    for(int i = 0; i< 10; i++)    {        cv::Mat testImage = cv::imread(EXAMPLE_FRAME);        processingFrame(testImage);    }        Utils::dual_write(""CNN is ready, example frame was processed"");    m_readyFlag.store(true);}void TestClass::processingFrame(cv::Mat& frame){    Q_ASSERT(q_ptr);    const clock_t begin_time = clock();    QMutexLocker locker(&m_mutex);    qDebug() << ""cv mat size: "" << width << height;    cvtColor(frame, frame, cv::COLOR_BGR2RGB);    // Resize for model input    cv::resize(frame, frame, cv::Size(width, height));    if (input_tensor->type != kTfLiteUInt8 ||           //        input_tensor->dims->data[0] != 1 ||             //        input_tensor->dims->data[1] != height ||  //        input_tensor->dims->data[2] != width ||   //        input_tensor->dims->data[3] != channels) {    std::cerr << ""Input tensor shape does not match input image"" << std::endl;    return;    }    uint8_t* dst = input_tensor->data.uint8;    for (int row = 0; row < height; row++) {        memcpy(dst, frame.ptr(row), row_elems);        dst += row_elems;    }    if(interpreter->Invoke() != kTfLiteOk)        qDebug() << ""Invoke is broken"";    qDebug() << ""Invoke is done!"";    const float* detection_locations = output_locations->data.f;    const float* detection_classes = output_classes->data.f;    const float* detection_scores = output_scores->data.f;    const int num_detections = *(num_detections_->data.f);    for (int i = 0; i < num_detections; i++) {        const float score = detection_scores[i];        const std::string label = std::to_string(uint8_t(detection_classes[i]));        const float yMin = detection_locations[4 * i + 0];        const float xMin = detection_locations[4 * i + 1];        const float yMax = detection_locations[4 * i + 2];        const float xMax = detection_locations[4 * i + 3];        if (score > thresholdScore) {            std::cout << label << "" score:"" << score << std::endl;            emit q_ptr->returnBoundingBoxes(frame, yMin, xMin, yMax, xMax, score, label, true);        }    }    std::cout << ""time: "" << float( clock () - begin_time ) /  CLOCKS_PER_SEC << std::endl;    emit q_ptr->finishedCNNProcessing(frame);}```**LOGS**```Checking readiness of Coral deviceEDGE TPU path: /sys/bus/usb/devices/2-1cv mat size:  640 480Invoke is done!1 score:0.902344time: 0.022215cv mat size:  640 480Invoke is done!1 score:0.902344time: 0.012465cv mat size:  640 480Invoke is done!1 score:0.902344time: 0.011841cv mat size:  640 480Invoke is done!1 score:0.902344time: 0.011659cv mat size:  640 480Invoke is done!1 score:0.902344time: 0.014413cv mat size:  640 480Invoke is done!1 score:0.902344time: 0.011502cv mat size:  640 480Invoke is done!1 score:0.902344time: 0.012496cv mat size:  640 480Invoke is done!1 score:0.902344time: 0.012136cv mat size:  640 480Invoke is done!1 score:0.902344time: 0.012898cv mat size:  640 480Invoke is done!1 score:0.902344time: 0.012129Thu Dec  9 11:52:59 2021:  CNN is ready, example frame was processedcv mat size:  640 480Segmentation fault (core dumped)```**GDB out**```0x000000000067d47c in tflite::ops::custom::detection_postprocess::DecodeCenterSizeBoxes(TfLiteContext*, TfLiteNode*, tflite::ops::custom::detection_postprocess::OpData*)```
"
53356,0,775,27,0,1,leandro-gracia-gil,0,"title:Passing empty tensors to TFLite converted signatures fails with an exception description:### 1. System information- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04- TensorFlow installation (pip package or built from source): pip- TensorFlow library (version, if pip package or github SHA, if built from source): tf-nightly 2.8.0-dev20211203 and 2.7 are both affected### 2. Code```pythonimport tensorflow as tfclass TestModel(tf.keras.models.Model):  @tf.function  def test(self, x):    return xtest_model = TestModel()signatures = [test_model.test.get_concrete_function(tf.TensorSpec([None], tf.float32))]converter = tf.lite.TFLiteConverter.from_concrete_functions(signatures, test_model)converter.optimizations = [tf.lite.Optimize.DEFAULT]converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]tflite_model = converter.convert()interpreter = tf.lite.Interpreter(model_content=tflite_model)interpreter.allocate_tensors()# This raises ""ValueError: Cannot set tensor: Tensor is unallocated. Try calling allocate_tensors() first""result = interpreter.get_signature_runner()(x=tf.zeros([0], tf.float32))```### 3. Failure after conversionConversion seems to work fine, but trying to run the signature fails with the following exception if an empty tensor is passed.`ValueError: Cannot set tensor: Tensor is unallocated. Try calling allocate_tensors() first`Despite the message, calling `allocate_tensors()` before has no effect. Passing a tensor that is not empty does not cause the failure.While passing an empty tensor might seem absurd at first, I'm hitting this issue in much more subtle scenarios like passing shapes of tensors (it fails if the shape is for a scalar because it's empty), or when using ragged tensors where one of its internal row splits happens to be empty.Note that returning empty tensors from within a TFLite converted function also fails. In that case you get this other exception:`ValueError: Invalid tensor size.`A workaround would be much appreciated if possible, though it seems unlikely without a proper fix.
"
53345,0,6427,280,0,0,frgfm,0,"title:CTC Loss throwing errors in half/mixed precision description:Hello there :wave: I have been running some experiments with automatic mixed-precision using TensorFlow recently. And I encountered an issue with the CTC loss, which throws an obscure dtype incompatibility issue. And I'm not sure how to solve this :thinking: Any ideas?**System information**- Have I written custom code: yes, the code snippet- OS Platform and Distribution: Linux Ubuntu 20.04- TensorFlow installed from: binary, via pip- TensorFlow version: 2.6.0- Python version: 3.8- CUDA/cuDNN version: CUDA 11.4 (cuDNN 8.2.0)- GPU model and memory: NVIDIA GeForce RTX 2070 with Max-Q Design**Describe the current behavior**As of now, running the snippet further down below throws the following error:```---------------------------------------------------------------------------InvalidArgumentError                      Traceback (most recent call last)<ipython-input-5-2c1811c7c1ed> in <module>     13 model_output = tf.random.uniform((batch_size, out_chans, vocab_size + 1), minval=0, maxval=1, dtype=tf.float16)     14 ---> 15 tf.nn.ctc_loss(gt, model_output, seq_len, input_length, logits_time_major=False, blank_index=vocab_size)~/miniconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)    204     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""    205     try:--> 206       return target(*args, **kwargs)    207     except (TypeError, ValueError):    208       # Note: convert_to_eager_tensor currently raises a ValueError, not a~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/ctc_ops.py in ctc_loss_v3(labels, logits, label_length, logit_length, logits_time_major, unique, blank_index, name)    960     blank_index = 0    961 --> 962   return ctc_loss_dense(    963       labels=labels,    964       logits=logits,~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/ctc_ops.py in ctc_loss_dense(labels, logits, label_length, logit_length, logits_time_major, unique, blank_index, name)   1094       return result[0], grad   1095 -> 1096     return compute_ctc_loss(*args)   1097    1098 ~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/custom_gradient.py in __call__(self, *a, **k)    307     308   def __call__(self, *a, **k):--> 309     return self._d(self._f, a, k)    310     311 ~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/custom_gradient.py in decorated(wrapped, args, kwargs)    261     262     if context.executing_eagerly():--> 263       return _eager_mode_decorator(wrapped, args, kwargs)    264     else:    265       return _graph_mode_decorator(wrapped, args, kwargs)~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/custom_gradient.py in _eager_mode_decorator(f, args, kwargs)    489   """"""Implement custom gradient decorator for eager mode.""""""    490   with tape_lib.VariableWatcher() as variable_watcher:--> 491     result, grad_fn = f(*args, **kwargs)    492   args = nest.flatten(args)    493   all_inputs = list(args) + list(kwargs.values())~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/ctc_ops.py in compute_ctc_loss(logits_t, labels_t, label_length_t, logit_length_t, *unique_t)   1086       if unique_t:   1087         kwargs[""unique""] = unique_t-> 1088       result = ctc_loss_and_grad(**kwargs)   1089       def grad(grad_loss):   1090         grad = [array_ops.reshape(grad_loss, [1, -1, 1]) * result[1]]~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/ctc_ops.py in ctc_loss_and_grad(logits, labels, label_length, logit_length, unique)    689     690   ilabel_log_probs = nn_ops.log_softmax(logits)--> 691   state_log_probs = _ilabel_to_state(labels, num_labels, ilabel_log_probs)    692   state_trans_probs = _ctc_state_trans(labels)    693   initial_state_log_probs, final_state_log_probs = ctc_state_log_probs(~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/ctc_ops.py in _ilabel_to_state(labels, num_labels, ilabel_log_probs)    591   one_hot = array_ops.expand_dims(one_hot, axis=0)    592   ilabel_log_probs = array_ops.expand_dims(ilabel_log_probs, axis=2)--> 593   state_log_probs = math_ops.reduce_sum(ilabel_log_probs * one_hot, axis=3)    594   state_log_probs = array_ops.concat([state_log_probs, blank], axis=2)    595   return array_ops.pad(~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)   1365         #   r_binary_op_wrapper use different force_same_dtype values.   1366         x, y = maybe_promote_tensors(x, y, force_same_dtype=False)-> 1367         return func(x, y, name=name)   1368       except (TypeError, ValueError) as e:   1369         # Even if dispatching the op failed, the RHS may be a tensor aware~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py in _mul_dispatch(x, y, name)   1708     return sparse_tensor.SparseTensor(y.indices, new_vals, y.dense_shape)   1709   else:-> 1710     return multiply(x, y, name=name)   1711    1712 ~/miniconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)    204     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""    205     try:--> 206       return target(*args, **kwargs)    207     except (TypeError, ValueError):    208       # Note: convert_to_eager_tensor currently raises a ValueError, not a~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py in multiply(x, y, name)    528   """"""    529 --> 530   return gen_math_ops.mul(x, y, name)    531     532 ~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py in mul(x, y, name)   6234       return _result   6235     except _core._NotOkStatusException as e:-> 6236       _ops.raise_from_not_ok_status(e, name)   6237     except _core._FallbackException:   6238       pass~/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)   6939   message = e.message + ("" name: "" + name if name is not None else """")   6940   # pylint: disable=protected-access-> 6941   six.raise_from(core._status_to_exception(e.code, message), None)   6942   # pylint: enable=protected-access   6943 ~/miniconda3/lib/python3.8/site-packages/six.py in raise_from(value, from_value)InvalidArgumentError: cannot compute Mul as input #1(zero-based) was expected to be a half tensor but is a float tensor [Op:Mul]```**Describe the expected behavior**the snippet running smoothly**Standalone code to reproduce the issue**```pythonimport tensorflow as tffrom tensorflow.keras import mixed_precisionmixed_precision.set_global_policy('mixed_float16')batch_size = 2out_chans = 32vocab_size = 10gt = tf.zeros((batch_size, 5), dtype=tf.int32)seq_len = [3, 4]input_length = tf.fill((batch_size,), out_chans)model_output = tf.random.uniform((batch_size, out_chans, vocab_size + 1), minval=0, maxval=1, dtype=tf.float16)tf.nn.ctc_loss(gt, model_output, seq_len, input_length, logits_time_major=False, blank_index=vocab_size)```
"
53339,1,1429,1,0,0,AdarshWase,0,"title:TensorFlow TextVectorization producing Ragged Tensor with no padding after loading it from pickle. description:This is my TextVectorization layer:```strip_chars = string.punctuation + '濠?strip_chars = strip_chars.replace('[', '')strip_chars = strip_chars.replace(']', '')vocab_size = 15000sequence_length = 20batch_size = 64def custom_standardization(input_string):  lowercase = tf.strings.lower(input_string)  return tf.strings.regex_replace(lowercase, '[%s]' % re.escape(strip_chars), '')eng_vectorization = TextVectorization(max_tokens = vocab_size,                                      output_mode = 'int',                                      output_sequence_length = sequence_length)spa_vectorization = TextVectorization(max_tokens = vocab_size,                                      output_mode = 'int',                                      output_sequence_length = sequence_length + 1,                                      standardize = custom_standardization)train_eng_texts = [pair[0] for pair in train_pairs]train_spa_texts = [pair[1] for pair in train_pairs]eng_vectorization.adapt(train_eng_texts)spa_vectorization.adapt(train_spa_texts)```I have saved it using:```pickle.dump({'config': eng_vectorization.get_config(), 'weights': eng_vectorization.get_weights()},             open(""english_vocab.pkl"", ""wb""))```But after loading it again:```from_disk = pickle.load(open(""english_vocab.pkl"", ""rb""))new_eng = TextVectorization.from_config(from_disk['config'])new_eng.adapt(tf.data.Dataset.from_tensor_slices([""xyz""]))new_eng.set_weights(from_disk['weights'])```It is not behaving as the original one. It is outputting `RaggedTensor`, how to resolve this? Here is the link to my Google Colab - https://colab.research.google.com/drive/1rEkPEnG1odsEObzQrRFVVll7L1Kz33ud?usp=sharing
"
53335,1,1117,0,0,0,sj120698,0,"title:Getting [java.lang.IllegalArgumentException: Internal error: Error applying delegate] error while applying NNAPI Delegate description:**Describe the current behavior**I am using the tensorflow [model_personalization ](https://github.com/tensorflow/examples/tree/master/lite/examples/model_personalization) example and I have modified the android application code ([LiteMultipleSignatureModel.java](https://github.com/tensorflow/examples/blob/master/lite/examples/model_personalization/android/transfer_api/src/main/java/org/tensorflow/lite/examples/transfer/api/LiteMultipleSignatureModel.java)) to run the TF Lite interpreter on NNAPI delegate in the following manner. ```Interpreter.Options options = (new Interpreter.Options());NnApiDelegate nnApiDelegate = null;if(Build.VERSION.SDK_INT >= Build.VERSION_CODES.P) {  nnApiDelegate = new NnApiDelegate();  options.addDelegate(nnApiDelegate);}this.interpreter = new Interpreter(tfLiteModel, options);```I am getting the following error on initializing the interpreter with the NNAPI delegate option : ```12-07 15:35:51.585 11444 11444 E AndroidRuntime: java.lang.IllegalArgumentException: Internal error: Error applying delegate: 12-07 15:35:51.585 11444 11444 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)12-07 15:35:51.585 11444 11444 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:93)12-07 15:35:51.585 11444 11444 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:66)12-07 15:35:51.585 11444 11444 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:44)12-07 15:35:51.585 11444 11444 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:226)```I suspect that this error is arising due to the ""train"" signature of the tfliteModel which uses the [optimizer.apply_gradients](https://github.com/tensorflow/examples/blob/1e624527d2fea0333894156ad7a59d8d455b2c73/lite/examples/model_personalization/transfer_learning/generate_training_model.py#L96) function to update the model weights, as on creating a different tfliteModel with  this particular line commented, the above error is not observed.  **Describe the expected behavior**Is there any way to add NNAPI delegate to the TF Lite interpreter for this trainable model?
"
53283,1,6913,5,0,0,leelew,0,"title:Bug for training GAN using TensorFlow v2.0 description:Hi, I run GAN (train as following code) using TensorFlow 2.0.```python  @tf.function  def train_step(self, X, y):      with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:          predictions = self.generator(X, training=True)           generated_frame = predictions          real_frame = tf.cast(y, tf.float32)          concatenate_inputs = tf.concat([real_frame, generated_frame], axis=0)                      concatenate_outputs = self.discriminator(concatenate_inputs, training=True)          score_real, score_generated = tf.split(concatenate_outputs, 2, axis=0)          discriminator_loss = self.loss_hinge_disc(score_real, score_generated)          generator_loss = self.generator_loss(real_frame, generated_frame) + 0.05*K.mean(score_generated)          gen_gradients = gen_tape.gradient(generator_loss, self.generator.trainable_variables)          disc_gradients = disc_tape.gradient(discriminator_loss, self.discriminator.trainable_variables)                    self.gen_optimizer.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))          self.disc_optimizer.apply_gradients(zip(disc_gradients, self.discriminator.trainable_variables))```But I got the following error and the system kill this task.```2021-12-02 20:52:43.795872: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 8137342976 exceeds 10% of free system memory.2021-12-02 20:52:48.641269: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 8137342976 exceeds 10% of free system memory.2021-12-02 20:52:59.829117: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)2021-12-02 20:53:00.006395: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3699850000 Hz2021-12-02 20:53:00.818017: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:592] layout failed: Invalid argument: MutableGraphView::SortTopologically error: detected edge(s) creating cycle(s) {'Func/gradient_tape/model/conv_lst_m2d/while/model/conv_lst_m2d/while_grad/body/_915/input/_3381' -> 'gradient_tape/model/conv_lst_m2d/while/model/conv_lst_m2d/while_grad/body/_915/gradient_tape/model/conv_lst_m2d/while/gradients/AddN', 'Func/gradient_tape/model_1/conv_lst_m2d_5/while/model_1/conv_lst_m2d_5/while_grad/body/_1629/input/_3841' -> 'gradient_tape/model_1/conv_lst_m2d_5/while/model_1/conv_lst_m2d_5/while_grad/body/_1629/gradient_tape/model_1/conv_lst_m2d_5/while/gradients/AddN', 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/mul_2' -> 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/add_5', 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/convolution_6' -> 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/add_4', 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/clip_by_value' -> 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/mul_3', 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/clip_by_value_2' -> 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/mul_5', 'Func/gradient_tape/model_1/conv_lst_m2d_4/while/model_1/conv_lst_m2d_4/while_grad/body/_1819/input/_3957' -> 'gradient_tape/model_1/conv_lst_m2d_4/while/model_1/conv_lst_m2d_4/while_grad/body/_1819/gradient_tape/model_1/conv_lst_m2d_4/while/gradients/AddN', 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/mul_2' -> 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/add_5', 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/clip_by_value_2' -> 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/mul_5', 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/clip_by_value' -> 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/mul_3', 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/convolution_6' -> 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/add_4', 'Func/gradient_tape/model_1/conv_lst_m2d_3/while/model_1/conv_lst_m2d_3/while_grad/body/_2009/input/_4073' -> 'gradient_tape/model_1/conv_lst_m2d_3/while/model_1/conv_lst_m2d_3/while_grad/body/_2009/gradient_tape/model_1/conv_lst_m2d_3/while/gradients/AddN', 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/mul_2' -> 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/add_5', 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/convolution_6' -> 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/add_4', 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/clip_by_value_2' -> 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/mul_5', 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/clip_by_value' -> 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/mul_3', 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/mul_2' -> 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/add_5', 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/convolution_6' -> 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/add_4', 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/clip_by_value_2' -> 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/mul_5', 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/clip_by_value' -> 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/mul_3', 'Func/gradient_tape/model/conv_lst_m2d_1/while/model/conv_lst_m2d_1/while_grad/body/_725/input/_3267' -> 'gradient_tape/model/conv_lst_m2d_1/while/model/conv_lst_m2d_1/while_grad/body/_725/gradient_tape/model/conv_lst_m2d_1/while/gradients/AddN', 'Func/gradient_tape/model/conv_lst_m2d_2/while/model/conv_lst_m2d_2/while_grad/body/_535/input/_3151' -> 'gradient_tape/model/conv_lst_m2d_2/while/model/conv_lst_m2d_2/while_grad/body/_535/gradient_tape/model/conv_lst_m2d_2/while/gradients/AddN', 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/mul_2' -> 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/add_5', 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/clip_by_value' -> 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/mul_3', 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/convolution_6' -> 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/add_4', 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/clip_by_value_2' -> 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/mul_5', 'Func/model/conv_lst_m2d/while/body/_1/input/_2804' -> 'model/conv_lst_m2d/while/body/_1/model/conv_lst_m2d/while/mul_2', 'model/conv_lst_m2d/while/body/_1/model/conv_lst_m2d/while/mul_5' -> 'model/conv_lst_m2d/while/body/_1/model/conv_lst_m2d/while/Identity_4'}.```So how can I train GAN using Keras properly?Thanks!
"
53271,1,0,71,0,0,elda27,0,"title:Code completion of Keras Module no longer work since 2.6 description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 20H2- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1/- Python version: 3.8.9- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source): N/A- CUDA/cuDNN version:  N/A- GPU model and memory: N/A**Describe the current behavior**Code completion is no longer work for `tf.keras` on the VSCode. since TensorFlow 2.6.![image](https://user-images.githubusercontent.com/6873761/144241798-4239b2ac-39b4-4c96-98ca-7c89918900e1.png)**Describe the expected behavior**Code completion is work for `tf.keras` on the VSCode until TensorFlow 2.5.![image](https://user-images.githubusercontent.com/6873761/144241133-3a2f04f8-3513-491e-8a43-243666c4d164.png)A related issue found #52031.The cause of this problem is lazy import of Keras.A similar problem exists in TensorFlow Probability.One solution is to prepare a stub file, but that is a lot of work.Are you aware that this problem is a VSCode/Pylance problem?**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): yes, but I don't have a solution.- Briefly describe your candidate solution(if contributing): N/A.
"
53240,1,957,0,0,0,Abhijit16,0,"title:Adding GPU delegate fails for (mediapipe) Hand landmarks TFLite model in C API description:I am trying to run handlandmark TFLite model in a Visual C++ 2022 project (running it on Windows 10 desktop) and am using TensorFlowLite's C API (I have built tensorflow 2.6.0 from source & am using it's dynamic library dll linked with my project) to locate the hand landmark points, after using the palm detector model. I am using GPU delegate for both palm detector model as well as hand landmark model, downloaded from below mediapipe link:https://google.github.io/mediapipe/solutions/models.html#handsHowever, while adding & using GPU delegate for palm detector model works prefectly, adding GPU delegate for hand landmark model fails (maybe in TfLiteInterpreterOptionsAddDelegate()), as the function TfLiteInterpreterCreate() returns NULL.Note that if I do not use GPU delegate, the interpreter creation works fine, & I am also able to locate and display the 21 hand landmarks correctly in the image/frame.There is no error or crash reported, only that debugging indicates that the interpreter creation function returns NULL, if GPU delegate is added to interpreter options.Below is the code snippet:```//string hand_model_name = <path to hand landmark model>.TfLiteModel *m_hand_landmark_model = TfLiteModelCreateFromFile(hand_model_name.c_str());		if (!m_hand_landmark_model)		return kTfLiteError;	// set GPU delegate for faster inference in TFLite.	TfLiteGpuDelegateOptionsV2 gpu_options = TfLiteGpuDelegateOptionsV2Default();	TfLiteDelegate *m_hand_gpu_delegate = TfLiteGpuDelegateV2Create(&gpu_options);	TfLiteInterpreterOptions *m_tfl_hand_interpreter_options = TfLiteInterpreterOptionsCreate();		TfLiteInterpreterOptionsAddDelegate(m_tfl_hand_interpreter_options, m_hand_gpu_delegate); // hand interpreter creation fails due to this function failing. If comment this line, below function creates the interpreter as desired.	// (2) create interpreter.	TfLiteInterpreter *m_hand_interpreter = TfLiteInterpreterCreate(m_hand_landmark_model, m_tfl_hand_interpreter_options);	if (!m_hand_interpreter)		return kTfLiteError;```   Can anyone please help me out on why this issue is coming & how to resolve it ? Thanks,Abhijit.
"
53204,1,1220,299,0,0,albertz,0,"title:Keras Nadam optimizer behaves different to `ApplyAdamOp` with `use_nesterov` option description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 (but irrelevant)- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No- TensorFlow installed from (source or binary): binary (but irrelevant)- TensorFlow version (use command below): 2.3, 2.7.0, current master- Python version: 3.7- Bazel version (if compiling from source): -- GCC/Compiler version (if compiling from source): -- CUDA/cuDNN version: -- GPU model and memory: -**Describe the current behavior**Nadam as implemented in the kernel via the `use_nesterov` option (`tensorflow/tensorflow/core/kernels/training_ops_gpu.cu.cc`) behaves different to `tf.keras.optimizers.Nadam`.We observed this problem (here: https://github.com/rwth-i6/returnn/issues/766#issuecomment-979216833) where the model converged in TF 1 with the old Nadam and does not anymore with TF 2 with the new Nadam.Specifically, in the kernel, you have this code:```cppconst T mul_factor = (*lr_) * sqrt(static_cast<T>(1.0) - (*beta2_power_)) /                     (static_cast<T>(1.0) - (*beta1_power_));...auto m_i = m[i];auto g_i = grad[i];auto v_i = v[i];m_i += one_minus_beta1 * (g_i - m_i);v_i += one_minus_beta2 * (g_i * g_i - v_i);if (use_nesterov) {  var[i] -= mul_factor * (m_i * beta1 + one_minus_beta1 * g_i) /            (epsilon + sqrt(v_i));} else {  var[i] -= mul_factor * m_i / (epsilon + sqrt(v_i));}m[i] = m_i;v[i] = v_i;```And in the Keras Nadam optimizer, we have this code:```g_prime = grad / coefficients['one_minus_m_schedule_new']m_t = (coefficients['beta_1_t'] * m +       coefficients['one_minus_beta_1_t'] * grad)m_t = tf.compat.v1.assign(m, m_t, use_locking=self._use_locking)m_t_prime = m_t / coefficients['one_minus_m_schedule_next']v_t = (coefficients['beta_2_t'] * v +       coefficients['one_minus_beta_2_t'] * tf.square(grad))v_t = tf.compat.v1.assign(v, v_t, use_locking=self._use_locking)v_t_prime = v_t / coefficients['v_t_prime_denominator']m_t_bar = (coefficients['one_minus_m_t'] * g_prime +           coefficients['m_t_1'] * m_t_prime)var_t = var - coefficients['lr_t'] * m_t_bar / (    tf.sqrt(v_t_prime) + coefficients['epsilon'])```There are differences is the calculation of `m_t` and `v_t`. While mathematically the same, the variant in the kernel is probably numerically more stable.Another difference is `mul_factor` vs `lr`.**Describe the expected behavior**I would expect that Nadam as implemented in the kernel via the `use_nesterov` option (`tensorflow/tensorflow/core/kernels/training_ops_gpu.cu.cc`, available since TF 1, also still in current master), which could be used via `tensorflow.contrib.opt.NadamOptimizer` in TF 1, is conceptually the same as `tf.keras.optimizers.Nadam`.
"
53189,0,0,269,0,0,elfringham,0,"title:Unit test //tensorflow/compiler/xla/tests:xla_hlo_profile_test_cpu gives illegal instruction on AARCH64 description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a- TensorFlow installed from (source or binary): source- TensorFlow version (use command below): git HEAD- Python version: 3.8.10- Bazel version (if compiling from source): 3.7.2- GCC/Compiler version (if compiling from source): 11.2.0- CUDA/cuDNN version: n/a- GPU model and memory: n/aYou can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**Test fails as illegal instruction is thrown.**Describe the expected behavior**Test passes**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --remote_http_cache=""""  --remote_cache_proxy="""" --noremote_accept_cached --config=nonccl --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --copt=-ffp-contract=off --cxxopt=-ffp-contract=off --copt=-Og --copt=-ggdb --verbose_failures -- //tensorflow/compiler/xla/tests:xla_hlo_profile_test_cpu**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.bazel-bin/tensorflow/compiler/xla/tests/xla_hlo_profile_test_cpu[==========] Running 2 tests from 1 test suite.[----------] Global test environment set-up.[----------] 2 tests from HloProfileTest[ RUN      ] HloProfileTest.ProfileSingleComputation2021-11-24 17:03:03.560320: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x3c904070 initialized for platform Host (this does not guarantee that XLA will be used). Devices:2021-11-24 17:03:03.560415: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): Host, Default Version2021-11-24 17:03:03.560854: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x3c904d60 initialized for platform Interpreter (this does not guarantee that XLA will be used). Devices:2021-11-24 17:03:03.560885: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): Interpreter, <undefined>Illegal instruction (core dumped)When running under gdbThread 70 ""xla_hlo_profile"" received signal SIGILL, Illegal instruction.[Switching to Thread 0xfffec6ffcd80 (LWP 420853)]0x0000fffff7ff8014 in ProfileSingleComputation.5 ()(gdb) disassDump of assembler code for function ProfileSingleComputation.5:   0x0000fffff7ff8000 <+0>:	str	d12, [sp, #-48]!   0x0000fffff7ff8004 <+4>:	stp	d11, d10, [sp, #16]   0x0000fffff7ff8008 <+8>:	stp	d9, d8, [sp, #32]   0x0000fffff7ff800c <+12>:	mov	x10, xzr   0x0000fffff7ff8010 <+16>:	ldp	x9, x13, [x3, #8]=> 0x0000fffff7ff8014 <+20>:	mrs	x8, pmccntr_el0   0x0000fffff7ff8018 <+24>:	add	x11, x9, #0x20   0x0000fffff7ff801c <+28>:	ldr	x9, [x3]   0x0000fffff7ff8020 <+32>:	add	x12, x9, #0x30   0x0000fffff7ff8024 <+36>:	add	x13, x13, #0x20   0x0000fffff7ff8028 <+40>:	mov	x14, xzr   0x0000fffff7ff802c <+44>:	add	x15, x11, x14   0x0000fffff7ff8030 <+48>:	add	x16, x13, x14   0x0000fffff7ff8034 <+52>:	ldp	q0, q1, [x15, #-32]   0x0000fffff7ff8038 <+56>:	ldp	q2, q3, [x16, #-32]   0x0000fffff7ff803c <+60>:	ldp	q4, q5, [x15]   0x0000fffff7ff8040 <+64>:	fadd	v0.4s, v0.4s, v2.4s   0x0000fffff7ff8044 <+68>:	fadd	v1.4s, v1.4s, v3.4s   0x0000fffff7ff8048 <+72>:	ldp	q2, q3, [x16]   0x0000fffff7ff804c <+76>:	fadd	v2.4s, v4.4s, v2.4s   0x0000fffff7ff8050 <+80>:	add	x15, x12, x14   0x0000fffff7ff8054 <+84>:	stp	q0, q1, [x15, #-48]   0x0000fffff7ff8058 <+88>:	fadd	v0.4s, v5.4s, v3.4s   0x0000fffff7ff805c <+92>:	stp	q2, q0, [x15, #-16]   0x0000fffff7ff8060 <+96>:	add	x14, x14, #0x40   0x0000fffff7ff8064 <+100>:	cmp	x14, #0x400   0x0000fffff7ff8068 <+104>:	b.ne	0xfffff7ff802c <ProfileSingleComputation.5+44>  // b.any   0x0000fffff7ff806c <+108>:	add	x10, x10, #0x1   0x0000fffff7ff8070 <+112>:	add	x11, x11, #0x400   0x0000fffff7ff8074 <+116>:	add	x12, x12, #0x400   0x0000fffff7ff8078 <+120>:	add	x13, x13, #0x400   0x0000fffff7ff807c <+124>:	cmp	x10, #0x100   0x0000fffff7ff8080 <+128>:	b.ne	0xfffff7ff8028 <ProfileSingleComputation.5+40>  // b.any   0x0000fffff7ff8084 <+132>:	mov	x10, xzr   0x0000fffff7ff8088 <+136>:	mrs	x11, pmccntr_el0   0x0000fffff7ff808c <+140>:	mov	w12, #0xb717                	// #46871   0x0000fffff7ff8090 <+144>:	movk	w12, #0x39d1, lsl #16   0x0000fffff7ff8094 <+148>:	dup	v0.4s, w12   0x0000fffff7ff8098 <+152>:	mov	w12, #0x25c0                	// #9664   0x0000fffff7ff809c <+156>:	movk	w12, #0xa59f, lsl #16   0x0000fffff7ff80a0 <+160>:	dup	v1.4s, w12   0x0000fffff7ff80a4 <+164>:	mov	w12, #0x337e                	// #13182   0x0000fffff7ff80a8 <+168>:	movk	w12, #0x2a61, lsl #16   0x0000fffff7ff80ac <+172>:	dup	v2.4s, w12   0x0000fffff7ff80b0 <+176>:	mov	w12, #0x37ff                	// #14335   0x0000fffff7ff80b4 <+180>:	movk	w12, #0xaebd, lsl #16   0x0000fffff7ff80b8 <+184>:	dup	v3.4s, w12   0x0000fffff7ff80bc <+188>:	ldr	x12, [x5, #24]   0x0000fffff7ff80c0 <+192>:	sub	x11, x11, x8   0x0000fffff7ff80c4 <+196>:	add	x11, x11, x12   0x0000fffff7ff80c8 <+200>:	str	x11, [x5, #24]   0x0000fffff7ff80cc <+204>:	mov	w11, #0x41                  	// #65   0x0000fffff7ff80d0 <+208>:	movk	w11, #0x335c, lsl #16--Type <RET> for more, q to quit, c to continue without paging--qSo the problem seems to be reading the performance counter register as the illegal instruction flagged is ""mrs	x8, pmccntr_el0""
"
53180,1,2690,18,0,0,breadbread1984,0,"title:group conv2d can't backprop properly description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04.3- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.7.0- Python version: 3.8- Bazel version (if compiling from source): n/a- GCC/Compiler version (if compiling from source): n/a- CUDA/cuDNN version: n/a- GPU model and memory: n/aYou can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`v2.7.0-rc1-69-gc256c071bb2 2.7.0**Describe the current behavior**I extend tf.keras.layers.Conv2D with the public member function convolution_op() to do group convolution. error occurs at backpropagation. if I change the group number to 1, backpropagation is ok.**Describe the expected behavior**extended convolution op should support backpropagation of group convolution.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.```pythonimport tensorflow as tfclass WSConv2D(tf.keras.layers.Conv2D):  def build(self, input_shape):    super(WSConv2D, self).build(input_shape);    self.gain = self.add_weight(shape = (tf.shape(self.kernel)[-1],), dtype = tf.float32, initializer = tf.keras.initializers.Ones()); # self.gain.shape = (cout,)  def call(self, inputs):    input_shape = inputs.shape;    if self._is_causal:  # Apply causal padding to inputs for Conv1D.      inputs = array_ops.pad(inputs, self._compute_causal_padding(inputs));    # standardize weight    mean, var = tf.nn.moments(self.kernel, axes = [0, 1, 2], keepdims = True);    fan_in = tf.cast(tf.math.reduce_prod(tf.shape(self.kernel)[:-1]), dtype = tf.float32); # fan_in.shape = ()    kernel = self.gain * (self.kernel - mean) / tf.math.sqrt(tf.math.maximum(var * fan_in, 1e-4));    # convolution    outputs = self.convolution_op(inputs, kernel);    if self.use_bias:      outputs = outputs + self.bias;    if self.activation is not None:      return self.activation(outputs);    return outputs;inputs = tf.random.normal(shape = (4,224,224,256));conv2d = WSConv2D(256, (3,3), groups = 4, padding = 'same');with tf.GradientTape() as tape:  outputs = conv2d(inputs);# NOTE: error occurs heregrads = tape.gradient(outputs, conv2d.trainable_weights);```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.```shellTraceback (most recent call last):  File ""test.py"", line 27, in <module>    grads = tape.gradient(outputs, conv2d.trainable_weights);  File ""/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py"", line 1084, in gradient    flat_grad = imperative_grad.imperative_grad(  File ""/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py"", line 71, in imperative_grad    return pywrap_tfe.TFE_Py_TapeGradient(  File ""/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py"", line 159, in _gradient_function    return grad_fn(mock_op, *out_grads)  File ""/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/ops/nn_grad.py"", line 581, in _Conv2DGrad    gen_nn_ops.conv2d_backprop_input(  File ""/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 1247, in conv2d_backprop_input    _ops.raise_from_not_ok_status(e, name)  File ""/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 7107, in raise_from_not_ok_status    raise core._status_to_exception(e) from None  # pylint: disable=protected-accesstensorflow.python.framework.errors_impl.InvalidArgumentError: Computed input depth 256 doesn't match filter input depth 64 [Op:Conv2DBackpropInput]```
"
53168,1,4089,0,0,0,Ponytai1j,0,"title:Failed precondition: Attempting to use uninitialized description:My code:```graph = tf.Graph()with graph.as_default():    # input_his: num_samples:216,9,9,144    input_hsi = tf.placeholder(tf.float32, [None, x_train.shape[1], x_train.shape[2], x_train.shape[3], 1],                               name='input_hsi')    labels_hsi = tf.placeholder(tf.float32, [None, nb_classes], name='labels_hsi')    is_train = tf.placeholder(tf.bool, shape=[], name='is_train')    learning_rate = tf.placeholder(tf.float32, shape=[], name='learning_rate')    keep_prob = tf.placeholder(tf.float32, shape=[], name='keep_prob')    ##################    ## # Wrong here ###    logits, prob = aucn_model.build_model(input_hsi, nb_classes, is_train=is_train, keep_prob=keep_prob) # Wrong here    #################    pred = tf.argmax(prob, 1)    tf.add_to_collection('pred', pred)```In function aucn_model.build_model```def build_model(input_images, num_output, is_train, keep_prob):    k = 36    first_channel = 64    t_channel = 48    print(input_images.get_shape())    x1_0 = first_conv(input_images, channel=first_channel)    # build spectral blocks    print('the input shape of spectral blocks is: ', x1_0.get_shape())    x1, x_1_transit_feature = loop_block(x1_0, channels_per_layer=k, kernel_size=(1, 1, 7), layer_num=2,                                         is_train=is_train, block_name='spectral_block', loop_num=1)    # transition layer    print('the output shape of spectral blocks is: ', x1.get_shape())    x1 = bn_relu(x1, is_train=is_train)    tran1_var = conv_var(kernel_size=(1, 1, x1.get_shape()[3]), in_channels=x1.get_shape()[4], out_channels=t_channel,                         init_method='msra', name='first_transition')    tran1 = tf.nn.conv3d(x1, tran1_var, [1, 1, 1, 1, 1], padding='VALID')    tran1 = bn_relu(tran1, is_train=is_train)    print(tran1.get_shape())    tran2 = Reshape((tran1.get_shape()[1], tran1.get_shape()[2], tran1.get_shape()[4], 1))(tran1)    print(tran2.get_shape())    tran2_var = conv_var(kernel_size=(3, 3, t_channel), in_channels=1, out_channels=first_channel,                         init_method='msra', name='second_transition')    x2_0 = tf.nn.conv3d(tran2, tran2_var, [1, 1, 1, 1, 1], padding='VALID')    print('the input of spatial block:', x2_0.get_shape())    # build spatial blocks    x2, x_2_transit_feature = loop_block(x2_0, channels_per_layer=k, kernel_size=(3, 1, 1), layer_num=2,                                         is_train=is_train, block_name='spatial_block_1', loop_num=1)    print('the output of spatial block:', x2.get_shape())    x3, x_3_transit_feature = loop_block(x2_0, channels_per_layer=k, kernel_size=(1, 3, 1), layer_num=2,                                         is_train=is_train, block_name='spatial_block_2', loop_num=1)    x4 = tf.concat([x2, x3], axis=4)    # Classifier block    pool1 = tf.nn.avg_pool3d(x4, ksize=[1, x4.get_shape()[1], x4.get_shape()[2], 1, 1],                             strides=[1, 1, 1, 1, 1], padding='VALID')    print(pool1.get_shape()) ### WRONG  HERE ####    flatten = tf.layers.flatten(pool1)# ---when changed to below it works---    other_dim = pool1.get_shape()[1]*pool1.get_shape()[2]*pool1.get_shape()[3]*pool1.get_shape()[4]    flatten = Reshape((other_dim,))(pool1)#######################    #print(tf.__version__)    #flatten = tf.contrib.layers.flatten(pool1)    print(flatten.get_shape())    # flatten = tf.nn.dropout(flatten, keep_prob=keep_prob)    wfc = tf.get_variable(name='FC_W', shape=[flatten.get_shape()[1], num_output],                          initializer=tf.contrib.layers.xavier_initializer())    bfc = tf.get_variable(name='FC_b', initializer=tf.constant(0.0, shape=[num_output]))    logits = tf.matmul(flatten, wfc) + bfc    print(logits.get_shape())    prob = tf.nn.softmax(logits)    return logits, prob```When I change to this code it works```flatten = tf.layers.flatten(pool1) # it doesnt workother_dim = pool1.get_shape()[1]*pool1.get_shape()[2]*pool1.get_shape()[3]*pool1.get_shape()[4]flatten = Reshape((other_dim,))(pool1) # it works```_**System information**colab,TF1.15i just confused that why collapsed in making computational graph informed error 闂傚倸鍊搁崐鐑芥嚄閸洖纾婚柕濞炬櫅绾惧潡鏌ら悡搴☆暢濞存粎绌砳nitialized闂?i even not try to excute there.
"
53166,0,0,269,0,0,elfringham,0,"title:Unit tests //tensorflow/core/ir/... fail or crash depending on optimization level description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 8.4- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a- TensorFlow installed from (source or binary): source- TensorFlow version (use command below): git HEAD- Python version: 3.6.8- Bazel version (if compiling from source): 3.7.2- GCC/Compiler version (if compiling from source): 10.3.0- CUDA/cuDNN version: n/a- GPU model and memory: n/aYou can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**Tests fail with errors like<stdin>:4:22: error: custom op 'tfg.AddV2' attribute '_mlir_device' occurs more than once in the attribute list    %AddV2, %ctl_0 = AddV2(%placeholder, %placeholder_1) device(""GPU"") assigned_device(""TPU"") {_mlir_device = ""GPU"", some_attribute = ""some attr!""} : (tensor<*xi32>, tensor<*xi32>) -> (tensor<*xi32>)                     ^FileCheck error: '<stdin>' is empty.FileCheck command line:  /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/llvm-project/llvm/FileCheck /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/org_tensorflow/tensorflow/core/ir/tests/ops.mlir**Describe the expected behavior**All tests pass**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): yes- Briefly describe your candidate solution(if contributing): Correct improper use of ArrayRef and StringRef**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --remote_http_cache=""""  --remote_cache_proxy="""" --noremote_accept_cached --config=nonccl --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --copt=-ffp-contract=off --copt=-O0 --copt=-ggdb --verbose_failures -- //tensorflow/core/ir/tests:ops.mlir.test**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.==================================================================================================== Test output for //tensorflow/core/ir/tests:ops.mlir.test:-- Testing: 1 tests, 1 workers --FAIL: MLIR tests :: ops.mlir (1 of 1)******************** TEST 'MLIR tests :: ops.mlir' FAILED ********************Script:--: 'RUN: at line 1';   /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/org_tensorflow/tensorflow/core/ir/tests/tfg-opt-no-passes /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/org_tensorflow/tensorflow/core/ir/tests/ops.mlir | /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/org_tensorflow/tensorflow/core/ir/tests/tfg-opt-no-passes | /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/llvm-project/llvm/FileCheck /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/org_tensorflow/tensorflow/core/ir/tests/ops.mlir--Exit Code: 2Command Output (stderr):--<stdin>:4:22: error: custom op 'tfg.AddV2' attribute '_mlir_device' occurs more than once in the attribute list    %AddV2, %ctl_0 = AddV2(%placeholder, %placeholder_1) device(""GPU"") assigned_device(""TPU"") {_mlir_device = ""GPU"", some_attribute = ""some attr!""} : (tensor<*xi32>, tensor<*xi32>) -> (tensor<*xi32>)                     ^FileCheck error: '<stdin>' is empty.FileCheck command line:  /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/llvm-project/llvm/FileCheck /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/org_tensorflow/tensorflow/core/ir/tests/ops.mlir--****************************************Failed Tests (1):  MLIR tests :: ops.mlirTesting Time: 0.11s  Failed: 1================================================================================
"
53153,1,0,4,0,0,nistarlwc,0,"title:tensorflow2.4 can't train 2 model together on 2 different GPU description:**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows10- TensorFlow installed from (source or binary): pip install - TensorFlow version (use command below): tensorflow-gpu==2.4- Python version: python3 .8- CUDA/cuDNN version: cuda11.1 cudnn8.0- GPU model and memory: RTX2080ti * 2, 12G**Describe the current behavior**my train code was tensorflow1.13,  use:         `import tensorflow.compat.v1 as tf`         ` tf.disable_v2_behavior()` Now it can train in tensorflow2.4.  I try to train 2 modeles with different learning_rate and same batchsize on 2 different GPU,   first can run with  ` os.environ['CUDA_VISIBLE_DEVICES'] = '0'`,  but secend use `os.environ['CUDA_VISIBLE_DEVICES'] = '1'`, can't run for `out of memory``failed to allocate 9.20G (9874664192 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory`
"
53144,1,82,0,0,0,markorakita,0,"title:PyCharm doesn't resolve anything under tensorflow.keras description:**System information**- OS Platform and Distribution: Windows 10- TensorFlow installed from: binary- TensorFlow version: 2.7.0- Python version: 3.9- Installed using: Conda- CUDA/cuDNN version: 11.2/8.1**Describe the problem**I've upgraded tensorflow from 2.5 to 2.7 and now PyCharm doesn't resolve anything under tensorflow.keras.![keras-no-autocompletion](https://user-images.githubusercontent.com/25534110/142740868-4f37d8c6-77ff-4ce4-af24-c48d111e933a.png)Other modules of tensorflow work, it's only keras that's problematic. I believe this has something to do with the change in TF 2.6 where keras has been split into a separate PIP package.People have also been reporting this problem to JetBrains (PyCharm developers): https://youtrack.jetbrains.com/issue/PY-50318**Provide the exact sequence of commands / steps that you executed before running into the problem**1. Create the conda environment:~ conda create --name tensorflow272. Activate it:~ conda activate tensorflow273. Install Python:~ conda install python=3.94. Install TensorFlow:~ pip install tensorflow5. Create new project in PyCharm6. Write this code:```import tensorflow as tftf.keras.preprocessing.image_dataset_from_directory()```7. When you move mouse to image_dataset_from_directory function there will be no autocomplete.8. Writing _tf.keras._ yields no modules/functions**Any other info / logs**/
"
53133,1,0,0,0,0,xjj210130,0,"title:undefined reference to `tensorflow::str_util::EndsWith闂?description:On  centos 8TensorFlow 2.6  installed from source.Python 3.8.2gcc version 8.4.1 20200928First:  I build libtensorflow  use cmd: bazel build //tensorflow:libtensorflow_cc.sosecond: I build example: tensorflow/tensorflow/examples/label_image/main.cc g++ -g  -std=c++14  -DLINUX -fpermissive  -fPIC -DHAVE_INTTYPES_H -DHAVE_NETINET_IN_H   -I/usr/local/include/ -I/opensource/tf/  -I/usr/local/include/eigen3   -I/opensource/tf/third_party -I /opensource/tf/third_party/eigen3  -c src/main.cc -o src/main.oThis is wrong message闂傚倸鍊搁崐鐑芥倿閿旈敮鍋撶粭娑樻噽閻瑩鏌熼悜姗嗘闁轰礁妫濋弻锝夊Ψ瑜庣亸鐖€fined reference to `tensorflow::str_util::EndsWith(absl::string_view, absl::string_view)'undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::string_view, std::vector<tensorflow::StackFrame, std::allocator<tensorflow::StackFrame> >&&)'undefined reference to `tensorflow::strings::internal::CatPieces[abi:cxx11](std::initializer_list<absl::string_view>)'however, I run cmd:nm -Ca libtensorflow_cc.so |grep EndsWithnm -Ca libtensorflow_framework.soThe function is in the lib. I use std=c++11, c++17 ,It doesn't work.Any help?Thanks. 
"
53077,1,6643,3,0,0,Yuri-Su,0,"title:macOS12 tensorflow-metal error description:**System information**- OS Platform and Distribution : macOS 12.1 intel- TensorFlow installed from (source or binary):source- TensorFlow version (use command below):tensorflow-metal 2.6- Python version:3.8- GPU model and memory: AMD Radeon RX 6600Errors will be reported when training the model```tensorflow.python.framework.errors_impl.AlreadyExistsError: Another metric with the same name already exists.```For example, the following program```import tensorflow as tfprint(tf.config.list_physical_devices())model = tf.keras.models.Sequential([    tf.keras.layers.Flatten(input_shape=[28, 28]),    tf.keras.layers.Dense(128, activation='relu'),    tf.keras.layers.Dense(10, activation='softmax')])print(model.summary())[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]2021-11-16 10:59:28.127916: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/api/keras/optimizersTraceback (most recent call last):  File ""/Volumes/Workspace/PythonProject/centernet2-tf2/main.py"", line 5, in <module>    model = tf.keras.models.Sequential([  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/tensorflow/python/util/lazy_loader.py"", line 62, in __getattr__    module = self._load()  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/tensorflow/python/util/lazy_loader.py"", line 45, in _load    module = importlib.import_module(self.__name__)  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/importlib/__init__.py"", line 127, in import_module    return _bootstrap._gcd_import(name[level:], package, level)  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load_unlocked  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load_unlocked  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load_unlocked  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load  File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked  File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked  File ""<frozen importlib._bootstrap_external>"", line 843, in exec_module  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/__init__.py"", line 25, in <module>    from keras import models  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/models.py"", line 20, in <module>    from keras import metrics as metrics_module  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/metrics.py"", line 26, in <module>    from keras import activations  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/activations.py"", line 20, in <module>    from keras.layers import advanced_activations  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/layers/__init__.py"", line 23, in <module>    from keras.engine.input_layer import Input  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/engine/input_layer.py"", line 21, in <module>    from keras.engine import base_layer  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/engine/base_layer.py"", line 43, in <module>    from keras.mixed_precision import loss_scale_optimizer  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/mixed_precision/loss_scale_optimizer.py"", line 18, in <module>    from keras import optimizers  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/optimizers.py"", line 26, in <module>    from keras.optimizer_v2 import adadelta as adadelta_v2  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/optimizer_v2/adadelta.py"", line 22, in <module>    from keras.optimizer_v2 import optimizer_v2  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py"", line 36, in <module>    keras_optimizers_gauge = tf.__internal__.monitoring.BoolGauge(  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/tensorflow/python/eager/monitoring.py"", line 360, in __init__    super(BoolGauge, self).__init__('BoolGauge', _bool_gauge_methods,  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/tensorflow/python/eager/monitoring.py"", line 135, in __init__    self._metric = self._metric_methods[self._label_length].create(*args)tensorflow.python.framework.errors_impl.AlreadyExistsError: Another metric with the same name already exists.```These are not the issues with the CPU version. It's OK to do simple things with the GPU, but these errors will occur as soon as you train```import tensorflow as tfprint(tf.config.list_physical_devices())with tf.device('/GPU'):    a = tf.random.normal(shape=(2,), dtype=tf.float32)    b = tf.nn.relu(a)    print(a)[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]Metal device set to: AMD Radeon RX 6600systemMemory: 16.00 GBmaxCacheSize: 3.99 GB2021-11-16 10:53:19.473055: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.2 AVX AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2021-11-16 10:53:19.473518: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.2021-11-16 10:53:19.473738: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)tf.Tensor([-1.0673397 -1.3157675], shape=(2,), dtype=float32)```   
"
53067,0,0,269,0,1,elfringham,0,"title:Unit test //tensorflow/compiler/xla/service/cpu:vectorized_reduce_with_no_vector_registers_test fails on AARCH64 description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a- TensorFlow installed from (source or binary): source- TensorFlow version (use command below): git HEAD- Python version: 3.6.9- Bazel version (if compiling from source): 3.7.2- GCC/Compiler version (if compiling from source): 10.3.0- CUDA/cuDNN version: n/a- GPU model and memory: n/aYou can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**Test fails with==================== Test output for //tensorflow/compiler/xla/service/cpu:vectorized_reduce_with_no_vector_registers_test:[==========] Running 1 test from 1 test suite.[----------] Global test environment set-up.[----------] 1 test from CodegenReduceOnArchWithNoVectorRegisters[ RUN      ] CodegenReduceOnArchWithNoVectorRegisters.Testtensorflow/compiler/xla/service/cpu/vectorized_reduce_with_no_vector_registers_test.cc:85: FailureValue of: _status_or_value7.status().ok()  Actual: falseExpected: trueINTERNAL: TargetRegistry::lookupTarget failed: No available targets are compatible with triple ""i686-none-android""[  FAILED  ] CodegenReduceOnArchWithNoVectorRegisters.Test (13 ms)[----------] 1 test from CodegenReduceOnArchWithNoVectorRegisters (13 ms total)[----------] Global test environment tear-down[==========] 1 test from 1 test suite ran. (13 ms total)[  PASSED  ] 0 tests.[  FAILED  ] 1 test, listed below:[  FAILED  ] CodegenReduceOnArchWithNoVectorRegisters.Test 1 FAILED TEST================================================================================**Describe the expected behavior**Test passes**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): yes- Briefly describe your candidate solution(if contributing): Add tag to allow test to be excluded on AARCH64 builds**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.The test has a hard coded link to an x86 platform triple. But more than that, the test simply does not make sense on AARCH64 as it wants to test the behaviour if there were no vector registers available, but that is never going to be the case on AARCH64. So rather than attempting to fix the test, which would probably bypass any content on AARCH64, just add a tag to the definition to allow it to be excluded.
"
53066,1,8969,5,0,0,makonaga,0,"title:On-Device Training with TensorFlow Lite cannot train on GPU. description:**1.System information****1) using colab**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab w/GPU- TensorFlow version (use command below): 2.7**2) using local machine**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04- TensorFlow version (use command below): 2.7- Python version:3.8.8- CUDA/cuDNN version: 11.2 / 8.1- GPU model and memory: RTX3090**2. Code**I use On-Device Training with TensorFlow Lite example.https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynbI used to download a overview.py when I use my local machine.**3. Current behavior**When running on Colab with GPU backend, unexpected error occurs when executing the following Cell.```restore(checkpoint_path=np.array(""/tmp/model.ckpt"", dtype=np.string_))``````NUM_EPOCHS = 10BATCH_SIZE = 100epochs = np.arange(1, NUM_EPOCHS + 1, 1)losses = np.zeros([NUM_EPOCHS])m = Model()for i in range(NUM_EPOCHS):  for batch_idx in range(len(train_images) // BATCH_SIZE):    batched_images = train_images[BATCH_SIZE*(batch_idx) : BATCH_SIZE * (batch_idx + 1)]    batched_labels = train_labels[BATCH_SIZE*(batch_idx) : BATCH_SIZE * (batch_idx + 1)]    result = train(        x=tf.constant(batched_images, shape=(BATCH_SIZE, IMG_SIZE, IMG_SIZE),                      dtype=tf.float32),        y=tf.constant(batched_labels, shape=(BATCH_SIZE, 10), dtype=tf.float32))  losses[i] = result['loss']  print('Finished {0} epochs, current loss: {1}'.format(i + 1, losses[i]))plt.plot(epochs, losses)plt.show()```When running on a local machine with a GPU, Segmentation fault (Core dumped) occurs in the same place as Colab.```$ python overview.py TensorFlow version: 2.7.02021-11-15 22:07:23.390165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2021-11-15 22:07:23.394521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2021-11-15 22:07:23.394982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2021-11-15 22:07:23.395740: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2021-11-15 22:07:23.396617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2021-11-15 22:07:23.397092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2021-11-15 22:07:23.397548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2021-11-15 22:07:23.701187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2021-11-15 22:07:23.701686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2021-11-15 22:07:23.702127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2021-11-15 22:07:23.702560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21492 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.62021-11-15 22:07:24.482198: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.Finished 10 epochs, current loss: 5.802193641662598Finished 20 epochs, current loss: 5.773608207702637Finished 30 epochs, current loss: 5.7503509521484375Finished 40 epochs, current loss: 5.731655120849609Finished 50 epochs, current loss: 5.665677070617676Finished 60 epochs, current loss: 5.549953460693359Finished 70 epochs, current loss: 3.9862852096557617Finished 80 epochs, current loss: 3.8360049724578857Finished 90 epochs, current loss: 3.7696609497070312Finished 100 epochs, current loss: 3.72278237342834472021-11-15 22:09:14.922369: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.WARNING:absl:Importing a function (__inference_internal_grad_fn_421422) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.WARNING:absl:Importing a function (__inference_internal_grad_fn_421450) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.2021-11-15 22:09:15.342590: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.2021-11-15 22:09:15.342614: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.2021-11-15 22:09:15.342618: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:372] Ignored change_concat_input_ranges.2021-11-15 22:09:15.343225: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: saved_model2021-11-15 22:09:15.344752: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }2021-11-15 22:09:15.344766: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: saved_model2021-11-15 22:09:15.354429: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.2021-11-15 22:09:15.376079: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: saved_model2021-11-15 22:09:15.393471: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 50247 microseconds.2021-11-15 22:09:15.416095: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.2021-11-15 22:09:15.478737: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1891] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):Flex ops: FlexBroadcastGradientArgs, FlexReluGrad, FlexRestore, FlexSaveDetails:        tf.BroadcastGradientArgs(tensor<2xi32>, tensor<2xi32>) -> (tensor<?xi32>, tensor<?xi32>) : {device = """"}        tf.ReluGrad(tensor<?x128xf32>, tensor<?x128xf32>) -> (tensor<?x128xf32>) : {device = """"}        tf.Restore(tensor<!tf_type.string>, tensor<!tf_type.string>) -> (tensor<10xf32>) : {device = """", preferred_shard = -1 : i64}        tf.Restore(tensor<!tf_type.string>, tensor<!tf_type.string>) -> (tensor<128x10xf32>) : {device = """", preferred_shard = -1 : i64}        tf.Restore(tensor<!tf_type.string>, tensor<!tf_type.string>) -> (tensor<128xf32>) : {device = """", preferred_shard = -1 : i64}        tf.Restore(tensor<!tf_type.string>, tensor<!tf_type.string>) -> (tensor<784x128xf32>) : {device = """", preferred_shard = -1 : i64}        tf.Save(tensor<!tf_type.string>, tensor<4x!tf_type.string>, tensor<784x128xf32>, tensor<128xf32>, tensor<128x10xf32>, tensor<10xf32>) -> () : {device = """"}See instructions: https://www.tensorflow.org/lite/guide/ops_selectWARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loadedINFO: Created TensorFlow Lite delegate for select TF ops.2021-11-15 22:09:15.503954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2021-11-15 22:09:15.504198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2021-11-15 22:09:15.504370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2021-11-15 22:09:15.504576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2021-11-15 22:09:15.504741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2021-11-15 22:09:15.504886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21492 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 17 nodes with 0 partitions.Segmentation fault (core dumped)```**4. Question**Is it currently possible to do On-Device Training using the GPU?
"
53064,1,0,8,1,0,janhartman,0,"title:TF2 does not load optimizer weights when restoring a saved model description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, very little- OS Platform and Distribution: Ubuntu 20.04 + Colab- TensorFlow installed from: binary- TensorFlow version: 2.7.0 (also happens on 2.6 and 2.5)- Python version: 3.8.10**Describe the current behavior**When loading a saved (Keras) model, the optimizer weights are not restored. No error or warning is printed to make the user aware of this. [The documentation](https://www.tensorflow.org/guide/keras/save_and_serialize) also does not mention this. **Describe the expected behavior**The optimizer weights should be restored automatically when loaded a saved model that was saved with its optimizer.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no**Standalone code to reproduce the issue**- [Notebook that shows the issue in TF2](https://colab.research.google.com/drive/1WyRoHFnNvoFocqm7jKC1CiH0KzzKh2lk?usp=sharing)- [Notebook that shows that TF1 does not have the same issue](https://colab.research.google.com/drive/1HDcX9wauIuArJpBlTmfDRlk0Yhr3ngQT?usp=sharing)**Other info / logs** This is a TF2 bug, it does not occur when using TF1 compat mode. It is a very significant bug - it means that models cannot be trained correctly if we want the training to be able to stop and resume. Workarounds such as setting the optimizer weights manually are buggy too - see https://github.com/keras-team/keras/issues/15298.Issue https://github.com/tensorflow/tensorflow/issues/52346 is related, but not the same - that one is about using checkpoints and getting errors, while here I am using the SavedModel format and not getting any errors (where there should be one).
"
53057,1,0,0,0,0,LittleDijkstraZ,0,"title:Error during training of EfficientDet-lite0 on GPU using the code model_maker_object_detection.ipynb description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Code: [model_maker_object_detection.ipynb](https://github.com/tensorflow/tensorflow/blob/3beeeff5abbaf24562722c4cbe3af8614346286c/tensorflow/lite/g3doc/tutorials/model_maker_object_detection.ipynb)- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab Pro Ubuntu 18.05- TensorFlow installed from (source or binary): commands in the notebook (!pip install --upgrade tensorflow==2.5.0)- TensorFlow version (use command below): 2.5.0- Python version: 3.7.12- CUDA/cuDNN version: 11.2- GPU model and memory: P100-PCIE 16GB## Current behaviorThe line:` model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data)`will run into the following error:UnknownError: 2 root error(s) found.  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.	 [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/efficientnet-lite0/StatefulPartitionedCall/stem/conv2d/Conv2D}}]]	 [[Func/cond_6/then/_3438/input/_6900/_104]]  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.	 [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/efficientnet-lite0/StatefulPartitionedCall/stem/conv2d/Conv2D}}]]0 successful operations.0 derived errors ignored. [Op:__inference_train_function_96848]Function call stack:train_function -> train_function## Expected behaviorIt should be able to train on GPU.## Full OutputEpoch 1/50UnknownError                              Traceback (most recent call last)<ipython-input-5-187f39c1697e> in <module>()----> 1 model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data)9 frames/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py in create(cls, train_data, model_spec, validation_data, epochs, batch_size, train_whole_model, do_train)    285     if do_train:    286       tf.compat.v1.logging.info('Retraining the models...')--> 287       object_detector.train(train_data, validation_data, epochs, batch_size)    288     else:    289       object_detector.create_model()/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py in train(self, train_data, validation_data, epochs, batch_size)    156       return self.model_spec.train(self.model, train_ds, steps_per_epoch,    157                                    validation_ds, validation_steps, epochs,--> 158                                    batch_size, val_json_file)    159     160   def evaluate(self,/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py in train(self, model, train_dataset, steps_per_epoch, val_dataset, validation_steps, epochs, batch_size, val_json_file)    270         callbacks=train_lib.get_callbacks(config.as_dict(), val_dataset),    271         validation_data=val_dataset,--> 272         validation_steps=validation_steps)    273     return model    274 /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)   1181                 _r=1):   1182               callbacks.on_train_batch_begin(step)-> 1183               tmp_logs = self.train_function(iterator)   1184               if data_handler.should_sync:   1185                 context.async_wait()/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)    887     888       with OptionalXlaContext(self._jit_compile):--> 889         result = self._call(*args, **kwds)    890     891       new_tracing_count = self.experimental_get_tracing_count()/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)    948         # Lifting succeeded, so variables are initialized and we can run the    949         # stateless function.--> 950         return self._stateless_fn(*args, **kwds)    951     else:    952       _, _, _, filtered_flat_args = \/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)   3022        filtered_flat_args) = self._maybe_define_function(args, kwargs)   3023     return graph_function._call_flat(-> 3024         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access   3025    3026   @property/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)   1959       # No tape is watching; skip to running the function.   1960       return self._build_call_outputs(self._inference_function.call(-> 1961           ctx, args, cancellation_manager=cancellation_manager))   1962     forward_backward = self._select_forward_and_backward_functions(   1963         args,/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)    594               inputs=args,    595               attrs=attrs,--> 596               ctx=ctx)    597         else:    598           outputs = execute.execute_with_cancellation(/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)     58     ctx.ensure_initialized()     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,---> 60                                         inputs, attrs, num_outputs)     61   except core._NotOkStatusException as e:     62     if name is not None:UnknownError: 2 root error(s) found.  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.	 [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/efficientnet-lite0/StatefulPartitionedCall/stem/conv2d/Conv2D}}]]	 [[Func/cond_6/then/_3438/input/_6900/_104]]  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.	 [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/efficientnet-lite0/StatefulPartitionedCall/stem/conv2d/Conv2D}}]]0 successful operations.0 derived errors ignored. [Op:__inference_train_function_96848]Function call stack:train_function -> train_function
"
53055,1,0,0,0,0,Gandalf401,0,"title:Current implementation only supports equal length strides in the row and column dimensions. description:When I use layers related  to DepthwiseConv2d operation, such as tf.keras.layers.SeparableConv2d, exception occurs like that: 'Current implementation only supports equal length strides in the row and column dimensions. [Op: DepthwiseConv2dNative]'. It means that i cannot use parameter 'strides' like [1, 2], however, that conflicts with the documentation, which allows 'strides'  with list format without mentioning that elements within the list should be the same.I'm using TensorFlow 2.0.0, and I guess this bug exisits in 3D scenariothe and further edition.![1](https://user-images.githubusercontent.com/94270103/141666163-3ba1d244-6efb-49b2-aa49-322d0b4d0889.png)
"
53053,1,0,0,0,0,moumed,0,"title:Error when Saving model with data augmentation layer on Tensorflow 2.7  description:I am getting an error when trying to save a model with data augmentation layers in last tensorflow version (2.7.0).Here is the code of data augmentation:>     input_shape_rgb = (img_height, img_width, 3)    data_augmentation_rgb = tf.keras.Sequential(      [         layers.RandomFlip(""horizontal""),        layers.RandomFlip(""vertical""),        layers.RandomRotation(0.5),        layers.RandomZoom(0.5),        layers.RandomContrast(0.5),        RandomColorDistortion(name='random_contrast_brightness/none'),      ]    )Now I build my model like this:>      input_shape = (img_height, img_width, 3)    model = Sequential([    layers.Input(input_shape),    data_augmentation_rgb,    layers.Rescaling((1./255)),      layers.Conv2D(16, kernel_size, padding=padding, activation='relu', strides=1,        data_format='channels_last'),    layers.MaxPooling2D(),    layers.BatchNormalization(),      layers.Conv2D(32, kernel_size, padding=padding, activation='relu'), # best 4    layers.MaxPooling2D(),    layers.BatchNormalization(),      layers.Conv2D(64, kernel_size, padding=padding, activation='relu'), # best 3    layers.MaxPooling2D(),    layers.BatchNormalization(),      layers.Conv2D(128, kernel_size, padding=padding, activation='relu'), # best 3    layers.MaxPooling2D(),    layers.BatchNormalization(),      layers.Flatten(),    layers.Dense(128, activation='relu'), # best 1    layers.Dropout(0.1),    layers.Dense(128, activation='relu'), # best 1    layers.Dropout(0.1),    layers.Dense(64, activation='relu'), # best 1    layers.Dropout(0.1),    layers.Dense(num_classes, activation = 'softmax')     ])       model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=metrics)     model.summary()Then after the training is done I just make:>      model.save(""./"")And I'm getting this error:>             ---------------------------------------------------------------------------      KeyError                                  Traceback (most recent call last)      <ipython-input-84-87d3f09f8bee> in <module>()      ----> 1 model.save(""./"")                  /usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py in        error_handler(*args, **kwargs)       65     except Exception as e:  # pylint: disable=broad-except       66       filtered_tb = _process_traceback_frames(e.__traceback__)       ---> 67       raise e.with_traceback(filtered_tb) from None       68     finally:       69       del filtered_tb             /usr/local/lib/python3.7/dist-        packages/tensorflow/python/saved_model/function_serialization.py in        serialize_concrete_function(concrete_function, node_ids, coder)       66   except KeyError:       67     raise KeyError(       ---> 68         f""Failed to add concrete function '{concrete_function.name}' to        object-""       69         f""based SavedModel as it captures tensor {capture!r} which is        unsupported""       70         "" or not reachable from root. ""             KeyError: ""Failed to add concrete function        'b'__inference_sequential_46_layer_call_fn_662953'' to object-based SavedModel as it        captures tensor <tf.Tensor: shape=(), dtype=resource, value=<Resource Tensor>> which        is unsupported or not reachable from root. One reason could be that a stateful        object or a variable that the function depends on is not assigned to an attribute of        the serialized trackable object (see SaveTest.test_captures_unreachable_variable).""I inspected the reason of getting this error by changing the architecture of my model and I just found that the reason came from the `data_augmentation` layer since the `RandomFlip` and `RandomRotation` and others are changed from `layers.experimental.prepocessing.RandomFlip` to `layers.RandomFlip`, but still the error appears.
"
53051,1,5048,126,0,0,ProtossDragoon,0,"title:[TPU, keras preprocessing layer] Some Op must be a compile-time constant. description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): y- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): google colab- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: - TensorFlow installed from (source or binary): google colab- TensorFlow version (use command below): 2.7- Python version: google colab- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version: TPU issue- GPU model and memory: TPU issueYou can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**Hi!TPU error raises especially with Kears preprocessing layers.I've tried to connect two models, augmentation model that contains preprocessing layer and segmentation model.```python3def new_concatenated_model(    image_input_hw,    mask_input_hw,    class_n):    seg_model = create_segmentation_model(class_n)    aug_model = create_augmentation_model(        image_input_hw, mask_input_hw, class_n)        image_input_shape = list(image_input_hw) + [3]    @auto_tpu(device=CURRENT_DEVICE) # decorator `auto_tpu` is just context manager.    def create():        im = seg_model.input        model = AugConcatedSegModel(            inputs=im,            outputs=seg_model(im),            augmentation_model=aug_model,            name='seg_model_train_with_aug'        )        return model        model = create()    return model```<br>`train_step()` function code was mainly came from tensorflow [official tutorial document](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit).```python3class AugConcatedSegModel(tf.keras.Model):    def __init__(        self,        inputs=None,        outputs=None,        augmentation_model=None,         **kwargs    ):        super().__init__(inputs=inputs, outputs=outputs, **kwargs)        self.augmentation_model = augmentation_model    def train_step(self, data):        im, ma = data        im, ma = self.augmentation_model((im, ma))        with tf.GradientTape() as tape:            ma_pred = self(im, training=True)  # Forward pass            # Compute the loss value            # (the loss function is configured in `compile()`)            loss = self.compiled_loss(ma, ma_pred, regularization_losses=self.losses)        # Compute gradients        trainable_vars = self.trainable_variables        gradients = tape.gradient(loss, trainable_vars)        # Update weights        self.optimizer.apply_gradients(zip(gradients, trainable_vars))        # Update metrics (includes the metric that tracks the loss)        self.compiled_metrics.update_state(ma, ma_pred)        # Return a dict mapping metric names to current value        return {m.name: m.result() for m in self.metrics}```<br>**Describe the expected behavior**Expected to train successfully without error.same code were tested on:- [x] CPU : No errors- [x] GPU : No errors- [x] TPU : ErrorYou could reproduce this error very fasthttps://colab.research.google.com/drive/1LhHj1FrkZE9QnFhY-NOO8mn7aiXhZgNh?usp=sharing `Runtime - Run all`.+ When I changed `augmentation model` to just plain `Conv2D` layers, the error disappeared.<br>**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.https://colab.research.google.com/drive/1LhHj1FrkZE9QnFhY-NOO8mn7aiXhZgNh?usp=sharing **Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.```python3InvalidArgumentError: 9 root error(s) found.  (0) INVALID_ARGUMENT: {{function_node __inference_train_function_692915}} Input 0 to node `sequential_augmentation_model/sequential_augmentation_layers/random_flip/stateless_random_flip_left_right/stateless_random_uniform/StatelessRandomUniformV2` with op StatelessRandomUniformV2 must be a compile-time constant.XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.	 [[{{node sequential_augmentation_model/sequential_augmentation_layers/random_flip/stateless_random_flip_left_right/stateless_random_uniform/StatelessRandomUniformV2}}]]	 [[TPUReplicate/_compile/_1646634736830564460/_4]]  (1) INVALID_ARGUMENT: {{function_node __inference_train_function_692915}} Input 0 to node `sequential_augmentation_model/sequential_augmentation_layers/random_flip/stateless_random_flip_left_right/stateless_random_uniform/StatelessRandomUniformV2` with op StatelessRandomUniformV2 must be a compile-time constant.XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.	 [[{{node sequential_augmentation_model/sequential_augmentation_layers/random_flip/stateless_random_flip_left_right/stateless_random_uniform/StatelessRandomUniformV2}}]]	 [[TPUReplicate/_compile/_1646634736830564460/_4]]	 [[tpu_compile_succeeded_assert/_5094882425795608634/_5/_47]]  (2) INVALID_ARGUMENT: {{function_node __inference_train_function_692915}} Input 0 to node `sequential_augmentation_model/sequential_augmentation_layers/random_flip/stateless_random_flip_left_right/stateless_random_uniform/StatelessRandomUniformV2` with op StatelessRandomUniformV2 must be a compile-time constant.XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.	 [[{{node sequential_augmentation_model/sequential_augmentation_layers/random_flip/stateless_random_flip_left_right/stateless_random_uniform/StatelessRandomUniformV2}}]]	 [[TPUReplicate/_compile/_1646634736830564460/_4]]	 [[tpu_compile_succeeded_assert/_5094882425795608634/_5/_159]]  (3) INVALID_ARGUMENT: {{function_node __inference_train_function_692915}} Input 0 to node `sequential_augmentation_model/sequential_a ... [truncated]```
"
53045,1,580,0,0,1,dvtran221,0,"title:SpaceToDepth and DepthToSpace were not named as expected description:Hello,In the API, we can give a name for the SpaceToDepth (or DepthToSpace) operator. However, I found that whatever the name I provided, tensorflow always fixes the name of SpaceToDepth as **tf.nn.space_to_depth** (and the same problem for DepthToSpace).I'm using Windows 10, version 21H1, build 19043.1348 and Tensorflow 2.6You can easily reproduce the issue with a minimal example as follows:```import tensorflow as tfimport jsondef sample_network(input_layer):    s2d = tf.nn.space_to_depth(input_layer, block_size=2, name=""Space2Depth"")    d2s = tf.nn.depth_to_space(s2d, block_size=2, name=""Depth2Space"")    return d2sif __name__ == ""__main__"":    input_net = tf.keras.Input(shape=(64, 64, 3), dtype=tf.float32)    output = sample_network(input_net)    model = tf.keras.Model(inputs=input_net, outputs=output)    model_json = json.loads(model.to_json())    with open(""github_issue.json"", ""w"") as f:        json.dump(model_json, f, indent=2)```I would expect the name of these operators are Space2Depth and Depth2Space, but they are not.![DepthToSpace](https://user-images.githubusercontent.com/12828532/141489907-6f5d4263-abf1-4ac7-b117-e650bcbae8ac.PNG)![SpaceToDepth](https://user-images.githubusercontent.com/12828532/141489910-c9d24747-fcc6-459a-9899-a939655ed544.PNG)Thank you for your consideration.
"
53042,0,0,55,0,0,NobuoTsukamoto,0,"title:XNNPACK delegate not enabled in TensorFlow Lite Python Interpreter. description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 (x64) and Raspberry Pi OS 64bit (raspios_arm64-2021-04-09)- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry Pi 4- TensorFlow installed from (source or binary): Source- TensorFlow version (use command below): v2.7.0 and 7b290f9fd9fbf2ac4352b3cbe327e1067e5a3574- Python version: 3.7.3 (Raspberry Pi OS 64bit)- Bazel version (if compiling from source): - (Build with CMake)- GCC/Compiler version (if compiling from source): 8.3.0 (Raspberry Pi OS 64bit)- CUDA/cuDNN version: - - GPU model and memory: -**Describe the current behavior**XNNPACK delegate not enabled in TensorFlow Lite Python Interpreter. Building with either CMake or Bazel will not take effect.The following log is not output.> INFO: Created TensorFlow Lite XNNPACK delegate for CPU.    Delegate lazy initialization was included in the 3d3c6db1ca2d50f6f07722cd800144f8f736167c commit.For C ++ IF, Interpreter::AllocateTensors calls ApplyLazyDelegateProviders to enable the XNNPACK delegate.https://github.com/tensorflow/tensorflow/blob/v2.7.0/tensorflow/lite/interpreter.cc#L176However, for Python IF, the XNNPACK delegate is not enabled because ApplyLazyDelegateProviders is not called in InterpreterWrapper::AllocateTensors.https://github.com/tensorflow/tensorflow/blob/v2.7.0/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L259**Describe the expected behavior**The XNNPACK delegate is enabled in the TensorFlow Lite Python Interpreter.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): Yes- Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue****Other info / logs** Include any logs or source code that would be helpful to
"
53038,1,0,5,0,0,hecheng64,0,"title:built-in method __contains__ of dict object at 0xffff8b79a200  on aarch64闂?ubuntu20.04闂?tf闂傚倸鍊搁崐鐑芥倿閿旈敮鍋撶粭娑樻噽閻瑩鏌熼悜姗嗘闁?.5.0闂?py: v3.8.0 description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):- OS Platform and Distribution闂傚倸鍊搁崐鐑芥倿閿旈敮鍋撶粭娑樻噽閻瑩鏌熼悜姗嗘畷闁稿鍊栭幈銊︾節閸愩劍鏆璶tu 20.04):- Mobile device 闂傚倸鍊搁崐鐑芥倿閿旈敮鍋撶粭娑樻噽閻瑩鏌熼悜姗嗘畷闁稿鍊濋弻锝咁煥閸曨収鍎宑h64- TensorFlow installed from (source or binary): source - TensorFlow version (use command below): V2.5.0- Python version: 3.8- Bazel version (if compiling from source): 3.7.2- GCC/Compiler version (if compiling from source): 9.3.0You can collect some of this information using our environment capture闂傚倸鍊搁崐鐑芥倿閿旈敮鍋撶粭娑樻噽閻瑩鏌熼悜姗嗘畷闁稿鍊楅埀顒傛嚀鐎氼噣鎯冮悮顩昻 3.8.11 (default, Aug  6 2021, 14:51:49) [GCC 10.2.0] :: Anaconda, Inc. on linuxType ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> from tensorflow.keras import models,layersRuntimeError: module compiled against API version 0xe but this version of numpy is 0xdTraceback (most recent call last):  File ""<stdin>"", line 1, in <module>  File ""/root/miniconda3/envs/mlsql/lib/python3.8/site-packages/tensorflow/__init__.py"", line 41, in <module>    from tensorflow.python.tools import module_util as _module_util  File ""/root/miniconda3/envs/mlsql/lib/python3.8/site-packages/tensorflow/python/__init__.py"", line 40, in <module>    from tensorflow.python.eager import context  File ""/root/miniconda3/envs/mlsql/lib/python3.8/site-packages/tensorflow/python/eager/context.py"", line 37, in <module>    from tensorflow.python.client import pywrap_tf_session  File ""/root/miniconda3/envs/mlsql/lib/python3.8/site-packages/tensorflow/python/client/pywrap_tf_session.py"", line 23, in <module>    from tensorflow.python._pywrap_tf_session import *ImportError: SystemError: <built-in method __contains__ of dict object at 0xffff8b79a200> returned a result with an error set>>> 
"
53029,1,282,84,0,0,topherbuckley,0,"title:Adding images with no bounding box to tflite model maker object detection dataset description:I want to add images without any bounding boxes to the dataset used to train an object detector using tflite model maker. According to the [docs](https://cloud.google.com/vision/automl/object-detection/docs/csv-format) I can add >one row for each image with no bounding box (such as row 4 below).``` TRAIN,gs://folder/image1.png,car,0.1,0.1,,,0.3,0.3,, TRAIN,gs://folder/image1.png,bike,.7,.6,,,.8,.9,, UNASSIGNED,gs://folder/im2.png,car,0.1,0.1,0.2,0.1,0.2,0.3,0.1,0.3 TEST,gs://folder/im3.png,,,,,,,,,```When doing so I am getting a ValueError ""could not convert string to float"", which is caused by an attempt to cast the None object to a float in the following [line](https://github.com/tensorflow/examples/blob/c930d0af2983ee438dcc005f9373188734f1624f/tensorflow_examples/lite/model_maker/core/data_util/object_detector_dataloader_util.py#L337-L338):```    xmin, ymin = float(line[3]) * width, float(line[4]) * height```How can I properly add such images without bounding boxes? Possibly a more important question; are these empty images even helpful towards retraining the model (i.e. does this give higher precision to the default classifier of None or Empty)?
"
53024,1,4167,0,0,0,Krzha,0,"title:Error with higher batch size in stateful LSTM layer description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0- Python version: 3.9.5- CUDA/cuDNN version: 11.2/8.1.1- GPU model and memory: GTX 1070 Ti**Describe the current behavior**When using a stateful LSTM layer with any batch size > 32 I get an error:```batch_size = 33input_tensor = tf.keras.Input(shape=[5], batch_size=batch_size)lstm_input = tf.keras.layers.Dense(16, activation=tf.keras.activations.elu)(input_tensor)lstm_input = tf.expand_dims(lstm_input, axis=1)lstm_layer = tf.keras.layers.LSTM(16, stateful=True, return_state=True)lstm_out, h, c = lstm_layer(lstm_input)out = tf.keras.layers.Dense(1, activation=tf.keras.activations.linear)(lstm_out)``````2021-11-10 19:32:33.397959: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cudnn_rnn_ops.cc:1517 : INVALID_ARGUMENT: Invalid input_h shape: [1,33,16] [1,32,16]```Any batch size <= 32 works fineChanging the LSTM layer to stateful=False and the Input layer to batch_size=None makes the issue disappear (changing only stateful to False does not)**Describe the expected behavior**No difference when changing the batch size.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no**Standalone code to reproduce the issue**```pythonimport tensorflow as tfimport numpy as npbatch_size = 33input_tensor = tf.keras.Input(shape=[5], batch_size=batch_size)lstm_input = tf.keras.layers.Dense(16, activation=tf.keras.activations.elu)(input_tensor)lstm_input = tf.expand_dims(lstm_input, axis=1)lstm_layer = tf.keras.layers.LSTM(16, stateful=True, return_state=True)lstm_out, h, c = lstm_layer(lstm_input)out = tf.keras.layers.Dense(1, activation=tf.keras.activations.linear)(lstm_out)model = tf.keras.Model(    inputs=[input_tensor],    outputs=[out, h, c])model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.MeanSquaredError())model.summary()input_arr = np.random.random(size=(batch_size, 5))model.predict([input_arr])```**Other info / logs** Include any logs or source code that would be helpful toExecuting the above program with any batch size > 32 gives the following error:```2021-11-10 19:45:47.216169: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2021-11-10 19:45:47.636164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6615 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1Model: ""model""_________________________________________________________________ Layer (type)                Output Shape              Param #   ================================================================= input_1 (InputLayer)        [(33, 5)]                 0                                                                           dense (Dense)               (33, 16)                  96                                                                          tf.expand_dims (TFOpLambda)  (33, 1, 16)              0                                                                           lstm (LSTM)                 [(33, 16),                2112                                    (33, 16),                                                        (33, 16)]                                                                                            dense_1 (Dense)             (33, 1)                   17                                                                         =================================================================Total params: 2,225Trainable params: 2,225Non-trainable params: 0_________________________________________________________________2021-11-10 19:45:48.667605: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cudnn_rnn_ops.cc:1517 : INVALID_ARGUMENT: Invalid input_h shape: [1,33,16] [1,32,16]Traceback (most recent call last):  File ""***\PyCharmCE2021.2\scratches\scratch.py"", line 22, in <module>    model.predict([input_arr])  File ""***\venv\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler    raise e.with_traceback(filtered_tb) from None  File ""***\venv\lib\site-packages\tensorflow\python\eager\execute.py"", line 58, in quick_execute    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,tensorflow.python.framework.errors_impl.InvalidArgumentError:    Invalid input_h shape: [1,33,16] [1,32,16]	 [[{{node CudnnRNN}}]]	 [[model/lstm/PartitionedCall]] [Op:__inference_predict_function_1045]Function call stack:predict_function -> predict_function -> predict_functionProcess finished with exit code 1```
"
53013,1,8734,0,0,0,ghosinski,0,"title:tflite-model-maker: audio_classifier's DataLoader loads .wav files as int32, resulting in TypeError when training model description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom Code- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10- TensorFlow version (use command below): 2.7.0 (also repeated on 2.6 and 2.5)- Python version: 3.8**Describe the current behavior**Following this tutorial: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_audio_classification.ipynbWhen using my own .wav files (16kHz, Mono, float32) as the test and train data, the built in DataLoader function (audio_classifier.DataLoader) loads the arrays as int32's, which leads to this TypeError when I attempt to train the model:```""TypeError: Input 'y' of 'FloorDiv' Op has type float32 that does not match type int32 of argument 'x'."" ```This is because the YAMnet model expects float32 as input. However, I can use the exact same .wav files as input into the TF Lite YAMnet, and the model processes an output array (1, 521) as expected - meaning these .wav files are in the correct format. It is only when I attempt to train the model via this transfer learning tutorial that this error appears, which leads me to believe it's a bug with the DataLoader. **Describe the expected behavior**The DataLoader should process .wav files as float32s, and therefore allow model training.**Standalone code to reproduce the issue**my wav files: https://storage.googleapis.com/glass_net_wavs_zip/glass_data.zip```pythonfrom tflite_model_maker import audio_classifierimport osdata_dir = os.getcwd() + '\\glass_data\\sample_data'spec = audio_classifier.YamNetSpec(    keep_yamnet_and_custom_heads=False,    frame_step=0.5 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH,    frame_length=1 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH)train_data = audio_classifier.DataLoader.from_folder(spec, os.path.join(data_dir, 'train'), cache=True)test_data = audio_classifier.DataLoader.from_folder(spec, os.path.join(data_dir, 'test'), cache=True)train_data, validation_data = train_data.split(0.8)# data = audio_classifier.DataLoader.from_folder(spec=spec, data_path=data_dir)# train_data, rest_data = data.split(0.8)# validation_data, test_data = rest_data.split(0.5)batch_size = 16epochs = 100print('Training the model')model = audio_classifier.create(    train_data,    spec,    validation_data,    batch_size=batch_size,    epochs=epochs)model.evaluate(test_data)```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.```Traceback (most recent call last):  File ""<input>"", line 2, in <module>  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\audio_classifier.py"", line 148, in create    task.train(train_data, validation_data, epochs, batch_size)  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\audio_classifier.py"", line 55, in train    train_ds, _ = self._get_dataset_and_steps(  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\audio_classifier.py"", line 42, in _get_dataset_and_steps    dataset = tf.distribute.get_strategy().distribute_datasets_from_function(  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\distribute\distribute_lib.py"", line 1159, in distribute_datasets_from_function    return self._extended._distribute_datasets_from_function(  # pylint: disable=protected-access  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\distribute\distribute_lib.py"", line 3574, in _distribute_datasets_from_function    return dataset_fn(InputContext())  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\custom_model.py"", line 81, in _dataset_fn    dataset = input_data.gen_dataset(  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow_examples\lite\model_maker\core\data_util\audio_dataloader.py"", line 360, in gen_dataset    ds = spec.preprocess_ds(ds, is_training=is_training, cache_fn=_cache_fn)  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\model_spec\audio_spec.py"", line 523, in preprocess_ds    ds = ds.map(self._frame, num_parallel_calls=autotune).unbatch()  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1927, in map    return ParallelMapDataset(  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 4522, in __init__    self._map_func = StructuredFunctionWrapper(  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 3712, in __init__    self._function = fn_factory()  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\function.py"", line 3134, in get_concrete_function    graph_function = self._get_concrete_function_garbage_collected(  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\function.py"", line 3100, in _get_concrete_function_garbage_collected    graph_function, _ = self._maybe_define_function(args, kwargs)  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\function.py"", line 3444, in _maybe_define_function    graph_function = self._create_graph_function(args, kwargs)  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\function.py"", line 3279, in _create_graph_function    func_graph_module.func_graph_from_py_func(  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 999, in func_graph_from_py_func    func_outputs = python_func(*func_args, **func_kwargs)  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 3687, in wrapped_fn    ret = wrapper_helper(*args)  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 3617, in wrapper_helper    ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\def_function.py"", line 889, in __call__    result = self._call(*args, **kwds)  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\def_function.py"", line 933, in _call    self._initialize(args, kwds, add_initializers_to=initializers)  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\def_function.py"", line 763, in _initialize    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\function.py"", line 3050, in _get_concrete_function_internal_garbage_collected    graph_function, _ = self._maybe_define_function(args, kwargs)  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\function.py"", line 3444, in _maybe_define_function    graph_function = self._create_graph_function(args, kwargs)  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\function.py"", line 3279, in _create_graph_function    func_graph_module.func_graph_from_py_func(  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 999, in func_graph_from_py_func    func_outputs = python_func(*func_args, **func_kwargs)  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\def_function.py"", line 672, in wrapped_fn    out = weak_wrapped_fn().__wrapped__(*args, **kwds)  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\function.py"", line 3971, in bound_method_wrapper    return wrapped_fn(*args, **kwargs)  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 986, in wrapper    raise e.ag_error_metadata.to_exception(e)TypeError: in user code:    C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\model_spec\audio_spec.py:497 _frame  *        clips = tf.signal.frame(    C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\util\dispatch.py:206 wrapper  **        return target(*args, **kwargs)    C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\ops\signal\shape_ops.py:181 frame        0, 1 + (length_samples - frame_length) // frame_step)    C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\ops\math_ops.py:1250 binary_op_wrapper        raise e    C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\ops\math_ops.py:1234 binary_op_wrapper        return func(x, y, name=name)    C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\util\dispatch.py:206 wrapper        return target(*args, **kwargs)    C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\ops\math_ops.py:1526 floordiv        return gen_math_ops.floor_div(x, y, name=name)    C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\ops\gen_math_ops.py:3796 floor_div        _, _, _op, _outputs = _op_def_library._apply_op_helper(    C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\framework\op_def_library.py:555 _apply_op_helper        raise TypeError(    TypeError: Input 'y' of 'FloorDiv' Op has type float32 that does not match type int32 of argument 'x'.```
"
53009,1,0,90,0,0,ghost,0,"title:model.fit() never calls custom metric description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu / OSX- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): v2.6.0-0-g919f693420e 2.6.0, v2.7.0-rc1-69-gc256c071bb2 2.7.0- Python version: 3.7, 3.9You can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**Following the [documentation](https://keras.io/api/metrics/) I'm trying to implement a custom metric, however the metric never gets called in the first place. I added sanity checks that should produce errors when the metric is called. You can find the full example in this [notebook][1]    def my_metric_fn(y_true, y_pred):        1 / 0        while True:            pass        squared_difference = tf.square(y_true - y_pred)        return tf.reduce_mean(squared_difference, axis=-1) The code runs without a problem after passing the metric    model.compile(optimizer=opt, metrics=[my_metric_fn])    history = model.fit(    train_dataset,    validation_data=validation_dataset,    epochs=epochs,    callbacks=[early_stopping]    )What I actually get:    Epoch 1/100    59/59 [==============================] - 27s 451ms/step - loss: 16.3928 - my_metric_fn: 0.0000e+00 - val_loss: 16.5252 - val_my_metric_fn: 0.0000e+00    Epoch 2/100    59/59 [==============================] - 25s 420ms/step - loss: 16.3508 - my_metric_fn: 0.0000e+00 - val_loss: 16.5316 - val_my_metric_fn: 0.0000e+00    Epoch 3/100    59/59 [==============================] - 25s 420ms/step - loss: 16.3420 - my_metric_fn: 0.0000e+00 - val_loss: 16.5372 - val_my_metric_fn: 0.0000e+00    Epoch 4/100    59/59 [==============================] - 25s 417ms/step - loss: 16.3365 - my_metric_fn: 0.0000e+00 - val_loss: 16.5287 - val_my_metric_fn: 0.0000e+00    Epoch 5/100    59/59 [==============================] - 25s 418ms/step - loss: 16.3251 - my_metric_fn: 0.0000e+00 - val_loss: 16.5271 - val_my_metric_fn: 0.0000e+00  [1]: https://colab.research.google.com/drive/1YohoIRqUWuvAQvEot5cFXWRfAPxm82cm?usp=sharing**Describe the expected behavior**The metric should be called followed by an error.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**I included a notebook earlier**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
52986,0,26844,11,0,0,anth2o,0,"title: Custom training with tf.distribute.Strategy fails with BatchNorm description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary): using Docker image `tensorflow/tensorflow:2.5.0-gpu`- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0- Python version: Python 3.8.0- Bazel version (if compiling from source): None- GCC/Compiler version (if compiling from source): None- CUDA/cuDNN version: 11.2/8.1- GPU model and memory:```bash+-----------------------------------------------------------------------------+| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     ||-------------------------------+----------------------+----------------------+| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. ||                               |                      |               MIG M. ||===============================+======================+======================||   0  TITAN X (Pascal)    Off  | 00000000:02:00.0 Off |                  N/A || 23%   43C    P0    69W / 250W |      0MiB / 12196MiB |      0%      Default ||                               |                      |                  N/A |+-------------------------------+----------------------+----------------------+|   1  TITAN X (Pascal)    Off  | 00000000:03:00.0 Off |                  N/A || 23%   43C    P0    69W / 250W |      0MiB / 12196MiB |      0%      Default ||                               |                      |                  N/A |+-------------------------------+----------------------+----------------------+|   2  TITAN X (Pascal)    Off  | 00000000:82:00.0 Off |                  N/A || 23%   37C    P0    57W / 250W |      0MiB / 12196MiB |      0%      Default ||                               |                      |                  N/A |+-------------------------------+----------------------+----------------------+|   3  TITAN X (Pascal)    Off  | 00000000:83:00.0 Off |                  N/A || 23%   42C    P0    67W / 250W |      0MiB / 12196MiB |      0%      Default ||                               |                      |                  N/A |+-------------------------------+----------------------+----------------------+```You can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**I tried adapting [this tutorial](https://www.tensorflow.org/tutorials/distribute/custom_training) on custom training loop with tf.distribute.Strategy, but I am facing an issue. I get the following exception if one of the replicas receives an empty batch:```bashAssertionError: in user code:    /tmp/ipykernel_21821/3562653524.py:98 distributed_train_step  *        per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:1285 run  **        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica        return self._call_for_each_replica(fn, args, kwargs)    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_strategy.py:678 _call_for_each_replica        return mirrored_run.call_for_each_replica(    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py:104 call_for_each_replica        return _call_for_each_replica(strategy, fn, args, kwargs)    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py:245 _call_for_each_replica        coord.join(threads)    /usr/local/lib/python3.8/dist-packages/tensorflow/python/training/coordinator.py:389 join        six.reraise(*self._exc_info_to_raise)    /usr/local/lib/python3.8/dist-packages/six.py:703 reraise        raise value    /usr/local/lib/python3.8/dist-packages/tensorflow/python/training/coordinator.py:297 stop_on_exception        yield    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py:220 _call_for_each_replica        merge_args = distribute_utils.regroup(    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_utils.py:62 regroup        regrouped_tuple = tuple(    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_utils.py:63 <genexpr>        regroup(tuple(v[i] for v in values), wrap_class, always_wrap)    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_utils.py:61 regroup        assert len(v) == len(v0)    AssertionError: ```I think this issue happens when one of of the GPU receives an empty per-replica batch. I tried reproducing this bug with the minimal example provided [here](https://www.tensorflow.org/tutorials/distribute/custom_training), and it didn't happen. I suppose it's because this issue happens only if our model uses **BatchNorm**. So, I adapted this minimal example by using a ResNet (provided in tf.keras.applications) as a model. My full code is available below.**Describe the expected behavior**This issue shouldn't happen. Also, I suppose this issue is raised because the **BatchNorm** statistics are computed locally, which can be a problem because each device would use different statistics.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**I couldn't reproduce this issue directly on Colab because it requires at least 2 GPUs:```python# Import TensorFlowimport tensorflow as tf# Helper librariesimport numpy as npimport osprint(tf.__version__)fashion_mnist = tf.keras.datasets.fashion_mnist(train_images, train_labels), _ = fashion_mnist.load_data()# Adding a dimension to the array -> new shape == (28, 28, 1)# We are doing this because the first layer in our model is a convolutional# layer and it requires a 4D input (batch_size, height, width, channels).# batch_size dimension will be added later on.train_images = train_images[..., None]# Getting the images in [0, 1] range.train_images = train_images / np.float32(255)# Padding images because ResNet requires a miniaml shape of (32, 32)padded_train_images = np.concatenate([    np.zeros((len(train_images), 2, 28, 1)),     train_images,     np.zeros((len(train_images), 2, 28, 1))], axis=1)padded_train_images = np.concatenate([    np.zeros((len(train_images), 32, 2, 1)),     padded_train_images,     np.zeros((len(train_images), 32, 2, 1))], axis=2)# If the list of devices is not specified in the# `tf.distribute.MirroredStrategy` constructor, it will be auto-detected.strategy = tf.distribute.MirroredStrategy()print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))BUFFER_SIZE = len(train_images)BATCH_SIZE_PER_REPLICA = 64GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_syncEPOCHS = 10# We keep only the first images, so that the last GPU receives an empty batchpadded_train_images = padded_train_images[:strategy.num_replicas_in_sync-1]train_labels = train_labels[:strategy.num_replicas_in_sync-1]train_dataset = tf.data.Dataset.from_tensor_slices((padded_train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE) train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)def create_model():  inputs = tf.keras.Input((32, 32, 1))  preprocessed = tf.keras.layers.Conv2D(3, (1, 1))(inputs) # ResNet requires 3 channels  features = tf.keras.applications.ResNet50V2(      include_top=False,       input_tensor=preprocessed,       pooling=""avg"", weights=None).output  logits = tf.keras.layers.Dense(10)(features)  return tf.keras.Model(inputs, features)with strategy.scope():  # Set reduction to `none` so we can do the reduction afterwards and divide by  # global batch size.  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(      from_logits=True,      reduction=tf.keras.losses.Reduction.NONE)  def compute_loss(labels, predictions):    per_example_loss = loss_object(labels, predictions)    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)# model, optimizer, and checkpoint must be created under `strategy.scope`.with strategy.scope():  model = create_model()  optimizer = tf.keras.optimizers.Adam()  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)def train_step(inputs):  images, labels = inputs  with tf.GradientTape() as tape:    predictions = model(images, training=True)    loss = compute_loss(labels, predictions)  gradients = tape.gradient(loss, model.trainable_variables)  optimizer.apply_gradients(zip(gradients, model.trainable_variables))  return loss # `run` replicates the provided computation and runs it# with the distributed input.@tf.functiondef distributed_train_step(dataset_inputs):  per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,                         axis=None)for epoch in range(EPOCHS):  # TRAIN LOOP  total_loss = 0.0  num_batches = 0  for x in train_dist_dataset:    total_loss += distributed_train_step(x)    num_batches += 1  train_loss = total_loss / num_batches  print(f""Epoch {epoch+1}, Loss: {train_loss}"")```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.Here is the full trace of the execution of the code above on 4 GPUs:```bash2021-11-08 14:37:06.516502: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.02.5.0WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.2021-11-08 14:37:14.416516: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.12021-11-08 14:37:14.609629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: pciBusID: 0000:02:00.0 name: TITAN X (Pascal) computeCapability: 6.1coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 447.48GiB/s2021-11-08 14:37:14.610983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: pciBusID: 0000:03:00.0 name: TITAN X (Pascal) computeCapability: 6.1coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 447.48GiB/s2021-11-08 14:37:14.612266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 2 with properties: pciBusID: 0000:82:00.0 name: TITAN X (Pascal) computeCapability: 6.1coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 447.48GiB/s2021-11-08 14:37:14.613980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 3 with properties: pciBusID: 0000:83:00.0 name: TITAN X (Pascal) computeCapability: 6.1coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 447.48GiB/s2021-11-08 14:37:14.614027: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.02021-11-08 14:37:14.619791: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.112021-11-08 14:37:14.619850: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.112021-11-08 14:37:14.620958: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.102021-11-08 14:37:14.621767: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.102021-11-08 14:37:14.622819: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.112021-11-08 14:37:14.623705: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.112021-11-08 14:37:14.623948: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.82021-11-08 14:37:14.651130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1, 2, 32021-11-08 14:37:14.652061: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2021-11-08 14:37:15.287138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: pciBusID: 0000:02:00.0 name: TITAN X (Pascal) computeCapability: 6.1coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 447.48GiB/s2021-11-08 14:37:15.289012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: pciBusID: 0000:03:00.0 name: TITAN X (Pascal) computeCapability: 6.1coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 447.48GiB/s2021-11-08 14:37:15.290747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 2 with properties: pciBusID: 0000:82:00.0 name: TITAN X (Pascal) computeCapability: 6.1coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 447.48GiB/s2021-11-08 14:37:15.292413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 3 with properties: pciBusID: 0000:83:00.0 name: TITAN X (Pascal) computeCapability: 6.1coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 447.48GiB/s2021-11-08 14:37:15.303710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1, 2, 32021-11-08 14:37:15.303805: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.02021-11-08 14:37:17.190690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:2021-11-08 14:37:17.190759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 2 3 2021-11-08 14:37:17.190771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N Y N N 2021-11-08 14:37:17.190779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   Y N N N 2021-11-08 14:37:17.190786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 2:   N N N Y 2021-11-08 14:37:17.190793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 3:   N N Y N 2021-11-08 14:37:17.198780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11436 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0, compute capability: 6.1)2021-11-08 14:37:17.200356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11436 MB memory) -> physical GPU (device: 1, name: TITAN X (Pascal), pci bus id: 0000:03:00.0, compute capability: 6.1)2021-11-08 14:37:17.202384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 11436 MB memory) -> physical GPU (device: 2, name: TITAN X (Pascal), pci bus id: 0000:82:00.0, compute capability: 6.1)2021-11-08 14:37:17.203758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 11436 MB memory) -> physical GPU (device: 3, name: TITAN X (Pascal), pci bus id: 0000:83:00.0, compute capability: 6.1)INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')Number of devices: 4INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).2021-11-08 14:37:18.079248: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:695] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: ""TensorSliceDataset/_2""op: ""TensorSliceDataset""input: ""Placeholder/_0""input: ""Placeholder/_1""attr {  key: ""Toutput_types""  value {    list {      type: DT_DOUBLE      type: DT_UINT8    }  }}attr {  key: ""output_shapes""  value {    list {      shape {        dim {          size: 32        }        dim {          size: 32        }        dim {          size: 1        }      }      shape {      }    }  }}2021-11-08 14:37:21.185304: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)2021-11-08 14:37:21.205000: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2199965000 HzWARNING:tensorflow:Gradients do not exist for variables ['conv2_block1_preact_bn/gamma:0', 'conv2_block1_preact_bn/beta:0', 'conv2_block1_1_bn/gamma:0', 'conv2_block1_1_bn/beta:0', 'conv2_block1_2_bn/gamma:0', 'conv2_block1_2_bn/beta:0', 'conv2_block2_preact_bn/gamma:0', 'conv2_block2_preact_bn/beta:0', 'conv2_block2_1_bn/gamma:0', 'conv2_block2_1_bn/beta:0', 'conv2_block2_2_bn/gamma:0', 'conv2_block2_2_bn/beta:0', 'conv2_block3_preact_bn/gamma:0', 'conv2_block3_preact_bn/beta:0', 'conv2_block3_1_bn/gamma:0', 'conv2_block3_1_bn/beta:0', 'conv2_block3_2_bn/gamma:0', 'conv2_block3_2_bn/beta:0', 'conv3_block1_preact_bn/gamma:0', 'conv3_block1_preact_bn/beta:0', 'conv3_block1_1_bn/gamma:0', 'conv3_block1_1_bn/beta:0', 'conv3_block1_2_bn/gamma:0', 'conv3_block1_2_bn/beta:0', 'conv3_block2_preact_bn/gamma:0', 'conv3_block2_preact_bn/beta:0', 'conv3_block2_1_bn/gamma:0', 'conv3_block2_1_bn/beta:0', 'conv3_block2_2_bn/gamma:0', 'conv3_block2_2_bn/beta:0', 'conv3_block3_preact_bn/gamma:0', 'conv3_block3_preact_bn/beta:0', 'conv3_block3_1_bn/gamma:0', 'conv3_block3_1_bn/beta:0', 'conv3_block3_2_bn/gamma:0', 'conv3_block3_2_bn/beta:0', 'conv3_block4_preact_bn/gamma:0', 'conv3_block4_preact_bn/beta:0', 'conv3_block4_1_bn/gamma:0', 'conv3_block4_1_bn/beta:0', 'conv3_block4_2_bn/gamma:0', 'conv3_block4_2_bn/beta:0', 'conv4_block1_preact_bn/gamma:0', 'conv4_block1_preact_bn/beta:0', 'conv4_block1_1_bn/gamma:0', 'conv4_block1_1_bn/beta:0', 'conv4_block1_2_bn/gamma:0', 'conv4_block1_2_bn/beta:0', 'conv4_block2_preact_bn/gamma:0', 'conv4_block2_preact_bn/beta:0', 'conv4_block2_1_bn/gamma:0', 'conv4_block2_1_bn/beta:0', 'conv4_block2_2_bn/gamma:0', 'conv4_block2_2_bn/beta:0', 'conv4_block3_preact_bn/gamma:0', 'conv4_block3_preact_bn/beta:0', 'conv4_block3_1_bn/gamma:0', 'conv4_block3_1_bn/beta:0', 'conv4_block3_2_bn/gamma:0', 'conv4_block3_2_bn/beta:0', 'conv4_block4_preact_bn/gamma:0', 'conv4_block4_preact_bn/beta:0', 'conv4_block4_1_bn/gamma:0', 'conv4_block4_1_bn/beta:0', 'conv4_block4_2_bn/gamma:0', 'conv4_block4_2_bn/beta:0', 'conv4_block5_preact_bn/gamma:0', 'conv4_block5_preact_bn/beta:0', 'conv4_block5_1_bn/gamma:0', 'conv4_block5_1_bn/beta:0', 'conv4_block5_2_bn/gamma:0', 'conv4_block5_2_bn/beta:0', 'conv4_block6_preact_bn/gamma:0', 'conv4_block6_preact_bn/beta:0', 'conv4_block6_1_bn/gamma:0', 'conv4_block6_1_bn/beta:0', 'conv4_block6_2_bn/gamma:0', 'conv4_block6_2_bn/beta:0', 'conv5_block1_preact_bn/gamma:0', 'conv5_block1_preact_bn/beta:0', 'conv5_block1_1_bn/gamma:0', 'conv5_block1_1_bn/beta:0', 'conv5_block1_2_bn/gamma:0', 'conv5_block1_2_bn/beta:0', 'conv5_block2_preact_bn/gamma:0', 'conv5_block2_preact_bn/beta:0', 'conv5_block2_1_bn/gamma:0', 'conv5_block2_1_bn/beta:0', 'conv5_block2_2_bn/gamma:0', 'conv5_block2_2_bn/beta:0', 'conv5_block3_preact_bn/gamma:0', 'conv5_block3_preact_bn/beta:0', 'conv5_block3_1_bn/gamma:0', 'conv5_block3_1_bn/beta:0', 'conv5_block3_2_bn/gamma:0', 'conv5_block3_2_bn/beta:0', 'post_bn/gamma:0', 'post_bn/beta:0'] when minimizing the loss.INFO:tensorflow:Error reported to Coordinator: Traceback (most recent call last):  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception    yield  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py"", line 220, in _call_for_each_replica    merge_args = distribute_utils.regroup(  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_utils.py"", line 62, in regroup    regrouped_tuple = tuple(  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_utils.py"", line 63, in <genexpr>    regroup(tuple(v[i] for v in values), wrap_class, always_wrap)  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_utils.py"", line 61, in regroup    assert len(v) == len(v0)AssertionError---------------------------------------------------------------------------AssertionError                            Traceback (most recent call last)/tmp/ipykernel_21821/3562653524.py in <module>    105   num_batches = 0    106   for x in train_dist_dataset:--> 107     total_loss += distributed_train_step(x)    108     num_batches += 1    109   train_loss = total_loss / num_batches/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)    887     888       with OptionalXlaContext(self._jit_compile):--> 889         result = self._call(*args, **kwds)    890     891       new_tracing_count = self.experimental_get_tracing_count()/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)    931       # This is the first call of __call__, so we have to initialize.    932       initializers = []--> 933       self._initialize(args, kwds, add_initializers_to=initializers)    934     finally:    935       # At this point we know that the initialization is complete (or less/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)    761     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)    762     self._concrete_stateful_fn = (--> 763         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access    764             *args, **kwds))    765 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)   3048       args, kwargs = None, None   3049     with self._lock:-> 3050       graph_function, _ = self._maybe_define_function(args, kwargs)   3051     return graph_function   3052 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)   3442    3443           self._function_cache.missed.add(call_context_key)-> 3444           graph_function = self._create_graph_function(args, kwargs)   3445           self._function_cache.primary[cache_key] = graph_function   3446 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)   3277     arg_names = base_arg_names + missing_arg_names   3278     graph_function = ConcreteFunction(-> 3279         func_graph_module.func_graph_from_py_func(   3280             self._name,   3281             self._python_function,/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)    997         _, original_func = tf_decorator.unwrap(python_func)    998 --> 999       func_outputs = python_func(*func_args, **func_kwargs)   1000    1001       # invariant: `func_outputs` contains only Tensors, CompositeTensors,/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)    670         # the function a weak reference to itself to avoid a reference cycle.    671         with OptionalXlaContext(compile_with_xla):--> 672           out = weak_wrapped_fn().__wrapped__(*args, **kwds)    673         return out    674 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)    984           except Exception as e:  # pylint:disable=broad-except    985             if hasattr(e, ""ag_error_metadata""):--> 986               raise e.ag_error_metadata.to_exception(e)    987             else:    988               raiseAssertionError: in user code:    /tmp/ipykernel_21821/3562653524.py:98 distributed_train_step  *        per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:1285 run  **        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica        return self._call_for_each_replica(fn, args, kwargs)    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_strategy.py:678 _call_for_each_replica        return mirrored_run.call_for_each_replica(    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py:104 call_for_each_replica        return _call_for_each_replica(strategy, fn, args, kwargs)    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py:245 _call_for_each_replica        coord.join(threads)    /usr/local/lib/python3.8/dist-packages/tensorflow/python/training/coordinator.py:389 join        six.reraise(*self._exc_info_to_raise)    /usr/local/lib/python3.8/dist-packages/six.py:703 reraise        raise value    /usr/local/lib/python3.8/dist-packages/tensorflow/python/training/coordinator.py:297 stop_on_exception        yield    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py:220 _call_for_each_replica        merge_args = distribute_utils.regroup(    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_utils.py:62 regroup        regrouped_tuple = tuple(    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_utils.py:63 <genexpr>        regroup(tuple(v[i] for v in values), wrap_class, always_wrap)    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_utils.py:61 regroup        assert len(v) == len(v0)    AssertionError: ```
"
52976,1,25,1,0,0,Lakedaemon,0,"title: Crash with tensorflow lite description:**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):- TensorFlow installed from (source or binary):- TensorFlow version (or github SHA if from source):**Provide the text output from tflite_convert**```# Copy and paste here```**Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.Also, please include a link to a GraphDef or the model if possible.I am using this model        model = Sequential()                model.add(Conv2D(32, (3, 3), input_shape=inputShape))        model.add(Activation('relu'))        model.add(Conv2D(32, (3, 3)))        model.add(Activation('relu'))        model.add(MaxPooling2D(pool_size=(2, 2)))        model.add(Dropout(0.5))        model.add(Conv2D(64, (3, 3)))        model.add(Activation('relu'))        model.add(Conv2D(64, (3, 3)))        model.add(Activation('relu'))        model.add(MaxPooling2D(pool_size=(2, 2)))        model.add(Dropout(0.5))                model.add(Flatten())        model.add(Dense(768))#1024))#256))        model.add(Activation('relu'))        model.add(Dropout(0.5))        model.add(Dense(nbClasses))        model.add(Activation('softmax'))then I convert it like this :     converter = tf.lite.TFLiteConverter.from_keras_model(model)    converter.optimizations = [tf.lite.Optimize.DEFAULT]    converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]And on Android with with tensorflowlite 2.6. 0 I got a  crash because    java.lang.UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present, and, if using a custom native library, have been properly loaded via System.loadLibrary():      java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol ""wcsnrtombs"" referenced by ""libtensorflowlite_jni.so""...Then I switched to tensorflowlite 2.7.0-rc0And I got this crashjava.lang.UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present, and, if using a custom native library, have been properly loaded via System.loadLibrary():      java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol ""getpagesize"" referenced by ""libtensorflowlite_jni.so""...I'll now try without this : [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]**Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem.If including tracebacks, please include the full traceback. Large logs and filesshould be attached.
"
52959,1,2911,0,0,0,xingkong1994,0,"title:data.Dataset.from_generator  Report Error in Distributed Training description:the message is ""Cannot assign a device for operation PyFunc: {{node PyFunc}} was explicitly assigned to /job:localhost/replica:0/task:0 but available devices are [ /job:worker/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.""  but my codes run in TF 2.2.0,  the training workers fine.    my demo code test.py is :```pythonimport tensorflow.compat.v1 as tfimport timeimport randomfrom tensorflow.python.framework import opsimport loggingimport numpy as nptf.disable_v2_behavior()tf.app.flags.DEFINE_string(""ps_hosts"", """", ""One of 'ps', 'worker'"")tf.app.flags.DEFINE_string(""worker_hosts"", """", ""One of 'ps', 'worker'"")tf.app.flags.DEFINE_string(""job_name"", ""worker"", ""One of 'ps', 'worker'"")tf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")tf.app.flags.DEFINE_string(""buckets"", None, ""oss buckets"")tf.app.flags.DEFINE_integer(""interval"", 60, ""interval"")tf.app.flags.DEFINE_string(""protocol"", ""grpc"", ""interval"")FLAGS = tf.app.flags.FLAGSdef data_generator():    dataset = np.array(range(1000))    for d in dataset:        yield ddef train(task_index, cluster, is_chief, target, buckets):  available_worker_device = ""/job:worker/task:%d"" % (task_index)  with tf.device(      tf.train.replica_device_setter(worker_device=available_worker_device,                                     cluster=cluster)):    dataset = tf.data.Dataset.from_generator(data_generator, (tf.int32), (tf.TensorShape([])))    dataset = dataset.repeat(3)    dataset = dataset.batch(4)    iterator = dataset.make_one_shot_iterator()    one_element = iterator.get_next()    local_step = 1    config = tf.ConfigProto(inter_op_parallelism_threads=7)    with tf.train.MonitoredTrainingSession(master=target,                                           is_chief=is_chief,                                           hooks=[],                                           save_checkpoint_secs=60,                                           checkpoint_dir=buckets,                                           config=config) as mon_sess:      while True:        l = mon_sess.run(one_element)        local_step += 1        if local_step % 100 == 0:          logging.info(l)def main(argv):  print(""job name = %s"" % FLAGS.job_name)  print(""task index = %d"" % FLAGS.task_index)  is_chief = FLAGS.task_index == 0  ps_spec = FLAGS.ps_hosts.split("","")  worker_spec = FLAGS.worker_hosts.split("","")  cluster = tf.train.ClusterSpec({""ps"": ps_spec, ""worker"": worker_spec})  config = tf.ConfigProto(inter_op_parallelism_threads=8)  server = tf.distribute.Server(cluster,                                job_name=FLAGS.job_name,                                protocol=FLAGS.protocol,                                task_index=FLAGS.task_index,                                config=config)  # join the ps server  if FLAGS.job_name == ""ps"":    server.join()  # start the training  print(""start trainig"")  print(FLAGS.buckets)  train(task_index=FLAGS.task_index,        cluster=cluster,        is_chief=is_chief,        target=server.target,        buckets=FLAGS.buckets)if __name__ == ""__main__"":  tf.app.run()```my start command is below:python test.py --ps_hosts=127.0.0.1:9200 --worker_hosts=127.0.0.1:9100 --task_index=0 --job_name=pspython test.py --ps_hosts=127.0.0.1:9200 --worker_hosts=127.0.0.1:9100 --task_index=0 --job_name=workercould you please tell me what's wrong with my code? my code works fine in TF2.2.0闂傚倸鍊搁崐椋庢濮橆剦鐒界憸宥堢亱闂佸搫鍊介鎶藉籍閸屾粎鐓斿┑锛勫亾濞?for your answer, thanks!
"
52957,1,0,7,0,1,Crispisu,0,"title:TypeError: Parameter to MergeFrom() must be instance of same class: expected tensorflow.TensorShapeProto got tensorflow.TensorShapeProto. description:Hi I am still getting the 'TypeError: Parameter to MergeFrom() must be instance of same class: expected tensorflow.TensorShapeProto got tensorflow.TensorShapeProto.' even after applying the fix mentioned previously.Any suggestions?
"
52954,1,999,17,0,1,zldrobit,0,"title:TFLite int8 quantization failure in TensorFlow 2.7.0 description:### 1. System information- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- TensorFlow installation (pip package or built from source): pip- TensorFlow library (version, if pip package or github SHA, if built from source): v2.7.0### 2. CodeThe MWE is provided in the following colab notebook:https://colab.research.google.com/drive/1K-Hwq6LEfjFr-4dSLEI4dhQd6pCR0K-F?usp=sharing### 3. Failure after conversionIf the conversion is successful, but the generated model is wrong, then state what is wrong:- Model produces wrong results and/or has lesser accuracy.- Model produces correct results, but it is slower than expected.### 4. (optional) RNN conversion supportIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.### 5. (optional) Any other info / logsInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.The log is attached as below:```INFO:tensorflow:Assets written to: /tmp/tmp0cb_yd9c/assetsINFO:tensorflow:Assets written to: /tmp/tmp0cb_yd9c/assets---------------------------------------------------------------------------TypeError                                 Traceback (most recent call last)<ipython-input-15-674e2748ac1c> in <module>()     16 converter.inference_output_type = tf.uint8  # or tf.int8     17 converter.experimental_new_quantizer = False---> 18 tflite_model = converter.convert()10 frames/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/schema_py_generated.py in Pack(self, builder)   5704             for i in reversed(range(len(self.operatorCodes))):   5705                 builder.PrependUOffsetTRelative(operatorCodeslist[i])-> 5706             operatorCodes = builder.EndVector(len(self.operatorCodes))   5707         if self.subgraphs is not None:   5708             subgraphslist = []TypeError: EndVector() takes 1 positional argument but 2 were given```This error only occurs with `flabbuffers==2.0`. I could convert an int8 quantization model with `tensorflow==2.7.0` and `flatbuffers==1.12`. This problem is related to the PR https://github.com/tensorflow/tensorflow/pull/51504. In the  issue https://github.com/google/flatbuffers/issues/6858, it is discussed that the number of `EndVector()`'s arguments changed from two to one with the flatbuffers version changing from `1.12` to `2.0`.
"
52951,0,6390,5,0,1,karanveersingh5623,0,"title:Cannot register 2 metrics with the same name: /tensorflow/api/keras/optimizers description:<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): - Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary): Docker image - TensorFlow version: 2.6.1- Keras version: Getting the above error when import- Python version: Python 3.6- Installed using virtualenv? pip? conda?: pip (Docker image)- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version: 11.4- GPU model and memory: Nvidia A100 40GB **Describe the problem**Looks like this issue , https://github.com/keras-team/keras/issues/15579Is it the right one ? because after applying the fix mentioned in above link ""pip install tensorflow==2.6.2"" , getting below error```root@6a893d98bbb1:/app/project# python3 download_process.py --helpWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.Instructions for updating:non-resource variables are not supported in the long termTraceback (most recent call last):  File ""download_process.py"", line 13, in <module>    from utils import *  File ""/app/project/utils.py"", line 4, in <module>    from object_detection.inputs import train_input  File ""/usr/local/lib/python3.6/dist-packages/object_detection/inputs.py"", line 26, in <module>    from object_detection.builders import model_builder  File ""/usr/local/lib/python3.6/dist-packages/object_detection/builders/model_builder.py"", line 29, in <module>    from object_detection.builders import matcher_builder  File ""/usr/local/lib/python3.6/dist-packages/object_detection/builders/matcher_builder.py"", line 23, in <module>    from object_detection.matchers import bipartite_matcher  # pylint: disable=g-import-not-at-top  File ""/usr/local/lib/python3.6/dist-packages/object_detection/matchers/bipartite_matcher.py"", line 20, in <module>    from tensorflow.contrib.image.python.ops import image_opsModuleNotFoundError: No module named 'tensorflow.contrib'```**Provide the exact sequence of commands / steps that you executed before running into the problem**- Build Image by running below dockerfile```FROM tensorflow/tensorflow:2.3.0-gpuRUN apt-get update --fix-missing && apt-get install -y \    ffmpeg \    git \    git-core \    g++ \    pkg-config \    python3-pip \    unzip \    vim \    wget \    zip \    zlib1g-devWORKDIR /appCOPY requirements.txt .RUN pip3 install -r requirements.txtRUN pip3 install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPIENV TF_CPP_MIN_LOG_LEVEL=2RUN wget https://github.com/protocolbuffers/protobuf/releases/download/v3.13.0/protoc-3.13.0-linux-x86_64.zip && \    unzip protoc-3.13.0-linux-x86_64.zip -d /app/protobuf/ENV PATH ""$PATH:/app/protobuf/bin""RUN git clone https://github.com/tensorflow/models.git && \    cd /app/models/research/ && \    protoc object_detection/protos/*.proto --python_out=. && \    cp object_detection/packages/tf2/setup.py . &&\    python -m pip install .```- Spawned container and inside it trying to run a pre-processing script. Below are packages used in it   ```import argparseimport ioimport osimport subprocessimport rayimport tensorflow.compat.v1 as tftf.disable_v2_behavior()#import tensorflow as tffrom PIL import Imagefrom psutil import cpu_counttf.disable_v2_behavior()from utils import *from object_detection.utils import dataset_util, label_map_util```- Error Log Output```2021-11-04 10:25:18.325660: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/api/keras/optimizersTraceback (most recent call last):  File ""download_process.py"", line 13, in <module>    from utils import *  File ""/app/project/utils.py"", line 4, in <module>    from object_detection.inputs import train_input  File ""/usr/local/lib/python3.6/dist-packages/object_detection/inputs.py"", line 26, in <module>    from object_detection.builders import model_builder  File ""/usr/local/lib/python3.6/dist-packages/object_detection/builders/model_builder.py"", line 25, in <module>    from object_detection.builders import box_predictor_builder  File ""/usr/local/lib/python3.6/dist-packages/object_detection/builders/box_predictor_builder.py"", line 20, in <module>    from object_detection.predictors import convolutional_box_predictor  File ""/usr/local/lib/python3.6/dist-packages/object_detection/predictors/convolutional_box_predictor.py"", line 26, in <module>    from object_detection.core import box_predictor  File ""/usr/local/lib/python3.6/dist-packages/object_detection/core/box_predictor.py"", line 137, in <module>    class KerasBoxPredictor(tf.keras.layers.Layer):  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/lazy_loader.py"", line 62, in __getattr__    module = self._load()  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/lazy_loader.py"", line 45, in _load    module = importlib.import_module(self.__name__)  File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module    return _bootstrap._gcd_import(name[level:], package, level)  File ""/usr/local/lib/python3.6/dist-packages/keras/__init__.py"", line 25, in <module>    from keras import models  File ""/usr/local/lib/python3.6/dist-packages/keras/models.py"", line 20, in <module>    from keras import metrics as metrics_module  File ""/usr/local/lib/python3.6/dist-packages/keras/metrics.py"", line 26, in <module>    from keras import activations  File ""/usr/local/lib/python3.6/dist-packages/keras/activations.py"", line 20, in <module>    from keras.layers import advanced_activations  File ""/usr/local/lib/python3.6/dist-packages/keras/layers/__init__.py"", line 23, in <module>    from keras.engine.input_layer import Input  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/input_layer.py"", line 21, in <module>    from keras.engine import base_layer  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py"", line 43, in <module>    from keras.mixed_precision import loss_scale_optimizer  File ""/usr/local/lib/python3.6/dist-packages/keras/mixed_precision/loss_scale_optimizer.py"", line 18, in <module>    from keras import optimizers  File ""/usr/local/lib/python3.6/dist-packages/keras/optimizers.py"", line 26, in <module>    from keras.optimizer_v2 import adadelta as adadelta_v2  File ""/usr/local/lib/python3.6/dist-packages/keras/optimizer_v2/adadelta.py"", line 22, in <module>    from keras.optimizer_v2 import optimizer_v2  File ""/usr/local/lib/python3.6/dist-packages/keras/optimizer_v2/optimizer_v2.py"", line 37, in <module>    ""/tensorflow/api/keras/optimizers"", ""keras optimizer usage"", ""method"")  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/monitoring.py"", line 361, in __init__    len(labels), name, description, *labels)  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/monitoring.py"", line 135, in __init__    self._metric = self._metric_methods[self._label_length].create(*args)tensorflow.python.framework.errors_impl.AlreadyExistsError: Another metric with the same name already exists.```**Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
52941,1,278,0,0,0,FHof,0,"title:tf.keras.backend.set_floatx does not change the default dtype in tensor initialisation description:I would like to change the default float precision for initialising tensors and found the [tf.keras.backend.set_floatx](https://www.tensorflow.org/api_docs/python/tf/keras/backend/set_floatx) function in the documentation. When I use this function, `tf.keras.backend.floatx()` shows the configured float type but unexpectedly this type does not affect the type of created tensors:```python3python3 -c 'import tensorflow as tf; tf.keras.backend.set_floatx(""float64""); print(tf.keras.backend.floatx(), tf.constant(1.0))'```The command outputs `float64 tf.Tensor(1.0, shape=(), dtype=float32)` when I execute it.I have tested it with intel_tensorflow 2.6.0, tensorflow 2.6.0 from conda-forge, tensorflow-gpu 2.4.1 from conda-forge and tensorflow 2.6.1 from pip3. The type of the created tensor never matched the configured default type.**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I didn't execute an official example script.- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux and Ubuntu 20.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: not applicable- TensorFlow installed from (source or binary): conda-forge and pip3- TensorFlow version (use command below): intel_tensorflow 2.6.0, tensorflow 2.6.0 from conda-forge, tensorflow-gpu 2.4.1 from conda-forge and tensorflow 2.6.1 from pip3- Python version: 3.9.7- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version: CUDA does not yet work for me- GPU model and memory: no nvidia GPU and Nvidia GeForce RTX 3080**Describe the current behavior**The default float precision is used when creating a tensor.**Describe the expected behavior**If I don't explicitly specify a dtype argument, the dtype is float32 and not the current keras backend floatx.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**```python3python3 -c 'import tensorflow as tf; tf.keras.backend.set_floatx(""float64""); print(tf.keras.backend.floatx(), tf.constant(1.0))'```**Other info / logs**
"
52937,0,4709,280,0,0,frgfm,0,"title:tf.keras import raises an AlreadyExistsError with keras 2.7 description:Hello there :wave:I encountered a CI problem with a build job today that wasn't happening yesterday. So I checked the difference in terms of dependency and the only difference was keras. So I inspected the traceback and ended up tracking the import from keras that causes trouble. I already reported this to the keras team in https://github.com/keras-team/keras/issues/15585 but I figured it might be of importance to you folks considering this impacts several imports from tensorflow itself!**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 LTS- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.6.0- Python version: 3.8.10- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version: CUDA 11.4.100 & cuDNN 8.2.2 - GPU model and memory: NVIDIA GeForce RTX 2070 with Max-Q Design**Describe the current behavior**Running the standalone code throws an `AlreadyExistsError`**Describe the expected behavior**Not raising any error.- Do you want to contribute a PR? (yes/no): happy to do so, but I'm not sure how to solve this- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**```pythonfrom tensorflow.keras.utils import img_to_array```**Other info / logs** Include any logs or source code that would be helpful to```---------------------------------------------------------------------------AlreadyExistsError                        Traceback (most recent call last)<ipython-input-1-a2ab22b98110> in <module>----> 1 from tensorflow.keras.utils import img_to_array~/miniconda3/lib/python3.8/site-packages/keras/api/_v2/keras/__init__.py in <module>      8 import sys as _sys      9 ---> 10 from keras import __version__     11 from keras.api._v2.keras import __internal__     12 from keras.api._v2.keras import activations~/miniconda3/lib/python3.8/site-packages/keras/__init__.py in <module>     23      24 # See b/110718070#comment18 for more details about this import.---> 25 from keras import models     26      27 from keras.engine.input_layer import Input~/miniconda3/lib/python3.8/site-packages/keras/models.py in <module>     18 import tensorflow.compat.v2 as tf     19 from keras import backend---> 20 from keras import metrics as metrics_module     21 from keras import optimizer_v1     22 from keras.engine import functional~/miniconda3/lib/python3.8/site-packages/keras/metrics.py in <module>     24      25 import numpy as np---> 26 from keras import activations     27 from keras import backend     28 from keras.engine import base_layer~/miniconda3/lib/python3.8/site-packages/keras/activations.py in <module>     18      19 from keras import backend---> 20 from keras.layers import advanced_activations     21 from keras.utils.generic_utils import deserialize_keras_object     22 from keras.utils.generic_utils import serialize_keras_object~/miniconda3/lib/python3.8/site-packages/keras/layers/__init__.py in <module>     21      22 # Generic layers.---> 23 from keras.engine.input_layer import Input     24 from keras.engine.input_layer import InputLayer     25 from keras.engine.input_spec import InputSpec~/miniconda3/lib/python3.8/site-packages/keras/engine/input_layer.py in <module>     19 from keras import backend     20 from keras.distribute import distributed_training_utils---> 21 from keras.engine import base_layer     22 from keras.engine import keras_tensor     23 from keras.engine import node as node_module~/miniconda3/lib/python3.8/site-packages/keras/engine/base_layer.py in <module>     41 from keras.engine import node as node_module     42 from keras.mixed_precision import autocast_variable---> 43 from keras.mixed_precision import loss_scale_optimizer     44 from keras.mixed_precision import policy     45 from keras.saving.saved_model import layer_serialization~/miniconda3/lib/python3.8/site-packages/keras/mixed_precision/loss_scale_optimizer.py in <module>     16      17 from keras import backend---> 18 from keras import optimizers     19 from keras.mixed_precision import loss_scale as keras_loss_scale_module     20 from keras.optimizer_v2 import optimizer_v2~/miniconda3/lib/python3.8/site-packages/keras/optimizers.py in <module>     24 from keras.optimizer_v1 import Optimizer     25 from keras.optimizer_v1 import TFOptimizer---> 26 from keras.optimizer_v2 import adadelta as adadelta_v2     27 from keras.optimizer_v2 import adagrad as adagrad_v2     28 from keras.optimizer_v2 import adam as adam_v2~/miniconda3/lib/python3.8/site-packages/keras/optimizer_v2/adadelta.py in <module>     20 import numpy as np     21 from keras import backend_config---> 22 from keras.optimizer_v2 import optimizer_v2     23 from tensorflow.python.util.tf_export import keras_export     24 ~/miniconda3/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py in <module>     34      35 ---> 36 keras_optimizers_gauge = tf.__internal__.monitoring.BoolGauge(     37     ""/tensorflow/api/keras/optimizers"", ""keras optimizer usage"", ""method"")     38 ~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/monitoring.py in __init__(self, name, description, *labels)    358       *labels: The label list of the new metric.    359     """"""--> 360     super(BoolGauge, self).__init__('BoolGauge', _bool_gauge_methods,    361                                     len(labels), name, description, *labels)    362 ~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/monitoring.py in __init__(self, metric_name, metric_methods, label_length, *args)    133           self._metric_name, len(self._metric_methods)))    134 --> 135     self._metric = self._metric_methods[self._label_length].create(*args)    136     137   def __del__(self):AlreadyExistsError: Another metric with the same name already exists.```
"
52883,1,3292,61,0,1,Rakagami,0,"title:AttributeError in tensorflowjs wizard with tensorflow_estimator version 2.7.0 (but can be fixed by downgrading modules) description:**System information**- Manjaro Linux x86_64- TensorFlow installed from anaconda (implicitly thorugh `pip install ""tensorflowjs[wizard]""`)- TensorFlow version : 2.6.0- Python version: 3.6.13- CUDA/cuDNN version: cuda_11.4.r11.4- GPU model and memory: NVIDIA GeForce GTX 1660 Ti, 6GB memory**Current Behavior**When I execute `tensorflowjs_wizard`, I get the following output:```Traceback (most recent call last):  File ""/home/raka/.conda/envs/tfjs_error/bin/tensorflowjs_wizard"", line 5, in <module>    from tensorflowjs.converters.wizard import pip_main  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflowjs/__init__.py"", line 21, in <module>    from tensorflowjs import converters  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflowjs/converters/__init__.py"", line 21, in <module>    from tensorflowjs.converters.converter import convert  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflowjs/converters/converter.py"", line 37, in <module>    from tensorflowjs.converters import tf_saved_model_conversion_v2  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py"", line 39, in <module>    import tensorflow_hub as hub  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflow_hub/__init__.py"", line 88, in <module>    from tensorflow_hub.estimator import LatestModuleExporter  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflow_hub/estimator.py"", line 62, in <module>    class LatestModuleExporter(tf.compat.v1.estimator.Exporter):  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py"", line 62, in __getattr__    module = self._load()  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py"", line 45, in _load    module = importlib.import_module(self.__name__)  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/importlib/__init__.py"", line 126, in import_module    return _bootstrap._gcd_import(name[level:], package, level)  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflow_estimator/__init__.py"", line 10, in <module>    from tensorflow_estimator._api.v1 import estimator  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 10, in <module>    from tensorflow_estimator._api.v1.estimator import experimental  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 10, in <module>    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 27, in <module>    from tensorflow_estimator.python.estimator import estimator  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 70, in <module>    @doc_controls.inheritable_header(""""""\AttributeError: module 'tensorflow.tools.docs.doc_controls' has no attribute 'inheritable_header'```**Expected Behavior**The expected behavior is the output of the `tensorflowjs_wizard`  command as documented [here](https://github.com/tensorflow/tfjs/blob/master/tfjs-converter/README.md)The error seems to have been caused by `inheritable_header` which is called from `tensorflow_estimator.python.estimator` as seen by log above. [This commit](https://github.com/tensorflow/estimator/commit/1cf920e090b02c6b3186b66a2bb2e0114476db40) shows, that the call from the estimator module was recently added with the 2.7.0 version which released yesterday.**How I currently solved it for me**Through downgrading tensorflow-estimator to 2.6.0 and tensorflowjs to 3.9.0 I was able to get the command `tensorflowjs_wizard` to **behave as expected**. I downgraded the python packages with the following command:```pip install --upgrade tensorflow-estimator==2.6.0pip install --upgrade tensorflowjs==3.9.0```**[Contributing](https://www.tensorflow.org/community/contribute)**I don't know how to fix this issue because how I solved my issue was through manual downgrading. Maybe someone who knows the code better knows what to do.**Standalone code to reproduce the issue**```bash# Assuming you have already installed conda and are currently in no environmentconda create --name tfjs python=3.6conda activate tfjspip install tensorflowjs[wizard]# Here you have to refresh the terminal somehowtensorflowjs_wizard```[edit: forgot to add the tensorflow version]
"
52882,1,126,0,0,0,yogeesh-alphaics,0,"title:tf.nn.batch_normalization and tf.keras.layers.BatchNormalization outputs gives a diff (10^-2) range description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04I have implemented a classification model , which has Batchnormalization layers into it. While training the model , i used ```tf.keras.layers.BatchNormalization``` layer for training. Then manually extracted the bn weights (gamma, beta, mean , var) from checkpoint file and called the bn using ```tf.nn.batch_normalization``` layer using above statistics for inference. I observed a drop in testing accuracy (~75%) as compared to the scenario where i load the weights in the model using keras itself ```model.load_weights(chkpnt_file)``` (~92%).   Though if i replace the ```tf.nn.batch_normalization``` call to keras bn layer call by calling ```set_weights``` function , both the cases gived 0.0 diff, which apprently led me to beleive that there is diff in both the calls while the input and bn_weights are exactly same. epsilon is also same.  **Standalone code to reproduce the issue**Please refer to this colab notebook(https://colab.research.google.com/drive/1nysYyCR5Ay_jkPlh1WD9A1Mf01eVvhMu? usp=sharing) for a small example. Here the error is coming to be a margin of 10^-2 (which i think is also big given all inputs are same) , but in my project there is huge diff.
"
52849,1,0,5,0,0,xuchen-dev,0,"title:tensorflow 1.4.0  python 2.7 description:ubantu18.0.4   python2.7  tensorflow 1.4.0  when i use q = tf.FIFOQueue(capacity=capacity, dtypes=dtypes, shapes=shapes)it report   q = tf.FIFOQueue(capacity=capacity, dtypes=dtypes, shapes=shapes)  File ""/data/conda/app/envs/GRnet/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py"", line 756, in __init__    name=name)  File ""/data/conda/app/envs/GRnet/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 810, in _fifo_queue_v2    name=name)  File ""/data/conda/app/envs/GRnet/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 343, in _apply_op_helper    raise RuntimeError(""Unrecognized Op name "" + op_type_name)RuntimeError: Unrecognized Op name FIFOQueueV2
"
52796,1,1380,0,0,1,tusneemA,0,"title:Title: Input shape compatibility of ZIP-object of (image and image ImageDataGenerator objects) and CNN description:Please go to Stack Overflow for help and support:https://stackoverflow.com/questions/tagged/tensorflowIf you open a GitHub issue, here is our policy:1.  It must be a bug, a feature request, or a significant problem with the    documentation (for small docs fixes please send a PR instead).2.  The form below must be filled out.3.  It shouldn't be a TensorBoard issue. Those go    [here](https://github.com/tensorflow/tensorboard/issues).**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.------------------------### System information-   **Have I written custom code (as opposed to using a stock example script    provided in TensorFlow)**: custom code -   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows10-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue    happens on a mobile device**: NA-   **TensorFlow installed from (source or binary)**: binary (added as library to Anaconda )-   **TensorFlow version (use command below)**: 2.1.0-   **Python version**: 3.7-   **Bazel version (if compiling from source)**: NA-   **GCC/Compiler version (if compiling from source)**: NA-   **CUDA/cuDNN version**: 10.1 -   **GPU model and memory**:- 	NVIDIA GeForce RTX 2070 with Max-Q Design	Driver version:	30.0.14.7196	Driver date:	8/27/2021	DirectX version:	12 (FL 12.1)	Physical location:	PCI bus 1, device 0, function 0	Utilization	0%	Dedicated GPU memory	0.0/8.0 GB	Shared GPU memory	0.1/7.9 GB	GPU Memory	0.1/15.9 GB-   **Exact command to reproduce**:You can collect some of this information using our environment capture script:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.shYou can obtain the TensorFlow version with:```bashpython -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""```### Describe the problemI have 64 RGB images with their corresponding masks. I use ImageDataGenerator and flow_from_directory function to read the images and masks from the directory folder.The folder structures are as follows:Images---- imgs-------- ---ROI----------------1.tiff----------------2.tiffMasks------imgs------- -----ROI------------------1.tiff------------------2.tiffImages and masks are stored with the same file names. In my case, ImageDataGenerator is an Iterator of size 2 with a batch size of 32. I did the following:I read the images and masks into lists named as 闂傚倸鍊搁崐鐑芥嚄閸洖纾婚柕濞炬櫅绾惧潡鏌ょ喊鍗炲缂佺姵妫冮弻娑樜旈崘顭戝殯ges闂?and 闂傚倸鍊搁崐鐑芥嚄閸洖纾婚柕濞炬櫅绾惧潡鏌ら懝鐗堢【缂傚秵妫冨缁樼瑹閳ь剟鍩€椤掑倸浠滈柤娲诲灡閺呭爼顢涢悙瀵稿幗濠电偞鍨兼ご鎼佸箟缁s闂傚倸鍊搁崐鐑芥嚄閸洖纾婚柕濞炬櫅绾惧潡鏌＄仦璇插姕闁稿绻濋弻鏇＄疀鐎ｎ亖鍋撻弴銏″€?respectively.size = 231 images = []for directory_path in glob.glob(闂? /Images/imgs/ROI/""):    for img_path in glob.glob(os.path.join(directory_path, ""*.tiff""):        img = cv2.imread(img_path, cv2.IMREAD_COLOR)               img = cv2.resize(img, (size, size))        images.append(img)        images = np.array(images)masks = [] for directory_path in glob.glob(闂? /Masks/imgs/ROI/""):    for mask_path in glob.glob(os.path.join(directory_path, ""*.tiff""):        mask = cv2.imread(mask_path, 0)               mask = cv2.resize(mask, (size, size))        masks.append(mask)     masks = np.array(masks)masks = np.expand_dims(masks, axis=3)Note: I obtained images of shape (64, 231,231,3) and masks of shape (64, 231, 231, 1)I then used keras documentation the process of transforming the images and masks together as follows:  a.	I created a dict to specify the augmentation parameters as follows for both images and masks.  data_gen_args_imgs = dict rotation range=90,                       rescale = 1. /255.)  data_gen_args_masks = dict rotation range=90)                 b.	The I created ImageDataGenerator objects for both images and masks as follows:  ```  image_datagen = ImageDataGenerator(**data_gen_args_imgs)  mask_datagen = ImageDataGenerator(**data_gen_args_masks)  ```    c.	I use the fit and flow functions using the same seeds and keyword arguments for images and masks as follows:   ```  seed = 1    image_datagen.fit(images, augment=True, seed=seed)  mask_datagen.fit(masks, augment=True, seed=seed)    image_generator = image_datagen.flow_from_directory(      'data/images',      class_mode=None,      shuffle= False,      seed=seed)    mask_generator = mask_datagen.flow_from_directory(      闂? /Images/imgs/闂?      class_mode=None,      shuffle= False,      seed=seed)  ```    d.	Then I created an iterator on the zip object to return a matched pair of images and corresponding masks as follows:    `train_generator = (pair for pair in zip (image_generator, mask_generator))`    e.      Then I build a CNN model as follows:   ```    activation= 'relu'  model = Sequential()  model.add (Conv2D(16, kernel_size=(3, 3),activation='relu',input_shape=(231,231,3),padding='same'))  model.add (Conv2D(16, kernel_size=(3, 3),activation= activation,    padding='same'))  model.add (Flatten())  model.add (Dense(5))  model.add (Dense(3))  model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])  model.summary()    ```    e.	Then I used fit_generator to fit the model as follows:     `model.fit_generator (train_generator, steps_per_epoch= 15, epochs=2, verbose=1)`### Source code / logsimport numpy as np import pandas as pd import matplotlib.pyplot as pltimport globimport cv2import osimport globimport tensorflow as tfimport picklefrom scipy import ndimage as ndfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Conv2Dfrom tensorflow.keras.layers import BatchNormalizationfrom tensorflow.keras.layers import MaxPooling2Dfrom tensorflow.keras.layers import Densefrom tensorflow.keras.layers import Flattenfrom tensorflow .keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_imgimport sklearn.utils from tensorflow.keras.optimizers import Adadelta, RMSprop,SGD,Adamprint(tf.__version__)train_path=""C://Users//Tusneem//Documents//Untitled Folder//Tusneem_Seg//SEG_OBJ1//BAS//ROI_1//""mask_path=""C://Users//Tusneem//Documents//Untitled Folder//Tusneem_Seg//SEG_OBJ1//BAS//ROI_2//""############################################################################size = 231 images = []for directory_path in glob.glob(""C://Users//Tusneem//Documents//Untitled Folder//Tusneem_Seg//SEG_OBJ1//BAS//ROI_1//ROI_train//""):    for img_path in glob.glob(os.path.join(directory_path, ""*.tiff"")):        img = cv2.imread(img_path, cv2.IMREAD_COLOR)               img = cv2.resize(img, (size, size))        #img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)        images.append(img)        #train_labels.append(label)        images = np.array(images)masks = [] for directory_path in glob.glob(""C://Users//Tusneem//Documents//Untitled Folder//Tusneem_Seg//SEG_OBJ1//BAS//ROI_2//ROI_train//""):    for mask_path in glob.glob(os.path.join(directory_path, ""*.tiff"")):        mask = cv2.imread(mask_path, 0)               mask = cv2.resize(mask, (size, size))        #mask = cv2.cvtColor(mask, cv2.COLOR_RGB2BGR)        masks.append(mask)        #train_labels.append(label)        masks = np.array(masks)masks=np.expand_dims(masks, axis=3)#######################Example of transforming images and masks together.import tensorflow as tf# we create two instances with the same argumentsdata_gen_args = dict(featurewise_center=True,                     featurewise_std_normalization=True,                     rotation_range=90)                    image_datagen = ImageDataGenerator(data_gen_args)mask_datagen = ImageDataGenerator(data_gen_args)# Provide the same seed and keyword arguments to the fit and flow methodsseed = 1image_datagen.fit(images, augment=True, seed=seed)mask_datagen.fit(masks, augment=True, seed=seed)image_generator = image_datagen.flow_from_directory(    train_path,    class_mode=None,    shuffle= False,    seed=seed)mask_generator = mask_datagen.flow_from_directory(    ""C://Users//Tusneem//Documents//Untitled Folder//Tusneem_Seg//SEG_OBJ1//BAS//ROI_2//"",    class_mode=None,    shuffle= False,    seed=seed)# combine generators into one which yields image and masks#train_generator = zip(image_generator, mask_generator)train_generator = (pair for pair in zip(image_generator, mask_generator))activation= 'relu'model = Sequential()model.add(Conv2D(16, kernel_size=(3, 3),activation='relu',input_shape=(231,231,3),padding='same'))model.add(Conv2D(16, kernel_size=(3, 3),activation='relu',padding='same'))model.add(Flatten())model.add(Dense(5))model.add(Dense(3))model.compile( loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])model.summary()model.fit_generator(train_generator, steps_per_epoch= 15, epochs=2, verbose=1)**But I got the following error regarding input dimensionality:**```InvalidArgumentError:  Input to reshape is a tensor with 33554432 values, but the requested shape requires a multiple of 853776 [[node sequential_8/flatten_8/Reshape (defined at <ipython-input-91-2871596287c2>:12) ]] [Op:__inference_distributed_function_7252]Function call stack:distributed_function```**my question is: How can I fix the problem of train_generator and CNN input shapes??**### attachments[summary of ImageDataGenarator_ZIP object_CNN.docx](https://github.com/tensorflow/tensorflow/files/7432552/summary.of.ImageDataGenarator_ZIP.object_CNN.docx)[The code.txt](https://github.com/tensorflow/tensorflow/files/7432554/The.code.txt).
"
52763,1,0,1,0,0,ammarchalifah,0,"title:Model weight and training loss turn to nan During and after training description:I'm using Keras layer to define a multi-input single output model. I've check the calculation by passing a dummy data to the layer using functional API and model.predict before calling model.fit, and the model works fine. It computes the output as a float, and calculate the loss value as expected. However, whenever I call model.fit, the training loss turns to `nan`. I tried to see the model weight before and after training, and it turns out the weight was exist before calling `model.fit`, but turns to nan after fitting the model. I assume this issue was caused by gradient calculation, but I don't know for sure.Here is the sample of what happened:<img width=""1104"" alt=""Screen Shot 2021-10-28 at 12 14 11"" src=""https://user-images.githubusercontent.com/38188988/139191329-f6e45e8a-1ff1-4c82-b647-d7d945881136.png""><img width=""1115"" alt=""Screen Shot 2021-10-28 at 12 14 33"" src=""https://user-images.githubusercontent.com/38188988/139191365-a51f54d8-4800-4b71-959e-2a01a4094438.png""><img width=""1119"" alt=""Screen Shot 2021-10-28 at 12 14 49"" src=""https://user-images.githubusercontent.com/38188988/139191386-afe14fb7-2489-4f4b-8c4b-bc35382effaa.png""><img width=""1113"" alt=""Screen Shot 2021-10-28 at 12 15 06"" src=""https://user-images.githubusercontent.com/38188988/139191407-9254f87e-e10d-4cae-af37-8b799c0e5eba.png""><img width=""815"" alt=""Screen Shot 2021-10-28 at 12 15 16"" src=""https://user-images.githubusercontent.com/38188988/139191424-09062211-fcf8-4fe8-a88b-86a45fad5d52.png"">I'm experiencing this issue on multiple devices: AWS SageMaker CPU and GPU instance, and my MacOS local jupyter notebook.**Tensorflow Version** : 2.6.0**Tensorflow Installed from**: PyPI**Python ver**: 3.7.10
"
52680,1,0,22,0,1,Ending2015a,0,"title:Inconsistent results on every run with GPU description:**System information**- OS Platform and Distribution: Colab- TensorFlow version: 2.6.0 GPU**Describe the current behavior**Hello, I'm using TensorFlow 2.4.0 with GPU support on my local machine to train a small CNN model. I can get the same training results when I train it on CPU mode if I fixed the random seed, but always get inconsistent results on GPU mode.*So I test it on Colab, which has TensorFlow 2.6.0 installed in default, but still got inconsistent results on GPU mode.***Describe the expected behavior**The results should be the same.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing): x**Standalone code to reproduce the issue**Here is the test code[Colab](https://colab.research.google.com/drive/1Xr_p9rsKM0bk3DDt9CL3L-dN7Vvz179Q?usp=sharing)Run it for many times with GPU enabled, you will get different results on each run.**Other info / logs*I found that only when I use `relu`, `leaky_relu` or `tf.maximum` as the activation function the results change on every run. But if I use `sigmoid`, `tanh`, or other non-relu-like activation functions the results will remain the same.
"
52678,1,239,4,0,0,zuozhen,0,"title:Shapes are incompatible when train pose_classification description:Error ""ValueError: Shapes (16, 1) and (16, 5) are incompatible"" occurs when I'm running ""pose_classification.ipynb"" on custom dataset at this line```history = model.fit(X_train, y_train,                        epochs=200,                        batch_size=16,                        validation_data=(X_val, y_val),                        callbacks=[checkpoint, earlystopping])```-------Update---------It seems not a bug, the problem should be the dataset and its labeling. I'll close it now.Sorry for inconvenience.
"
52677,1,0,0,0,0,WangHuachen,0,"title:import tensorflow.experimental.numpy as tnp description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary):- TensorFlow version (use command below):- Python version:- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version:- GPU model and memory:You can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior****Describe the expected behavior****[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
52676,0,0,269,0,0,elfringham,0,"title:Undefined behaviour in Range description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): all- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a- TensorFlow installed from (source or binary): source- TensorFlow version (use command below): git HEAD- Python version: 3.6.8- Bazel version (if compiling from source): 3.7.2- GCC/Compiler version (if compiling from source): 10.3.0- CUDA/cuDNN version: n/a- GPU model and memory: n/aYou can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**https://github.com/tensorflow/tensorflow/blob/0b67dee3f02e2e055230ca6dd6cc7d090af72baa/tensorflow/core/ops/math_ops.cc#L1484 has undefined behaviour when size is greater than std::numeric_limits<int64_t>::max()This leads to the unit test RangeTest.testLargeStarts failing on AARCH64 where the g++ implements different behaviour from x86. On x86 the result of the cast is large and -ve, on AARCH64 it is large and +ve. Neither is incorrect as the behaviour of casting into a type that cannot hold the value is undefined.**Describe the expected behavior**The code should be written to avoid relying on undefined behaviour of the source.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): yes- Briefly describe your candidate solution(if contributing):Test the variable 'size' for exceeding the greatest possible value that can be safely cast to int64_t and throw an error if found.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.$ bazel test --flaky_test_attempts=3 --test_output=all --cache_test_results=no --remote_http_cache=""""  --remote_cache_proxy="""" --noremote_accept_cached --config=nonccl --verbose_failures -- //tensorflow/python/kernel_tests:init_ops_test**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.======================================================================ERROR: testLargeStarts (__main__.RangeTest)RangeTest.testLargeStarts----------------------------------------------------------------------Traceback (most recent call last):  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/init_ops_test.py"", line 553, in testLargeStarts    v = math_ops.range(start=-1e+38, limit=1)  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/util/traceback_utils.py"", line 141, in error_handler    return fn(*args, **kwargs)  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/util/dispatch.py"", line 1092, in op_dispatch_handler    return dispatch_target(*args, **kwargs)  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/math_ops.py"", line 2113, in range    return gen_math_ops._range(start, limit, delta, name=name)  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/gen_math_ops.py"", line 7737, in _range    _ops.raise_from_not_ok_status(e, name)  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 7131, in raise_from_not_ok_status    raise core._status_to_exception(e) from None  # pylint: disable=protected-accesstensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[9223372036854775807] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:Range]
"
52657,1,0,0,0,1,wei-v-wang,0,"title:Numpy 1.21.2/3 Causing Feature Column Unit Test Failures description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary): source- TensorFlow version (use command below): latest test was with https://github.com/tensorflow/tensorflow/commit/94bc26a5f71ad14f2390def5a6ecead451f7c9bc (an Oct'25 2021 commit on main branch)- Python version: 3.8- Bazel version (if compiling from source): ""We have bazel 3.7.2 installed.""- GCC/Compiler version (if compiling from source): gcc-7- CUDA/cuDNN version: NA- GPU model and memory: NAYou can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**This commit bumped up numpy version. https://github.com/tensorflow/tensorflow/commit/f96917ea887d1108358132c9dc9a0a0366e7d69eMany CI relies on the above file to run their nightlies. //tensorflow/python/feature_column:feature_column_test unit test failures with numpy 1.21.2 or numpy 1.21.3 with the following error message. numpy 1.19.5 was successful. [2021-10-25T12:40:03.025Z] ERROR: test_fills_cols_to_vars (__main__.LinearModelTest)[2021-10-25T12:40:03.025Z] LinearModelTest.test_fills_cols_to_vars[2021-10-25T12:40:03.025Z] ----------------------------------------------------------------------[2021-10-25T12:40:03.025Z] Traceback (most recent call last):[2021-10-25T12:40:03.025Z]   File ""/home/tensorflow_ci_jenkins/workspace/workspace/workspace/tensorflow-eigen-test/bazel-ci_build-cache/.cache/bazel/_bazel_tensorflow_ci_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/feature_column/feature_column_test.runfiles/org_tensorflow/tensorflow/python/feature_column/feature_column_test.py"", line 1612, in test_fills_cols_to_vars[2021-10-25T12:40:03.025Z]     self.assertAllEqual(cols_to_vars['bias'], [bias])[2021-10-25T12:40:03.025Z]   File ""/home/tensorflow_ci_jenkins/workspace/workspace/workspace/tensorflow-eigen-test/bazel-ci_build-cache/.cache/bazel/_bazel_tensorflow_ci_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/feature_column/feature_column_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1390, in decorated[2021-10-25T12:40:03.025Z]     return f(*args, **kwds)[2021-10-25T12:40:03.025Z]   File ""/home/tensorflow_ci_jenkins/workspace/workspace/workspace/tensorflow-eigen-test/bazel-ci_build-cache/.cache/bazel/_bazel_tensorflow_ci_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/feature_column/feature_column_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 3055, in assertAllEqual[2021-10-25T12:40:03.025Z]     a = self._GetNdArray(a)[2021-10-25T12:40:03.025Z]   File ""/home/tensorflow_ci_jenkins/workspace/workspace/workspace/tensorflow-eigen-test/bazel-ci_build-cache/.cache/bazel/_bazel_tensorflow_ci_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/feature_column/feature_column_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2799, in _GetNdArray[2021-10-25T12:40:03.025Z]     return np.array(a)[2021-10-25T12:40:03.025Z]   File ""/home/tensorflow_ci_jenkins/workspace/workspace/workspace/tensorflow-eigen-test/bazel-ci_build-cache/.cache/bazel/_bazel_tensorflow_ci_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/feature_column/feature_column_test.runfiles/org_tensorflow/tensorflow/python/ops/resource_variable_ops.py"", line 534, in __array__[2021-10-25T12:40:03.025Z]     return np.asarray(self.numpy(), dtype=dtype)[2021-10-25T12:40:03.025Z]   File ""/home/tensorflow_ci_jenkins/workspace/workspace/workspace/tensorflow-eigen-test/bazel-ci_build-cache/.cache/bazel/_bazel_tensorflow_ci_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/feature_column/feature_column_test.runfiles/org_tensorflow/tensorflow/python/ops/resource_variable_ops.py"", line 674, in numpy[2021-10-25T12:40:03.025Z]     raise NotImplementedError([2021-10-25T12:40:03.025Z] NotImplementedError: numpy() is only available when eager execution is enabled.**Describe the expected behavior**The unit test should pass. **[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): yes- Briefly describe your candidate solution(if contributing):- Downgrade numpy to ~1.19.2 as workaround. Not sure about the root-cause and fix. **Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.Simply run the //tensorflow/python/feature_column:feature_column_test unit test. **Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.An example full failure log can be found in Intel's public CI page via https://tensorflow-ci.intel.com/job/tensorflow-eigen-test/492/artifact/eigen_test.log/*view*/ (subject to expiration). 
"
52653,0,1441,0,0,0,GPhilo,0,"title:TFLite: XNNPACK delegate failed to delegate FULLY_CONNECTED node description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -- TensorFlow installed from (source or binary): Source for inference, Binary for model generation- TensorFlow version (use command below): r2.6.0 (from git tag) for the inference code.  2.4.1 for model generation.- Python version: 3.8.5 (model generation side only)- Bazel version (if compiling from source): N/A, I'm building TFLite via CMake- GCC/Compiler version (if compiling from source): MSVC 19.29.30136.0- CUDA/cuDNN version: N/A- GPU model and memory: N/A**Describe the current behavior**When loading the attached MobileNet model via the TFLite C API, the XNNPACK delegate fails with the following output:```INFO: Created TensorFlow Lite XNNPACK delegate for CPU.ERROR: failed to delegate FULLY_CONNECTED node #67ERROR: Node number 83 (TfLiteXNNPackDelegate) failed to prepare.```**Describe the expected behavior**The model loads correctly.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing): -**Standalone code to reproduce the issue**Model file: [model.fp32.tflite.zip](https://github.com/tensorflow/tensorflow/files/7409553/model.fp32.tflite.zip)### Model generation (if not using the attached tflite model):```pythonimport tensorflow as tffrom pathlib import Pathstrategy = tf.distribute.MirroredStrategy()with strategy.scope():  base_model = tf.keras.applications.mobilenet_v2.MobileNetV2(        input_shape=(224,224,3), alpha=1.0, weights='imagenet',        classes=6, include_top=False, pooling='avg')  output = tf.keras.layers.Dense(6, name='logits', dtype='float32')(base_model.output)  model = tf.keras.Model(base_model.inputs, output)converter = tf.lite.TFLiteConverter.from_keras_model(model)converter.optimizations = [tf.lite.Optimize.DEFAULT]tflite_model = converter.convert()Path('model.fp32.tflite').write_bytes(tflite_model)```### Model loading in Cpp```cpp#include <tensorflow/lite/c/c_api.h>#include <tensorflow/lite/delegates/xnnpack/xnnpack_delegate.h>int main(){    auto m_model = TfLiteModelCreateFromFile(""model.fp32.tflite"");    auto m_options = TfLiteInterpreterOptionsCreate();    TfLiteXNNPackDelegateOptions opt = TfLiteXNNPackDelegateOptionsDefault();    auto m_xnnpack_delegate = TfLiteXNNPackDelegateCreate(&opt);    TfLiteInterpreterOptionsAddDelegate(m_options, m_xnnpack_delegate);    auto m_interpreter = TfLiteInterpreterCreate(m_model, m_options); // This returns nullptr and prints error messages in the console}```
"
52638,1,313,0,0,0,yogeesh-alphaics,0,"title:ValueError: Converting a list of object of custom class (ExtensionType) to tensor error description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):linux ubuntu 20.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no- TensorFlow installed from (source or binary):binary- TensorFlow version (use command below):2.8.0-dev20211023- Python version:3.8.10- Bazel version (if compiling from source):no- GCC/Compiler version (if compiling from source):no- CUDA/cuDNN version: nil- GPU model and memory: nili have implemented a simple class in python extending from tf.exerimental.ExtensionType which stores three tensors into it. My goal is to create a list of these objects which then can be returned from a function after converting this list into tensor. I have tried creating a tf.Variable object , tf.TensorArray object , tf.constant obect , passed numpy object of this list into these function too but getting an error that i cannot convert to tensor or ""classname"" is not identified as proper tf.Dtype. **Standalone code to reproduce the issue**https://colab.research.google.com/drive/1nea-BQ6nRsbiAAKFMnmeKSeqAeHiq563?usp=sharing**Other info / logs** Include any logs or source code that would be helpful to```ValueError: Attempt to convert a value (Array(values=<tf.Tensor: shape=(5,), dtype=int32, numpy=array([1, 2, 3, 4, 5], dtype=int32)>, matched=<tf.Tensor: shape=(), dtype=bool, numpy=True>, iou=<tf.Tensor: shape=(), dtype=float32, numpy=53.46>)) with an unsupported type (<class '__main__.Array'>) to a Tensor.```
"
52627,1,7901,0,0,0,yogeesh-alphaics,0,"title:TypeError: To be compatible with tf.eager.defun, Python functions must return zero or more Tensors; description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux ubuntu 20.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.6.0- Python version: 3.8.10- Bazel version (if compiling from source): no- GCC/Compiler version (if compiling from source): no- CUDA/cuDNN version: cuda_11.2- GPU model and memory: nvidia geforce rtx 2060 / 6gbI am implementing a function which is calld by tf.keras model.fit function while running the validation dataset after every epoch, hence the fucntion will be runnig in graph mode. The problem is when i am reurning parameters from this function . I am getting a typeerror stating that 'i should return 0 or more tensors' , whereas currently I am returning a list. *Since the list contains numpy array and each np array is collection of custom class in python. I have tried converting this numpy array in tensor variable , tensorarray (dtype is a problem ) to no avail . Hence i am unable to figure out how should i return the Box objects in form of tensor.*As this function is running in eager mode  **prefectly fine** , i am wondering is it a strict signature constraint to return tensors and not even numpy array from a function decorated under tf.fucntion decorator.**Standalone code to reproduce the issue**  https://colab.research.google.com/drive/1Ei2t9coPNEVrfmejzaRUDIs-tm05EZFD?usp=sharing**Other info / logs** Include any logs or source code that would be helpful to```TypeError                                 Traceback (most recent call last)/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in convert(x)    962         try:--> 963           x = ops.convert_to_tensor_or_composite(x)    964         except (ValueError, TypeError):20 frames/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor_or_composite(value, dtype, name)   1688   return internal_convert_to_tensor_or_composite(-> 1689       value=value, dtype=dtype, name=name, as_ref=False)   1690 /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor_or_composite(value, dtype, name, as_ref)   1727         as_ref=as_ref,-> 1728         accepted_result_types=(Tensor, composite_tensor.CompositeTensor))   1729 /usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py in wrapped(*args, **kwargs)    162           return func(*args, **kwargs)--> 163       return func(*args, **kwargs)    164 /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)   1565     if ret is None:-> 1566       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)   1567 /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)     51   del as_ref  # Unused.---> 52   return constant_op.constant(value, dtype, name=name)     53 /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)    271   return _constant_impl(value, dtype, shape, name, verify_shape=False,--> 272                         allow_broadcast=True)    273 /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)    289           value, dtype=dtype, shape=shape, verify_shape=verify_shape,--> 290           allow_broadcast=allow_broadcast))    291   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)    563         ""Element type not supported in TensorProto: %s"" % numpy_dtype.name)--> 564   append_fn(tensor_proto, proto_values)    565 tensorflow/python/framework/fast_tensor_util.pyx in tensorflow.python.framework.fast_tensor_util.AppendObjectArrayToTensorProto()/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/compat.py in as_bytes(bytes_or_text, encoding)     86     raise TypeError('Expected binary or unicode string, got %r' %---> 87                     (bytes_or_text,))     88 TypeError: Expected binary or unicode string, got <__main__.BoundingBox object at 0x7f3f3b798a50>During handling of the above exception, another exception occurred:TypeError                                 Traceback (most recent call last)<ipython-input-119-e9ab93aab472> in <module>()     29      30 if __name__ == ""__main__"":---> 31     test_postprocess()<ipython-input-119-e9ab93aab472> in test_postprocess()     24         batch_labels = [np.random.uniform(0,1 , size = [1,13,13,3,7]).astype(np.float32) , np.random.uniform(0,1 , size = [1,26,26,3,7]).astype(np.float32) , np.random.uniform(0,1 , size = [1,52,52,3,7]).astype(np.float32)]     25         # box_objects = post_process(batch_labels , anchors)---> 26         box_objects = post_process(batch_labels , anchors)     27         # print(box_objects)     28 /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)    883     884       with OptionalXlaContext(self._jit_compile):--> 885         result = self._call(*args, **kwds)    886     887       new_tracing_count = self.experimental_get_tracing_count()/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)    931       # This is the first call of __call__, so we have to initialize.    932       initializers = []--> 933       self._initialize(args, kwds, add_initializers_to=initializers)    934     finally:    935       # At this point we know that the initialization is complete (or less/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)    758     self._concrete_stateful_fn = (    759         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access--> 760             *args, **kwds))    761     762     def invalid_creator_scope(*unused_args, **unused_kwds):/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)   3064       args, kwargs = None, None   3065     with self._lock:-> 3066       graph_function, _ = self._maybe_define_function(args, kwargs)   3067     return graph_function   3068 /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)   3461    3462           self._function_cache.missed.add(call_context_key)-> 3463           graph_function = self._create_graph_function(args, kwargs)   3464           self._function_cache.primary[cache_key] = graph_function   3465 /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)   3306             arg_names=arg_names,   3307             override_flat_arg_shapes=override_flat_arg_shapes,-> 3308             capture_by_value=self._capture_by_value),   3309         self._function_attributes,   3310         function_spec=self.function_spec,/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)   1010       # TensorArrays and `None`s.   1011       func_outputs = nest.map_structure(convert, func_outputs,-> 1012                                         expand_composites=True)   1013    1014       check_mutation(func_args_before, func_args, original_func)/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)    867     868   return pack_sequence_as(--> 869       structure[0], [func(*x) for x in entries],    870       expand_composites=expand_composites)    871 /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py in <listcomp>(.0)    867     868   return pack_sequence_as(--> 869       structure[0], [func(*x) for x in entries],    870       expand_composites=expand_composites)    871 /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in convert(x)    967               ""must return zero or more Tensors; in compilation of %s, found ""    968               ""return value of type %s, which is not a Tensor."" %--> 969               (str(python_func), type(x)))    970       if add_control_dependencies:    971         x = deps_ctx.mark_as_return(x)TypeError: To be compatible with tf.eager.defun, Python functions must return zero or more Tensors; in compilation of <function post_process at 0x7f3f368f9b00>, found return value of type <class 'numpy.ndarray'>, which is not a Tensor.```
"
52626,1,3535,72,0,0,stefanistrate,0,"title:Cannot load back model with no-op Concatenate layer description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.6- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0- Python version: 3.9.7- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source): N/A- CUDA/cuDNN version: N/A- GPU model and memory: N/AYou can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**When I create a simple model with a dummy Concatenate layer (i.e. the concatenation receives one single element), I am able to save it successfully, but the subsequent model loading fails.**Describe the expected behavior**The model loading should finish without errors.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): No- Briefly describe your candidate solution(if contributing): N/A**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.```pythonimport tensorflow as tfif __name__ == ""__main__"":    input_layer = tf.keras.Input(shape=[100])    dense_layer = tf.keras.layers.Dense(1)(input_layer)    concatenate_layer = tf.keras.layers.Concatenate()([dense_layer])    model = tf.keras.Model([input_layer], [concatenate_layer])    model.compile(optimizer=""adam"", loss=""mean_absolute_error"")    model.save(""model.h5"")    loaded_model = tf.keras.models.load_model(""model.h5"")```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.Full traceback:```bashTraceback (most recent call last):  File ""/Users/stefan/workspace/tierra/bug.py"", line 10, in <module>    loaded_model = tf.keras.models.load_model(""model.h5"")  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/saving/save.py"", line 200, in load_model    return hdf5_format.load_model_from_hdf5(filepath, custom_objects,  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/saving/hdf5_format.py"", line 180, in load_model_from_hdf5    model = model_config_lib.model_from_config(model_config,  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/saving/model_config.py"", line 52, in model_from_config    return deserialize(config, custom_objects=custom_objects)  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/layers/serialization.py"", line 208, in deserialize    return generic_utils.deserialize_keras_object(  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/utils/generic_utils.py"", line 674, in deserialize_keras_object    deserialized_obj = cls.from_config(  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/functional.py"", line 662, in from_config    input_tensors, output_tensors, created_layers = reconstruct_from_config(  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/functional.py"", line 1283, in reconstruct_from_config    process_node(layer, node_data)  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/functional.py"", line 1231, in process_node    output_tensors = layer(input_tensors, **kwargs)  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/base_layer.py"", line 976, in __call__    return self._functional_construction_call(inputs, args, kwargs,  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/base_layer.py"", line 1114, in _functional_construction_call    outputs = self._keras_tensor_symbolic_call(  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/base_layer.py"", line 848, in _keras_tensor_symbolic_call    return self._infer_output_signature(inputs, args, kwargs, input_masks)  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/base_layer.py"", line 886, in _infer_output_signature    self._maybe_build(inputs)  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/base_layer.py"", line 2659, in _maybe_build    self.build(input_shapes)  # pylint:disable=not-callable  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/utils/tf_utils.py"", line 259, in wrapper    output_shape = fn(instance, input_shape)  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/layers/merge.py"", line 489, in build    raise ValueError('A `Concatenate` layer should be called 'ValueError: A `Concatenate` layer should be called on a list of at least 1 input.```
"
52619,1,1374,101,0,0,isarandi,0,"title:MirroredVariable has different values on replicas (only first device is correct) description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, two lines- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -- TensorFlow installed from (source or binary): binary (pip)- TensorFlow version (use command below): 2.6.0 (but I also tried tf-nightly with the same result)- Python version: 3.8- Bazel version (if compiling from source): -- GCC/Compiler version (if compiling from source): -- CUDA/cuDNN version: 11.2 / 8.1.1 (same behavior also with 11.4 / 8.2.4)- GPU model and memory: 4 x NVIDIA A40 (48GB)**Describe the current behavior**Minimal code example:```pythonimport tensorflow as tf    with tf.distribute.MirroredStrategy().scope():    print(tf.Variable(1.))```Output is:```INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')MirroredVariable:{  0: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>,  1: <tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=0.0>,  2: <tf.Variable 'Variable/replica_2:0' shape=() dtype=float32, numpy=0.0>,  3: <tf.Variable 'Variable/replica_2:0' shape=() dtype=float32, numpy=0.0>}```The problem is, as seen above, that the replicas do not contain the correct variable value, all are zero values except on the first device (the `numpy=0.0` parts). This is the same with 2 or 3 devices as well, not just with all 4.(The same code does produce the expected behavior on a different machine with 2x Titan RTX GPUs.)This is simply the minimal reproducing example. The real-world consequence when performing multi-gpu training is that the first forward pass succeeds, but after the first SGD update, things become NaN.**Describe the expected behavior**Expected output would be:```INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')MirroredVariable:{  0: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>,  1: <tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=1.0>,  2: <tf.Variable 'Variable/replica_2:0' shape=() dtype=float32, numpy=1.0>,  3: <tf.Variable 'Variable/replica_2:0' shape=() dtype=float32, numpy=1.0>}```(Note the `numpy=1.0` parts.)**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.```pythonimport tensorflow as tf    with tf.distribute.MirroredStrategy().scope():    print(tf.Variable(1.))```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.The server in question is a Dell PowerEdge R750xa with 4x Nvidia A40 GPUs.
"
52617,1,0,0,0,0,yogeesh-alphaics,0,"title:InaccessibleTensoreorrr: tensor not acessible here , present in another or code block description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux ubuntu 20.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.6.0- Python version: 3.8.10- Bazel version (if compiling from source): no- GCC/Compiler version (if compiling from source): no- CUDA/cuDNN version: cuda_11.2- GPU model and memory: nvidia geforce rtx 2060 / 6gb I have implemented a post processing function for object detction model , which will also be used in mAP calcuation using tf,keras custom metric class support. Though while running the scipt in eager mode gives no porblem whatsoever,  but running the script in graph mode (by putting the main function in tf.function scope ), gives an InaccesibleTensorerror.**Standalone code to reproduce the issue**link to notebook : https://colab.research.google.com/drive/1Ei2t9coPNEVrfmejzaRUDIs-tm05EZFD?usp=sharing **error**InaccessibleTensorError: in user code:    <ipython-input-30-c33ccae08316>:147 post_process  *        keep_index = [index for index in range(len(final_probs)) if final_probs[index] > filter_threshold]    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py:1817 wrapper        return fn(x, y, *args, **kwargs)    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py:3962 greater        ""Greater"", x=x, y=y, name=name)    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:750 _apply_op_helper        attrs=attr_protos, op_def=op_def)    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:597 _create_op_internal        inp = self.capture(inp)    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:647 capture        % (tensor, tensor.graph, self))    InaccessibleTensorError: The tensor 'Tensor(""while/while_1/cond/strided_slice_5:0"", shape=(), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=while_while_1_cond_true_56189, id=140026056052816); accessed from: FuncGraph(name=while_body_55673, id=140026059019408).
"
52596,1,5873,4,0,0,ghup1,0,"title:Mixed precision training incompatible with BinaryCrossentropy label smoothing description:### System information-   **Have I written custom code (as opposed to using a stock example script    provided in TensorFlow)**: Yes-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows Server 2012 R2-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue    happens on a mobile device**: --   **TensorFlow installed from (source or binary)**: binary-   **TensorFlow version (use command below)**: v2.6.0-rc2-32-g919f693420e 2.6.0-   **Python version**: 3.9.6-   **Bazel version (if compiling from source)**: --   **GCC/Compiler version (if compiling from source)**: --   **CUDA/cuDNN version**: 11.2.2 / 8.1-   **GPU model and memory**: NVIDIA Titan X (Pascal), 12288 MiB-   **Exact command to reproduce**: see example below### Describe the problemReporting a possible bug. When setting float16 mixed precision policy and using label smoothing in BinaryCrossentropy, training returns a TypeError. Turning off either the mixed precision policy or label smoothing gives no errors. Passing a float16 to the label_smoothing argument does not help.### Source code / logs**Reproducible example**```import tensorflow as tfimport numpy as np# Set mixed precision policytf.keras.mixed_precision.set_global_policy('mixed_float16')# Create some random datainputs  = tf.random.normal((64, 256, 256, 1))targets = tf.constant(np.random.choice(    a       = [0, 1],    size    = (64, 256, 256, 1),    replace = True))# Create simple modelmodel = tf.keras.Sequential([    tf.keras.Input(shape=(256, 256, 1)),    tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu')])model.compile(    optimizer = 'adam',    loss      = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1))# Train modelmodel.fit(inputs, targets, batch_size=8)```**Traceback**```Traceback (most recent call last):  File ""G:\[redacted]\src\debug1.py"", line 36, in <module>    model.fit(inputs, targets, batch_size=8)  File ""F:\conda_env\[redacted]\lib\site-packages\keras\engine\training.py"", line 1184, in fit    tmp_logs = self.train_function(iterator)  File ""F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\eager\def_function.py"", line 885, in __call__    result = self._call(*args, **kwds)  File ""F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\eager\def_function.py"", line 933, in _call    self._initialize(args, kwds, add_initializers_to=initializers)  File ""F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\eager\def_function.py"", line 759, in _initialize    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access  File ""F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\eager\function.py"", line 3066, in _get_concrete_function_internal_garbage_collected    graph_function, _ = self._maybe_define_function(args, kwargs)  File ""F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\eager\function.py"", line 3463, in _maybe_define_function    graph_function = self._create_graph_function(args, kwargs)  File ""F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\eager\function.py"", line 3298, in _create_graph_function    func_graph_module.func_graph_from_py_func(  File ""F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 1007, in func_graph_from_py_func    func_outputs = python_func(*func_args, **func_kwargs)  File ""F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\eager\def_function.py"", line 668, in wrapped_fn    out = weak_wrapped_fn().__wrapped__(*args, **kwds)  File ""F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 994, in wrapper    raise e.ag_error_metadata.to_exception(e)TypeError: in user code:    F:\conda_env\[redacted]\lib\site-packages\keras\engine\training.py:853 train_function  *        return step_function(self, iterator)    F:\conda_env\[redacted]\lib\site-packages\keras\engine\training.py:842 step_function  **        outputs = model.distribute_strategy.run(run_step, args=(data,))    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:1286 run        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:2849 call_for_each_replica        return self._call_for_each_replica(fn, args, kwargs)    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:3632 _call_for_each_replica        return fn(*args, **kwargs)    F:\conda_env\[redacted]\lib\site-packages\keras\engine\training.py:835 run_step  **        outputs = model.train_step(data)    F:\conda_env\[redacted]\lib\site-packages\keras\engine\training.py:788 train_step        loss = self.compiled_loss(    F:\conda_env\[redacted]\lib\site-packages\keras\engine\compile_utils.py:201 __call__        loss_value = loss_obj(y_t, y_p, sample_weight=sw)    F:\conda_env\[redacted]\lib\site-packages\keras\losses.py:141 __call__        losses = call_fn(y_true, y_pred)    F:\conda_env\[redacted]\lib\site-packages\keras\losses.py:245 call  **        return ag_fn(y_true, y_pred, **self._fn_kwargs)    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\util\dispatch.py:206 wrapper        return target(*args, **kwargs)    F:\conda_env\[redacted]\lib\site-packages\keras\losses.py:1805 binary_crossentropy        y_true = tf.__internal__.smart_cond.smart_cond(label_smoothing, _smooth_labels,    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\framework\smart_cond.py:56 smart_cond        return true_fn()    F:\conda_env\[redacted]\lib\site-packages\keras\losses.py:1803 _smooth_labels        return y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\ops\math_ops.py:1383 binary_op_wrapper        raise e    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\ops\math_ops.py:1367 binary_op_wrapper        return func(x, y, name=name)    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\ops\math_ops.py:1710 _mul_dispatch        return multiply(x, y, name=name)    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\util\dispatch.py:206 wrapper        return target(*args, **kwargs)    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\ops\math_ops.py:530 multiply        return gen_math_ops.mul(x, y, name)    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\ops\gen_math_ops.py:6244 mul        _, _, _op, _outputs = _op_def_library._apply_op_helper(    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\framework\op_def_library.py:555 _apply_op_helper        raise TypeError(    TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.```
"
52594,1,1032,0,0,0,bias,0,"title:Failed to invoke the interpreter with error: Provided data count XXX must match the required count 3. description:### 1. System information- OSX 11.5.2, iOS 14.7.1, CentOS 8.4.2105 - TensorFlow v2.6.0 custom built for server (CentOS 8.4.2105)- TensorFlow library 'TensorFlowLiteSwift', '~> 2.6'The CentOS custom built tensorflow was built with the following optimization flags:```-march=native -msse4_1 -msse4_2 -mssse3 -mcx16 -mpopcnt```On the CentOS 8 box I successfully trained a custom model using `faster_rcnn_resnet152_v1` for it's configuration. I'm able to run the model on images and mp4 files successfully. I converted the model on the CentOS box for tflite. I copied the model into the `tensorflow/examples/tree/master/lite/examples/object_detection/ios` example project and updated the Podfile to use tflite 2.6.### 2. Code```import tensorflow as tfsaved_model_dir = ""exported-models/barbell_3/saved_model""# Convert the modelconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)converter.target_spec.supported_ops = [  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.]tflite_model = converter.convert()# Save the model.with open('model.tflite', 'wb') as f:  f.write(tflite_model)```ModelDataHandler.swiftCase 1```  // MARK: Model parameters  let batchSize = 1  let inputChannels = 3  let inputWidth = 1024  let inputHeight = 1024```Case 2```  // MARK: Model parameters  let batchSize = 1  let inputChannels = 3  let inputWidth = 1  let inputHeight = 1```### 3. Failure after conversion#### Case 1- Interpreter can't load model```Failed to invoke the interpreter with error: Provided data count 3145728 must match the required count 3.```Note 1024 * 1024 * 3 = 3145728Note the netron graph entry point shows the input should be `1x1x1=3` followed by a while loop whose 3rd element is (1x1024x1024x3).Presumably, these are the correct parameters however see case 2.#### Case 2If I set the inputWidth and inputHeight to 1x1 the interpreter loads, the app starts and then crashes with an Out of Memory error (in a popup window) - however, nothing is logged in the XCode console and no errors are given for tensorflow lite.```2021-10-20 09:28:46.565241-0500 ObjectDetection[723:92493] Initialized TensorFlow Lite runtime.INFO: Initialized TensorFlow Lite runtime.```Is it the case that the correct parameters are inputHeight=1 and inputWidth=1 and that my model is using too much OS memory on the phone?If that's true are there guidelines on which model training config params are suitable for phones (both iOS and Andriod)? That is, models with better input size and hidden layer size etc?### 4. (optional) RNN conversion support### 5. (optional) Any other info / logs<img width=""1394"" alt=""netron"" src=""https://user-images.githubusercontent.com/128980/138111306-28d3f186-6461-4795-a798-9777cb203496.png"">
"
52588,1,500,0,0,0,ashishsb0307,0,"title:Modifying the Categorical Cross entropy for Mirrored strategy/Distributed training causing low validation accuracy. description:### System information-   **Have I written custom code (yes)-   **OS Platform and Distribution (Windows)-   **TensorFlow installed from (binary)-   **TensorFlow version (2.4.1)-   **Python version(3.7.9)-   **CUDA/cuDNN version(11.1/8/0):-   **GPU model and memory(Titan XP, 12 GB):### Describe the problemFor **normal (single GPU training)** current loss is being calculated as follows:```def compute_loss(labels, predictions):    loss = tf.reduce_mean(    tf.keras.losses.categorical_crossentropy(y_true=labels, y_pred=predictions)    )    return loss```For **Mirrored strategy/Distributed training (8 GPU)**, I am computing loss as follows:```loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=False,              reduction=tf.keras.losses.Reduction.NONE)def compute_loss(labels, predictions):    per_example_loss = loss_object(labels, predictions)     return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)```But in the Distributed strategy the loss is not converging as fast and getting very poor validation accuracy as compared to the original.
"
52575,1,1802,0,0,1,timmy-ops,0,"title:TF estimator train_and_evaluate: loss = None and model does not train description:Hi guys, I am working on a premade estimator model of tensorflow (DNNRegressor) in an old tensorflow version. I already found some similar issues but their solution didnt work out for me (they said it would be solved by setting max_steps to None). TF version: 1.15.1```train_spec = tf.estimator.TrainSpec(    input_fn=lambda: read_train(data_folder, params),    max_steps=None)```max_steps was 1400000 before and i tried now to set it to None, but it didnt work for me. My input pipeline looks like this:```clvf = CLVFeatures(ignore_crosses=True)def parse_csv(csv_row):  columns = tf.decode_csv(csv_row, record_defaults = clvf.get_all_defaults())  features = dict(zip(clvf.get_all_names(), columns))    for column_name in clvf.get_unused():    features.pop(column_name)  target = features.pop(clvf.get_target_name())  return features, target#@tf.functiondef dataset_input_fn(data_folder, prefix=None, mode=None, params=None, count=None):  shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False  filenames = tf.matching_files('{}{}*.csv'.format(data_folder, prefix))  dataset = tf.data.TextLineDataset(filenames)#skip(1)  dataset = dataset.map(parse_csv)  if shuffle:    dataset = dataset.shuffle(buffer_size=params.buffer_size)  dataset = dataset.repeat(count=count)  dataset = dataset.batch(params.batch_size)  iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)#tf.compat.v1.data.make_one_shot_iterator(dataset)/#tf.compat.v1.data.make_initializable_iterator(dataset)    features, target = iterator.get_next()  return features, targetdef read_train(data_folder, params):  return dataset_input_fn(      data_folder=data_folder,      prefix='train',      params=params,      mode=tf.estimator.ModeKeys.TRAIN)def read_eval(data_folder, params):  return dataset_input_fn(data_folder=data_folder,                          prefix='eval',                          params=params)def read_test(data_folder, params):  return dataset_input_fn(data_folder=data_folder,                          prefix='test',                          params=params,                          count=1)```I would appreciate some help, so much. This is for school haha thank you!
"
52537,1,361,4,0,0,MalcolmMielle,0,"title:Ref() does not return the same ref for the element. description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 34- TensorFlow installed from (source or binary): 2.6.0- Python version: 3.9.7**Describe the current behavior**When loading MINST and requesting the ref() of the first element, it is not equal to itself.**Describe the expected behavior**The element should have the same ref at all time.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no, I don't know why it's doing this.**Standalone code to reproduce the issue**```import tensorflow_datasets as tfds# %% Train dataset(ds_train_original, ds_test_original), ds_info = tfds.load(    ""mnist"",    split=[""train"", ""test""],    shuffle_files=True,    as_supervised=True,    with_info=True,)iterator = iter(ds_train_original)el = iterator.get_next()[0]el[0].ref() == el[0].ref()   # <- this should be True```
"
52536,1,0,0,0,1,mbkgh,0,"title:Tensorflow serving in docker causes ""Invalid reduction dimension (1 for input with 1 dimension(s)... description:I am using tensorflow-serving which runs with the build-in examples well and without issues:cd c:\tmp\tfservingPS C:\tmp\tfserving> C:\programme\git\bin\git clone https://github.com/tensorflow/servingset-variable -Name ""TESTDATA"" -Value ""$(pwd)/serving/tensorflow_serving/servables/tensorflow/testdata""docker run -t --rm -p 8501:8501 -v ""$TESTDATA/saved_model_half_plus_two_cpu:/models/half_plus_two"" -e MODEL_NAME=half_plus_two tensorflow/servingcurl -d ""{\""instances\"": [1.0, 2.0, 5.0]}"" -X POST http://localhost:8501/v1/models/half_plus_two:predictHowever, by trying to run the following model in a recent docker container it causes:""Invalid reduction dimension (1 for input with 1 dimension(s)\Steps to verify:I used the example code ...https://keras.io/examples/structured_data/structured_data_classification_from_scratch/and at the end saved the model model.save('my-model.tf')and put it into the serving directory where it was recognized:C:\tmp\tfserving\serving\tensorflow_serving\servables\tensorflow\testdatacd \tmp\tfservingset-variable -Name ""TESTDATA"" -Value ""$(pwd)/serving/tensorflow_serving/servables/tensorflow/testdata""docker run -t --rm -p 8501:8501 -v ""$TESTDATA/my_model:/models/my_model"" -e MODEL_NAME=my_model tensorflow/servingcurl http://localhost:8501/v1/models/my_model{  ""model_version_status"": [  { ""version"": ""1"", ""state"": ""AVAILABLE"", ""status"": { ""error_code"": ""OK"", [...]However, trying to run a prediction curl -d ""{ \""instances\"": [ {\""age\"": 50,\""sex\"": 1,\""cp\"": 1,\""trestbps\"": 145,\""chol\"": 133,\""fbs\"": 1,\""restecg\"": 2,\""thalach\"": 150,\""exang\"": 0,\""oldpeak\"": 2.3,\""slope\"": 3,\""ca\"": 0,\""thal\"": \""fixed\"" } ]}"" -X POST http://localhost:8501/v1/models/my_model:predict... returns:>>""error"": ""Invalid reduction dimension (1 for input with 1 dimension(s)\n\t [[{{node model/integer_lookup_5/bincount/Max}}]]""**System information**- Running on Windows with no GPU-Support. No further changes were made on the example code- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n.a.- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below):  v2.6.0-rc2-32-g919f693420e 2.6.0- Python version: 3.8.11 (default, Aug  6 2021, 09:57:55) [MSC v.1916 64 bit (AMD64)]- Bazel version (if compiling from source): have also tried to compile on linux with bazel but with the same result- GCC/Compiler version (if compiling from source): n.a.- CUDA/cuDNN version: n.a.- GPU model and memory: n.a.
"
52517,0,0,0,1,0,jooho-enerzai,0,"title:Error occurs during quantization usingTFLiteConvert. description:### 1. System information- Linux Ubunut 20.04- pip install tensorflow==2.6.0- pip install tensorflow_model_optimization==0.7.0### 2. Codehttps://colab.research.google.com/drive/1kOgZevD3dkM0hxehTh8vNOW_1zXW4VDE?usp=sharing### 3. Failure after conversionerror: 'tfl.max_pool_2d' op quantization parameters violate the same scale constraint: !quant.uniform<i8:f32, 0.010792325524722828> vs. !quant.uniform<i8:f32, 0.0039215686274509803:-128>(failure on converting)### 5. (optional) Any other info / logsI build two very similar models, but one is successfully converted to quantized model, while other is failed.Same with avgpool instead of maxpoolThere was a  similar issue  #46754 (https://github.com/tensorflow/tensorflow/issues/46754),but I think concat isn't problem because one of my model including concat works.Thank you
"
52380,0,7743,0,0,0,aewhite,0,"title:Meshgrid does not work with tf.function description:The following code fails at runtime:```import tensorflow as tfdef f(x, y):    return tf.meshgrid(x, y)@tf.functiondef g(x, y):    return tf.meshgrid(x, y)def main():    print(f""tensorflow version: {tf.version.VERSION}"")    all_values = tf.range(0.0, 1.0, .1)    x = y = tf.expand_dims(all_values, -1)    print(f(x, y))  # This works    print(g(x, y)) # This failsif __name__ == '__main__':    main()```The output:```2021-10-14 12:51:02.388948: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.02021-10-14 12:51:03.952753: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.12021-10-14 12:51:04.028174: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected2021-10-14 12:51:04.028234: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (removed): /proc/driver/nvidia/version does not exist2021-10-14 12:51:04.028962: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.tensorflow version: 2.5.1[<tf.Tensor: shape=(10, 10), dtype=float32, numpy=array([[0.        , 0.1       , 0.2       , 0.3       , 0.4       ,        0.5       , 0.6       , 0.70000005, 0.8000001 , 0.9000001 ],       [0.        , 0.1       , 0.2       , 0.3       , 0.4       ,        0.5       , 0.6       , 0.70000005, 0.8000001 , 0.9000001 ],       [0.        , 0.1       , 0.2       , 0.3       , 0.4       ,        0.5       , 0.6       , 0.70000005, 0.8000001 , 0.9000001 ],       [0.        , 0.1       , 0.2       , 0.3       , 0.4       ,        0.5       , 0.6       , 0.70000005, 0.8000001 , 0.9000001 ],       [0.        , 0.1       , 0.2       , 0.3       , 0.4       ,        0.5       , 0.6       , 0.70000005, 0.8000001 , 0.9000001 ],       [0.        , 0.1       , 0.2       , 0.3       , 0.4       ,        0.5       , 0.6       , 0.70000005, 0.8000001 , 0.9000001 ],       [0.        , 0.1       , 0.2       , 0.3       , 0.4       ,        0.5       , 0.6       , 0.70000005, 0.8000001 , 0.9000001 ],       [0.        , 0.1       , 0.2       , 0.3       , 0.4       ,        0.5       , 0.6       , 0.70000005, 0.8000001 , 0.9000001 ],       [0.        , 0.1       , 0.2       , 0.3       , 0.4       ,        0.5       , 0.6       , 0.70000005, 0.8000001 , 0.9000001 ],       [0.        , 0.1       , 0.2       , 0.3       , 0.4       ,        0.5       , 0.6       , 0.70000005, 0.8000001 , 0.9000001 ]],      dtype=float32)>, <tf.Tensor: shape=(10, 10), dtype=float32, numpy=array([[0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ],       [0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ],       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ,        0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],       [0.3       , 0.3       , 0.3       , 0.3       , 0.3       ,        0.3       , 0.3       , 0.3       , 0.3       , 0.3       ],       [0.4       , 0.4       , 0.4       , 0.4       , 0.4       ,        0.4       , 0.4       , 0.4       , 0.4       , 0.4       ],       [0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ],       [0.6       , 0.6       , 0.6       , 0.6       , 0.6       ,        0.6       , 0.6       , 0.6       , 0.6       , 0.6       ],       [0.70000005, 0.70000005, 0.70000005, 0.70000005, 0.70000005,        0.70000005, 0.70000005, 0.70000005, 0.70000005, 0.70000005],       [0.8000001 , 0.8000001 , 0.8000001 , 0.8000001 , 0.8000001 ,        0.8000001 , 0.8000001 , 0.8000001 , 0.8000001 , 0.8000001 ],       [0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,        0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ]],      dtype=float32)>]Traceback (most recent call last):  File ""/mnt/workspace/tmp/pycharm_project_951/python_gamma/meshgrid_bug.py"", line 23, in <module>    main()  File ""/mnt/workspace/tmp/pycharm_project_951/python_gamma/meshgrid_bug.py"", line 19, in main    print(g(x, y))  File ""/usr/local/lib64/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 889, in __call__    result = self._call(*args, **kwds)  File ""/usr/local/lib64/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 933, in _call    self._initialize(args, kwds, add_initializers_to=initializers)  File ""/usr/local/lib64/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 764, in _initialize    *args, **kwds))  File ""/usr/local/lib64/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3050, in _get_concrete_function_internal_garbage_collected    graph_function, _ = self._maybe_define_function(args, kwargs)  File ""/usr/local/lib64/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3444, in _maybe_define_function    graph_function = self._create_graph_function(args, kwargs)  File ""/usr/local/lib64/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3289, in _create_graph_function    capture_by_value=self._capture_by_value),  File ""/usr/local/lib64/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 999, in func_graph_from_py_func    func_outputs = python_func(*func_args, **func_kwargs)  File ""/usr/local/lib64/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 672, in wrapped_fn    out = weak_wrapped_fn().__wrapped__(*args, **kwds)  File ""/usr/local/lib64/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 986, in wrapper    raise e.ag_error_metadata.to_exception(e)NotImplementedError: in user code:    /mnt/workspace/tmp/pycharm_project_951/python_gamma/meshgrid_bug.py:11 g  *        return tf.meshgrid(x, y)    /usr/local/lib64/python3.7/site-packages/tensorflow/python/util/dispatch.py:206 wrapper  **        return target(*args, **kwargs)    /usr/local/lib64/python3.7/site-packages/tensorflow/python/ops/array_ops.py:3644 meshgrid        mult_fact = ones(shapes, output_dtype)    /usr/local/lib64/python3.7/site-packages/tensorflow/python/util/dispatch.py:206 wrapper        return target(*args, **kwargs)    /usr/local/lib64/python3.7/site-packages/tensorflow/python/ops/array_ops.py:3212 ones        output = _constant_if_small(one, shape, dtype, name)    /usr/local/lib64/python3.7/site-packages/tensorflow/python/ops/array_ops.py:2896 _constant_if_small        if np.prod(shape) < 1000:    <__array_function__ internals>:6 prod            /mnt/workspace/python/numpy/core/fromnumeric.py:3052 prod        keepdims=keepdims, initial=initial, where=where)    /mnt/workspace/python/numpy/core/fromnumeric.py:86 _wrapreduction        return ufunc.reduce(obj, axis, dtype, out, **passkwargs)    /usr/local/lib64/python3.7/site-packages/tensorflow/python/framework/ops.py:870 __array__        "" a NumPy call, which is not supported"".format(self.name))    NotImplementedError: Cannot convert a symbolic Tensor (meshgrid/Size_1:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported```This is running the AWS DLAMI for TF 2.5.1 using the following AMI: ami-09ddcd88a97c092e5. The EC2 instance type is a t3.small. 
"
52379,1,0,90,0,0,ghost,0,"title:ctc_loss_calculator.h:499] No valid path found description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOs Big Sur 11.6- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0- Python version: 3.9.7- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version:- GPU model and memory:You can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**The issue is addressed in multiple SO questions and none provides a clear solution if any answers at all - [CTC loss Tensorflow, No valid path found](https://stackoverflow.com/questions/47461331/ctc-loss-tensorflow-no-valid-path-found) - [ctc_loss error ""No valid path found.""](https://stackoverflow.com/questions/45130184/ctc-loss-error-no-valid-path-found) - [CTC Loss bug: no valid path found? OCR difficulties in Tf.keras](https://stackoverflow.com/questions/64676073/ctc-loss-bug-no-valid-path-found-ocr-difficulties-in-tf-keras) - [Tensorflow ctc_loss_calculator: No valid path found](https://stackoverflow.com/questions/44195801/tensorflow-ctc-loss-calculator-no-valid-path-found)Here's the code I run ...    from itertools import groupby    from pathlib import Path        import numpy as np    import tensorflow as tf    from matplotlib import pyplot as plt    from tensorflow.keras import Model    from tensorflow.keras.callbacks import EarlyStopping    from tensorflow.keras.layers import (LSTM, BatchNormalization, Bidirectional,                                         Conv2D, Dense, Input, Lambda, Layer,                                         MaxPool2D)    from tensorflow.keras.layers.experimental.preprocessing import StringLookup    from tensorflow.keras.optimizers import Adam    from tensorflow.keras.preprocessing.sequence import pad_sequences            class CTCLayer(Layer):        def __init__(self, *args, **kwargs):            super().__init__(*args, **kwargs)            self.loss_fn = tf.keras.backend.ctc_batch_cost            def call(self, y_true, *args, **kwargs):            y_pred = args[0]            batch_len = tf.cast(tf.shape(y_true)[0], dtype='int64')            input_length = tf.cast(tf.shape(y_pred)[1], dtype='int64')            label_length = tf.cast(tf.shape(y_true)[1], dtype='int64')            input_length = input_length * tf.ones(shape=(batch_len, 1), dtype='int64')            label_length = label_length * tf.ones(shape=(batch_len, 1), dtype='int64')            loss = self.loss_fn(y_true, y_pred, input_length, label_length)            self.add_loss(loss)            return y_pred            class TrainingManager:        def __init__(            self, images, labels, batch_size=256, validation_size=0.1, resize=(32, 128)        ):            self.images = images            self.labels = labels            self.batch_size = batch_size            self.validation_size = validation_size            self.resize = resize            self.vocabulary = sorted(set(''.join(self.labels)))            self.max_label_length = len(max(self.labels, key=len))            self.char_to_num = StringLookup(                vocabulary=self.vocabulary, num_oov_indices=0, mask_token=None            )            self.num_to_char = StringLookup(                vocabulary=self.char_to_num.get_vocabulary(), mask_token=None, invert=True            )            def process_example(self, img_path, label):            img = tf.io.read_file(img_path)            img = tf.io.decode_png(img, channels=1)            img = tf.image.convert_image_dtype(img, tf.float32)            img = tf.image.resize(img, self.resize)            return {'image': img, 'label': label}            def preview_dataset(self, dataset, n_rows, n_cols, fig_size=(15, 10)):            fig, ax = plt.subplots(n_rows, n_cols, figsize=fig_size)            for batch in dataset.take(1):                images = batch['image']                labels = batch['label']                for i in range(n_rows * n_cols):                    img = (images[i] * 255).numpy().astype('uint8')                    label = (                        tf.strings.reduce_join(self.num_to_char(labels[i] + 1))                        .numpy()                        .decode('utf-8')                        .replace('[UNK]', '')                    )                    ax[i // n_rows, i % n_cols].imshow(img[:, :, 0], cmap='gray')                    ax[i // n_rows, i % n_cols].set_title(label)                    ax[i // n_rows, i % n_cols].axis('off')            def decode_predictions(self, predictions):            text_list = []            prediction_indices = np.argmax(predictions, axis=2)            for i in range(prediction_indices.shape[0]):                text = ''                for p, _ in groupby(prediction_indices[i]):                    if p != len(self.vocabulary):                        text += self.vocabulary[p]                text_list.append(text)            return text_list            def create_dataset(self, x, y):            dataset = tf.data.Dataset.from_tensor_slices((x, y))            return (                dataset.map(                    self.process_example, num_parallel_calls=tf.data.experimental.AUTOTUNE                )                .batch(self.batch_size)                .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)            )            def split_data(self):            separation_idx = int(len(self.images) * (self.validation_size - 1))            train_images = self.images[:separation_idx]            valid_images = self.images[separation_idx:]            labels = [                self.char_to_num(tf.strings.unicode_split(label, input_encoding='UTF-8'))                for label in self.labels            ]            labels = pad_sequences(labels, self.max_label_length, padding='post')            train_labels = labels[:separation_idx]            valid_labels = labels[separation_idx:]            return train_images, valid_images, train_labels, valid_labels            def create_datasets(self):            train_images, valid_images, train_labels, valid_labels = self.split_data()            train_dataset = self.create_dataset(train_images, train_labels)            valid_dataset = self.create_dataset(valid_images, valid_labels)            return train_dataset, valid_dataset            def create_model(self, training=True):            x0 = Input(shape=(32, 128, 1), name='image')            x = Conv2D(32, (3, 3), activation='selu', padding='same')(x0)            x = MaxPool2D(pool_size=(2, 2))(x)            x = Conv2D(64, (3, 3), activation='selu', padding='same')(x)            x = MaxPool2D(pool_size=(2, 2))(x)            x = Conv2D(128, (3, 3), activation='selu', padding='same')(x)            x = Conv2D(128, (3, 3), activation='selu', padding='same')(x)            x = MaxPool2D(pool_size=(2, 1))(x)            x = Conv2D(256, (3, 3), activation='selu', padding='same')(x)            x = BatchNormalization()(x)            x = Conv2D(256, (3, 3), activation='selu', padding='same')(x)            x = BatchNormalization()(x)            x = MaxPool2D(pool_size=(2, 1))(x)            x = Conv2D(64, (2, 2), activation='selu')(x)            x = Lambda(lambda i: tf.squeeze(i, 1))(x)            x = Bidirectional(LSTM(128, return_sequences=True))(x)            x = Bidirectional(LSTM(128, return_sequences=True))(x)            output = Dense(len(self.vocabulary) + 1, activation='softmax', name='dense')(x)            if not training:                return Model(x0, output)            labels = Input(name='label', shape=(None,), dtype='float32')            output = CTCLayer(name='ctc_loss')(labels, output)            return Model(inputs=[x0, labels], outputs=output)            if __name__ == '__main__':        photos, texts = [], []        for line in open('labels.txt'):            photo_path, photo_text = line.split(',')            photos.append((Path('examples') / photo_path).as_posix())            texts.append(photo_text.strip())        manager = TrainingManager(photos, texts, batch_size=32)        optimizer = Adam()        model = manager.create_model()        model.compile(optimizer=optimizer, metrics=[tf.keras.metrics.Accuracy()])        model.summary()        tds, vds = manager.create_datasets()        manager.preview_dataset(tds, 2, 2)        plt.show()        history = model.fit(            tds,            epochs=100,            validation_data=vds,            verbose=1,            callbacks=[EarlyStopping(verbose=1, patience=3, restore_best_weights=True)],            shuffle=True,        )**`examples` + `labels.txt` (inside examples folder)**[examples.tar.gz](https://drive.google.com/file/d/1hBVhUNvlpwhENgLf5Ui_AS7eiSb_L39B/view?usp=sharing)**Note:** The code works perfectly fine for labels that are 15 characters long or shorter. The labels and respective photos in the example above have 1-20 characters each. What exactly do I need to modify, to make it work, given a label of length n? **Describe the expected behavior****[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.2021-10-14 05:22:31.610 Python[18731:595755] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/hr/61r_7jcx2r3cnklwrr2zwbqw0000gn/T/org.python.python.savedState    2021-10-14 05:22:31.746089: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)    Epoch 1/100    2021-10-14 05:22:41.480510: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:41.480551: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:41.480614: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:41.480785: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    1/1 [==============================] - ETA: 0s - loss: inf - accuracy: 0.0000e+002021-10-14 05:22:44.004554: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.004595: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.004646: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.004705: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.004822: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.004859: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.004890: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.004907: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.005056: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.005073: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.005204: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.005233: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.005292: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.005322: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.160657: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.160745: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.160787: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.160862: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.160886: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.160959: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.161019: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.161058: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.161081: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.161108: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.161236: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.161306: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.161352: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.161394: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.161416: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.161439: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.161504: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.161650: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.315489: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.315528: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.315676: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.316045: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.316060: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.316151: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.316276: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.316282: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.467841: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.467882: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.467911: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.468036: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.468267: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.468388: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.468427: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.468476: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.468526: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.468723: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.468737: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.510596: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    1/1 [==============================] - 9s 9s/step - loss: inf - accuracy: 0.0000e+00 - val_loss: inf - val_accuracy: 0.0000e+00    Epoch 2/100    2021-10-14 05:22:44.622235: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.622277: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.622401: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:44.622617: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    1/1 [==============================] - ETA: 0s - loss: inf - accuracy: 0.0000e+002021-10-14 05:22:45.075456: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.075495: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.075544: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.075600: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.075660: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.075716: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.075740: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.075760: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.075806: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.075877: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.075995: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.076016: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.076178: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.076210: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.234284: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.234392: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.234438: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.234483: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.234530: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.234544: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.234596: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.234639: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.234768: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.234837: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.234882: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.234913: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.234959: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.234991: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.235021: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.235153: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.235235: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.235299: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.391722: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.391763: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.391838: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.391867: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.391914: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.392013: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.392028: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.392276: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.545771: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.545837: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.545860: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.546005: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.546136: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.546205: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.546389: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.546450: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.546475: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.546489: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.546529: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.587576: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    1/1 [==============================] - 1s 1s/step - loss: inf - accuracy: 0.0000e+00 - val_loss: inf - val_accuracy: 0.0000e+00    Epoch 3/100    2021-10-14 05:22:45.698230: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.698437: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.698533: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:45.698647: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    1/1 [==============================] - ETA: 0s - loss: inf - accuracy: 0.0000e+002021-10-14 05:22:46.127968: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.128016: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.128060: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.128121: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.128177: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.128236: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.128244: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.128368: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.128391: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.128524: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.128631: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.128675: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.128696: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.128926: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.290187: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.290274: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.290309: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.290390: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.290411: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.290485: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.290545: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.290574: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.290605: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.290663: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.290756: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.290822: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.290884: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.290895: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.290993: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.291033: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.291057: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.291095: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.448088: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.448374: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.448476: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.448486: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.448636: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.448658: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.448730: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.448954: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.604924: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.604977: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.604992: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.605256: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.605280: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.605321: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.605429: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.605523: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.605583: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.605591: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.605646: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    2021-10-14 05:22:46.646805: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.    1/1 [==============================] - 1s 1s/step - loss: inf - accuracy: 0.0000e+00 - val_loss: inf - val_accuracy: 0.0000e+00    Restoring model weights from the end of the best epoch.    Epoch 00003: early stopping
"
52368,1,5243,20,0,0,SiLiKhon,0,"title:keras.models.load_model resets the optimizer's state description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `yes, mostly based on the example from https://www.tensorflow.org/guide/keras/save_and_serialize`- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): google colab (`Linux 59a52e5448f6 5.4.104+ #1 SMP Sat Jun 5 09:50:34 PDT 2021 x86_64 x86_64 x86_64 GNU/Linux`)- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: `no`- TensorFlow installed from (source or binary): `google colab version`- TensorFlow version (use command below): `v2.6.0-0-g919f693420e 2.6.0`- Python version: `3.7.12 (default, Sep 10 2021, 00:21:48)  [GCC 7.5.0]`- Bazel version (if compiling from source): `no`- GCC/Compiler version (if compiling from source): `no`- CUDA/cuDNN version: `11.2`- GPU model and memory: `Tesla K80, 11441MiB`You can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**When restoring a keras model with `keras.models.load_model`, the returned model's optimizer is in the reset state (e.g. its `weights` attribute is empty).**Describe the expected behavior**The original call:```pythonreconstructed_model = tf.keras.models.load_model(""my_model"")```should have restored and kept the optimizer's weights.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): `no`- Briefly describe your candidate solution(if contributing): `-`**Standalone code to reproduce the issue**```pythonimport tensorflow as tfimport numpy as npdef get_model():    # Create a simple model.    inputs = tf.keras.Input(shape=(32,))    outputs = tf.keras.layers.Dense(1)(inputs)    model = tf.keras.Model(inputs, outputs)    model.compile(optimizer=""adam"", loss=""mean_squared_error"")    return modelmodel = get_model()# Train the model.test_input = np.random.random((128, 32))test_target = np.random.random((128, 1))model.fit(test_input, test_target)# Calling `save('my_model')` creates a SavedModel folder `my_model`.model.save(""my_model"")# It can be used to reconstruct the model identically.reconstructed_model = tf.keras.models.load_model(""my_model"")print(reconstructed_model.optimizer.weights)```output:> 4/4 [==============================] - 1s 4ms/step - loss: 0.1829INFO:tensorflow:Assets written to: my_model/assets[]If we additionally provide a `compile=False` argument, the optimizer's weights are restored:```pythonreconstructed_model = tf.keras.models.load_model(""my_model"", compile=False)for w in reconstructed_model.optimizer.weights:    print(w.shape)```output:>(32, 1)(1,)(32, 1)(1,)However, trying to use the restored optimizer fails with an exception:```pythonreconstructed_model.compile(reconstructed_model.optimizer, loss=""mean_squared_error"")reconstructed_model.fit(test_input, test_target)```output:```---------------------------------------------------------------------------NotImplementedError                       Traceback (most recent call last)<ipython-input-3-22a4ff24818b> in <module>()      1 reconstructed_model.compile(reconstructed_model.optimizer, loss=""mean_squared_error"")----> 2 reconstructed_model.fit(test_input, test_target)9 frames/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)    992           except Exception as e:  # pylint:disable=broad-except    993             if hasattr(e, ""ag_error_metadata""):--> 994               raise e.ag_error_metadata.to_exception(e)    995             else:    996               raiseNotImplementedError: in user code:    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:853 train_function  *        return step_function(self, iterator)    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:842 step_function  **        outputs = model.distribute_strategy.run(run_step, args=(data,))    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1286 run        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica        return self._call_for_each_replica(fn, args, kwargs)    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica        return fn(*args, **kwargs)    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:835 run_step  **        outputs = model.train_step(data)    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:791 train_step        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)    /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:522 minimize        return self.apply_gradients(grads_and_vars, name=name)    /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:660 apply_gradients        apply_state)    /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:707 _distributed_apply        var, apply_grad_to_update_var, args=(grad,), group=False)    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2595 update        var, fn, args=args, kwargs=kwargs, group=group)    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2473 _replica_ctx_update        return replica_context.merge_call(merge_fn, args=args, kwargs=kwargs)    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3064 merge_call        return self._merge_call(merge_fn, args, kwargs)    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3071 _merge_call        return merge_fn(self._strategy, *args, **kwargs)    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2471 merge_fn  **        return self.update(var, fn, merged_args, merged_kwargs, group=group)    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2592 update        return self._update(var, fn, args, kwargs, group)    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3646 _update        return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3652 _update_non_slot        result = fn(*args, **kwargs)    /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:689 apply_grad_to_update_var  **        update_op = self._resource_apply_dense(grad, var, **apply_kwargs)    /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:1241 _resource_apply_dense        raise NotImplementedError(""Must be implemented in subclasses."")    NotImplementedError: Must be implemented in subclasses.```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
52366,1,0,0,1,0,maryamxasghari,0,"title:tf failed to allocate memory on GPU  description:I have a similar problem. I tried batch size 64 or 32,16,8 all failed. I also downgrade to tensorflow and tensorflow-gpu to 2.5 and still getting the same error. tensorflow.python.framework.errors_impl.ResourceExhaustedError: failed to allocate memory [Op:Mul]Here is the link to the error: https://pastebin.com/raw/TMSCH9VaI ran the same code yesterday and it was fine with batch size 64. But I needed to stop to change something and after that I keep getting this error. <img width=""731"" alt=""image"" src=""https://user-images.githubusercontent.com/74671619/137164238-6f39cb77-740d-40ff-9040-31eaed1307de.png"">There is no running process and still Sum Total of in-use chunks: 7.17GiB2021-10-13 11:14:03.092854: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] total_region_allocated_bytes_: 7773224960 memory_limit_: 7773224960 available bytes: 0 curr_region_allocation_bytes_: 155464499202021-10-13 11:14:03.092863: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Stats:Limit:                      7773224960InUse:                      7702514176MaxInUse:                   7702514688NumAllocs:                        2285MaxAllocSize:               1761195520Reserved:                            0PeakReserved:                        0LargestFreeBlock:                    02021-10-13 11:14:03.092920: W tensorflow/core/common_runtime/bfc_allocator.cc:467] ***************************************************************************************************x$pip freezeabsl-py==0.14.1alembic==1.4.3argon2-cffi==20.1.0asn1crypto==0.24.0astunparse==1.6.3async-generator==1.10attrs==20.3.0backcall==0.2.0backports.entry-points-selectable==1.1.0bleach==3.2.1cached-property==1.5.2cachetools==4.2.4certifi==2020.12.5certipy==0.1.3cffi==1.12.3chardet==3.0.4clang==5.0conda==4.8.1conda-package-handling==1.3.11cryptography==2.7cycler==0.10.0decorator==4.4.2defusedxml==0.6.0distlib==0.3.3entrypoints==0.3filelock==3.3.0flatbuffers==1.12future==0.18.2gast==0.4.0google-auth==1.35.0google-auth-oauthlib==0.4.6google-pasta==0.2.0grpcio==1.34.1h5py==3.1.0idna==2.8importlib-metadata==3.4.0ipykernel==5.4.3ipython==7.19.0ipython-genutils==0.2.0ipywidgets==7.5.1jedi==0.18.0Jinja2==2.11.2joblib==1.1.0json5==0.9.5jsonschema==3.2.0jupyter-client==6.1.11jupyter-core==4.7.0jupyter-telemetry==0.1.0jupyterhub==1.2.2jupyterlab==2.2.9jupyterlab-pygments==0.1.2jupyterlab-server==1.2.0keras==2.6.0keras-nightly==2.5.0.dev2021032900Keras-Preprocessing==1.1.2kiwisolver==1.3.2libarchive-c==2.8libclang==11.1.0llvmlite==0.37.0Mako==1.1.4Markdown==3.3.4MarkupSafe==1.1.1matplotlib==3.4.3mistune==0.8.4mtcnn==0.1.1nbclient==0.5.1nbconvert==6.0.7nbformat==5.1.1nbgitpuller==0.9.0nbresuse==0.3.6nest-asyncio==1.4.3notebook==6.1.5nteract-on-jupyter==2.1.3numba==0.54.1numpy==1.19.5oauthlib==3.1.0opencv-python==4.5.3.56opt-einsum==3.3.0packaging==20.8pamela==1.0.0pandas==1.3.3pandocfilters==1.4.3parso==0.8.1pexpect==4.8.0pickleshare==0.7.5Pillow==8.3.2platformdirs==2.4.0prometheus-client==0.9.0prompt-toolkit==3.0.10protobuf==3.18.1psutil==5.8.0ptyprocess==0.7.0pyasn1==0.4.8pyasn1-modules==0.2.8pycosat==0.6.3pycparser==2.19Pygments==2.7.4pyOpenSSL==19.0.0pyparsing==2.4.7pyrsistent==0.17.3PySocks==1.7.0python-dateutil==2.8.1python-editor==1.0.4python-json-logger==2.0.1pytz==2021.3pyzmq==21.0.0requests==2.22.0requests-oauthlib==1.3.0rsa==4.7.2ruamel.yaml.clib==0.2.2ruamel_yaml==0.15.46scikit-learn==1.0Send2Trash==1.5.0six==1.15.0SQLAlchemy==1.3.22tb-nightly==2.7.0a20211010tensorboard==2.6.0tensorboard-data-server==0.6.1tensorboard-plugin-wit==1.8.0tensorflow==2.5.0tensorflow-estimator==2.5.0tensorflow-gpu==2.5.0tensorflow-io-gcs-filesystem==0.21.0termcolor==1.1.0terminado==0.9.2testpath==0.4.4threadpoolctl==3.0.0torch==1.6.0tornado==5.1.1tqdm==4.32.1traitlets==5.0.5typing-extensions==3.7.4.3urllib3==1.24.2virtualenv==20.8.1virtualenv-clone==0.5.7wcwidth==0.2.5webencodings==0.5.1Werkzeug==2.0.2widgetsnbextension==3.5.1wrapt==1.12.1zipp==3.4.0_Originally posted by @maryamxasghari in https://github.com/tensorflow/tensorflow/issues/16768#issuecomment-942425438_
"
52365,1,1247,96,0,0,napsta32,0,"title:'tf.Selu' op is neither a custom op nor a flex op description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):- OS: Windows 10- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0- Python version: 3.8- CUDA/cuDNN version: 11.4- GPU model and memory: GTX 1060**Describe the current behavior**Error when quantizing a model with selu activation.**Describe the expected behavior**Quantize selu activation.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no (sorry)**Standalone code to reproduce the issue**```import tensorflow as tfimport pathlibdef get_model():    input_layer = tf.keras.Input(shape=[128], dtype=tf.float32)    dense = tf.keras.layers.Dense(128, kernel_initializer='lecun_normal', activation='selu')(input_layer)    return tf.keras.models.Model(inputs=input_layer, outputs=dense)def quantize(model):    converter = tf.lite.TFLiteConverter.from_keras_model(model)    converter.optimizations = [tf.lite.Optimize.DEFAULT]    converter.target_spec.supported_types = [tf.float16]    tflite_model = converter.convert()    tflite_models_dir = pathlib.Path('.')    tflite_models_dir.mkdir(exist_ok=True, parents=True)    tflite_model_file = tflite_models_dir/'unquantizable.tflite'    tflite_model_file.write_bytes(tflite_model)    passif __name__ == '__main__':    model = get_model()    quantize(model)    pass```**Other info / logs**```error: 'tf.Selu' op is neither a custom op nor a flex operror: failed while converting: 'main':Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_selectTF Select ops: SeluDetails:        tf.Selu(tensor<?x128xf32>) -> (tensor<?x128xf32>) : {device = """"}```
"
52364,1,0,0,0,0,Jon573,0,"title:Performance of a model loading the same weights changes depending on the version of tensorflow  description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu20.04 LTS- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.4.3 & 2.6.0- Python version: 3.6.13**Describe the current behavior**I received weights of 3 keras models (ResNet50V2, EfficientNetB0, and VGG16) that had been trained with a version of tensorflow 2.x (I don't know and can't find out). I do know the weights were created with the keras model checkpoint callback. I created a new venv and pip installed version 2.6.0 and loaded the weights and tested out the performance on the task. It was completely different to expected performance and terrible. It occurred for all three networks. I spent a while trying to trouble shoot and eventually thought I would try with an older version (2.4.3), and performance was as expected.**Describe the expected behavior**Performance should not be different for models loading the same weights under 2.4.3 and 2.6.0.
"
52346,0,189,5,0,0,RomanSteinberg,0,"title:Restore from checkpoint loads optimizer incorrectly description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0- Python version: 3.8- Bazel version (if compiling from source): -- GCC/Compiler version (if compiling from source): -- CUDA/cuDNN version: 11.4- GPU model and memory: 1080TI 11Gb**Describe the current behavior**1. After checkpoint restoration optimizer weights are different from optimizer weights before saving checkpoint.2. As `assert_consumed` notifies checkpoint file has unresolved optimizer slots (variables). ```Unresolved object in checkpoint (root).optimizer.iter: attributes {  name: ""VARIABLE_VALUE""  full_name: ""Adam/iter""  checkpoint_key: ""optimizer/iter/.ATTRIBUTES/VARIABLE_VALUE""}```**Describe the expected behavior**1. The optimizer weights should be the same before save and after load.2. The `assert_consumed` should not warn about anything.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing): - **Standalone code to reproduce the issue**The reproducible code example is presented in [colab](https://colab.research.google.com/drive/1R01obneq7_jSfBRdUabTAMQIYxuzShqe?usp=sharing).**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
52344,1,699,4,0,0,anargyri,0,"title:Migration script inserts loss_reduction argument description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): using code from OSS repo mentioned below- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow installed from (source or binary): PyPI binary- TensorFlow version (use command below): 1.15.3- Python version: 3.7.11- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source): N/A- CUDA/cuDNN version: 10.0.130 / 7.6.5- GPU model and memory: Tesla T4 16GBYou can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**After applying the [migration script](https://www.tensorflow.org/guide/migrate/upgrade) `tf_upgrade_v2 ` to [the recommenders repo](https://github.com/microsoft/recommenders/tree/main/recommenders), tests on [this code](https://github.com/microsoft/recommenders/blob/main/recommenders/models/wide_deep/wide_deep_utils.py) fail with the error```E       AttributeError: module 'tensorflow.python.keras.api._v1.keras.losses' has no attribute 'Reduction'```The cause of the error is that the script has inserted an additional argument `loss_reduction=tf.keras.losses.Reduction.SUM`in lines https://github.com/microsoft/recommenders/blob/27709229cdc4aa7d39ab715789f093a2d21d2661/recommenders/models/wide_deep/wide_deep_utils.py#L176https://github.com/microsoft/recommenders/blob/27709229cdc4aa7d39ab715789f093a2d21d2661/recommenders/models/wide_deep/wide_deep_utils.py#L186https://github.com/microsoft/recommenders/blob/27709229cdc4aa7d39ab715789f093a2d21d2661/recommenders/models/wide_deep/wide_deep_utils.py#L200**Describe the expected behavior**It seems that the migration script should not insert the `loss_reduction` argument in the code. Because after removing this argument the tests pass successfully. **[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing):N/A**Standalone code to reproduce the issue**From a conda environment with `python=3.7 cudatoolkit=10.0 ""cudnn>=7"" tensorflow==1.15` do```git clone https://github.com/microsoft/recommenders.gitcd recommenderstf_upgrade_v2 --intree recommenders --outtree recommenders_v2/recommenders --reportfile recommenders_report.txttf_upgrade_v2 --intree tests --outtree recommenders_v2/tests --reportfile tests_report.txtmv recommenders_v2/recommenders recommendersmv recommenders_v2/tests tests```Deactivate this env and then do```conda create -n tf1_15 python=3.7 cudatoolkit=10.0 ""cudnn>=7""conda activate tf1_15pip install --upgrade pip setuptoolspip install .[gpu,dev]pytest tests/unit/recommenders/models/test_wide_deep_utils.py::test_wide_model```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.[log.txt](https://github.com/tensorflow/tensorflow/files/7329622/log.txt)
"
52335,1,455,25,0,0,AngCamp,0,"title:tensoflow.keras.preprocessing.image_dataset_from_directory doesn't recognize there are files in a directory when labels are supplied as a list/tupple description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>https://keras.io/api/preprocessing/image/#imagedatasetfromdirectory-functionMy code:```img_width = 224img_hieght = 224batch_size = 100train_labels = train_metadat_df.sort_values(by='filename')['sirna'].tolist()train_ds = tf.keras.preprocessing.image_dataset_from_directory(  ""../input/recursion-cellular-image-classification-224-jpg/train/train"",  validation_split=0.2,  subset=""training"",  labels= train_labels,  label_mode= ""categorical"",  seed=123,  image_size=(img_height, img_width),  batch_size=batch_size)```This function normally takes a pointer for a directory with images organized by lab into directories ( i.e. all flowers in a file called flowers, all animals in a file called animals) and then outputs a dataset object.  But there is also an option to provide a list of When providing a list of file labels instead but when this happens the following error is produced:`ValueError: Expected the lengths of `labels` to match the number of files in the target directory. len(labels) is 73030 while we found 0 files in ../input/recursion-cellular-image-classification-224-jpg/train/train.`There are ~73000 .jpeg files in that directory and when I run it on ../input/recursion-cellular-image-classification-224-jpg/train it finds ~54000 of them and trains them all on one class (because they are all stored in one file).**System information**- using Tensorflow in a kaggle notebook- currently no GPU is turned on- tensorflow version 2.4.1You can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior****Describe the expected behavior****[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
52329,1,4703,25,0,0,Koratun,0,"title:Invalid argument error description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10- TensorFlow installed from (source or binary):pip install tensorflow- TensorFlow version (use command below):2.6.0- Python version:3.7- CUDA/cuDNN version:11.3- GPU model and memory:NVIDIA 2060 SUPER, compute capability: 7.56010MBI am trying to create a model that takes two images (one taken right after the other) and train it so that it can predict how much movement has occurred. I use a smaller model that processes one image at a time, then concatenate the two outputs in a larger model.I tried testing it and the model compiles just fine, but it crashes when I call fit() and gives me an invalid argument error.```2021-10-11 10:46:28.321065: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)2021-10-11 10:46:29.319124: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at transpose_op.cc:143 : Invalid argument: transpose expects a vector of size 5. But input(1) is a vTraceback (most recent call last):2021-10-11 10:46:29.319416: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at transpose_op.cc:143 : Invalid argument: transpose expects a vector of size 5. But input(1) is a vTraceback (most recent call last):  File ""d:/.../Deep Sight/deep_sight.py"", line 155, in <module>    main()  File ""d:/.../Deep Sight/deep_sight.py"", line 150, in main    final_model.fit(train_data, epochs=5)  File ""C:\Users\...\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\engine\training.py"", line 1184, in fit    tmp_logs = self.train_function(iterator)  File ""C:\Users\...\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\eager\def_function.py"", line 885, in __call__    result = self._call(*args, **kwds)  File ""C:\Users\...\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\eager\def_function.py"", line 950, in _call    return self._stateless_fn(*args, **kwds)  File ""C:\Users\...\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\eager\function.py"", line 3040, in __call__    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access  File ""C:\Users\...\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\eager\function.py"", line 1964, in _call_flat    ctx, args, cancellation_manager=cancellation_manager))  File ""C:\Users\...\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\eager\function.py"", line 596, in call    ctx=ctx)  File ""C:\Users\...\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\eager\execute.py"", line 60, in quick_execute    inputs, attrs, num_outputs)tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.  (0) Invalid argument:  transpose expects a vector of size 5. But input(1) is a vector of size 4         [[{{node gradient_tape/model_1/model/conv2d/Conv2D/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]         [[Func/mean_squared_error/cond/then/_0/input/_29/_48]]  (1) Invalid argument:  transpose expects a vector of size 5. But input(1) is a vector of size 4         [[{{node gradient_tape/model_1/model/conv2d/Conv2D/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]0 successful operations.0 derived errors ignored. [Op:__inference_train_function_3095]Function call stack:train_function -> train_function```**Standalone code to reproduce the issue**My batch size is 32 as defined in my dataset. I believe it has something to do with the squeeze method because in an earlier version of this model I did not use the squeeze or split methods and it ran perfectly fine. However, because my dataset is much larger now, I shifted to using tf.Datasets to input the data. This required me to input the 2 images together in a single tensor as the input.The shape of my dataset is: (batch, features, labels) with the label being a scalar, and each feature being of shape: (2, 256, 256, 3)```pythondef main():    # Create model    # Start with smaller model that processes the two images in the same way.    single_image_input = keras.Input(shape=(256,256,3))    image = layers.Conv2D(64, (3,3))(single_image_input)    image = layers.LeakyReLU()(image)    image = layers.BatchNormalization()(image)    # Run through MaxPool2D to help the algorithm identify features in different areas of the image.    # Has the effect of downsampling and cutting the dimensions in half.    image = layers.MaxPool2D()(image)    image = layers.Conv2D(128, (3, 3))(image)    image = layers.LeakyReLU()(image)    image = layers.BatchNormalization()(image)    image = layers.Dropout(.3)(image)    image_model = keras.Model(single_image_input, image)        # Create larger model    image_inputs = keras.Input(shape=(2,256,256,3))    first_image, second_image = tf.split(image_inputs, num_or_size_splits=2, axis=0)    first_image, second_image = tf.squeeze(first_image), tf.squeeze(second_image)    image_outputs = [image_model(first_image), image_model(second_image)]    model = layers.Concatenate()(image_outputs)    model = layers.Flatten()(model)    model = layers.Dense(128)(model)    model = layers.LeakyReLU()(model)    model = layers.BatchNormalization()(model)    model = layers.Dropout(.3)(model)    # Output is change in y-position of drone    out_layer = layers.Dense(1, activation='linear')(model)    final_model = keras.Model(image_inputs, out_layer)    final_model.compile(loss=""mse"", optimizer=optimizers.Adam(lr=0.0003, beta_1=0.7))    image_model.summary()    final_model.summary()    #Preprocess data    print(""Loading and processing data..."")    train_data = tf_load_data()    #Train model    final_model.fit(train_data, epochs=5)```
"
52325,1,0,0,1,0,jeisinge,0,"title:keras.layers.IntegerLookup fails to deserialize vocubulary description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.6- Python version: 3.7**Describe the current behavior**When creating a keras.layers.IntegerLookup, we provide a vocabulary.  This is saved off via serialization.  However, upon deserialization, the vocabulary is not loaded correctly --- the layer continues to work, however, one cannot serialize again.**Describe the expected behavior**We should be able to serialize and deserialize IntegerLookups any number of times.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.To reproduce, see https://colab.research.google.com/drive/1tpXdEsfYKyax5QhYLR7KAV4P2vQHIT8D?usp=sharing .  It is interesting to note that I cannot reproduce by just serializing to JSON --- I have to serialize to SavedModel.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.From looking at the code, https://github.com/keras-team/keras/blob/v2.6.0/keras/layers/preprocessing/index_lookup.py#L200 , it appears that we are setting `vocabulary`, but not `input_vocabulary`.  Since `vocabulary` is set, the layer works fine.  But, upon the next serialization, we do serialize an empty vocab: https://github.com/keras-team/keras/blob/v2.6.0/keras/layers/preprocessing/index_lookup.py#L333 .It is not clear to me the difference between `input_vocabulary` and `vocabulary`.This is also related to https://github.com/tensorflow/tensorflow/issues/43834
"
52322,1,0,0,0,1,spaghettix,0,"title:tensorflow_addons.losses.metric_learning.pairwise_distance. Cannot convert a symbolic Tensor (2nd_target:0) to a numpy array description:Whent I try to use the pairwise_distance function is the the following error ""Cannot convert a symbolic Tensor (2nd_target:0) to a numpy array"".I found that is because **line 67 in the pairwise_distance function** is written as:**tf.ones([num_data])**Instead of: **tf.ones((num_data))**Please check if this is correct and fix it.The function works perfectly after I changed that line of code.
"
52318,0,1397,53,0,0,bkgoodman,0,"title:tf.lite.Interpreter set_tensor failing to properly recognize uint8 input tensors description:**System information**- Ubuntu 20.0.04- Intel Atom- Binary installation installed from build without SSE3 instructions- TensorFlow 2.3.0- Python 3.6- No CUDA/GPU/TPU**Describe the current behavior**I have a working `.tflite` model (which takes 180x180 float greyscale image) as input, and returns 6 float sigmoid outputs. All works as-expected yielding expected results with test images.I am trying to quantize `.tflite` model to `uint8`. Notice I am setting the input and output types to `unit8` (edited for brevity):```model = tf.keras.models.load_model(""bkgmodel.h5"")converter = tf.lite.TFLiteConverter.from_keras_model(model)converter.optimizations = [tf.lite.Optimize.DEFAULT]converter.representative_dataset = representative_dataset_genconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]converter.inference_input_type = tf.uint8converter.inference_output_type = tf.uint8```When I try to run inference on it with the following code (brevity):``` img_array= np.array(np.expand_dims(img_array/1.0,0), dtype=np.uint8)  print (""IMAGE_ARRAY type"",img_array.dtype)  interpreter = tf.lite.Interpreter(model_path=""bkgmodel_quant.tflite"")  interpreter.resize_tensor_input(0, [1, 180, 180, 1])  interpreter.allocate_tensors()  print (""INPUT TENSOR"",interpreter.get_input_details())  input = interpreter.tensor(interpreter.get_input_details()[0][""index""])  output = interpreter.tensor(interpreter.get_output_details()[0][""index""])  interpreter.set_tensor(0, img_array)```When I run I get the following error:`ValueError: Cannot set tensor: Got value of type UINT8 but expected type INT8 for input 0, name: input_2_int8`When I look at the types of the image data and input tensors, they inded _are_ `uint8`:```IMAGE_ARRAY type uint8INPUT TENSOR [{'name': 'input_2', 'index': 22, 'shape': array([  1, 180, 180,   1], dtype=int32), 'shape_signature': array([ -1, 180, 180,   1], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (1.0, 0), 'quantization_parameters': {'scales': array([1.], dtype=float32), 'zero_points': array([0], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]```(look only at `dtype` in the input tensor, I assume).So - indeed, the input tensor _should_ be a uint8, and the image _is_ a uint8 - yet I get this error.As an experiment - I tried changing _only_ my inference code to set the image data to `int8`. i.e:```img_array= np.array(np.expand_dims(img_array/1.0,0), dtype=np.int8)```When I do, the error goes away and the inference runs (as expected) - no error, *however* my models predictions are all wrong.(It is notable that the output values all add up to _almost_ 256. I am assuming this means that the model is working correctly and yielding valid data - and I am also assuming that 8-bit sigmoids are expected to have a bit of a roundoff error where they don't add up to exactly 256??)If I am doing something wrong, or is this a bug in Tensorflow-lite??**Describe the expected behavior**I would expect that when I leave it as:`img_array= np.array(np.expand_dims(img_array/1.0,0), dtype=np.uint8)`...that it runs with no error, and yields expected inference results against these known files  - which give expected results running all the same code (except changing the `uint8` and `int8`s above to floats) with a non-quantitized, float `.tflite` model.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**See `liteinfer.py` and `convert.py` (quantize) at:https://github.com/bkgoodman/espcam_training_tools
"
52294,1,0,5,0,0,fernandobperezm,0,"title:Using the same seed kwarg returns different values between GlorotUniform and GlorotUniformV2 description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.5 LTS- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary): Pip (binary)- TensorFlow version (use command below): v2.6.0-0-g919f693420e 2.6.0- Python version: 3.7.12- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version: NA- GPU model and memory: NAYou can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**Setting the `operation` seed returns different tensors when using GlorotUniform when imported from `tf.compat.v1` and `tensorflow.keras.initializers`**Describe the expected behavior**Both tensors must be equal**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.[Colab link](https://colab.research.google.com/drive/13V8v7a7fCQQMroaLz1PGvgrQ5Wf81IRp?usp=sharing)In the notebook, I share reproducible code about the current behavior and the expected behavior, as well as *why* this might be happening. **Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
52288,1,0,7,1,0,jvcrnc,0,"title:save_model() still fails with custom layer and SavedModel format description:Basically same issue that was closed before:https://github.com/tensorflow/tensorflow/issues/40912With TF 2.6 I'm experiencing exactly the same problem.Also with tf-nightly the issue is still there.
"
52281,1,0,22,0,0,ymzlygw,0,"title:RuntimeError: Encountered unresolved custom op: ReorderAxes. See instructions: https://www.tensorflow.org/lite/guide/ops_customNode number 284 (ReorderAxes) failed to prepare. description:**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab- TensorFlow installed from (source or binary): Colab - TensorFlow version (or github SHA if from source): convert model in TF1.x / interpreter in  latest tf-nightly.I convert a model to tflite using following codes:    co = tf.compat.v1.lite.TFLiteConverter.from_saved_model(modelName)    co.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]    co.allow_custom_ops=TrueThere are no error in converting.But when I try to interpreter the tfliter model, I encoder error:--> 916     self._interpreter.Invoke()    917     918   def reset_all_variables(self):**RuntimeError: Encountered unresolved custom op: ReorderAxes.**See instructions: https://www.tensorflow.org/lite/guide/ops_customNode number 284 (ReorderAxes) failed to prepare.Could you please tell me how to solve this?
"
52263,1,853,14,0,0,niciBume,0,"title:Pruned TFLite model has invalid model identifier after post training quantisation description:### 1. System information- Linux Ubuntu 20.04- Pip TensorFlow package version 2.6.0### 2. CodeReproducable code in this [gist](https://colab.research.google.com/gist/niciBume/2fb8d9eb36d5e13d8faaea9e1273e980/tensorflow-lite-debugger-colab.ipynb#scrollTo=RD0CEfccGN2C)### 3. DescriptionAfter pruning a keras model according to [tf colab](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras), the post training quantized tflite can't be invoked (the quantisation works for the un-pruned model).`interpreter` = tf.lite.Interpreter(model_path=MODEL_TF_PRUNE)`Leads to the error:```ValueError                                Traceback (most recent call last)<ipython-input-56-edee2625144a> in <module>()----> 1 interpreter = tf.lite.Interpreter(model_path=MODEL_TF_PRUNE)/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/interpreter.py in __init__(self, model_path, model_content, experimental_delegates, num_threads, experimental_op_resolver_type, experimental_preserve_all_tensors)    365           _interpreter_wrapper.CreateWrapperFromFile(    366               model_path, op_resolver_id, custom_op_registerers_by_name,--> 367               custom_op_registerers_by_func, experimental_preserve_all_tensors))    368       if not self._interpreter:    369         raise ValueError('Failed to open {}'.format(model_path))ValueError: Model provided has model identifier '', should be 'TFL3'```
"
52255,0,1693,0,0,0,stist1111,0,"title:Model Maker Object Detection Tutorial Bug description:I ran the Model Maker Object Detection Tutorial via Colab.(https://colab.research.google.com/drive/1DhxMGuQ9ep9mrfDBrFBx47zmOeEOn9_W#scrollTo=qhl8lqVamEty)However, a problem occurred in `model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data)`.```Epoch 1/50---------------------------------------------------------------------------UnknownError                              Traceback (most recent call last)<ipython-input-5-187f39c1697e> in <module>()----> 1 model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data)9 frames/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)     58     ctx.ensure_initialized()     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,---> 60                                         inputs, attrs, num_outputs)     61   except core._NotOkStatusException as e:     62     if name is not None:UnknownError: 2 root error(s) found.  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.	 [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/efficientnet-lite0/StatefulPartitionedCall/stem/conv2d/Conv2D}}]]  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.	 [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/efficientnet-lite0/StatefulPartitionedCall/stem/conv2d/Conv2D}}]]	 [[Func/cond/then/_3378/input/_6828/_56]]0 successful operations.0 derived errors ignored. [Op:__inference_train_function_96849]Function call stack:train_function -> train_function```Please solve this problem.
"
52236,1,2034,7,0,0,WingsOfPanda,0,"title:mixed_precision returns gradient zeros when the model input size is large description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Noe- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0- Python version: 3.9- CUDA/cuDNN version: 11.1/8.1.1- GPU model and memory: RTX 3090/24GB**Describe the current behavior**When using mixed_precision policy described in [https://www.tensorflow.org/guide/mixed_precision](url) with large model input size, for example `(256, 368, 368,)`, the returned gradient are constantly ZEROS. However, if remove the mixed_precision policy, the returned gradient is normal with non-zeros numbers.Moreover, if we use small model input size, let's say `(16, 16, 16)`, the returned gradient is normal no matter the mixed_precision is allowed or not. My model is a typical U-net like model.**Describe the expected behavior**With large model input size like `(256, 368, 368)` used above, the returned gradient should be at least non-zeros. Otherwise the model won't be trained. **Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.The code below set the input shape to `[1, 256, 368, 368, 1]` and allows `mixed_precision.Policy('mixed_float16')`. It will return zeros gradients in the end (hence, no training at all). Setting `tf16_flag=False` will returns normal gradient behavior.Also, by change `shape = [1, 16, 16, 16, 1]`, the gradient behaves normally no matter allows `mixed_precision.Policy('mixed_float16')` or not```import tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras import layersfrom tensorflow.keras import mixed_precisionimport numpy as npfrom tqdm import tqdmgpus = tf.config.experimental.list_physical_devices('GPU')for gpu in gpus:    tf.config.experimental.set_memory_growth(gpu, True)tf16_flag = Trueif tf16_flag:    policy = mixed_precision.Policy('mixed_float16')    mixed_precision.set_global_policy(policy)shape = [1, 256, 368, 368, 1]# shape = [1, 16, 16, 16, 1]def forward_conv(x, filters, kernels, name='forward', padding='same'):    i = 0    for flt, kernel in zip(filters, kernels):        x = layers.Conv3D(flt, kernel, activation='relu', padding=padding, dilation_rate=(1, 1, 1),                          use_bias=False, name=str(i) + '_' + name)(x)        x = layers.BatchNormalization(name=str(i) + '_bn_' + name)(x)        i += 1    return xdef part_one(ipt):    l1 = forward_conv(ipt, (4, 4), (3, 3), name='enc1')    d2 = layers.MaxPool3D(pool_size=(2, 2, 2))(l1)    l2 = forward_conv(d2, (4, 4), (3, 3), name='enc2')    return l1, l2def part_inner(ipt1, ipt2):    l1 = forward_conv(ipt1, (4, 4), (3, 3), name='enc1')    l2 = forward_conv(ipt2, (4, 4), (3, 3), name='enc2')    return l1, l2def part_two(ipt1, ipt2):    l2 = forward_conv(ipt2, (4, 4), (3, 3), name='dec2')    u1 = layers.UpSampling3D(size=(2, 2, 2))(l2)    r1 = forward_conv(ipt1 + u1, (4, 4), (3, 3), name='dec1')    return r1initial = tf.ones(shape, dtype=tf.float16) if tf16_flag \    else tf.ones(shape, dtype=tf.float32)tf.random.set_seed(1)with tf.GradientTape() as g:    g.watch(initial)    l1_, l2_ = part_one(initial)    for _ in range(2):        l1_, l2_ = part_inner(l1_, l2_)    opt_ = part_two(l1_, l2_)    loss = tf.reduce_mean(l1_) + tf.reduce_mean(opt_)    gd = g.gradient(loss, initial)    print('-' * 100)    print(f'loss is {loss} and grad is {np.sum(gd)} with input shape {shape}')```
"
52218,1,0,1,0,0,Bhavay192,0,"title:Converted tf.gfile.GFile to tf.io.gfile.GFile  description:**System information**- OS Platform and Distribution (Google Colab):- TensorFlow version (tensorflow 2.6.0):I was working with the tensorflow's object detection api and as soon as i ran the following code:PATH_TO_LABELS = '/content/models/research/object_detection/data/mscoco_label_map.pbtxt'category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)It showed the following error.AttributeError                            Traceback (most recent call last)<ipython-input-82-651c9b9bcbff> in <module>()      1 PATH_TO_LABELS = '/content/models/research/object_detection/data/mscoco_label_map.pbtxt'----> 2 category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)2 frames/usr/local/lib/python3.7/dist-packages/object_detection/utils/label_map_util.py in load_labelmap(path)    130   Returns:    131     a StringIntLabelMapProto--> 132   """"""    133   with tf.io.gfile.GFile(path, 'r') as fid:    134     label_map_string = fid.read()AttributeError: module 'tensorflow' has no attribute 'gfile'I tried to change the load_labelmap file too but it showed no result. Viewing this on stack overflow suggested that I should downgrade my tensorflow version to 1.x but for the particular program, I need to have the tensorflow version above 2.5What can be the possible solution to this problem.
"
52216,1,232,0,0,0,JTWang2000,0,"title:Mac m1 tf2.5.0: No layer for IntegerLookup, Normalization, StringLookup description:**System information**- OS Platform and Distribution: macOS Big Sur Version 11.3.1 with mac m1 chip- TensorFlow installed from: https://developer.apple.com/metal/tensorflow-plugin/- TensorFlow version (use command below): tf 2,5- Python version: 3.8Debug output:```import tensorflow as tfprint(tf.version.GIT_VERSION, tf.version.VERSION)```unknown 2.5.0**Describe the current behavior**```from tensorflow.keras.layers import IntegerLookupfrom tensorflow.keras.layers import Normalizationfrom tensorflow.keras.layers import StringLookup```ImportError                               Traceback (most recent call last)/var/folders/1z/77jbzk11477gc69_s8n2y0r00000gn/T/ipykernel_27569/1759035919.py in <module>----> 1 from tensorflow.keras.layers import IntegerLookupImportError: cannot import name 'IntegerLookup' from 'tensorflow.keras.layers' (/Users/username/miniforge3/envs/env/lib/python3.8/site-packages/tensorflow/keras/layers/__init__.py)**Standalone code to reproduce the issue**Follow link https://developer.apple.com/metal/tensorflow-plugin/ to install tf2.5 on mac and then run the import
"
52215,1,0,1,0,0,MattWolf74,0,"title:Creating dataset from large numpy arrays via from_tensor_slices crashes without any error message or warning description:I use tf 2.6 and when I try to create datasets from larger numpy arrays (>10GB) via from_tensor_slices the code breaks when I try to train via ""fit"" or even just attempt to iterate over the dataset. The code just breaks and exists, no warning, no error message, nothing. I have not found any similar issue mentioned anywhere else. What is the actual limitation here? I have over 128GB ram and due to the code breaking already on the dataset iteration part it is surely unrelated to my GPU and its memory (24GB). The numpy arrays load without issues into memory but once the iterator causes the execution of ""from_tensor_slices"", the code breaks shortly after. What are workable solutions here? DataGenerators? Creating TFRecord files? I try to avoid going the route of TFRecords because it appears very poorly documented how to create such binary files. As no warning or error message is output there is no log to show at all. The problem can be reproduced with a large numpy array (random data works) and the following code:#obtain training data    print(""loading training data..."")    train_data = np.load(os.path.join(os.getcwd(), ""source_datasets"", f""{train_data_id}_features.npy""))    train_targets = np.load(os.path.join(os.getcwd(), ""source_datasets"", f""{train_data_id}_targets.npy""))    print(""training dataset construction..."")    train_ds = tf.data.Dataset.from_tensor_slices((train_data, train_targets))    for x,y in train_ds:        do_something
"
52210,1,1339,0,0,0,kat25230,0,"title:Monitor TPU-VM utilization description:Hi Tensorflow Team,I'm trying to use the monitoring function: [`tf.profiler.experimental.client.monitor`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/profiler/profiler_client.py#L135), but getting the following error:```return _pywrap_profiler.monitor(tensorflow.python.framework.errors_impl.UnimplementedError: unimplemented.```My setup is the following: I `ssh` into a host tpu-vm and run everything locally on the host vm.(using the following `gcloud` command: `gcloud alpha compute tpus tpu-vm  ssh my-tpu ...`)Code looks something like this:```import tensorflow as tffrom tf.python.profiler import profiler_clientresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=""local"")tf.tpu.experimental.initialize_tpu_system(resolver)strategy = tf.distribute.TPUStrategy(resolver)class Monitor(object):    def init(self, service_addr, duration_ms):        self.service_addr = service_addr        self.duration_ms = duration_ms        self._stop = True        self.client = profiler_client    def _loop(self):       while not self._stop:            time.sleep(0.5)            try:                self.client.monitor(self.service_addr, duration_ms=self.duration_ms, level=1)            except Exception as e:                print(e)                time.sleep(1)    def start(self):        if self._stop:            self._thread = threading.Thread(target=self._loop, daemon=True)            self._stop = False            self._thread.start()    def stop(self):        if not self._stop:            self._stop = True            self._thread.join()tf.profiler.experimental.server.start(8466)tpu_monitor = Monitor(""grpc://localhost:8466"", 2000)tpu_monitor.start()train_model()tpu_monitor.stop()```Could you please advice what am i doing wrong?Is monitoring not supported when running locally on the tpu-host vm?Also alternatively tried doing this: (following this [tutorial](https://cloud.google.com/tpu/docs/cloud-tpu-tools#monitor_job))`pip3 install --upgrade ""cloud-tpu-profiler>=2.3.0""`running my training scrip in one shell and in another shell running (on the host tpu-vm): `capture_tpu_profile --service_addr localhost:8466 --monitoring_level 1 --num_queries 1000`, however looking at the code for the `capture_tpu_profile` script it uses the same `profiler_client.monitor(service_addr, duration_ms, monitoring_level)` function (so probably not surprising that i get the same error)
"
52178,1,0,142,0,0,prakharverma,0,"title:Different prediction on GPU between `tf.keras.models.load_model(..)` and `tf.saved_model.load(..)` description:**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.4.1- Python version: 3.8.2- CUDA/cuDNN version: 11.0- GPU model and memory: GeForce GTX 1050, 4 GB**Describe the current behavior**I have a CNN based regression model. Surprisingly, the model is predicting different outputs when loaded with `tf.keras.models.load_model(..)` and `tf.saved_model.load(..)`. However, this only occurs when I am using a GPU and also not always but ~5% of inference times. On CPU, they both produce the same outputs always. The difference is rather small, happens after `1e-7` but still big for my use case.**Describe the expected behavior**Irrespective of the loading method and whether GPU is used for inference or not, predicted values should always be the same.**Standalone code to reproduce the issue**The link to colab: `https://colab.research.google.com/drive/1JwXNx-MbVqB7HDXF4z9lqa91oWsfvpA0?usp=sharing`Colab uses different TF and python versions but the issue still exists.**Other info / logs** `AssertionError: [[0.12652352]] and [[0.12652355]]`
"
52166,1,530,0,0,0,duckduck-sys,0,"title:Model evaluate accuracy drops following Save + Load description:After training EfficientNetB0 on a custom dataset, i want to ensure that the model i save give the same accuracy after i load it:```# Evaluate modelprint(""After training: "")model.evaluate(dataset)# Save modelmodel.save('my_model.hdf5')# Reload the saved modelnew_model = tf.keras.models.load_model('my_model.hdf5')# Evaluate againprint(""After saving and reloading: "")new_model.evaluate(dataset)```Which gives the output :```After training : 1000/1000 [==============================] - 20s 19ms/step - loss: 0.4617 - accuracy: 0.8139After saving and reloading : 1000/1000 [==============================] - 20s 18ms/step - loss: 0.5586 - accuracy: 0.7688```Batch size 32. The model got much worse after save + reloadingI tried EfficientNetB1, B2, B3 as well, all same issueThen I tried simply switching to MobileNetV3 and Xception, they both work perfectly fine! The accuracy after Load is identical to that measured before Save...Whats going on? Using tf 2.6 with python 3.8 and cudnn 8.2
"
52164,0,0,269,0,0,elfringham,0,"title:unit test failure kernels:sparse_matmul_op_test on AARCH64 description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/a- TensorFlow installed from (source or binary): source- TensorFlow version (use command below): git HEAD- Python version: 3.6.9- Bazel version (if compiling from source): 3.7.2- GCC/Compiler version (if compiling from source): 10.3.0- CUDA/cuDNN version: n/a- GPU model and memory: n/aYou can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**Test fails**Describe the expected behavior**Test passes**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.bazel test //tensorflow/core/kernels:sparse_matmul_op_test**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.[ RUN      ] SparseMatmulOpTest.BroadcastPacketTest[0.170094 0.170094 0.170094 0.170094] != [  0.170094    0.14922 -0.0823886   0.026985], differences: [         0 -0.0208738  -0.252482  -0.143109]tensorflow/core/kernels/sparse_matmul_op_test.cc:329: FailureValue of: areApprox(ref, data2, PacketSize)Actual: falseExpected: true[  FAILED  ] SparseMatmulOpTest.BroadcastPacketTest (0 ms)
"
52156,1,3020,149,0,0,dwyatte,0,"title:Keras model saved with user-defined signature works with TensorFlow Serving but not Python description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Any- TensorFlow installed from (source or binary): Binary- TensorFlow version (use command below): TensorFlow 2.6 and Nightly- Python version: Python 3.7- CUDA/cuDNN version: N/A- GPU model and memory: N/A**Describe the current behavior**When manually specifying the signature in [tf.keras.Model.save](https://www.tensorflow.org/api_docs/python/tf/keras/Model#save) a correct model that works with TensorFlow Serving is saved. However, when reloading the same model in Python, TensorFlow tries to execute the incorrect concrete function.In the example below, I create a model that accepts a `[None, 4]` input but when saving, change it to a `[None, 2]` input that I simply concatenate along `axis=1` to turn it into a `[None, 4]` input. TensorFlow Serving can run this model without problems, but reloading the model with `restored_model = tf.keras.model.load_model(...)` and calling the restored model as a function incorrectly tries to run a function that expects a `[None, 4]` input instead of my user-specified one that expects a `[None, 2]` input.**Standalone code to reproduce the issue**```pythonimport tensorflow as tfinput_name = ""abc""###################################################################################################### model that accepts [None, 4] input#####################################################################################################class MyModel(tf.keras.Model):    def __init__(self):        super().__init__()        self.x = tf.keras.layers.Dense(1)    def call(self, inputs, training=None):        return self.x(inputs[input_name])inputs = tf.data.Dataset.from_tensor_slices(({input_name: [[1, 2, 3, 4]]}, [1])).batch(1)model = MyModel()model.compile(loss=""binary_crossentropy"", optimizer=""sgd"")model.fit(inputs)model.save(""4d_model"")########################################################################################################################################################################################################### override the signature to accept [None, 2] and simply concatenate it into [None, 4] for the model#####################################################################################################@tf.function(input_signature=[tf.TensorSpec([None, 2], dtype=tf.int32, name=input_name)])def serve(x):    return model({input_name: tf.concat((x, x), axis=1)})model.save(""2d_model"", signatures={""serving_default"": serve})#####################################################################################################restored_model = tf.keras.models.load_model(""2d_model"")x = next(iter(tf.data.Dataset.from_tensor_slices({input_name: [[1, 2]]}).batch(1)))outputs = restored_model(x)  # incorrectly requires [None, 4] instead of [None, 2]``````python>>> print(restored_model.signatures[""serving_default""].structured_input_signature)((), {'abc': TensorSpec(shape=(None, 2), dtype=tf.int32, name='abc')})```**Other info / logs** Include any logs or source code that would be helpful to```ValueError: Exception encountered when calling layer ""my_model"" (type MyModel).Could not find matching concrete function to call loaded from the SavedModel. Got:  Positional arguments (2 total):    * {'abc': <tf.Tensor 'inputs:0' shape=(1, 2) dtype=int32>}    * False  Keyword arguments: {} Expected these arguments to match one of the following 4 option(s):Option 1:  Positional arguments (2 total):    * {'abc': TensorSpec(shape=(None, 4), dtype=tf.int32, name='inputs/abc')}    * False  Keyword arguments: {}Option 2:  Positional arguments (2 total):    * {'abc': TensorSpec(shape=(None, 4), dtype=tf.int32, name='inputs/abc')}    * True  Keyword arguments: {}Option 3:  Positional arguments (2 total):    * {'abc': TensorSpec(shape=(None, 4), dtype=tf.int32, name='abc')}    * False  Keyword arguments: {}Option 4:  Positional arguments (2 total):    * {'abc': TensorSpec(shape=(None, 4), dtype=tf.int32, name='abc')}    * True  Keyword arguments: {}Call arguments received:  闂?args=({'abc': 'tf.Tensor(shape=(1, 2), dtype=int32)'},)  闂?kwargs={'training': 'None'}```
"
52109,0,2696,10,0,0,lankuohsing,0,"title:A bug for tf.keras.layers.TextVectorization when built from saved configs and weights description:I have tried writing a python program to save tf.keras.layers.TextVectorization to disk and load it with the answer of https://stackoverflow.com/questions/65103526/how-to-save-textvectorization-to-disk-in-tensorflow.The TextVectorization layer built from saved configs outputs a vector with wrong length when the arg output_sequence_length is not None and output_mode='int'.For example, if I set output_sequence_length= 10, and output_mode='int', it is expected that given a text, TextVectorization should output a vector with length of 10, see vectorizer and new_v2 in the code below.However, if TextVectorization's arg output_mode='int' is set from saved configs, it doesn't output a vector with length of 10(actually it is 9, the real length of the sentence. It seems like output_sequence_length is not set successfully). See the object new_v1 in the code below.The interesting thing is, I have compared from_disk['config']['output_mode'] and 'int', they equal to each other.```import tensorflow as tffrom tensorflow.keras.models import load_modelimport pickle# In[]max_len = 10  # Sequence length to pad the outputs to.text_dataset = tf.data.Dataset.from_tensor_slices([                                                   ""I like natural language processing"",                                                   ""You like computer vision"",                                                   ""I like computer games and computer science""])# Fit a TextVectorization layerVOCAB_SIZE = 10  # Maximum vocab size.vectorizer = tf.keras.layers.TextVectorization(        max_tokens=None,        standardize=""lower_and_strip_punctuation"",        split=""whitespace"",        output_mode='int',        output_sequence_length=max_len        )vectorizer.adapt(text_dataset.batch(64))# In[]#print(vectorizer.get_vocabulary())#print(vectorizer.get_config())#print(vectorizer.get_weights())# In[]# Pickle the config and weightspickle.dump({'config': vectorizer.get_config(),             'weights': vectorizer.get_weights()}            , open(""./models/tv_layer.pkl"", ""wb""))# Later you can unpickle and use# `config` to create object and# `weights` to load the trained weights.from_disk = pickle.load(open(""./models/tv_layer.pkl"", ""rb""))new_v1 = tf.keras.layers.TextVectorization(        max_tokens=None,        standardize=""lower_and_strip_punctuation"",        split=""whitespace"",        output_mode=from_disk['config']['output_mode'],        output_sequence_length=from_disk['config']['output_sequence_length'],        )# You have to call `adapt` with some dummy data (BUG in Keras)new_v1.adapt(tf.data.Dataset.from_tensor_slices([""xyz""]))new_v1.set_weights(from_disk['weights'])new_v2 = tf.keras.layers.TextVectorization(        max_tokens=None,        standardize=""lower_and_strip_punctuation"",        split=""whitespace"",        output_mode='int',        output_sequence_length=from_disk['config']['output_sequence_length'],        )# You have to call `adapt` with some dummy data (BUG in Keras)new_v2.adapt(tf.data.Dataset.from_tensor_slices([""xyz""]))new_v2.set_weights(from_disk['weights'])print (""*""*10)# In[]test_sentence=""Jack likes computer scinece, computer games, and foreign language""print(vectorizer(test_sentence))print (new_v1(test_sentence))print (new_v2(test_sentence))print(from_disk['config']['output_mode']=='int')```Here are the print() outputs:```**********tf.Tensor([ 1  1  3  1  3 11 12  1 10  0], shape=(10,), dtype=int64)tf.Tensor([ 1  1  3  1  3 11 12  1 10], shape=(9,), dtype=int64)tf.Tensor([ 1  1  3  1  3 11 12  1 10  0], shape=(10,), dtype=int64)True```Does anyone know why?I have also raised a same issue as this in the repo of Keras https://github.com/keras-team/keras/issues/15382 
"
52087,0,0,2,0,1,Snape3058,0,"title:Potential dangling-pointer bug in function `GetPyArrayDescrForTensor` by holding a reference in a list after all its references released (a static analyzer report) description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- TensorFlow installed from (source or binary): source- TensorFlow version (commit): faad219- Python version: 3.8.5**Static analysis results, no POC.**This static analysis report has been manually reviewed to verify its validity.**Describe the current behavior**The path provided by the static analyzer is as follows.1. A new reference is returned from `PyTuple_New` and pointed to by `field`.https://github.com/tensorflow/tensorflow/blob/faad219fc46032a0ae9576ccc3076612cc1f5f72/tensorflow/python/lib/core/ndarray_tensor.cc#L3412. A reference is stolen by function `PyList_SetItem`.https://github.com/tensorflow/tensorflow/blob/faad219fc46032a0ae9576ccc3076612cc1f5f72/tensorflow/python/lib/core/ndarray_tensor.cc#L3503. Refcnt decrement in macro `Py_CLEAR` will make the reference held in the list a dangling pointer.https://github.com/tensorflow/tensorflow/blob/faad219fc46032a0ae9576ccc3076612cc1f5f72/tensorflow/python/lib/core/ndarray_tensor.cc#L352**Contributing**- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing):A potential correct fix can be removing the following line.https://github.com/tensorflow/tensorflow/blob/faad219fc46032a0ae9576ccc3076612cc1f5f72/tensorflow/python/lib/core/ndarray_tensor.cc#L352
"
52083,1,0,0,0,0,acaywood,0,"title:Bug description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary):- TensorFlow version (use command below):- Python version:- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version:- GPU model and memory:You can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior****Describe the expected behavior****[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
52069,0,0,269,0,0,elfringham,0,"title:bazel test //tensorflow/tools/docs:tf_doctest fails on aarch64 description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a- TensorFlow installed from (source or binary): source- TensorFlow version (use command below): git HEAD- Python version: 3.8.10- Bazel version (if compiling from source): 3.7.2- GCC/Compiler version (if compiling from source): 10.3.0- CUDA/cuDNN version: n/a- GPU model and memory: n/aYou can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**Test fails**Describe the expected behavior**Test passes**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing): I think the test needs to be relaxed slightly to accept the values produced by AARCH64 CPUs.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.bazel test //tensorflow/tools/docs:tf_doctest**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.======================================================================FAIL: Tanh (tensorflow.python.ops.gen_math_ops)Doctest: tensorflow.python.ops.gen_math_ops.Tanh----------------------------------------------------------------------Traceback (most recent call last):  File ""/usr/lib/python3.8/doctest.py"", line 2204, in runTest    raise self.failureException(self.format_failure(new.getvalue()))AssertionError: Failed doctest test for tensorflow.python.ops.gen_math_ops.Tanh  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/ops/gen_math_ops.py"", line 408, in Tanh----------------------------------------------------------------------File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/ops/gen_math_ops.py"", line 416, in tensorflow.python.ops.gen_math_ops.TanhFailed example:    tf.math.tanh(x)Expected:    <tf.Tensor: shape=(8,), dtype=float32, numpy=    array([-1.        , -0.99990916, -0.46211717,  0.7615942 ,  0.8336547 ,            0.9640276 ,  0.9950547 ,  1.        ], dtype=float32)>Got:    <tf.Tensor: shape=(8,), dtype=float32, numpy=    array([-0.99999976, -0.99990916, -0.46211717,  0.7615942 ,  0.8336546 ,            0.9640276 ,  0.9950547 ,  0.99999976], dtype=float32)>    #############################################################    Check the documentation    (https://www.tensorflow.org/community/contribute/docs_ref) on how to write testable docstrings.    #############################################################======================================================================FAIL: tanh (tensorflow.python.ops.gen_math_ops)Doctest: tensorflow.python.ops.gen_math_ops.tanh----------------------------------------------------------------------Traceback (most recent call last):  File ""/usr/lib/python3.8/doctest.py"", line 2204, in runTest    raise self.failureException(self.format_failure(new.getvalue()))AssertionError: Failed doctest test for tensorflow.python.ops.gen_math_ops.tanh  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/ops/gen_math_ops.py"", line 11336, in tanh----------------------------------------------------------------------File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/ops/gen_math_ops.py"", line 11344, in tensorflow.python.ops.gen_math_ops.tanhFailed example:    tf.math.tanh(x)Expected:    <tf.Tensor: shape=(8,), dtype=float32, numpy=    array([-1.        , -0.99990916, -0.46211717,  0.7615942 ,  0.8336547 ,            0.9640276 ,  0.9950547 ,  1.        ], dtype=float32)>Got:    <tf.Tensor: shape=(8,), dtype=float32, numpy=    array([-0.99999976, -0.99990916, -0.46211717,  0.7615942 ,  0.8336546 ,            0.9640276 ,  0.9950547 ,  0.99999976], dtype=float32)>    #############################################################    Check the documentation    (https://www.tensorflow.org/community/contribute/docs_ref) on how to write testable docstrings.    #############################################################======================================================================FAIL: sigmoid (tensorflow.python.ops.math_ops)Doctest: tensorflow.python.ops.math_ops.sigmoid----------------------------------------------------------------------Traceback (most recent call last):  File ""/usr/lib/python3.8/doctest.py"", line 2204, in runTest    raise self.failureException(self.format_failure(new.getvalue()))AssertionError: Failed doctest test for tensorflow.python.ops.math_ops.sigmoid  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/ops/math_ops.py"", line 174, in sigmoid----------------------------------------------------------------------File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/ops/math_ops.py"", line 187, in tensorflow.python.ops.math_ops.sigmoidFailed example:    tf.math.sigmoid(x)Expected:    <tf.Tensor: shape=(4,), dtype=float32,    numpy=array([0.5      , 0.7310586, 1.       , 1.       ], dtype=float32)>Got:    <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.5      , 0.7310586, 0.9999998, 0.9999998], dtype=float32)>
"
52060,1,0,0,0,0,akankshaaa13,0,"title:ValueError: Shapes (None, 8) and (None, 7) are incompatible description:ValueError: in user code:    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:853 train_function  *        return step_function(self, iterator)    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:842 step_function  **        outputs = model.distribute_strategy.run(run_step, args=(data,))    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1286 run        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica        return self._call_for_each_replica(fn, args, kwargs)    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica        return fn(*args, **kwargs)    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:835 run_step  **        outputs = model.train_step(data)    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:789 train_step        y, y_pred, sample_weight, regularization_losses=self.losses)    /usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py:201 __call__        loss_value = loss_obj(y_t, y_p, sample_weight=sw)    /usr/local/lib/python3.7/dist-packages/keras/losses.py:141 __call__        losses = call_fn(y_true, y_pred)    /usr/local/lib/python3.7/dist-packages/keras/losses.py:245 call  **        return ag_fn(y_true, y_pred, **self._fn_kwargs)    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper        return target(*args, **kwargs)    /usr/local/lib/python3.7/dist-packages/keras/losses.py:1666 categorical_crossentropy        y_true, y_pred, from_logits=from_logits, axis=axis)    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper        return target(*args, **kwargs)    /usr/local/lib/python3.7/dist-packages/keras/backend.py:4839 categorical_crossentropy        target.shape.assert_is_compatible_with(output.shape)    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py:1161 assert_is_compatible_with        raise ValueError(""Shapes %s and %s are incompatible"" % (self, other))    ValueError: Shapes (None, 8) and (None, 7) are incompatiblePlease check the code in [link](https://colab.research.google.com/drive/1WHp2T_jOjtXUHeBhiYR3zaeumtXtSDwn#scrollTo=1rsivhQNf36Y&uniqifier=3)
"
52046,0,3251,15,0,0,NikoRepo,0,"title:Downloading ""Imdb_reviews"" from Tensorflow_datasets: UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd5 in position 30 invalid continuation byte description:- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10- TensorFlow version (use command below):- 2.3.0- Python version:- 3.8.8**Describe the current behavior**When I was downloading ""imbd_reviews"" dataset I am facing the below error,**'utf-8' codec can't decode byte 0xc5 in position 171: invalid continuation byte**``` import tensorflow_datasets as tfdsdatasets, info = tfds.load(""imdb_reviews"",as_supervised=True, with_info=True)Downloading and preparing dataset imdb_reviews (80.23 MiB) to C:\Users\desig\tensorflow_datasets\imdb_reviews\plain_text\0.1.0...Dl Completed...:0/0 [00:00<?, ? url/s]Dl Size...:0/0 [00:00<?, ? MiB/s]---------------------------------------------------------------------------UnicodeDecodeError                        Traceback (most recent call last)<ipython-input-6-f3ae52bd604b> in <module>      1 import numpy as np----> 2 datasets, info = tfds.load(""imdb_reviews"",as_supervised=True, with_info=True)      3 ~\anaconda3\lib\site-packages\tensorflow_datasets\core\api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)     50     _check_no_positional(fn, args, ismethod, allowed=allowed)     51     _check_required(fn, kwargs)---> 52     return fn(*args, **kwargs)     53      54   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter~\anaconda3\lib\site-packages\tensorflow_datasets\core\registered.py in load(name, split, data_dir, batch_size, in_memory, shuffle_files, download, as_supervised, decoders, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)    298   if download:    299     download_and_prepare_kwargs = download_and_prepare_kwargs or {}--> 300     dbuilder.download_and_prepare(**download_and_prepare_kwargs)    301     302   if as_dataset_kwargs is None:~\anaconda3\lib\site-packages\tensorflow_datasets\core\api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)     50     _check_no_positional(fn, args, ismethod, allowed=allowed)     51     _check_required(fn, kwargs)---> 52     return fn(*args, **kwargs)     53      54   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter~\anaconda3\lib\site-packages\tensorflow_datasets\core\dataset_builder.py in download_and_prepare(self, download_dir, download_config)    305         self.info.size_in_bytes = dl_manager.downloaded_size    306         # Write DatasetInfo to disk, even if we haven't computed the statistics.--> 307         self.info.write_to_directory(self._data_dir)    308     self._log_download_done()    309 ~\anaconda3\lib\contextlib.py in __exit__(self, type, value, traceback)    118         if type is None:    119             try:--> 120                 next(self.gen)    121             except StopIteration:    122                 return False~\anaconda3\lib\site-packages\tensorflow_datasets\core\file_format_adapter.py in incomplete_dir(dirname)    198   try:    199     yield tmp_dir--> 200     tf.io.gfile.rename(tmp_dir, dirname)    201   finally:    202     if tf.io.gfile.exists(tmp_dir):~\anaconda3\lib\site-packages\tensorflow\python\lib\io\file_io.py in rename_v2(src, dst, overwrite)    543     errors.OpError: If the operation fails.    544   """"""--> 545   _pywrap_file_io.RenameFile(    546       compat.as_bytes(src), compat.as_bytes(dst), overwrite)    547 UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc5 in position 171: invalid continuation byte```Do any one have  idea, Thank you.
"
52028,1,868,4,0,0,lvadke,0,"title:Reshape conversion produces invalid output shape (if one dim is -1) description:### 1. System information- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.4 LTS- TensorFlow installation (pip package or built from source): pip package- TensorFlow library (version, if pip package or github SHA, if built from source): 2.6.0### 2. Code```import numpy as npimport tensorflow as tfinputs = tf.keras.Input(shape=(1, 126))outputs = tf.keras.layers.Reshape((-1, 21))(inputs)#outputs = tf.reshape(inputs, (-1, 21))model = tf.keras.Model(inputs=inputs, outputs=outputs)model.summary()model.compile(optimizer='sgd', loss='mean_squared_error')inputs = np.random.rand(126).reshape((1, 1, 126))outputs = np.random.rand(126).reshape((1, 6, 21))model.fit(x=inputs, y=outputs, epochs=1)def representative_dataset_gen():  for input in inputs:    input = input.astype(np.float32)    yield [input]converter = tf.lite.TFLiteConverter.from_keras_model(model)converter.optimizations = [tf.lite.Optimize.DEFAULT]converter.representative_dataset = representative_dataset_gentflite_model = converter.convert()with open('model.tflite', 'wb') as f:  f.write(tflite_model)```### 3. Failure after conversionConversion produces a reshape operation (and all subsequent ops) with incorrect output shape, which makes TFLM fail (as it uses only the output shape as the reshape input parameter):![reshape](https://user-images.githubusercontent.com/52713197/133601423-e585f707-953e-4b20-9374-7453a5becd54.png)I tried both, tf.keras.layers.Reshape and tf.reshape, and the result is the same, except the branch in the blue box is generated only for the Keras operator. (The issue is extracted from a detection model with many reshapes.)
"
52023,1,0,0,0,0,ArunaKote,0,"title:Error in lowering tf.ResizeBilinear and tf.ResizeNearestNeighbor using tf-mlir-translate and tf-opt  description:Model for tf.ResizeNearestNeighbor looks like:model = tf.keras.Sequential([tf.keras.layers.Conv2D(4, (1, 1),input_shape = (10, 10, 3),batch_size=1, name='fpn_c5p5'),tf.keras.layers.UpSampling2D(size=(2, 2),interpolation='nearest', name=""fpn_p5upsampled"")])Model for tf.ResizeBilinear looks like:model = tf.keras.Sequential([tf.keras.layers.Conv2D(4, (1, 1),input_shape = (10, 10, 3),batch_size=1, name='fpn_c5p5'),tf.keras.layers.UpSampling2D(size=(2, 2),interpolation='nearest', name=""fpn_p5upsampled"")])Error in lowering tf.ResizeBilinear and tf.ResizeNearestNeighbor to HLO.I have attached log file and python code to get saved_model.pb[codeandlog.zip](https://github.com/tensorflow/tensorflow/files/7174773/codeandlog.zip)1.tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate --savedmodel-objectgraph-to-mlir --tf-savedmodel-exported-names=predict  -tf-enable-shape-inference-on-import=true $PWD -o sample.mlir2.tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt -canonicalize --tf-executor-to-functional-conversion --tf-shape-inference -xla-legalize-tf  --print-ir-before-all &>1  sample.mlirIf tf.ResizeNearestNeighbor if it has argument half_pixel_centers = false I able to lower to MHLO. But in the above case half_pixel_centers = true. and i am not able to lower to HLO.
"
52013,1,1325,6,0,0,pacospace,0,"title:Cannot load saved tf model description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora 33- TensorFlow installed from (source or binary): pip install tensorflow- TensorFlow version (use command below): 2.6.0- Python version: 3.8You can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`:`v2.6.0-rc2-32-g919f693420e 2.6.0`**Describe the current behavior**`tf.saved_model.load(model_path)` fails with the following error:```Traceback (most recent call last):  File ""convert_to_onnx.py"", line 3, in <module>    tf.saved_model.load('/home/fmurdaca/work/aicoe/elyra-aidevsecops-tutorial/models/210915165333-7b04047d1220f5cd')  File ""/home/fmurdaca/.local/share/virtualenvs/convert-onnx-KeTiB-gU/lib64/python3.8/site-packages/tensorflow/python/saved_model/load.py"", line 864, in load    result = load_internal(export_dir, tags, options)[""root""]  File ""/home/fmurdaca/.local/share/virtualenvs/convert-onnx-KeTiB-gU/lib64/python3.8/site-packages/tensorflow/python/saved_model/load.py"", line 902, in load_internal    loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,  File ""/home/fmurdaca/.local/share/virtualenvs/convert-onnx-KeTiB-gU/lib64/python3.8/site-packages/tensorflow/python/saved_model/load.py"", line 162, in __init__    self._load_all()  File ""/home/fmurdaca/.local/share/virtualenvs/convert-onnx-KeTiB-gU/lib64/python3.8/site-packages/tensorflow/python/saved_model/load.py"", line 259, in _load_all    self._load_nodes()  File ""/home/fmurdaca/.local/share/virtualenvs/convert-onnx-KeTiB-gU/lib64/python3.8/site-packages/tensorflow/python/saved_model/load.py"", line 448, in _load_nodes    slot_variable = optimizer_object.add_slot(AttributeError: '_UserObject' object has no attribute 'add_slot'```This also breaks tensorflow-onnx package: https://github.com/onnx/tensorflow-onnx/issues/1715**Describe the expected behavior**method is able to load model, so that other actions can be performed.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): yes- Briefly describe your candidate solution(if contributing): adjust attribute provided by that object**Standalone code to reproduce the issue**1. clone https://github.com/AICoE/elyra-aidevsecops-tutorial2. pip install tensorflow3. run python script with import tensoflow as tf and `tf.saved_model.load('./models/210124112759-d97fd1f46b13ee40')`**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
52003,1,0,8,0,0,shemliang,0,"title:shared_embedding_columns  description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary):- TensorFlow version (use command below):- Python version:- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version:- GPU model and memory:You can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior****Describe the expected behavior****[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
51978,0,2359,194,0,0,bersbersbers,0,"title:""Unimplemented:  Deterministic GPU implementation of unsorted segment reduction op not available"" with AUC metric and TF_DETERMINISTIC_OPS description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OpenSUSE LEAP 15.2- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0- Python version: Python 3.9.6- CUDA/cuDNN version: 11.2 and 8.1.1, I believe- GPU model and memory: Quadro RTX 6000Reproduces on Colab with GPU.**Describe the current behavior**```Traceback (most recent call last):[...]  File ""/home/bers/proj/bug.py"", line 12, in <module>    model.fit(x=data, y=data)  File ""/data2/bers/opt/pyenv/versions/3.9.6/lib/python3.9/site-packages/keras/engine/training.py"", line 1184, in fit    tmp_logs = self.train_function(iterator)  File ""/data2/bers/opt/pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py"", line 885, in __call__    result = self._call(*args, **kwds)  File ""/data2/bers/opt/pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py"", line 950, in _call    return self._stateless_fn(*args, **kwds)  File ""/data2/bers/opt/pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py"", line 3039, in __call__    return graph_function._call_flat(  File ""/data2/bers/opt/pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py"", line 1963, in _call_flat    return self._build_call_outputs(self._inference_function.call(  File ""/data2/bers/opt/pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py"", line 591, in call    outputs = execute.execute(  File ""/data2/bers/opt/pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,tensorflow.python.framework.errors_impl.UnimplementedError: 2 root error(s) found.  (0) Unimplemented:  Deterministic GPU implementation of unsorted segment reduction op not available.	 [[node UnsortedSegmentSum (defined at home/bers/proj/bug.py:12) ]]	 [[assert_less_equal/Assert/AssertGuard/pivot_f/_13/_39]]  (1) Unimplemented:  Deterministic GPU implementation of unsorted segment reduction op not available.	 [[node UnsortedSegmentSum (defined at home/bers/proj/bug.py:12) ]]0 successful operations.0 derived errors ignored. [Op:__inference_train_function_513]Function call stack:train_function -> train_function```**Describe the expected behavior**No error (works in TF 2.5.0)**Standalone code to reproduce the issue**```pythonimport osos.environ[""TF_DETERMINISTIC_OPS""] = ""True""import tensorflow as tfdata = tf.ones((1, 1))layer = tf.keras.layers.Input(shape=[1])model = tf.keras.models.Model(inputs=layer, outputs=layer)model.compile(loss=""categorical_crossentropy"", metrics=""AUC"")model.fit(x=data, y=data)```
"
51954,1,0,0,0,0,seragENTp,0,"title:GPU Delegate Issue ! description:### System information-   **Have I written custom code (as opposed to using a stock example script    provided in TensorFlow)**: No-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android OS Oreo (API Level 27)-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue    happens on a mobile device**: Surveillance Camera with QCS605 Qualcomm ship.-   **TensorFlow installed from (source or binary)**: binary-   **TensorFlow version (use command below)**: 2.5-   **Python version**: 3.8-   **Bazel version (if compiling from source)**:-   **GCC/Compiler version (if compiling from source)**:-   **CUDA/cuDNN version**:-   **GPU model and memory**: Adreno GPU 615-   **Exact command to reproduce**:-       final Interpreter.Options options = new Interpreter.Options();        CompatibilityList compatList = new CompatibilityList();        if (compatList.isDelegateSupportedOnThisDevice()) {            // if the device has a supported GPU, add the GPU delegate            GpuDelegate.Options delegateOptions = compatList.getBestOptionsForThisDevice();            GpuDelegate gpuDelegate = new GpuDelegate(delegateOptions);            options.addDelegate(gpuDelegate);            Log.i(LOGTAG, ""-----------Running using GPU Delegate-----------"");        } else {            // if the GPU is not supported, run on numThreads threads            options.setNumThreads(numThreads)                    .setAllowFp16PrecisionForFp32(allowFp16PrecisionForFp32)                    .setUseNNAPI(useNNAPI);            Log.i(LOGTAG, ""------------Running using CPU Delegate---------"");        }### Describe the problemTrying to run [Movinet ](https://tfhub.dev/google/collections/movinet) tflite model on a vendor camera with Adreno GPU 615. The network runs on the CPU without any issues. But it can't be run on the GPU.  I'm using Java TFLite run time with version 2.5. I tried the nightly version but didn't work also. ### Source code / logsI/Adreno: QUALCOMM build                   : e3ea17d, I2eff518144I/Adreno: Build Config                     : S L 4.0.10 AArch64I/zygote64: android::hardware::configstore::V1_0::ISurfaceFlingerConfigs::hasWideColorDisplay retrieved: 0E/libEGL: call to OpenGL ES API with no current context (logged once per thread)
"
51944,0,5043,238,0,0,fsx950223,0,"title:Init node head/predictions/class_string_lookup/table_init/LookupTableImportV2 doesn't exist in graph description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):debian 10- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary):binary- TensorFlow version (use command below):2.6.0- Python version:3.6.9- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version:- GPU model and memory:You can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**```2021-09-11 12:21:06.193462: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory2021-09-11 12:21:06.193522: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.2021-09-11 12:21:09.808281: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory2021-09-11 12:21:09.808359: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)2021-09-11 12:21:09.808396: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (penguin): /proc/driver/nvidia/version does not exist2021-09-11 12:21:09.808686: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2021-09-11 12:21:10.927031: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)2021-09-11 12:21:11.938166: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 02021-09-11 12:21:11.938620: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session2021-09-11 12:21:11.961714: E tensorflow/core/grappler/grappler_item_builder.cc:669] Init node head/predictions/class_string_lookup/table_init/LookupTableImportV2 doesn't exist in graphTraceback (most recent call last):  File ""tflite.py"", line 29, in <module>    model = converter.convert()  File ""/home/fsx950223/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 1396, in convert    return super(TFLiteConverterV2, self).convert()  File ""/home/fsx950223/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 729, in wrapper    return self._convert_and_export_metrics(convert_func, *args, **kwargs)  File ""/home/fsx950223/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 715, in _convert_and_export_metrics    result = convert_func(self, *args, **kwargs)  File ""/home/fsx950223/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 1201, in convert    self._freeze_concrete_function())  File ""/home/fsx950223/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/lite/python/convert_phase.py"", line 218, in wrapper    raise error from None  # Re-throws the exception.  File ""/home/fsx950223/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/lite/python/convert_phase.py"", line 208, in wrapper    return func(*args, **kwargs)  File ""/home/fsx950223/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 1177, in _freeze_concrete_function    self._funcs[0], lower_control_flow=False))  File ""/home/fsx950223/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 1229, in convert_variables_to_constants_v2_as_graph    aggressive_inlining=aggressive_inlining)  File ""/home/fsx950223/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 809, in __init__    aggressive_inlining)  File ""/home/fsx950223/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 1043, in _run_inline_graph_optimization    return tf_optimizer.OptimizeGraph(config, meta_graph)  File ""/home/fsx950223/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/grappler/tf_optimizer.py"", line 58, in OptimizeGraph    graph_id, strip_default_attributes)ValueError: Failed to import metagraph, check error log for more info.```**Describe the expected behavior**Works same as ```converter = tf.lite.TFLiteConverter.from_saved_model('./saved_model', signature_keys=[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY])```**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.```pythonimport tensorflow as tfimport tensorflow_text as textmodel = tf.saved_model.load('./saved_model')concrete_func = model.signatures[    tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]concrete_func.inputs[0].set_shape([1])converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])# converter = tf.lite.TFLiteConverter.from_saved_model('./saved_model', signature_keys=[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY])converter.optimizations=[tf.lite.Optimize.DEFAULT]converter.inference_type=tf.float32converter.target_spec.supported_ops=[tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]model = converter.convert()tf.io.write_file('guesslang.tflite', model)```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.Saved model: [saved_model.zip](https://github.com/tensorflow/tensorflow/files/7147206/saved_model.zip)
"
51936,1,502,0,0,0,lugalUrim,0,"title:tf.keras.layers.MaxPooling3D crashes description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.6.0- Python version: 3.6.8- Bazel version (if compiling from source): n/a- GCC/Compiler version (if compiling from source): n/a- CUDA/cuDNN version: n/a- GPU model and memory: n/a**Describe the current behavior**`tf.keras.layers.MaxPooling3D` crashes when `pool_size` contains `0`, and outputs a all-inf tensor when `pool_size` contains negative values.**Describe the expected behavior**Expect a `ValueError` to be thrown if the input `pool_size` contains zero or negative values.**Standalone code to reproduce the issue**If the `pool_size` has `0`:```import tensorflow as tfpool_size = [2, 2, 0]layer = tf.keras.layers.MaxPooling3D(strides=1, pool_size=pool_size)input_tensor = tf.random.uniform([3, 4, 10, 11, 12], dtype=tf.float32)res = layer(input_tensor) # crash```Outputs:```Floating point exception (core dumped)```If the `pool_size` has negative values:```import tensorflow as tfpool_size = [2, 2, -2]layer = tf.keras.layers.MaxPooling3D(strides=1, pool_size=pool_size,)input_tensor = tf.random.uniform([3, 4, 10, 11, 12], dtype=tf.float32)res = layer(input_tensor)print(res)```The output is a tensor with `shape`=`(3, 3, 9, 14, 12)` and all `inf` values.
"
51922,1,5256,16,0,0,EnderWiggin14,0,"title:Sub-classed constraint doesn't appear to be called description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): A proprietary variant of RedHat, outside of my control- TensorFlow installed from (source or binary): Binary- TensorFlow version (use command below): 2.5.0- Python version: 3.7.2- CUDA/cuDNN version: 11.1.0/8.1.1- GPU model and memory: Nvidia K40 12GB**Describe the current behavior**I have created a sub-classed layer that performs a weighted sum of the outputs of two sub_models.In this layer, I create weights with the `add_weight` function which includes the use of a constraint.The intention of the constraint is to normalize the weights of each input dimension/feature (i.e. sum to one).I have created a custom weight normalization constraint which is sub-classed from `krs.constraints.Constraint`. The output weights from the constraint function for model consisting of 2 sub-models with each producing a 200 dimension/feature output. Thus, the input to the constraint would be 2 x 200 weight tensor and the columnwise sums should all be 1.0.When providing an instance of my constraint class to the `add_weight` function, it appears that there it is never called. It is initialized though. When running `VectorWeightedSum_1` below the `tf.print` statement in the call function shows that the weights are never normalized. Also, the `tf.print` statement in `NormalizeSumWeights.call()` doesn't ever produce an output. (It also doesn't exit the program if I put a `sys.exit()` call inside the contraint's `call`)If I instead use `VectorWeightedSum_2`, the `tf.print` statement demonstrates that weights are indeed normalized properly. This was done by creating a new function inside the `VectorWeightedSum_2` class as opposed to a constraint class. The line for this code is commented out in the code below.**Describe the expected behavior**The sub-classed constraint should have it's `call` function called.The weights produced by the sub-classed constraint should sum to 1.0 along each column.In the example code, the `tf.print` statements should be printed for both `_constraint` and the use of the `NormalizeSumWeights` class.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): No**Standalone code to reproduce the issue**```import numpy as npimport tensorflow as tfimport tensorflow.keras as krsclass NormalizeSumWeights(krs.constraints.Constraint):    def __init__(self,**kwargs):        super(NormalizeSumWeights,self).__init__(**kwargs)    def call(self,w):        tf.print(""NormalizeSumWeights_________________________________"")        return tf.math.divide(w,tf.math.reduce_sum(w,axis=0))class WeightedSum(krs.layers.Layer):    def __init__( self, n_models = 2, **kwargs):        super( WeightedSum, self ).__init__( **kwargs)        self.n_models = n_models    def build(self,input_shape):        self.sum_weights = self.add_weight(name=""sum_weights"",shape=(self.n_models,1),                                    initializer = krs.initializers.RandomUniform(0,1.0),                                    constraint = NormalizeSumWeights(),                                    trainable = True)    def call(self,inputs):        output = tf.multiply(tf.cast(self.sum_weights[0,:],inputs[0].dtype),inputs[0])        for i in range(1,len(inputs)):            output += tf.multiply(inputs[i],tf.cast(self.sum_weights[i,0],inputs[0].dtype))        return output    def get_config(self):        data = { ""n_models"": self.n_models}        return dataclass VectorWeightedSum_1(krs.layers.Layer):    def __init__( self, n_models = 2,n_dimensions=200, **kwargs):        super( VectorWeightedSum_1, self ).__init__( **kwargs)        self.n_models = n_models        self.n_dimensions = n_dimensions    def build(self,input_shape):        self.sum_weights = self.add_weight(name=""sum_weights"",shape=(self.n_models,self.n_dimensions),                                    initializer = krs.initializers.RandomUniform(0,1.0),                                    constraint = NormalizeSumWeights(),                                    trainable = True)    def _constraint(self,w):        tf.print(""Weights Normalization__________________________"")        return tf.math.divide(w,tf.math.reduce_sum(w,axis=0))    def call(self,inputs):        output = tf.multiply(tf.cast(self.sum_weights[0,:],inputs[0].dtype),inputs[0])        for i in range(1,len(inputs)):            output += tf.multiply(inputs[i],tf.cast(self.sum_weights[i,:],inputs[0].dtype))        tf.print(""vectorWeightedSum___weights________________"")        tf.print(self.sum_weights)        return outputclass VectorWeightedSum_2(krs.layers.Layer):    def __init__( self, n_models = 2,n_dimensions=200, **kwargs):        super( VectorWeightedSum_2, self ).__init__( **kwargs)        self.n_models = n_models        self.n_dimensions = n_dimensions    def build(self,input_shape):        self.sum_weights = self.add_weight(name=""sum_weights"",shape=(self.n_models,self.n_dimensions),                                    initializer = krs.initializers.RandomUniform(0,1.0),                                    constraint = self._constraint,                                    trainable = True)    def _constraint(self,w):        tf.print(""Weights Normalization__________________________"")        return tf.math.divide(w,tf.math.reduce_sum(w,axis=0))    def call(self,inputs):        output = tf.multiply(tf.cast(self.sum_weights[0,:],inputs[0].dtype),inputs[0])        for i in range(1,len(inputs)):            output += tf.multiply(inputs[i],tf.cast(self.sum_weights[i,:],inputs[0].dtype))        tf.print(""vectorWeightedSum___weights________________"")        tf.print(self.sum_weights)        return outputinput_lf = krs.Input((4,))x = input_lfx = krs.layers.Dense(10,activation = 'relu')(x)x = krs.layers.Dense(10,activation = 'relu')(x)lf_out = krs.layers.Dense(200,activation = 'relu')(x)lf_mod = krs.Model(input_lf,lf_out,name='lf')input_hf_lin = krs.Input((204,))x = input_hf_linx = krs.layers.Dense(10)(x)x = krs.layers.Dense(10)(x)hf_lin_out = krs.layers.Dense(200,activation = 'relu')(x)hf_lin_mod = krs.Model(input_hf_lin,hf_lin_out,name='hf_linear')input_hf_nonlin = krs.Input((14,))x = input_hf_nonlinx = krs.layers.Dense(10,activation = 'relu')(x)x = krs.layers.Dense(10,activation = 'relu')(x)hf_nonlin_out = krs.layers.Dense(200,activation = 'relu')(x)hf_nonlin_mod = krs.Model(input_hf_lin,hf_lin_out,name='hf_nonlinear')input_hf = krs.Input((204,))x = input_hflin = hf_lin_mod(x)nonlin = hf_nonlin_mod(x)summed_out = VectorWeightedSum_1(n_models=2)([lin,nonlin])#summed_out = VectorWeightedSum_2(n_models=2)([lin,nonlin])hf_mod = krs.Model(input_hf,summed_out,name='hf')input_full_mod = krs.Input((4,))x = input_full_modlow = lf_mod(x)x = krs.layers.Concatenate()([low,x])full_out = hf_mod(x)full_mod = krs.Model(input_full_mod,outputs = {'low_fidelity':low,'high_fidelity':full_out},name='full_model')opt = krs.optimizers.Adam()loss = krs.losses.MSEfull_mod.compile(optimizer = opt,loss = loss)x_train = np.random.uniform(0,10,(20,4))y_train_low = np.random.uniform(0,10,(20,200))y_train_high = np.random.uniform(0,10,(20,200))y = {""low_fidelity"": y_train_low,     ""high_fidelity"": y_train_high}full_mod.fit(x_train,y,epochs=5,batch_size=1)```
"
51911,1,0,6,0,0,rafak360,0,"title:TensorFlow throws an exception when loading a model that was normalized like this. description:See my stackoverflow question that covers up everything:https://stackoverflow.com/questions/69112466/why-tensorflow-throws-this-exception-when-loading-a-model-that-was-normalized-li
"
51908,1,394,0,0,0,lugalUrim,0,"title:tf.pad crashes with large paddings description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.6.0- Python version: 3.6.8- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source): N/A- CUDA/cuDNN version: N/A- GPU model and memory: N/A**Describe the current behavior**`tf.pad` crashes when the argument ""paddings"" has large values.**Describe the expected behavior**Expect an exception to be thrown if the input `paddings` is unexpected.**Standalone code to reproduce the issue**```import tensorflow as tfinput_tensor = tf.random.uniform([1, 32, 32, 3], dtype=tf.float32)paddings = [[125106557, 1415887920], [747509374, 2136925906], [413308538, 904601717], [1900762018, 831358864]]res = tf.pad(input_tensor,paddings)```outputs:```2021-09-09 12:46:38.123113: F tensorflow/core/framework/tensor_shape.cc:352] Check failed: 0 <= new_num_elements (0 vs. -1)Aborted (core dumped)```
"
51902,1,0,0,0,0,kevinkotzen,0,"title:val_sample_weight not used in calculation of custom validation metrics description:**System information**-  Custome code using standard keras and tfa.metrics-  Operating System: CentOS Linux 7 (Core), Kernel: Linux 3.10.0-1160.el7.x86_64- Tensorflow installed with pip install tf-nightly-gpu- v1.12.1-63317-g034300c177d 2.7.0-dev20210907- NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3- A100-40GB**Describe the current behavior**All validation metrics other than loss (set in model.compile(metrics=['accuracy', 'another_metrics'])) and reported during model.fit(......,val_data=(x_val, y_val, val_sample_weight)) do not seem to take the val_sample_weight into consideration. The loss metrics works as expected.  **Describe the expected behavior**The val_sample_weight associated with the x_val and y_val should be used in the calculation of 'accuracy' and other metrics such as tfa.metrics.CohenKappa. This is important for early stopping of model based on validation metrics. **[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**import os import numpy as npimport tensorflow as tfimport tensorflow_addons as tfainputs = tf.keras.Input(shape=(10))x = tf.keras.layers.Dense(100, activation='relu')(inputs)x = tf.keras.layers.Dense(10, activation='relu')(x)outputs = tf.keras.layers.Dense(3, activation='softmax')(x)model = tf.keras.models.Model(inputs=inputs, outputs=outputs)model.compile('adam', loss='sparse_categorical_crossentropy', metrics=['accuracy', tfa.metrics.CohenKappa(num_classes=3, sparse_labels=True)], sample_weight_mode='temporal')model.summary()X =tf.random.uniform(shape=[100,10], minval=0, maxval=1, dtype=tf.float32)Y = tf.random.uniform(shape=[100,1], minval=0, maxval=3, dtype=tf.int64)W = tf.ones([100])W2 = tf.zeros([100])model.fit(X, Y, sample_weight=W, validation_data=(X,Y,W2), epochs=5, verbose=1)probs = model.predict(X)kappa = tfa.metrics.CohenKappa(num_classes=3, sparse_labels=True)kappa.update_state(Y, probs)print(""Kappa Without Weights"", kappa.result())kappa = tfa.metrics.CohenKappa(num_classes=3, sparse_labels=True)kappa.update_state(Y, probs,W2)print(""Kappa With Weights"", kappa.result())**Other info / logs** Include any logs or source code that would be helpful to>>>W2 is all zeros. Notice that the val_loss is 0.000 as expected, but the val_accuracy and val_kappa are not. See prints the end showing kappa behavior with and without weights.   Epoch 1/54/4 [==============================] - 1s 215ms/step - loss: 1.0857 - accuracy: 0.4000 - cohen_kappa: -0.0130 - val_loss: 0.0000e+00 - val_accuracy: 0.3900 - val_cohen_kappa: -0.0281Epoch 2/54/4 [==============================] - 0s 31ms/step - loss: 1.0766 - accuracy: 0.4000 - cohen_kappa: -0.0096 - val_loss: 0.0000e+00 - val_accuracy: 0.4000 - val_cohen_kappa: -0.0052Epoch 3/54/4 [==============================] - 0s 30ms/step - loss: 1.0736 - accuracy: 0.3900 - cohen_kappa: -0.0214 - val_loss: 0.0000e+00 - val_accuracy: 0.4000 - val_cohen_kappa: -0.0059Epoch 4/54/4 [==============================] - 0s 31ms/step - loss: 1.0706 - accuracy: 0.4000 - cohen_kappa: -0.0037 - val_loss: 0.0000e+00 - val_accuracy: 0.4000 - val_cohen_kappa: -0.0059Epoch 5/54/4 [==============================] - 0s 29ms/step - loss: 1.0666 - accuracy: 0.4000 - cohen_kappa: -0.0081 - val_loss: 0.0000e+00 - val_accuracy: 0.4200 - val_cohen_kappa: 0.0213Kappa Without Weights tf.Tensor(0.021262169, shape=(), dtype=float32)Kappa With Weights tf.Tensor(0.0, shape=(), dtype=float32)
"
51879,0,0,35,0,0,starboyvarun,0,"title:Bug: Loading the  older versions of tfs and keras description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: No![4 1](https://user-images.githubusercontent.com/26819449/132501060-50a9033b-f984-4828-9715-a77b50fa1c8a.JPG)![4 2](https://user-images.githubusercontent.com/26819449/132501064-9121e22f-bf76-4648-92fb-2b68456a500f.JPG)I wanted to run a Bioinformatics library, a deep learning model which supports particular versions.The before model which I wanted to run showed error in TensorFlow also but this model also requirement can't be met.I was using google-collab.I think the issue is with integrating with google-collab or maybe with TensorFlow only.Thanks!
"
51843,1,451,2,0,0,sidhomj,0,"title:tf.linalg.diag issue description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.6.0- Python version: 3.8.11**Describe the current behavior**When providing the tf.linalg.diag function an input > length of 32, the function returns this error:```tensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [32,32] vs. shape[1] = [1,1] [Op:ConcatV2] name: concat```**Describe the expected behavior**The function should return a tensor of shape [n,n] no mater how large the tensor is.- Do you want to contribute a PR? (yes/no): no**Standalone code to reproduce the issue**```import tensorflow as tfimport numpy as np#inputinput_data = np.ones(33)#modelinput = tf.keras.layers.Input([],dtype=tf.float32)out = tf.linalg.diag(input)model = tf.keras.Model(inputs=input, outputs=out)#inferencepred = model.predict(input_data)```
"
51842,1,290,85,0,0,hotstone1993,0,"title:The ModifyGraphWithDelegate function fails on some devices. description:<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac OS Big sur (11.3.1)- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  Galaxy s21 ultra- TensorFlow installed from (source or binary): source  - TensorFlow version: r2.6- Python version: 3.8.8- Installed using virtualenv? pip? conda?: pip- Bazel version (if compiling from source): 3.7.2I currently use libtensorflowlite_gpu_delegate.so which compiled from source(r2.6).ModifyGraphWithDelegate function works fine on Galaxy Note 10.But, it's not working in Galaxy s21 ultra.(returns kTfLiteApplicationError.)Does it work according to the device? (Can't use gpu delegate for Galaxy s21 ultra?)Or do I need to set additional options?(Currently I am setting it as the default as below.)```  TfLiteGpuDelegateOptionsV2 option = TfLiteGpuDelegateOptionsV2Default();  delegate_ = TfLiteGpuDelegateV2Create(/*default options=*/&option);  TfLiteStatus result = interpreter_->ModifyGraphWithDelegate(delegate_);  if (result != kTfLiteOk) {    return kAudioNodeError;  }```I built the libraries as follows.libtensorflowlite.so(3.7MB): bazel build -c opt --config android_x86 android_x86_64 android_arm android_arm64 --define tflite_with_xnnpack=truelibtensorflowlite_gpu_delegate.so(98.5MB): bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so Thanks!!**Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
51840,1,6079,16,0,0,EnderWiggin14,0,"title:Saving a composite model that includes a custom layer results in error - None has NoneType, but expected one of: bytes, unicode description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): A custom variant of RedHat, outside of my control- TensorFlow installed from (source or binary): Binary- TensorFlow version (use command below): 2.5.0- Python version: 3.7.2- CUDA/cuDNN version: 11.1.0/8.1.1- GPU model and memory: Nvidia K40 12GB**Describe the current behavior**I'm trying to save a model which is a composite model of composite models.The first model is a sequential model of two sequential models. Both of the two sub-models have custom layers that perform scaling operations like MinMax and cube root. This model saves and loads without any issues.The first model is then loaded in a different script without compiling. There is not issue with this. Let's call this model MODEL_1.The next step may be a bit confusing. There are then two more models added in parallel to each other but sequentially with MODEL_1. Let's call these models MODEL_2a and MODEL_2b. The output of MODEL_1 has the input of MODEL_1 concatenated to it and this serves as the input to MODEL_2a and MODEL_2b. It should be noted that that the ""MODEL_1 input"" goes through a custom scaling layer before being concatenated to the output of MODEL_1. The is scaling layer that was also implemented in MODEL_1 without any issues.Finally, there is a single, custom layer that performs a 'simple' weighted sum of the outputs of MODEL_2a and MODEL_2b to produce the model output. This weighting happens via `alpha*OUTPUT_2a + [1-alpha]*OUTPUT_2b`. 'alpha' is a trainable, scalar parameter. I haven't used this before in a model that I have saved, so I'm guessing this is the cause.The model compiles and trains, but fails to save.The custom weighted sum layer is this,```class WeightedSum(krs.layers.Layer):    def __init__( self, n_models = 2, name = 'weighted_sum_0' ):        super( WeightedSum, self ).__init__( name = name)        self.n_models = n_models        self.ensemble_weights = []        self.output_init = tf.Variable(0.,validate_shape=False,trainable=False)    def build(self,input_shape):        for i in range(self.n_models):            self.ensemble_weights.append( self.add_weight(shape=(1,),                                    initializer = 'ones',                                    trainable = True) )    def call(self,inputs):        new_normalizer = tf.convert_to_tensor(0.,dtype = inputs[0].dtype)        for i in range(self.n_models):            new_normalizer = new_normalizer + self.ensemble_weights[i]        new_normalizer = tf.constant(1.,dtype=new_normalizer.dtype)/new_normalizer        output = self.output_init        for i in range(self.n_models):            output = tf.add(output,tf.multiply(self.ensemble_weights[i],inputs[i]))        output = tf.multiply( output, new_normalizer )        return output```The save command is this, (NOTE: I use `import tensorflow.keras as krs`)```krs.models.save_model(linked_model,""test_failed_save.mdl"")```The error that is produced is this.```Traceback (most recent call last):  File ""multi_fidelity_training_full_link.py"", line 304, in <module>    main()  File ""multi_fidelity_training_full_link.py"", line 265, in main    krs.models.save_model(linked_model,""test_failed_save.mdl"")  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py"", line 151, in save_model    signatures, options, save_traces)  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save.py"", line 90, in save    model, filepath, signatures, options)  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py"", line 1104, in save_and_return_nodes    raise_metadata_warning))  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py"", line 1291, in _build_meta_graph    raise_metadata_warning)  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py"", line 1225, in _build_meta_graph_impl    options.namespace_whitelist)  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py"", line 713, in _fill_meta_graph_def    _call_function_with_mapped_captures, resource_map=resource_map)))  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/graph_view.py"", line 424, in frozen_saveable_objects    call_with_mapped_captures)  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/graph_view.py"", line 375, in _serialize_gathered_objects    slot_variables=slot_variables)  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/graph_view.py"", line 355, in _fill_object_graph_proto    child_proto.local_name = child.nameTypeError: None has type NoneType, but expected one of: bytes, unicode```**Describe the expected behavior**It should save the model.**Standalone code to reproduce the issue**```import numpy as npimport tensorflow.keras as krsimport tensorflow as tfclass WeightedSum(krs.layers.Layer):    def __init__( self, n_models = 2, **kwargs):        super( WeightedSum, self ).__init__( **kwargs)        self.n_models = n_models        self.ensemble_weights = []        self.output_init = tf.Variable(0.,validate_shape=False,trainable=False)    def build(self,input_shape):        for i in range(self.n_models):            self.ensemble_weights.append( self.add_weight(shape=(1,),                                    initializer = 'ones',                                    trainable = True) )    def call(self,inputs):        new_normalizer = tf.convert_to_tensor(0.,dtype = inputs[0].dtype)        for i in range(self.n_models):            new_normalizer = new_normalizer + self.ensemble_weights[i]        new_normalizer = tf.constant(1.,dtype=new_normalizer.dtype)/new_normalizer        output = tf.cast(self.output_init,dtype=inputs[0].dtype)        for i in range(self.n_models):            output = tf.add(output,tf.multiply(tf.cast(self.ensemble_weights[i],dtype=inputs[i].dtype),inputs[i]))        output = tf.multiply( output, new_normalizer )        return outputinput_lf = krs.Input((4,))x = input_lfx = krs.layers.Dense(10,activation = 'relu')(x)x = krs.layers.Dense(10,activation = 'relu')(x)lf_out = krs.layers.Dense(10,activation = 'relu')(x)lf_mod = krs.Model(input_lf,lf_out,name='lf')input_hf_lin = krs.Input((14,))x = input_hf_linx = krs.layers.Dense(10)(x)x = krs.layers.Dense(10)(x)hf_lin_out = krs.layers.Dense(10,activation = 'relu')(x)hf_lin_mod = krs.Model(input_hf_lin,hf_lin_out,name='hf_linear')input_hf_nonlin = krs.Input((14,))x = input_hf_nonlinx = krs.layers.Dense(10,activation = 'relu')(x)x = krs.layers.Dense(10,activation = 'relu')(x)hf_nonlin_out = krs.layers.Dense(10,activation = 'relu')(x)hf_nonlin_mod = krs.Model(input_hf_lin,hf_lin_out,name='hf_nonlinear')input_hf = krs.Input((14,))x = input_hflin = hf_lin_mod(x)nonlin = hf_nonlin_mod(x)summed_out = WeightedSum(n_models=2)([lin,nonlin])hf_mod = krs.Model(input_hf,summed_out,name='hf')input_full_mod = krs.Input((4,))x = input_full_modlow = lf_mod(x)x = krs.layers.Concatenate()([low,x])full_out = hf_mod(x)full_mod = krs.Model(input_full_mod,outputs = {'low_fidelity':low,'high_fidelity':full_out},name='full_model')opt = krs.optimizers.Adam()loss = krs.losses.MSEfull_mod.compile(optimizer = opt,loss = loss)x_train = np.random.uniform(0,10,(20,4))y_train_low = np.random.uniform(0,10,(20,10))y_train_high = np.random.uniform(0,10,(20,10))y = {""low_fidelity"": y_train_low,     ""high_fidelity"": y_train_high}full_mod.fit(x_train,y,epochs=5)krs.models.save_model(full_mod,""test_model.mdl"")```EDIT: I goofed during my copy and paste from VIM. I didn't get all of the code orginally.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
51825,0,3622,280,0,1,frgfm,0,"title:Grouped convolutions generate seriously obscure errors on CPU description:Hello there :wave: Today I ran into a cumbersome error that only happens when running on CPU instead of GPUs. I tracked the source of the error to grouped convolutions and managed to make a reproducible minimal snippet. I happened to suspect that it was because of grouped convolutions since I ran into some problems a few days ago with those using SavedModels but it's pure luck. It would be good to improve the error message or even get this fixed if possible :pray: Happy to help provided some directions!**System information**- Have I written custom code: yes, the code snippet- OS Platform and Distribution: Linux Ubuntu 20.04- TensorFlow installed from: binary, via pip- TensorFlow version: 2.5.0- Python version: 3.8- CUDA/cuDNN version: CUDA 11.4 (cuDNN 8.2.0)- GPU model and memory: NVIDIA GeForce RTX 2070 with Max-Q Design**Describe the current behavior**As of now, running the snippet further down below throws an error on CPU but not on GPU.**Describe the expected behavior**Simple:- having a better error (pointing the lack of support of grouped convolutions on CPU)- or even better, if that could get fixed :) **Standalone code to reproduce the issue**```pythonimport tensorflow as tffrom tensorflow.keras import layersfrom tensorflow.keras.models import Sequentialsamples = tf.zeros((1, 256, 256, 3), dtype=tf.float32)model = Sequential([layers.Conv2D(18, padding='same', kernel_size=3, groups=1), layers.GlobalAveragePooling2D(), layers.Dense(1)])trouble_model = Sequential([layers.Conv2D(18, padding='same', kernel_size=3, groups=3), layers.GlobalAveragePooling2D(), layers.Dense(1)])# Backprop on classic modelwith tf.GradientTape() as tape:    out = model(samples, training=True)grads = tape.gradient(out, model.trainable_weights)# Now with grouped convwith tf.GradientTape() as tape:    out = trouble_model(samples, training=True)grads = tape.gradient(out, trouble_model.trainable_weights)```which runs successfully on GPU but on CPU throws the following:```---------------------------------------------------------------------------InvalidArgumentError                      Traceback (most recent call last)<ipython-input-1-e03a8706f9a2> in <module>     19 with tf.GradientTape() as tape:     20     out = trouble_model(samples, training=True)---> 21 grads = tape.gradient(out, trouble_model.trainable_weights)~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)   1072                           for x in nest.flatten(output_gradients)]   1073 -> 1074     flat_grad = imperative_grad.imperative_grad(   1075         self._tape,   1076         flat_targets,~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)     69         ""Unknown value for unconnected_gradients: %r"" % unconnected_gradients)     70 ---> 71   return pywrap_tfe.TFE_Py_TapeGradient(     72       tape._tape,  # pylint: disable=protected-access     73       target,~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)    157       gradient_name_scope += forward_pass_name_scope + ""/""    158     with ops.name_scope(gradient_name_scope):--> 159       return grad_fn(mock_op, *out_grads)    160   else:    161     return grad_fn(mock_op, *out_grads)~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/nn_grad.py in _Conv2DGrad(op, grad)    579   # in Eager mode.    580   return [--> 581       gen_nn_ops.conv2d_backprop_input(    582           shape_0,    583           op.inputs[1],~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py in conv2d_backprop_input(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)   1245       return _result   1246     except _core._NotOkStatusException as e:-> 1247       _ops.raise_from_not_ok_status(e, name)   1248     except _core._FallbackException:   1249       pass~/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)   6895   message = e.message + ("" name: "" + name if name is not None else """")   6896   # pylint: disable=protected-access-> 6897   six.raise_from(core._status_to_exception(e.code, message), None)   6898   # pylint: enable=protected-access   6899 ~/miniconda3/lib/python3.8/site-packages/six.py in raise_from(value, from_value)InvalidArgumentError: Computed input depth 3 doesn't match filter input depth 1 [Op:Conv2DBackpropInput]```
"
51824,1,3134,115,0,0,howl-anderson,0,"title:Keras model with TPUStrategy get InternalError description:When I fit a Keras Model with TPUStrategy, it got following errors:```InternalError: 7 root error(s) found.  (0) Internal: {{function_node __inference_train_function_163744}} failed to connect to all addressesAdditional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0::{""created"":""@1630677855.531027249"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3009,""referenced_errors"":[{""created"":""@1630677855.531025671"",""description"":""failed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":398,""grpc_status"":14}]}	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]Executing non-communication op <MultiDeviceIteratorGetNextFromShard> originally returned UnavailableError, and was replaced by InternalError to avoid invoking TF network error handling logic.	 [[RemoteCall]]	 [[IteratorGetNextAsOptional]]	 [[Pad_9/paddings/_152]]  (1) Internal: {{function_node __inference_train_function_163744}} failed to connect to all addressesAdditional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0::{""created"":""@1630677855.531027249"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3009,""referenced_errors"":[{""created"":""@1630677855.531025671"",""description"":""failed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":398,""grpc_status"":14}]}	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]Executing non-communication op <MultiDeviceIteratorGetNextFromShard> originally returned UnavailableError, and was replaced by InternalError to avoid invoking TF network error handling logic.	 [[RemoteCall]]	 [[IteratorGetNextAsOptional]]	 [[cond_14/switch_pred/_140/_90]]  (2) Internal: {{function_node __inference_train_function_163744}} failed to connect to all addressesAdditional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0::{""created"":""@1630677855.531027249"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3009,""referenced_errors"":[{""created"":""@1630677855.531025671"",""description"":""failed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":398,""grpc_status"":14}]}	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]Executing non-communication op <MultiDeviceIteratorGetNextFromShard> originally returned UnavailableError, and was replaced by InternalError to avoid invoking TF network error handling logic.	 [[RemoteCall]]	 [[IteratorGetNextAsOptional]]	 [[TPUReplicate/_compile/_10521736726166110026/_4/_194]]  (3) Internal: {{function_node __inference_train_function_163744}} failed to connect to all addressesAdditional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:: ... [truncated]```TensorFlow Version: v2.6.0-0-g919f693420e 2.6.0**Standalone code to reproduce the issue**The code used to reproduce this error can be found at https://drive.google.com/file/d/1ceB5VBwFd87Z0ANa_91vhvrOdG4SJQLY/view?usp=sharing
"
51821,1,1179,9,0,0,ysyyork,0,"title:ctc_loss: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found. description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: - TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.4.0- Python version: 3.6.7- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version: CUDA 11.2- GPU model and memory: on CPU**Describe the current behavior**When using ctc_loss with dense tensor, it functions correctly. But when using it with sparse tensor, although the result is the right but it raised a warning saying: ```W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.```I'm not sure if this is a real bug or something else. **Describe the expected behavior**It shouldn't raise the warning. **Standalone code to reproduce the issue**```import tensorflow as tflabel = [	[1, 2, 1, 0, 0],	[1, 1, 0, 0, 0],	[1, 1, 1, 1, 1],]logits = [	[[0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0]],	[[0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0]],	[[0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0]],]labels_length = [3, 2, 5]logits_length = [5, 5, 5]labels_tensor = tf.convert_to_tensor(label, dtype=tf.int32)labels_tensor_sparse = tf.sparse.from_dense(labels_tensor)logits_tensor = tf.convert_to_tensor(logits, dtype=tf.float32)labels_length_tensor = tf.convert_to_tensor(labels_length, dtype=tf.int32)logits_length_tensor = tf.convert_to_tensor(logits_length, dtype=tf.int32)loss_dense = tf.nn.ctc_loss(labels_tensor, logits_tensor, labels_length_tensor, logits_length_tensor, logits_time_major=False)print(loss_dense.numpy()[0])loss_sparse = tf.nn.ctc_loss(labels_tensor_sparse, logits_tensor, None, logits_length_tensor, logits_time_major=False, blank_index=0)print(loss_sparse.numpy()[0])```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.![image](https://user-images.githubusercontent.com/6693605/132005260-d30cd4e5-e7f9-4e52-afc5-7668fd22122b.png)
"
51810,1,0,1,0,0,tanveer6715,0,"title:RuntimeError: tensorflow/lite/kernels/conv.cc:349 input->dims->data[3] != filter->dims->data[3] (64 != 1)Node number 13 (CONV_2D) failed to prepare. description:Hello I am using semantic segmentation model. The model is suclass custom model. It has been trained and saved successfully. i also convert it into tflite version. But when I tried for inference of tflite model it shows the mentioned error in allocating tensors. i have also tried it using tf-nightly 2.7 and tf version 2.5 but it shows same error while allocation tensor. Any help will be highly appreciated. Thanks### 1. System information- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):- TensorFlow version = 2.5:### 2. CodeProvide code to help us reproduce your issues using one of the following options:#### Option A: Reference colab notebooks1)  Reference [TensorFlow Model Colab]https://colab.research.google.com/drive/1v8SvJbMmjTyYVnRBeGwwCGSnRpkpR80p#scrollTo=L_PMowpPmaFx
"
51801,1,2323,0,0,0,nitsanhasson,0,"title:When loading a saved model with multiple outputs, outputs order is not deterministic description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No- TensorFlow installed from (source or binary): pip- TensorFlow version (use command below): 2.5- Python version: 3.6.9- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version: cuda 11.2- GPU model and memory: Nvidia Quadro T1000**Describe the current behavior**I have a model that outputs several tensors, each with a different shape and dtype, let's call it model A.Model's A output tensors are the input of another model B.What I would like to do is to be able to save model A (using model.save(""..."")), and then use it later by loading it.I found out that when loading model A and calling it, the outputs order in non deterministic, which causes a problem when I try to use those outputs as inputs to model B.**Describe the expected behavior**I'd like the model to maintain the original outputs order**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.```import tensorflow as tffrom tensorflow import kerasclass A_Pow(keras.layers.Layer):    def __init__(self, **kwargs):        super(A_Pow, self).__init__(**kwargs)        self.input_spec = tf.keras.layers.InputSpec(shape=(1, 2), dtype=tf.float32)    def call(self, input):        return input * inputclass B_AddX(keras.layers.Layer):    def __init__(self, x, **kwargs):        super(B_AddX, self).__init__(**kwargs)        self.x = x        self.input_spec = tf.keras.layers.InputSpec(shape=(1, 3), dtype=tf.int32)    def call(self, input):        return input + self.xpow_input = tf.keras.Input(shape=(2), batch_size=1, dtype=tf.float32)addx_input = tf.keras.Input(shape=(3), batch_size=1, dtype=tf.int32)pow_output = A_Pow()(pow_input)addx_output = B_AddX(2)(addx_input)model = tf.keras.Model(inputs=[pow_input, addx_input], outputs=[pow_output, addx_output])model_output = model([tf.constant([1,2], shape=(1,2), dtype=tf.float32), tf.constant([3,4,5], shape=(1,3), dtype=tf.int32)])model.compile()model.save(""multiple_outputs_model"")loaded_model = tf.saved_model.load(""multiple_outputs_model"")loaded_model = loaded_model.signatures[""serving_default""]loaded_model_output = loaded_model(input_1=tf.constant([1,2], shape=(1,2), dtype=tf.float32), input_2=tf.constant([3,4,5], shape=(1,3), dtype=tf.int32))print(model_output)print(loaded_model_output)# The order of the outputs in the loaded model is not determoinistic# one run output:#   [<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 4.]], dtype=float32)>, <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[5, 6, 7]], dtype=int32)>]#   {'b__add_x': <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[5, 6, 7]], dtype=int32)>, 'a__pow': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 4.]], dtype=float32)>}# another run output:#   [<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 4.]], dtype=float32)>, <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[5, 6, 7]], dtype=int32)>]#   {'a__pow': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 4.]], dtype=float32)>, 'b__add_x': <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[5, 6, 7]], dtype=int32)>}# The second run has the valid output (a__pow before b_pow)```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
51800,1,1017,15,0,0,letdivedeep,0,"title:Android GPU delegate gives same output from intermediate layers output  description:Hi TeamI have trained a mobilnetv2  model using the tfod API.  Along with the normal detector output (bbox, score,num_detection, confidences), I also wanted an intermediate layers output, so  while converting the model to tflite i specified the intemediate node from where i needed the output ('FeatureExtractor/MobilenetV2/expanded_conv_9/add') as illustrated below : `tflite_convert --graph_def_file=$1/tflite_graph/tflite_graph.pb --output_file=$1/tflite_graph/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3','FeatureExtractor/MobilenetV2/expanded_conv_9/add' --inference_type=FLOAT --allow_custom_ops`the resulted graph is like this : <img width=""1659"" alt=""Screenshot 2021-09-02 at 7 38 20 PM"" src=""https://user-images.githubusercontent.com/74127861/131858728-3b53a171-2e37-4848-ab57-2932f20d52b4.png"">When I run the tflite model on the android with the XNNPack or CPU, I am able to get the different output  of the intermediate layer which is coming at index 4 code snippet : ```try {      Interpreter.Options options = new Interpreter.Options();      options.setNumThreads(NUM_THREADS);      options.setUseXNNPACK(true);      d.tfLite = new Interpreter(modelFile, options);      // 4th index for intermediate output      int[] shape = d.tfLite.getOutputTensor(4).shape();      d.tfLiteOptions = options;    } catch (Exception e) {      throw new RuntimeException(e);    }```But when i run the same model on GPU delegate, I am getting the same output for every different frame I am passing. The other output like (bbox, score,num_detection, confidences) changes but the intermediate output remains the same ```try {      Interpreter.Options options = new Interpreter.Options();      CompatibilityList compatList = new CompatibilityList();      GpuDelegate.Options delegateOptions = compatList.getBestOptionsForThisDevice();      GpuDelegate gpuDelegate = new GpuDelegate(delegateOptions);      options.addDelegate(gpuDelegate);      d.tfLite = new Interpreter(modelFile, options);      // 4th index for intermediate output      int[] shape = d.tfLite.getOutputTensor(4).shape();      d.tfLiteOptions = options;    } catch (Exception e) {      throw new RuntimeException(e);    }```**System information**- Mobile device: OnePlus 6T, Mi Note5 pro- TFOD Api TensorFlow version : 1.15- Android Tensorflow Lite version: 2.4.0Need help to figure out what may be going wrong[detect_10.tflite.zip](https://github.com/tensorflow/tensorflow/files/7099629/detect_10.tflite.zip)
"
51769,1,0,14,0,0,old-school-kid,0,"title:Saving a NSL model description:**NotImplementedError: Saving `AdversarialRegularization` models is currently not supported. Consider using `save_weights` or saving the `base_model`.**I trained a Neural Structured Learning model with AdversarialRegularization and tried to save the model as a SavedModel format as well as a HDF5 model. And in both case I faced this error. Can anyone let me know how to save the model or a workaround.A simple example can be found in this [notebook](https://colab.research.google.com/drive/1CV6uCAys9lcbD33oZv39BvU5AfnHDYya?usp=sharing)
"
51756,1,650,294,0,0,seanshpark,0,"title:[tflite conversion] Invalid file dropped for invalid shape without error description:### 1. System information- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- TensorFlow installation (pip package or built from source): pip package version 2.6.0- TensorFlow library (version, if pip package or github SHA, if built from source): N/A### 2. Code_Provide code to help us reproduce your issues using one of the following options:_Download model:```wget https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_v3_2018_04_27.tgztar zxvf inception_v3_2018_04_27.tgz``````pyimport tensorflow as tfinput_path = ""./inception_v3.pb""output_path = ""./inception_v3.tflite""input_shapes = {""input"": [0, 299, 299, 3]}input_nodes = [""input""]output_nodes = [""InceptionV3/Predictions/Reshape_1""]converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(            input_path, input_nodes, output_nodes, input_shapes)converter.allow_custom_ops = Truetflite_model = converter.convert()open(output_path, ""wb"").write(tflite_model)```I suppose `input_shapes = {""input"": [0, 299, 299, 3]}` is invalid where batch is 0.And expect there be some error reporting.### 3. Failure after conversion_If the conversion is successful, but the generated model is wrong, then state what is wrong:_- tflite model has only input and output nodes without any valid network### 5. (optional) Any other info / logs_Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached._Invalid input shape is a NEGATIVE test to check TensorFlow is working OK.This time it didn't.TF2.3.0 showed this error. But TF 2.6.0 ended without error but dropped invalid tflite file.> loc(""InceptionV3/InceptionV3/Mixed_7c/concat""): error: 'tfl.concatenation' op failed to verify that values and output must have same element typeTraceback (most recent call last):File "".../tensorflow/lite/python/convert.py"", line 199, in toco_convert_protosenable_mlir_converter)File "".../tensorflow/lite/python/wrap_toco.py"", line 38, in wrapped_toco_convertenable_mlir_converter)Exception: :0: error: loc(""InceptionV3/InceptionV3/Mixed_7c/concat""): 'tfl.concatenation' op failed to verify that values and output must have same element type:0: note: loc(""InceptionV3/InceptionV3/Mixed_7c/concat""): see current operation: %0 = ""tfl.concatenation""() {axis = 3 : i32, fused_activation_function = ""NONE""} : () -> tensor<0x8x8x2048xf32>
"
51731,1,0,6,0,0,mgkumar138,0,"title:TF RNN subclass model called twice although sequence length is one description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No- TensorFlow installed from (source or binary): binary- TensorFlow version (use command below): 2.5.0- Python version: 3.8.3- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version: 11.2 / 8- GPU model and memory: RTX 2070 8 GB**Describe the current behavior**I am building a custom RNN layer using tensorflow's model subclass documentation. My input is a 1D vector while the RNN states are also fed into the model at each timestep i.e. my input is generated at each timestep, the RNN's state is obtained from the previous timestep and I need the model output at each timestep. The issue I am facing is that when a single input is given each timestep, the RNN's call function is called twice instead of just once i.e. print('--------- Loop --------') is displayed twice instead of just once when the input is only given once. This becomes a bigger issue as runtime is doubled when I am running a bigger network for a reinforcement learning problem where weights are updated at each timestep instead of the Monte Carlo method.**Describe the expected behavior**The expected behaviour would be that the call function is only called once at each timestep so that print('--------- Loop --------') runs once at each timestep.**Standalone code to reproduce the issue**`    class MinimalRNNCell(tf.keras.layers.Layer):        def __init__(self, units=100, **kwargs):            self.units = units            self.state_size = units            super(MinimalRNNCell, self).__init__(**kwargs)        def build(self, input_shape):            self.kernel = self.add_weight(shape=(input_shape[-1], self.units),                                      initializer='uniform',                                      name='kernel')            self.recurrent_kernel = self.add_weight(            shape=(self.units, self.units),            initializer='uniform',            name='recurrent_kernel')            self.built = True        def call(self, inputs, states):            prev_output = states[0]            h = tf.matmul(inputs, self.kernel)            output = h + tf.matmul(prev_output, self.recurrent_kernel)            print('--------- Loop --------')            return output, [output]    rnncell = MinimalRNNCell()    rnn = tf.keras.layers.RNN(rnncell, return_state=True, return_sequences=False, time_major=False, stateful=False)    x = tf.random.normal(shape=[1,67],stddev=1)    state = tf.random.normal(shape=[1,100],stddev=1)    for t in range(3):        r, state = rnn(x[None,:], state)        print('{} timestep done'.format(t+1))`**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.Run log: --------- Loop ----------------- Loop --------1 timestep done--------- Loop ----------------- Loop --------2 timestep done--------- Loop ----------------- Loop --------3 timestep done
"
51730,1,0,0,0,0,Silvio-Ma,0,"title:Failed to get compute capability major for device: UNKNOWN ERROR (1); 0 description:Dear All,I have installed tensorflow-cpu 2.6.0 on my anaconda. Both Tensorflow and Keras will be imported correctly, but running a cell containing them raises error:> RuntimeError Traceback (most recent call last) in 1 from tensorflow.python.client import device_lib ----> 2 print(device_lib.list_local_devices())> D:\Programs\Anaconda\lib\site-packages\tensorflow\python\client\device_lib.py in list_local_devices(session_config) 43 serialized_config = session_config.SerializeToString() 44 return [ ---> 45 _convert(s) for s in _pywrap_device_lib.list_devices(serialized_config) 46 ]> > RuntimeError: failed to get compute capability major for device: UNKNOWN ERROR (1); 0**System information**- Python 3.8.5 (Anaconda)- Tensorflow 2.6.0- keras 2.6.0- Windows 7 64Thank alotSilvio
"
51729,1,0,0,0,0,MePando,0,"title:Unable to generate train.record, but for text.record it works fine description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): nope- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary): I used the guide at https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html- TensorFlow version (use command below): 2.6.0- Python version: Python 3.8.8- Bazel version (if compiling from source): - GCC/Compiler version (if compiling from source): nope - CUDA/cuDNN version: nope- GPU model and memory: nopeYou can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior** When following the guide command to change format from xml to record, it doesn't work for train but works for text**Describe the expected behavior**it should output something like: Successfully created the TFRecord file: /home/jrhin/Tensorflow/workspace/training_demo/annotations/train.record**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.python generate_tfrecord.py -x ~/Tensorflow/workspace/training_demo/images/train -l ~/Tensorflow/workspace/training_demo/annotations/label_map.pbtxt -o ~/Tensorflow/workspace/training_demo/annotations/train.record**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.Traceback (most recent call last):  File ""generate_tfrecord.py"", line 172, in <module>    tf.app.run()  File ""/home/jrhin/anaconda3/lib/python3.8/site-packages/tensorflow/python/platform/app.py"", line 40, in run    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)  File ""/home/jrhin/anaconda3/lib/python3.8/site-packages/absl/app.py"", line 303, in run    _run_main(main, args)  File ""/home/jrhin/anaconda3/lib/python3.8/site-packages/absl/app.py"", line 251, in _run_main    sys.exit(main(argv))  File ""generate_tfrecord.py"", line 162, in main    tf_example = create_tf_example(group, path)  File ""generate_tfrecord.py"", line 136, in create_tf_example    classes.append(class_text_to_int(row['class']))  File ""generate_tfrecord.py"", line 105, in class_text_to_int    return label_map_dict[row_label]KeyError: 'w'
"
51720,1,1707,68,0,0,River861,0,"title:Inconsistency In CategoricalCrossentropy Calculation description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  **Yes**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**- TensorFlow installed from (source or binary): **binary**- TensorFlow version (use command below):  **2.6.0**- Python version:  **3.6.13**- GPU model and memory:  **CPU-only****Describe the current behavior**The `categorical_crossentropy` loss values calculated with different ways is inconsistent in a model built by myself. 1. In the first way, I calculate the loss value using `model.predict` and `CategoricalCrossentropy`2. In tne second way, I calculate the loss value using `model.evaluate`3. In the third way, I show the loss value using `model.fit`The small model, input data `x`, target data `y_true` and the testing code are here:  [cce_test.zip](https://github.com/tensorflow/tensorflow/files/7068915/cce_test.zip)The three results are: ```14.506287     287.1078     287.1078```Besides, I calculate the loss with the same model and data using `CNTK2.7` and Theano`1.0.4`闂傚倸鍊搁崐鐑芥倿閿旈敮鍋撶粭娑樻噽閻瑩鏌熸潏楣冩闁稿鏅犻弻锝夋晲閸曨垳鍙?their result are both `14.506287`. This makes me more suspicious to the calculation result of tensorflow.**Describe the expected behavior**```14.506287    14.506287     14.506287```**Standalone code to reproduce the issue**```pythonimport numpy as npimport tensorflow as tfmodel = tf.keras.models.load_model(""model.h5"")x = np.load(""x.npy"")y_true = np.load(""y_true.npy"")model.compile(loss='categorical_crossentropy', optimizer='adam')# calculate loss using model.predict and categorical_crossentropycce = tf.keras.losses.CategoricalCrossentropy()y_pred = model.predict(x=x)loss1 = cce(y_true, y_pred)print(""loss1 = "", loss1.numpy())# calculate loss using model.evaluateloss2 = model.evaluate(x=x, y=y_true)print(""loss2 = "", loss2)# show loss using model.fitmodel.fit(x=x, y=y_true)```The result is:```loss1 =  14.5062871/1 [==============================] - 0s 461ms/step - loss: 287.1078loss2 =  287.107849121093751/1 [==============================] - 2s 2s/step - loss: 287.1078```**Other info / logs** Here is the code do the same thing using CNTK2.7:```pythonimport numpy as npimport osos.environ['KERAS_BACKEND'] = 'cntk'import kerasmodel = keras.models.load_model(""model.h5"")x = np.load(""x.npy"")y_true = np.load(""y_true.npy"")model.compile(loss='categorical_crossentropy', optimizer='adam')# calculate loss using model.predict and categorical_crossentropycce = keras.losses.CategoricalCrossentropy()y_pred = model.predict(x=x)loss1 = cce([y_true], [y_pred])print(""loss1 = "", loss1.eval())# calculate loss using model.evaluateloss2 = model.evaluate(x=x, y=y_true)print(""loss2 = "", loss2)# show loss using model.fitmodel.fit(x=x, y=y_true)```The result is:```loss1 =  14.506287convolution engine.10/10 [==============================] - 0s 5ms/steploss2 =  14.50628662109375Epoch 1/110/10 [==============================] - 0s 21ms/step - loss: 14.5063```Any repiies will be appreciated.Thanks.
"
51718,1,1033,35,0,0,marcosrdac,0,"title:Symmetric convolved with symmetric returns non-symmetric description:**System information**- OS Platform and Distribution: Arch Linux 5.13.12- TensorFlow installed from: pip- TensorFlow version: v2.6.0-rc2-32-g919f693420e 2.6.0- Python version: 3.9- CUDA/cuDNN: running on CPU for nowWhen convolving a symmetric kernel with itself I expect the result to be also symmetric. This is what I get when using numpy, scipy or JAX, but for some reason it is not the case when using TensorFlow. I don't know if I could not understand something in the documentation, if it is like this on purpose, but it is surely not obvious. If this is not a bug, why does TensorFlow work like this, and how could I overcome this behaviour to get a 2D cross-correlation/convolution right?**Standalone code to reproduce the issue**See comparison bellow considering a (symmetric) laplacian filter being convolved with itself:```pythonimport numpy as npdef test_convolution_jax(filt):    from jax import numpy as jnp, lax    filt = jnp.asarray(filt)    return lax.conv(filt[:, :, None, None],                    filt[:, :, None, None],                    window_strides=(1, 1),                    padding='SAME')[:, :, 0, 0]def test_convolution_tf(filt):    import tensorflow as tf    filt = tf.Variable(filt)    return tf.nn.convolution(filt[:, :, None, None],                             filt[:, :, None, None],                             strides=(1, 1),                             padding='SAME')[:, :, 0, 0]filt = np.asarray([[0,  1, 0],                   [1, -4, 1],                   [0,  1, 0]],                   dtype=np.float32)print('JAX:', test_convolution_jax(filt), sep='\n', end=2 * '\n')print('TF:', test_convolution_tf(filt), sep='\n',)```Output:```JAX:[[ 1. -4.  1.] [-4. 18. -4.] [ 1. -4.  1.]]TF:tf.Tensor([[ 1. -4.  1.] [-8. 18. -8.] [ 1. -4.  1.]], shape=(3, 3), dtype=float32)```
"
51712,1,13872,22,0,0,OniReimu,0,"title:[tf2.6] Loaded subclassed-Model fails to be retrained description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): -- Dockerfile: ```FROM python:3.8WORKDIR /usr/src/appCOPY requirements.txt ./RUN pip install --no-cache-dir -r requirements.txtCOPY . .CMD [ ""python"", ""./test.py"" ]```-- requirements.txt:```numpytensorflow```and`docker build -t python-docker .`- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not tested- TensorFlow installed from (source or binary): Docker on Mac mini (2018)- TensorFlow version (use command below): TF2.6- Python version: 3.8- Bazel version (if compiling from source): Not tested, unlikely to be related to this issue- GCC/Compiler version (if compiling from source): Not tested, unlikely to be related to this issue- CUDA/cuDNN version: No CUDA on this Mac mini- GPU model and memory: N/A**Describe the current behavior**Typing `docker run -it --rm -v ""$PWD"":/usr/src/app -w /usr/src/app python-docker python test.py` in my environment will end up getting the following error. More specifically, `predict(x, y)` works perfectly, however, `fit(x, y)` is the cause.```ValueError: No gradients provided for any variable: ['abs:0', 'non_distributional_model/dense_1/kernel:0', 'non_distributional_model/dense_1/bias:0', 'non_distributional_model/dense_2/kernel:0', 'non_distributional_model/dense_2/bias:0', 'non_distributional_model/output_layer/kernel:0', 'non_distributional_model/output_layer/bias:0'].```**Describe the expected behavior**The loaded model should be retrainable.**Standalone code to reproduce the issue**```pythonimport numpy as npimport tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras.optimizers import Adamfrom tensorflow.keras.layers import DenseBATCH_SIZE = 2NUM_ACTION = 11STATE_DIM = 1""""""Customer Loss functions.""""""def _huber_loss(y_true, y_pred, max_grad=1.):    a = tf.abs(y_true - y_pred)    less_than_max = 0.5 * tf.square(a)    greater_than_max = max_grad * (a - 0.5 * max_grad)    return tf.where(a <= max_grad, x=less_than_max, y=greater_than_max)@tf.keras.utils.register_keras_serializable()class MeanHuberLoss(keras.losses.Loss):    def __init__(self, name='mean_huber_loss', **kwargs):        super(MeanHuberLoss, self).__init__(name=name, **kwargs)    def call(self, y_true, y_pred):        error = _huber_loss(y_true, y_pred)        # The reduce_mean is automatically done as default        return error@tf.keras.utils.register_keras_serializable()class DirectMappingForAbs(keras.metrics.Metric):    def __init__(self, name='direct_map_for_abs', **kwargs):        super(DirectMappingForAbs, self).__init__(name=name, **kwargs)        self.output_value = tf.Variable(initial_value=[], name='abs', shape=(None, ), validate_shape=False, dtype=tf.float32)    def update_state(self, values, sample_weight=None):        self.output_value.assign(values)    def result(self):        return self.output_value    def reset_state(self):        self.output_value.assign([])@tf.keras.utils.register_keras_serializable()class NonDistributionalModel(keras.Model):    def __init__(self,                  name=""non_distributional_model"",                 num_output=None,                 trainable=True):        super(NonDistributionalModel, self).__init__(name=name)        self.loss_tracker = keras.metrics.Mean(name=""loss"")        self.abs_metric = DirectMappingForAbs(name=""abs"") # Returns a tensor with the same shape of the input tensors        self.criterion = MeanHuberLoss()        self.layer_1 = Dense(10, trainable=trainable, activation='relu', name=""dense_1"")          self.layer_2 = Dense(10, trainable=trainable, activation='relu', name=""dense_2"")        self.output_layer = Dense(num_output, trainable=trainable, activation=None, name=""output_layer"")    def call(self, inputs):        inputs = tf.cast(inputs, tf.float32)        layer_1 = self.layer_1(inputs)        layer_2 = self.layer_2(layer_1)        output = self.output_layer(layer_2)        return output    @tf.function    def train_step(self, data):        states, targets = data        targets = tf.stop_gradient(targets)        with tf.GradientTape() as tape:            logits = self(states, training=True)  # Forward pass            loss = self.criterion(targets, logits)        trainable_vars = self.trainable_variables        grads = tape.gradient(loss, trainable_vars)        self.optimizer.apply_gradients(zip(grads, trainable_vars))        self.loss_tracker.update_state(loss)        self.abs_metric.update_state(tf.cast(tf.reduce_mean(tf.math.abs(targets - logits), axis=-1), tf.float32))                return {""loss"": self.loss_tracker.result(), ""abs"": self.abs_metric.result()}      @property    def metrics(self):        return [self.loss_tracker, self.abs_metric]   class History(keras.callbacks.Callback):    def on_train_begin(self, logs={}):        self.losses = []        self.abs = []    def on_train_batch_end(self, batch, logs={}):        self.losses.append(logs.get('loss'))        self.abs.append(logs.get('abs'))class _DQN_Model:    def __init__(self,                  alpha,                  batch_size,                  num_output,                  trainable=True):        self.alpha = alpha        self.batch_size = batch_size        self.num_output = num_output        self.trainable = trainable        self.model = self._build_model()    def _build_model(self):        lr_schedule = keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=self.alpha,                                                                       first_decay_steps=1000)        model = NonDistributionalModel(num_output=self.num_output, trainable=self.trainable)        model.compile(optimizer=Adam(lr_schedule))        return model    def predict(self, state):        return self.model.predict(state)    def train(self, states, targets):        history = History()        return self.model.fit(states, targets, batch_size=self.batch_size, epochs=1, verbose=0, callbacks=[history])class Critic(object):    def __init__(self, alpha, batch_size):        self.alpha = alpha        self.batch_size = batch_size        self._eval_model = _DQN_Model(alpha=alpha, batch_size=batch_size, num_output=NUM_ACTION, trainable=True)    def learn(self):        x = np.random.random((self.batch_size, 1))        y = np.random.random((self.batch_size, NUM_ACTION))        history = self._eval_model.train(x, y)        return tf.squeeze(history.history['loss'])critic = Critic(alpha=0.1, batch_size=BATCH_SIZE)critic.learn()critic._eval_model.model.save(""model_saved"")loaded_model = keras.models.load_model(""model_saved"")print(""Saved Model Weights: {}, Type: {}"".format(len(critic._eval_model.model.get_weights()), type(critic._eval_model.model)))print(""Loaded Model Weights: {}, Type: {}"".format(len(loaded_model.get_weights()), type(loaded_model)))critic._eval_model.model.summary()loaded_model.summary()x=np.random.random((BATCH_SIZE, 1))y=np.random.random((BATCH_SIZE, NUM_ACTION))# Let's check:np.testing.assert_allclose(    critic._eval_model.predict(x), loaded_model.predict(x))print(""Continue to train the loaded model\n"")history = History()result = loaded_model.fit(x, y, batch_size=BATCH_SIZE, epochs=1, verbose=0, callbacks=[history])```**Other info / logs** ```2021-08-27 12:28:38.619800: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory2021-08-27 12:28:38.619860: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.2021-08-27 12:28:40.667183: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory2021-08-27 12:28:40.667238: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)2021-08-27 12:28:40.667269: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (a3f95dfc3a77): /proc/driver/nvidia/version does not exist2021-08-27 12:28:40.667437: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2021-08-27 12:28:40.722464: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)WARNING:tensorflow:Gradients do not exist for variables ['abs:0'] when minimizing the loss.2021-08-27 12:28:41.133216: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.Saved Model Weights: 9, Type: <class '__main__.NonDistributionalModel'>Loaded Model Weights: 9, Type: <class 'keras.saving.saved_model.load.Custom>NonDistributionalModel'>Model: ""non_distributional_model""_________________________________________________________________Layer (type)                 Output Shape              Param #=================================================================dense_1 (Dense)              multiple                  20_________________________________________________________________dense_2 (Dense)              multiple                  110_________________________________________________________________output_layer (Dense)         multiple                  121=================================================================Total params: 253Trainable params: 251Non-trainable params: 2_________________________________________________________________Model: ""non_distributional_model""_________________________________________________________________Layer (type)                 Output Shape              Param #=================================================================dense_1 (Dense)              multiple                  20_________________________________________________________________dense_2 (Dense)              multiple                  110_________________________________________________________________output_layer (Dense)         multiple                  121=================================================================Total params: 253Trainable params: 251Non-trainable params: 2_________________________________________________________________Continue to train the loaded modelTraceback (most recent call last):  File ""test2.py"", line 182, in <module>    result = loaded_model.fit(x, y, batch_size=BATCH_SIZE, epochs=1, verbose=0, callbacks=[history])  File ""/usr/local/lib/python3.8/site-packages/keras/engine/training.py"", line 1184, in fit    tmp_logs = self.train_function(iterator)  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 885, in __call__    result = self._call(*args, **kwds)  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 933, in _call    self._initialize(args, kwds, add_initializers_to=initializers)  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 759, in _initialize    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3066, in _get_concrete_function_internal_garbage_collected    graph_function, _ = self._maybe_define_function(args, kwargs)  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3463, in _maybe_define_function    graph_function = self._create_graph_function(args, kwargs)  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3298, in _create_graph_function    func_graph_module.func_graph_from_py_func(  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 1007, in func_graph_from_py_func    func_outputs = python_func(*func_args, **func_kwargs)  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 668, in wrapped_fn    out = weak_wrapped_fn().__wrapped__(*args, **kwds)  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 994, in wrapper    raise e.ag_error_metadata.to_exception(e)ValueError: in user code:    /usr/local/lib/python3.8/site-packages/keras/engine/training.py:853 train_function  *        return step_function(self, iterator)    /usr/local/lib/python3.8/site-packages/keras/engine/training.py:842 step_function  **        outputs = model.distribute_strategy.run(run_step, args=(data,))    /usr/local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)    /usr/local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica        return self._call_for_each_replica(fn, args, kwargs)    /usr/local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica        return fn(*args, **kwargs)    /usr/local/lib/python3.8/site-packages/keras/engine/training.py:835 run_step  **        outputs = model.train_step(data)    /usr/local/lib/python3.8/site-packages/keras/engine/training.py:791 train_step        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)    /usr/local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:522 minimize        return self.apply_gradients(grads_and_vars, name=name)    /usr/local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:622 apply_gradients        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)    /usr/local/lib/python3.8/site-packages/keras/optimizer_v2/utils.py:72 filter_empty_gradients        raise ValueError(""No gradients provided for any variable: %s."" %    ValueError: No gradients provided for any variable: ['abs:0', 'non_distributional_model/dense_1/kernel:0', 'non_distributional_model/dense_1/bias:0', 'non_distributional_model/dense_2/kernel:0', 'non_distributional_model/dense_2/bias:0', 'non_distributional_model/output_layer/kernel:0', 'non_distributional_model/output_layer/bias:0'].```Thank you for your time.
"
51710,0,0,7,0,0,pokecheater,0,"title:Separable Conv2D allows dilation_rate!=1 and strides !=1 description:Hey Tensorflow Community :-),Nothing serious :).I just noticed in tensorflow 2.4 (sry can not update to 2.7 currently since I am stuck to this version because of other compatibilty issues, so I can't tell if this also affects your newest version) that the layer Separable Conv2D states in it's documentations that if I assign another value for the dilation rate than 1 the strides are not to be allowed to have another value than 1 and vice versa. _dilation_rate: An integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1._https://keras.io/api/layers/convolution_layers/separable_convolution2d/But in fact it is possible to set both parameters with values > 1. (Since this is also true (and you will get an exception if you try to apply other values than 1 for both parameters) for the normal covolutional layers I think it is either a bug or a documentation copycat issue.)Cheers :)
"
51707,1,0,0,0,0,kurdsh1993,0,"title:7502269190description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary):- TensorFlow version (use command below):- Python version:- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version:- GPU model and memory:You can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior****Describe the expected behavior****[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.[git_test-master.zip](https://github.com/tensorflow/tensorflow/files/7064174/git_test-master.zip)
"
51691,1,0,0,0,0,ArunaKote,0,"title:Error while converting mrcnn to tf dialect description:Actually I wanted to convert mrcnn model to tf dialect .I downloaded model from TensorFlow Hub. and I tried adding signatures to model using import tensorflow.compat.v2 as tfloaded_model = tf.saved_model.load(闂?maskedrcnn闂?call = loaded_model.call.get_concrete_function(tf.TensorSpec(shape=(1, 1024, 1024, 3), dtype=tf.uint8))signatures = {闂傚倸鍊搁崐鐑芥嚄閸洖纾婚柕濞炬櫅绾惧潡鏌ら悡搴☆暢濞存粎妫緀dict闂? call}tf.saved_model.save(loaded_model,闂?ex闂? signatures=signatures)I got warning as Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 125). These functions will not be directly callable after loading.           If I use tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate --savedmodel-objectgraph-to-mlir  /data/maskedrcnn -o sample.mlir I am getting error as tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate: Symbol `_ZN10tensorflow35_DeviceAttributes_default_instance_E' has different size in shared object, consider re-linkingtensorflow/bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate: Symbol `_ZN10tensorflow43_ConfigProto_Experimental_default_instance_E' has different size in shared object, consider re-linkingtensorflow/bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate: Symbol `_ZTVN10tensorflow4data11DatasetBaseE' has different size in shared object, consider re-linkingtensorflow/bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate: symbol lookup error:tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate: undefined symbol: _ZN10tensorflow28CollectiveParamResolverLocal22GetOrCreateInstanceRecEPKNS0_8GroupRecEPNS_16CollectiveParamsEPb                                                                                                                                                                                                                           
"
51686,0,0,177,0,0,maxhgerlach,0,"title:MaybeLockVariableInputMutexesInOrder() from training_op_helpers.h is broken when trying to lock only a subset of the input variables description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04- TensorFlow installed from (source or binary): binary from PyPI- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0- Python version: 3.7.5- CUDA/cuDNN version: 11.2, 8.1**Describe the current behavior**On current master `MaybeLockVariableInputMutexesInOrder()` does not behave correctly if I pass in `std::vector<int>{1}` for the argument `input_ids`.Rather than try to acquire a lock on the variable associated to input id `1` here, it tries to acquire one for that with input id `0`, which may already be locked of course and shouldn't be touched at all by this function.https://github.com/tensorflow/tensorflow/blob/3899a80bafd3fbb95edb717dded9ffd64af58b11/tensorflow/core/kernels/training_op_helpers.h#L176**Describe the expected behavior**It should lock the right variable.At the highlighted line in the source the mutex is pulled in for the wrong input_id. I think it would make more sense here to use the mutex pointer that has already been put into the `mutexes` vector a couple of lines above.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): not really- Briefly describe your candidate solution(if contributing): see under ""describe the expected behavior"", but I haven't tested that
"
51685,1,0,0,0,1,libbooze,0,"title:ABSL_HAVE_ADDRESS_SANITIZER use in Tensorflow headers breaks build for MSVC 16.11 with /fsanitize=address description:**System information**- TensorFlow installed from (source or binary): source, but does not really matter- TensorFlow version (use command below): 2.5**Describe the current behavior**When compiling in MSVC with address sanitizer TF enables code guarded by ABSL_HAVE_ADDRESS_SANITIZER.That guarded code (header includes, calls of functions) break the compile.I would guess this happens since till recently only clang and gcc had sanitizers, and I presume they support all the functionality TF uses. **Describe the expected behavior**Either ABSL_HAVE_ADDRESS_SANITIZER  should be fixed to be 1 only on gcc/clang or TF should use the ABSL_HAVE_ADDRESS_SANITIZER  only when gcc/clang are used. On MSVC this causes a ton of problems.Alternatively ask MSFT if they plan to add all the headers soon, I doubt it, but if they do you can close as WONTFIX.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing): I would discuss this with Abseil team and decide to either fix a macro in Abseil, or define a TF_SOMETHING macro that logically maps to ABSL_HAVE_ADDRESS_SANITIZER==1  &&  compiler_is_not_msvc**Standalone code to reproduce the issue**Include any header that will transitively include headers that use ABSL_HAVE_ADDRESS_SANITIZER  and compile on MSVC 16.11 with /fsanitize=addressExample of problematic header:cpu\include\absl\base\internal\dynamic_annotations.h
"
51680,1,4637,0,0,0,bayesian-mind,0,"title:Activity Regularizer not working with quantization aware training (QAT) description:**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04- TensorFlow installed from (source or binary): pip install- TensorFlow version (or github SHA if from source): TF 2.3**Provide the text output from tflite_convert**```  1/120 [..............................] - ETA: 0s - loss: 2.3153 - accuracy: 0.1040WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.120/120 [==============================] - 1s 11ms/step - loss: 2.2161 - accuracy: 0.3372Quantizing model[<tf.Tensor 'conv2d/ActivityRegularizer_2/truediv:0' shape=() dtype=float32>, <tf.Tensor: shape=(), dtype=float32, numpy=0.00021372623>, <tf.Tensor 'conv2d_1/ActivityRegularizer_2/truediv:0' shape=() dtype=float32>, <tf.Tensor: shape=(), dtype=float32, numpy=0.004322933>]Traceback (most recent call last):  File ""<path to python file>/tempTrain.py"", line 95, in <module>    print(quantized_model.losses.numpy())AttributeError: 'list' object has no attribute 'numpy'```**Standalone code to reproduce the issue** ```import numpy as npimport tensorflow as tffrom tensorflow_model_optimization.python.core.quantization.keras import quantizefrom tensorflow.python import kerasl = keras.layerstf.config.run_functions_eagerly(True)def layers_list():  return [      l.Conv2D(32, 5, padding='same', activation='relu',               input_shape=image_input_shape(), activity_regularizer=tf.keras.regularizers.l2(l=0.0001), kernel_regularizer=tf.keras.regularizers.l2(l=0.0001)),      l.MaxPooling2D((2, 2), (2, 2), padding='same'),      # TODO(pulkitb): Add BatchNorm when transformations are ready.      # l.BatchNormalization(),      l.Conv2D(64, 5, padding='same', activation='relu', activity_regularizer=tf.keras.regularizers.l2(l=0.0001), kernel_regularizer=tf.keras.regularizers.l2(l=0.0001)),      l.MaxPooling2D((2, 2), (2, 2), padding='same'),      l.Flatten(),      l.Dense(1024, activation='relu'),      l.Dropout(0.4),      l.Dense(10, activation='softmax')  ]def sequential_model():  return keras.Sequential(layers_list())def functional_model():  """"""Builds an MNIST functional model.""""""  inp = keras.Input(image_input_shape())  x = l.Conv2D(32, 5, padding='same', activation='relu', activity_regularizer=tf.keras.regularizers.l2(l=0.0001), kernel_regularizer=tf.keras.regularizers.l2(l=0.0001))(inp)  x = l.MaxPooling2D((2, 2), (2, 2), padding='same')(x)  # TODO(pulkitb): Add BatchNorm when transformations are ready.  # x = l.BatchNormalization()(x)  x = l.Conv2D(64, 5, padding='same', activation='relu', activity_regularizer=tf.keras.regularizers.l2(l=0.0001), kernel_regularizer=tf.keras.regularizers.l2(l=0.0001))(x)  x = l.MaxPooling2D((2, 2), (2, 2), padding='same')(x)  x = l.Flatten()(x)  x = l.Dense(1024, activation='relu')(x)  x = l.Dropout(0.4)(x)  out = l.Dense(10, activation='softmax')(x)  return keras.models.Model([inp], [out])def image_input_shape(img_rows=28, img_cols=28):  if tf.keras.backend.image_data_format() == 'channels_first':    return 1, img_rows, img_cols  else:    return img_rows, img_cols, 1def preprocessed_data(img_rows=28,                      img_cols=28,                      num_classes=10):  """"""Get data for mnist training and evaluation.""""""  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()  if tf.keras.backend.image_data_format() == 'channels_first':    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)  else:    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)  x_train = x_train.astype('float32')  x_test = x_test.astype('float32')  x_train /= 255  x_test /= 255  # convert class vectors to binary class matrices  y_train = tf.keras.utils.to_categorical(y_train, num_classes)  y_test = tf.keras.utils.to_categorical(y_test, num_classes)  return x_train, y_train, x_test, y_testmodel = functional_model() #sequential_model()model.summary()x_train, y_train, x_test, y_test = preprocessed_data()model.compile(    loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])model.fit(x_train, y_train, batch_size=500)_, model_accuracy = model.evaluate(x_test, y_test, verbose=0)print(""Quantizing model"")quantized_model = quantize.quantize_model(model)print(quantized_model.losses)quantized_model.compile(    loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])print(quantized_model.losses.numpy())quantized_model.fit(x_train, y_train, batch_size=500)_, quantized_model_accuracy = quantized_model.evaluate(    x_test, y_test, verbose=0)```**Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem.If including tracebacks, please include the full traceback. Large logs and filesshould be attached.
"
51675,1,640,68,0,0,River861,0,"title:A Suspected Bug in binary_crossentropy description:Hi, I found that the the calculation result of `binary_crossentropy` loss function is different from other deep learning libraries, such as theano.Here is an example code using tensorflow2.6.0:```pythonimport numpy as npimport tensorflow as tfy_true = np.array([0., 1., 0.], dtype=np.float32)y_pred = np.array([0.9999999, 0.9999999, 0.0000001], dtype=np.float32)res = tf.keras.backend.binary_crossentropy(y_true, y_pred)print(""loss = "", res.numpy())```The result is:```loss =  [15.333239 -0.       -0.      ]```And this is the code using theano1.0.4:```pythonimport numpy as npfrom theano import tensor as Ty_true = np.array([0., 1., 0.], dtype=np.float32)y_pred = np.array([0.9999999, 0.9999999, 0.0000001], dtype=np.float32)res = T.nnet.binary_crossentropy(y_pred, y_true)print(""loss = "", res.eval())```The result is:```loss =  [1.5942385e+01 1.1920930e-07 1.1920930e-07]```I then found the cause of the inconsistency is that, tensorflow use the `epsilon`  to caculate the loss value,  which I think is redundant because the output has been clipped using `epsilon` eariler. Here is the source code location:https://github.com/tensorflow/tensorflow/blob/fbd7286aba58ba180a2e3c8a280ed5379ee5435d/tensorflow/python/keras/backend.py#L5047-L5052Besides, I found that the `categorical_crossentropy` doesn't use the `epsilon` to caculate the loss value. This makes me more suspicious of the usage of `epsilon` in `binary_crossentropy`:https://github.com/tensorflow/tensorflow/blob/fbd7286aba58ba180a2e3c8a280ed5379ee5435d/tensorflow/python/keras/backend.py#L4906-L4908Thanks.
"
51665,1,7581,22,0,0,OniReimu,0,"title:[tf2.6] Model saving error for a customized model and loss function description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): -- Dockerfile: ```FROM python:3.8WORKDIR /usr/src/appCOPY requirements.txt ./RUN pip install --no-cache-dir -r requirements.txtCOPY . .CMD [ ""python"", ""./test.py"" ]```-- requirements.txt:```numpytensorflow```and`docker build -t python-docker .`- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not tested- TensorFlow installed from (source or binary): Docker on Mac mini (2018)- TensorFlow version (use command below): TF2.6- Python version: 3.8- Bazel version (if compiling from source): Not tested, unlikely to be related to this issue- GCC/Compiler version (if compiling from source): Not tested, unlikely to be related to this issue- CUDA/cuDNN version: No CUDA on this Mac mini- GPU model and memory: N/A**Describe the current behavior**Typing `docker run -it --rm -v ""$PWD"":/usr/src/app -w /usr/src/app python-docker python test.py` in my environment will end up getting the following error, **if the `self.compiled_loss(targets, logits)` and `model.compile(optimizer=Adam(lr_schedule), loss=mean_huber_loss)` are used.**```KeyError: ""Failed to add concrete function b'__inference_train_step_1086' to object based saved model as it captures tensor tf.Tensor(<unprintable>, shape=(), dtype=resource) which is unsupported or not reachable from root. One reason could be that a stateful object or a variable that the function depends on is not assigned to an attribute of the serialized trackable object (see SaveTest.test_captures_unreachable_variable).""```**If instead I use `loss = self.criterion(targets, logits)` and `model.compile(optimizer=Adam(lr_schedule))` everything works perfectly.****Describe the expected behavior**Model should be properly saved to the assigned folder regardless of the two different combinations above.**Standalone code to reproduce the issue**```pythonimport tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras.optimizers import AdamBATCH_SIZE = 2 ** 7NUM_ACTION = 11STATE_DIM = 1def _huber_loss(y_true, y_pred, max_grad=1.):    a = tf.abs(y_true - y_pred)    less_than_max = 0.5 * tf.square(a)    greater_than_max = max_grad * (a - 0.5 * max_grad)    return tf.where(a <= max_grad, x=less_than_max, y=greater_than_max)def mean_huber_loss(y_true, y_pred):    return tf.reduce_mean(_huber_loss(y_true, y_pred))class NonDistributionalModel(keras.Model):    def __init__(self, inputs, outputs):        super(NonDistributionalModel, self).__init__(inputs=inputs, outputs=outputs)        self.loss_tracker = keras.metrics.Mean(name=""loss"")        self.abs_metric = keras.metrics.MeanTensor(name=""abs"") # Returns a tensor with the same shape of the input tensors        self.criterion = mean_huber_loss    @tf.function    def train_step(self, data):        states, targets = data        with tf.GradientTape() as tape:            logits = self(states, training=True)            loss = self.compiled_loss(targets, logits)            # loss = self.criterion(targets, logits)        trainable_vars = self.trainable_variables        gradients = tape.gradient(loss, trainable_vars)        self.optimizer.apply_gradients(zip(gradients, trainable_vars))        self.loss_tracker.update_state(loss)        self.abs_metric.update_state(tf.reduce_mean(tf.math.abs(targets - logits), axis=-1))                return {""loss"": self.loss_tracker.result(), ""abs"": self.abs_metric.result()}         @property    def metrics(self):        # We list our `Metric` objects here so that `reset_states()` can be        # called automatically at the start of each epoch        # or at the start of `evaluate()`.        # If you don't implement this property, you have to call        # `reset_states()` yourself at the time of your choosing.        return [self.loss_tracker, self.abs_metric]   inputs = keras.Input(shape=(STATE_DIM,))outputs = keras.layers.Dense(NUM_ACTION)(inputs)model = NonDistributionalModel(inputs, outputs)lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=0.1,                                                                       first_decay_steps=1000)model.compile(optimizer=Adam(lr_schedule), loss=mean_huber_loss)# model.compile(optimizer=Adam(lr_schedule))x = np.random.random((BATCH_SIZE, 1))y = np.random.random((BATCH_SIZE, NUM_ACTION))model.fit(x, y, batch_size=BATCH_SIZE, epochs=1)model.save('model')```**Other info / logs** ```2021-08-25 08:06:24.613536: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory2021-08-25 08:06:24.613609: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.Memory usage: 0.2776603698730469 GBTotal processing time: 1.830595807s2021-08-25 08:06:26.613800: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory2021-08-25 08:06:26.613940: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)2021-08-25 08:06:26.613981: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (b9da2514de51): /proc/driver/nvidia/version does not exist2021-08-25 08:06:26.614187: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2021-08-25 08:06:26.669676: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)1/1 [==============================] - 1s 567ms/step - loss: 0.1817 - abs: 0.50332021-08-25 08:06:27.407536: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.Traceback (most recent call last):  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/saved_model/function_serialization.py"", line 65, in serialize_concrete_function    bound_inputs.append(node_ids[capture])  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/util/object_identity.py"", line 139, in __getitem__    return self._storage[self._wrap_key(key)]KeyError: <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>During handling of the above exception, another exception occurred:Traceback (most recent call last):  File ""test1.py"", line 238, in <module>    model.save('model')  File ""/usr/local/lib/python3.8/site-packages/keras/engine/training.py"", line 2145, in save    save.save_model(self, filepath, overwrite, include_optimizer, save_format,  File ""/usr/local/lib/python3.8/site-packages/keras/saving/save.py"", line 149, in save_model    saved_model_save.save(model, filepath, overwrite, include_optimizer,  File ""/usr/local/lib/python3.8/site-packages/keras/saving/saved_model/save.py"", line 90, in save    saved_nodes, node_paths = save_lib.save_and_return_nodes(  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py"", line 1228, in save_and_return_nodes    _build_meta_graph(obj, signatures, options, meta_graph_def))  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py"", line 1399, in _build_meta_graph    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py"", line 1362, in _build_meta_graph_impl    object_graph_proto = _serialize_object_graph(  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py"", line 936, in _serialize_object_graph    serialized = function_serialization.serialize_concrete_function(  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/saved_model/function_serialization.py"", line 67, in serialize_concrete_function    raise KeyError(KeyError: ""Failed to add concrete function b'__inference_train_step_1086' to object based saved model as it captures tensor tf.Tensor(<unprintable>, shape=(), dtype=resource) which is unsupported or not reachable from root. One reason could be that a stateful object or a variable that the function depends on is not assigned to an attribute of the serialized trackable object (see SaveTest.test_captures_unreachable_variable).""```Any ideas? Thank you for your time.
"
51657,1,227,4,0,0,CodeMonkey3435,0,"title:Compile TFLite wheel file for Raspberry Pi 4. (Tensorflow operators problem) description:**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Raspbian(Buster) (64 bit version)- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry Pi 4 (1GB RAM, but 2GB Swap memory I think)- TensorFlow installed from (source or binary): trying to install- TensorFlow version: 2.5.0, also TFLite 2..5.0- Python version: 3.7.3- Installed using virtualenv? pip? conda?: virtualenv, pip3- Bazel version (if compiling from source): Build label: 4.2.0- GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0- CUDA/cuDNN version: 11.2/8.1 on ubuntu 20.04- GPU model and memory: GTX 1070 8GBSo I started with a well working model on my computer and wanted to transfer it to the Raspberry Pi. My plan is to use TFLite on the RPI with verson 2.5.0(tflite), as the regular Tensorfow is also running on 2.5.0. My first instinct was to install the wheel file that was downloadable from the tensorflow website. (Here https://github.com/google-coral/pycoral/releases/)I used this:  tflite_runtime-2.5.0.post1-cp37-cp37m-linux_aarch64.whlIt installed fine with pip3 in my venv.Sadly, I get the following error:>     interpreter.invoke()>   File ""/home/pi/.local/lib/python3.7/site-packages/tflite_runtime/interpreter.py"", line 833, in invoke>     self._interpreter.Invoke()> RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 275 (FlexCropAndResize) failed to prepare.I understand, that I am using tensorflow operators that are not included in the tflite library. This is unfortunate but I believe there is one other option: To build the wheel file myself.I found a readme file deep down in tensorflow, located here: tensorflow/tensorflow/lite/tools/pip_package/README.mdI used this part:> ### Cross build with Flex for armhf Python 3.7> > ```sh> CI_DOCKER_EXTRA_PARAMS=""-e CUSTOM_BAZEL_FLAGS=--define=tflite_pip_with_flex=true"" \>   tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \>   tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh armhf> ```All I changed was, ""armhf"" to ""aarch64"" because of 64 bit OS and I added a flag ""--jobs=2"" to keep my computer from freezing up during the process.The process ended with exit 0, so I thought everything should be fine, but then I get this error message when running the same script as with the first error message:> Traceback (most recent call last):>   File ""tfliteTest.py"", line 2, in <module>>     import tflite_runtime.interpreter as tflite>   File ""/home/pi/.local/lib/python3.7/site-packages/tflite_runtime/interpreter.py"", line 42, in <module>>     from tflite_runtime import metrics_portable as metrics> ImportError: cannot import name 'metrics_portable' from 'tflite_runtime' (/home/pi/.local/lib/python3.7/site-packages/tflite_runtime/__init__.py)Please help me, and thank you already for reading this. Also reply if you have another way around this problem, even if I have to take another route.
"
51648,0,5310,0,1,0,SennriSyunnga,0,"title:闂傚倸鍊搁崐椋庢濮橆剦鐒界憸宥堢亱濠电偛妫欓幐鎼佹倿閸偁浜滈柟鐑樺灥閳ь剛鏁诲畷鎴﹀箻鐎靛摜鐣界紓浣哄劋缁岀丢I闂傚倸鍊搁崐椋庢濮橆剦鐒界憸宥堢亱濠电偛妫欓幐鎼佸磼閵娾晜鐓熸い鎺嶇琚galStateException happended while running a model loading from SavedModel and the graph instance cant close itself  description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, i will attach below.- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 &&  Window 10 1909- TensorFlow installed from (source or binary): Java Maven```xml        <dependency>            <groupId>org.tensorflow</groupId>            <artifactId>tensorflow</artifactId>            <version>1.15.0</version>        </dependency>        <dependency>            <groupId>org.tensorflow</groupId>            <artifactId>libtensorflow</artifactId>            <version>1.15.0</version>        </dependency>        <dependency>            <groupId>org.tensorflow</groupId>            <artifactId>libtensorflow_jni_gpu</artifactId>            <version>1.15.0</version>        </dependency>```- TensorFlow version (use command below): 1.15.0**Describe the current behavior**I try to load a Saved Model from keras. It all works well till I try to run the session闂傚倸鍊搁崐鐑芥嚄閸洖纾婚柕濞炬櫅绾惧潡鏌＄仦璇插姎闁藉啰鍠栭弻銊╂偄閸濆嫅銏ゆ煟閹烘垹浠涢柕鍥у楠炴帒顓奸崶褍顬夐梻浣烘嚀閸熸寧娼々姒搉ge thing occurs: the code just can't continue and throw no exception. When i use 'try catch finally' style instead of 'try with resource' style, i finally got such error message below:```java.lang.IllegalStateException: Error while reading resource variable dense_2/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/dense_2/kernel)	 [[{{node dense_2/MatMul/ReadVariableOp}}]]```And what's more, though I got the error message, the graph instance can't close itself,when I paused the test code in idea, i found the code stop at the Object.wait() method闂傚倸鍊搁崐鐑芥倿閿旈敮鍋撶粭娑樻噽閻瑩鏌熸潏楣冩闁稿骸锕﹂埀顒傛嚀鐎氱兘寮查惇淇?means that the graph.refcount kept 1 value all the time. The code couldn't escape from graph.close() method.To prove the correctness of saved model, I try to load model in python code like below:```pythonimport tensorflow as tfimport numpy as npexport_path = ""./test/"";input = np.random.random((1, 30));with tf.Session(graph=tf.Graph()) as sess:    loaded = tf.saved_model.loader.load(sess, [""serve""], export_path)    graph = tf.get_default_graph()    # print(graph.get_operations())    x = sess.graph.get_tensor_by_name('rp_input:0')    y = sess.graph.get_tensor_by_name('dense_2/Sigmoid:0')    scores = sess.run(y,                      feed_dict={x: input});    print(""predict: %d"" % (np.argmax(scores, 1)));```It works well and print predict result, in that case, I think the problem may not lie in the model. (maybe?) I tried hard to find solution or workaround on stackoverflow and issues here,I saw several similar problems to mine, but they all occurs in python, such as :https://github.com/tensorflow/tensorflow/issues/28287and https://github.com/tensorflow/tensorflow/issues/22362the second issues seems most alike, but the model export method is different.**Standalone code to reproduce the issue**Here is my model:[model.zip](https://github.com/tensorflow/tensorflow/files/7036923/model.zip)Here is the test code, because it fails all over the time, i ommit the code to close the resources.```java    public void test_09_justTestAPI() {        float[] a = new float[]{1.53672f, 2.047399f, 1.42194f, 1.494959f, -0.69123f, -0.39482f, 0.236573f, 0.733827f, -0.531855f, -0.973978f, 1.704854f, 2.085134f, 1.615931f, 1.723842f, 0.102458f, -0.017833f, 0.693043f, 1.263669f, -0.217664f, -1.058611f, 1.300499f, 2.260938f, 1.156857f, 1.291565f, -0.42401f, -0.069758f, 0.252202f, 0.808431f, -0.189161f, -0.490556f};        long[] shape = new long[]{1, 30};        try {            SavedModelBundle savedModelBundle = SavedModelBundle.load(""."", ""serve"");            Graph graph = savedModelBundle.graph();            Tensor<Float> data = Tensor.create(shape, FloatBuffer.wrap(a));            Session session = new Session(graph);            Session.Runner runner = session.runner()                    .feed(""rp_input"", data)                    .fetch(""dense_2/Sigmoid"");            float[][] res = new float[1][1];            Tensor<?> out = runner.run().get(0);            out.copyTo(res); // <artifactId>commons-io</artifactId>            BigDecimal pro = BigDecimal.valueOf(res[0][0]);        } catch (Exception e) {            throw e;        }    }```**Other info / logs** The model is produced by webank federal learining program,In their code, the model is build by code using keras api:```pydef _load_model(nn_struct_json):    return tf.keras.models.model_from_json(nn_struct_json, custom_objects={})```The json content is definded like:```json      ""nn_define"": {        ""class_name"": ""Sequential"",        ""config"": {          ""name"": ""sequential"",          ""layers"": [            {              ""class_name"": ""RepeatVector"",              ""config"": {                ""name"":""rp"",                ""n"":1              }            },            {              ""class_name"": ""LSTM"",              ""config"": {                ""name"":""lstm"",                ""units"":32              }            },            {              ""class_name"": ""Dense"",              ""config"": {                ""name"": ""dense"",                ""trainable"": true,                ""dtype"": ""float32"",                ""units"": 64,                ""activation"": ""relu""              }            },            {              ""class_name"": ""Dense"",              ""config"": {                ""name"": ""dense_2"",                ""trainable"": true,                ""dtype"": ""float32"",                ""units"": 1,                ""activation"": ""sigmoid""              }            }          ]        },        ""keras_version"": ""2.2.4-tf"",        ""backend"": ""tensorflow""      }```the model is saved by code below:```py    def export_model(self):        with tempfile.TemporaryDirectory() as tmp_path:            # try:            #     tf.keras.models.save_model(self._model, filepath=tmp_path, save_format=""tf"")            # except NotImplementedError:            #     import warnings            #     warnings.warn('Saving the model as SavedModel is still in experimental stages. '            #                   'trying tf.keras.experimental.export_saved_model...')            tf.keras.experimental.export_saved_model(self._model, saved_model_path=tmp_path)            model_bytes = _zip_dir_as_bytes(tmp_path)        return model_bytes``` You can check the code in this link : https://github.com/FederatedAI/FATE/blob/master/python/federatedml/nn/backend/tf_keras/nn_model.pyIn case, here is the log of my test code:```c2021-08-24 15:11:00.368680: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: .2021-08-24 15:11:00.377471: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }2021-08-24 15:11:00.382175: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX22021-08-24 15:11:00.390552: I tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.2021-08-24 15:11:00.409095: I tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: .2021-08-24 15:11:00.416363: I tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 47658 microseconds.```I really stuck on this problem.I would appreciate it if someone can help me out, many thanks!
"
51646,1,1310,0,0,0,Loganpi,0,"title:Autograph could not transform <function Model.make_test_function.<locals>.test_function at 0x14ec9d430> and will run it as-is. description:**System information**- Have I written custom code:- OS Platform and Distribution : MacOS 11.5.2- TensorFlow installed from TensorFlow_MacOS- TensorFlow version (use command below): 2.4- Python version: 3.8.10- GPU model and memory: Using apple M1 CPUYou can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Occasionally will return during training: ```WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x14c680d30> and will run it as-is.Please report this to the TensorFlow team.```****Describe the expected behavior: no error message****Standalone code to reproduce the issue**Training fashion-MNIST:```from functools import partialDefaultConv2D = partial(keras.layers.Conv2D,                        kernel_size=3, activation='relu', padding=""SAME"")model = keras.models.Sequential([    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),    keras.layers.MaxPooling2D(pool_size=2),    DefaultConv2D(filters=128),    DefaultConv2D(filters=128),    keras.layers.MaxPooling2D(pool_size=2),    DefaultConv2D(filters=256),    DefaultConv2D(filters=256),    keras.layers.MaxPooling2D(pool_size=2),    keras.layers.Flatten(),    keras.layers.Dense(units=128, activation='relu'),    keras.layers.Dropout(0.5),    keras.layers.Dense(units=64, activation='relu'),    keras.layers.Dropout(0.5),    keras.layers.Dense(units=10, activation='softmax'),])model.summary()model.compile(loss=""sparse_categorical_crossentropy"", optimizer=""nadam"", metrics=[""accuracy""])history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))score = model.evaluate(X_test, y_test)X_new = X_test[:10] # pretend we have new imagesy_pred = model.predict(X_new)```Sorry if I have not included the information correctly. I have never submitted a bug before.
"
51643,1,0,7,0,0,jiannanWang,0,"title:tf.math.xdivy decorated with @tf.function returns 0 when the input is large description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): - Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary): source- TensorFlow version (use command below): 2.6.0- Python version: 3.7.11- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version: running on CPU- GPU model and memory: running on CPU**Describe the current behavior**When we feed large inputs to tf.math.xdivy decorated with @tf.function, we get outputs of 0, which is wrong.**Describe the expected behavior**The function returns correct results, just like what it does in eager mode.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**    import pickle    import tensorflow as tf        input_path = ""tf_math_xdivy_inputs.p""    data = pickle.load(open(input_path, 'rb'))        fun = tf.math.xdivy        output1 = fun(**data)     print(output1)  # the results in eager mode is correct        @tf.function    def fun_wrapper(data):        return fun(**data)        output2 = fun_wrapper(data)    print(output2)  # the results of tf function is wrongThe inputs are {'x': array([-3.0127542e+38+0.j], dtype=complex64), 'y': (2.0609168319398798e+37-2.2877970645017637e+38j)}.Output1 is tf.Tensor([-0.11767363-1.3062797j], shape=(1,), dtype=complex64).Output2 is tf.Tensor([-0.+0.j], shape=(1,), dtype=complex64).This is the input for reproduction. Please decompress it before use.[tf_math_xdivy_inputs.p.zip](https://github.com/tensorflow/tensorflow/files/7034126/tf_math_xdivy_inputs.p.zip)We also detect similar issues with other apis. They are: tf.nn.compute_average_loss, tf.realdiv, tf.math.sign. We can provide corresponding inputs if necessary.
"
51640,1,2443,74,0,0,AdityaKane2001,0,"title:Cannot load model checkpoints: Two checkpoint references resolved to different objects description:I am trying to load model checkpoints saved during training. My model has custom layers and the full implementation can be found [here](https://github.com/AdityaKane2001/regnety/issues/15). My model looks as follows:```Model: ""sequential""_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================PreStem (PreStem)            (None, 224, 224, 3)       0         _________________________________________________________________Stem (Stem)                  (None, 111, 111, 32)      992       _________________________________________________________________Stage_0 (Stage)              (None, 56, 56, 24)        4542      _________________________________________________________________Stage_1 (Stage)              (None, 28, 28, 56)        12390     _________________________________________________________________Stage_2 (Stage)              (None, 14, 14, 152)       277400    _________________________________________________________________Stage_3 (Stage)              (None, 7, 7, 368)         2567444   _________________________________________________________________Head (Head)                  (None, 1000)              369000    =================================================================Total params: 3,231,768Trainable params: 3,210,920Non-trainable params: 20,848_________________________________________________________________```When I try to load the model checkpoint after initializing the model architecture, I get the following warning message with an error as described below. ```Two checkpoint references resolved to different objects (<regnety.models.blocks.Stage object at 0x7f3c2049e110> and <regnety.models.blocks.Stage object at 0x7f3c2058ed50>).WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program....(many such messages)...ValueError                                Traceback (most recent call last)<ipython-input-4-0c1dca79eafe> in <module>()      1 from regnety.models import RegNetY      2 ----> 3 model = RegNetY(""200mf"", load_weights=True)      4  (long traceback)/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py in assert_is_compatible_with(self, other)   1159     """"""   1160     if not self.is_compatible_with(other):-> 1161       raise ValueError(""Shapes %s and %s are incompatible"" % (self, other))   1162    1163   def most_specific_compatible_shape(self, other):ValueError: Shapes (56,) and (24,) are incompatible```Gist [here](https://colab.research.google.com/gist/AdityaKane2001/424e9f482c52b2e205d344d6cc27a8d9/groupedconvissue.ipynb?authuser=5). Other info:TF version: 2.6.0Environment: Colab with GPU
"
51635,1,0,6,0,0,chenghuige,0,"title:tf 2.6 break interfance of model.compile, unable to pass tf.keras.optimizers.Adam description:tf 2.6      from tensorflow.python.keras.optimizer_v1 import Optimizer      from tensorflow.python.keras.optimizer_v2 import optimizer_v2      optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)      ic(optimizer)      ic(isinstance(optimizer, (Optimizer, optimizer_v2.OptimizerV2)))ic| optimizer: <keras.optimizer_v2.adam.Adam object at 0x7fc5e03f32b0>ic| isinstance(optimizer, (Optimizer, optimizer_v2.OptimizerV2)): FalseSo we can not pass optimizer to model.compile now as in tensorflow/python/keras/optimizers.py@keras_export('keras.optimizers.get')def get(identifier):  """"""Retrieves a Keras Optimizer instance.  Args:      identifier: Optimizer identifier, one of          - String: name of an optimizer          - Dictionary: configuration dictionary. - Keras Optimizer instance (it            will be returned unchanged). - TensorFlow Optimizer instance (it            will be wrapped as a Keras Optimizer).  Returns:      A Keras Optimizer instance.  Raises:      ValueError: If `identifier` cannot be interpreted.  """"""  if isinstance(identifier, (Optimizer, optimizer_v2.OptimizerV2)):    return identifier
"
51633,1,0,14,0,0,old-school-kid,0,"title:Error in model.fit description:Made a Tensorflow fuctional API model on top of  TFAutoModelForSequenceClassification with 3 sentence as input. Training model directly on tokenized input raises**ValueError: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \'list\'> containing values of types {""<class \'tensorflow.python.framework.ops.EagerTensor\'>""})'}), (<class 'list'> containing values of types {""<class 'int'>""})**If I convert it into numpy array it raises **ValueError: Data cardinality is ambiguous:**`model(X_train[0])`  prduces the desired result in both cases but on training the model it raises errors.Code can be found in this [notebook](https://colab.research.google.com/drive/1wsVVHiaqBF8joIEsP_XSMF35fnDQS19D?usp=sharing)
"
51631,0,0,0,0,0,HannaLochOlszewska,0,"title:DeprecationWarning: the imp module is deprecated in favour of importlib description:- Python version: 3.8.10- Tensorflow version: 2.6.0Hi team,I keep on getting the deprecation warning while using tensorflow:`DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses`I know that's just a warning, but do you plan to update this library?Best!
"
51614,1,1479,79,0,1,cjfghk5697,0,"title:Resource exhausted: EfficientNetB7 OOM description:<h1>1. System information</h1>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ColabTensorFlow installation (pip package or built from source): 2.6.0TensorFlow library (version, if pip package or github SHA, if built from source): all other libraries cloned from https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/<h1>2. Error Code</h1>Full Error code was saved inhttps://github.com/cjfghk5697/Dacon_Code_Review/blob/main/cifar10.ipynbError code```pythonepochs = 40 hist = model.fit(ds_train, epochs=epochs, validation_data=ds_test, verbose=2)````Error```pythonResourceExhaustedError: 2 root error(s) found.  (0) Resource exhausted:  OOM when allocating tensor with shape[64,192,300,300] and type half on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc	 [[node sequential_2/model_2/efficientnetb7/block2a_expand_conv/Conv2D (defined at /lib/python3.7/threading.py:926) ]]Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.	 [[div_no_nan_1/ReadVariableOp/_26]]Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.  (1) Resource exhausted:  OOM when allocating tensor with shape[64,192,300,300] and type half on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc	 [[node sequential_2/model_2/efficientnetb7/block2a_expand_conv/Conv2D (defined at /lib/python3.7/threading.py:926) ]]Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.0 successful operations.0 derived errors ignored. [Op:__inference_train_function_308507]```I use a EfficientNetB7. But EfficientNetB7 fails fit model. Because the Resource exhausted in Colab. So I wish want to know how do fix this error.<h1>3. After an Error</h1>First, I added operations such as batch normalization because the efficientnetb7 parameter was too large to cause oom, but it didn't help.Second, I followed the code on https://stackoverflow.com/questions/49665757/how-to-add-report-tensor-allocations-upon-oom-to-runoptions-in-keras, but the tensorflow version was different, so I couldn't use it.ThirdI've tried similar code on https://stackoverflow.com/questions/64197155/tf2-add-report-tensor-allocations-upon-oom-to-runoptions, but I don't know if this is the solution. The error remained the same.
"
51603,1,0,15,0,0,jcwren,0,"title:Android audio classification example does not work on a Pixel with Android 10 description:## URL(s) with the issue:https://github.com/tensorflow/examples/tree/master/lite/examples/sound_classification/android## Description of issue (what needs changing):Perhaps indicate that although hardware may support Android 6+, it may not be capable enough to run the example.### Clear descriptionThe README.md file indicates that any device supporting Android 6+ with audio support is sufficient. While the example worked fine on my Pixel 4 XL (Android 11), it would not on my Pixel (Android 10). The screen controls are displayed, but it only displays ""Silence"" as the classification, and does not update the bar to the right as it does on my 4 XL. The slider control is also extremely slow to respond, somewhere on the order of 3/4 of a second to a second. I think the hardware just isn't capable enough to run TensorFlow even though it does support Android 10.
"
51600,1,0,0,0,0,arjunskumar,0,"title:TypeError: 'int' object is not callable description:TF Version: `2.6.0`using `tf.random.set_seed(7) `  produce  TypeError: 'int' object is not callable
"
51590,0,2251,9,0,0,neesetifa,0,"title:TypeError: EndVector() takes 1 positional argument but 2 were given description:### 1. System information- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  ArchLinux- TensorFlow installation (pip package or built from source):  pip package   - TensorFlow library (version, if pip package or github SHA, if built from source):  v2.5.0### 2. CodeProvide code to help us reproduce your issues using one of the following options:```import tensorflow as tfimport numpy as npimport pathlibgpus = tf.config.experimental.list_physical_devices(device_type='GPU')if gpus:    for gpu in gpus:        tf.config.experimental.set_memory_growth(device = gpu, enable = True)# Set keras model namekeras_model = ""weight.h5""        # Load MNIST datasetmnist = tf.keras.datasets.mnist(train_images, train_labels), (test_images, test_labels) = mnist.load_data()# Normalize the input image so that each pixel value is between 0 to 1.train_images = train_images.astype(np.float32) / 255.0test_images = test_images.astype(np.float32) / 255.0# Define the model architecturemodel = tf.keras.Sequential([  tf.keras.layers.InputLayer(input_shape=(28, 28)),  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),  tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),    tf.keras.layers.Flatten(),  tf.keras.layers.Dense(10)])# Set training detailsmodel.compile(optimizer='adam',              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),              metrics=['accuracy'])# Train modelmodel.fit(  train_images,  train_labels,  epochs=5,  validation_data=(test_images, test_labels))# Save modelmodel.save_weights(filepath = keras_model, save_format = 'h5')# Load keras model weightmodel.load_weights(filepath = keras_model)# Define representative datasetdef representative_data_gen():  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):    yield [input_value]# Do conversionconverter = tf.lite.TFLiteConverter.from_keras_model(model)converter.optimizations = [tf.lite.Optimize.DEFAULT]converter.representative_dataset = representative_data_genconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]converter.inference_input_type = tf.int8converter.inference_output_type = tf.int8tflite_model_quant = converter.convert()   # FAILED HERE# Save the quantized modeltflite_models_dir = pathlib.Path("""")tflite_model_quant_file = tflite_models_dir/tflite_modeltflite_model_quant_file.write_bytes(tflite_model_quant)```### 3. Failure after conversionNot able to convert### 4. (optional) RNN conversion supportN/A### 5. (optional) Any other info / logs2021-08-20 16:27:43.366934: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.fully_quantize: 0, inference_type: 6, input_inference_type: 9, output_inference_type: 9Traceback (most recent call last):  File ""/home/xxx/Desktop/TFLite_practice/convert_tflite.py"", line 56, in <module>    tflite_model_quant = converter.convert()  File ""/usr/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 1057, in convert    result = super(TFLiteKerasModelConverterV2,  File ""/usr/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 800, in convert    result = _modify_model_io_type(result, **flags_modify_model_io_type)  File ""/usr/lib/python3.9/site-packages/tensorflow/lite/python/util.py"", line 906, in modify_model_io_type    return _convert_model_from_object_to_bytearray(model_object)  File ""/usr/lib/python3.9/site-packages/tensorflow/lite/python/util.py"", line 556, in _convert_model_from_object_to_bytearray    model_offset = model_object.Pack(builder)  File ""/usr/lib/python3.9/site-packages/tensorflow/lite/python/schema_py_generated.py"", line 5630, in Pack    operatorCodes = builder.EndVector(len(self.operatorCodes))TypeError: EndVector() takes 1 positional argument but 2 were given
"
51589,1,0,1,0,0,wangwei-cmd,0,"title:google.protobuf.message.DecodeError: Error parsing message when using tf.function description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 21.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary): conda binary- TensorFlow version (use command below): tf.2.5- Python version: python 3.8- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version: cuda 11.4, cudatoolkit 11.0.221,  cudnn 8.2.1.32- GPU model and memory: NVIDIA TITAN RTX, 24GbYou can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`Successfully opened dynamic library libcudart.so.11.0 v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0**Describe the current behavior**I write a custom model involving the tf.gather_nd function. When the 'train_step' function is not decorated   by 'tf.function', the model can be well-trained. But when I use 'tf.function' to decorate the 'train_step' function, I get the error 'google.protobuf.message.DecodeError: Error parsing message'.**Describe the expected behavior**The model can be trained.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing): no**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.import tensorflow as tfimport numpy as npclass GatherModel(tf.keras.Model):    def __init__(self,ind1,w1):        super(GatherModel, self).__init__()        self.ind1=ind1        self.w1=tf.cast(w1,tf.float32)        self.lambda1 = tf.Variable(initial_value=tf.constant(0.1), trainable=True, name='lambda1')    def __call__(self, inputs,training=0):        y=inputs        for i in range(5):            y=tf.transpose(y, [1, 2, 3, 0])            y=tf.gather_nd(y*1.0, self.ind1)            y=y*self.w1            y=tf.reduce_sum(y,0)            y=tf.transpose(y,[3,0,1,2])            y = self.lambda1*y        return y# @tf.functiondef train_step(model, inputs, labels, Loss, optimizer):    with tf.GradientTape() as tape:        predictions = model(inputs, training=1)        loss = Loss(labels, predictions)    grads = tape.gradient(loss, model.trainable_variables)    optimizer.apply_gradients(zip(grads, model.trainable_variables))    return lossif __name__ == '__main__':    ind1=np.random.randint(0,300,[367, 217, 721, 2])    w1=np.random.normal(size=[367, 217, 721, 1, 1])    Model=GatherModel(ind1,w1)    inputs=tf.random.normal([2,256,256,1])    labels= tf.random.normal([2,217,721,1])    loss=tf.keras.losses.MeanSquaredError()    optimizer=tf.keras.optimizers.Adam(0.001)    for i in range(10):        LL=train_step(Model,inputs,labels,loss,optimizer)        print(LL)  In the google colab https://colab.research.google.com/drive/1TrTctTKjYOIrZ2rvKhQmdTDS77d0gmg9#scrollTo=WMcegEVFWW_M, it says that the program crashed because of  exhausting of RAM.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
51588,1,1463,1,0,0,wangwei-cmd,0,"title:google.protobuf.message.DecodeError: Error parsing message when using tf.function description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 21.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow installed from (source or binary): conda binary- TensorFlow version (use command below): tf.2.5- Python version: python 3.8- Bazel version (if compiling from source):- GCC/Compiler version (if compiling from source):- CUDA/cuDNN version: cuda 11.4, cudatoolkit 11.0.221,  cudnn 8.2.1.32- GPU model and memory: NVIDIA TITAN RTX, 24GbYou can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`Successfully opened dynamic library libcudart.so.11.0 v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0**Describe the current behavior**I write a custom model involving the tf.gather_nd function. When the 'train_step' function is not decorated   by 'tf.function', the model can be well-trained. But when I use 'tf.function' to decorate the 'train_step' function, I get the error 'google.protobuf.message.DecodeError: Error parsing message'.**Describe the expected behavior**The model can be trained.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no):- Briefly describe your candidate solution(if contributing): no**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.```import tensorflow as tfimport numpy as npclass GatherModel(tf.keras.Model):    def __init__(self,ind1,w1):        super(GatherModel, self).__init__()        self.ind1=ind1        self.w1=tf.cast(w1,tf.float32)        self.lambda1 = tf.Variable(initial_value=tf.constant(0.1), trainable=True, name='lambda1')    def __call__(self, inputs,training=0):        y=inputs        for i in range(5):            y=tf.transpose(y, [1, 2, 3, 0])            y=tf.gather_nd(y*1.0, self.ind1)            y=y*self.w1            y=tf.reduce_sum(y,0)            y=tf.transpose(y,[3,0,1,2])            y = self.lambda1*y        return y# @tf.functiondef train_step(model, inputs, labels, Loss, optimizer):    with tf.GradientTape() as tape:        predictions = model(inputs, training=1)        loss = Loss(labels, predictions)    grads = tape.gradient(loss, model.trainable_variables)    optimizer.apply_gradients(zip(grads, model.trainable_variables))    return lossif __name__ == '__main__':    ind1=np.random.randint(0,300,[367, 217, 721, 2])    w1=np.random.normal(size=[367, 217, 721, 1, 1])    Model=GatherModel(ind1,w1)    inputs=tf.random.normal([2,256,256,1])    labels= tf.random.normal([2,217,721,1])    loss=tf.keras.losses.MeanSquaredError()    optimizer=tf.keras.optimizers.Adam(0.001)    for i in range(10):        LL=train_step(Model,inputs,labels,loss,optimizer)        print(LL)```In the google colab   [https://colab.research.google.com/drive/1TrTctTKjYOIrZ2rvKhQmdTDS77d0gmg9#scrollTo=WMcegEVFWW_M](url), it says that the program is crashed because of  exhausting of RAM.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
51584,1,2037,3,0,0,sjtusmartboy,0,"title:how to record loss with tf.summary in the tf.function graph mode description:**tensorflow 2.5, python 3.7**```import tensorflow as tfimport numpy as npimport datetimeclass Dense(tf.Module):    def __init__(self, input_dim, output_size, name=None):        super(Dense, self).__init__(name=name)        self.w = tf.Variable(            tf.random.normal([input_dim, output_size]), name='w')        self.b = tf.Variable(tf.zeros([output_size]), name='b')    def __call__(self, x):        y = tf.matmul(x, self.w) + self.b        return tf.nn.relu(y)model = Dense(2,4)class Test(object):    def __init__(self):        self.output = model([[7.0, 3]])        self.optimizer = tf.compat.v1.train.AdamOptimizer(0.4)        self.step = 0        stamp = datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")        logdir = 'logs/test8/%s' % stamp        self.summary_writer = tf.summary.create_file_writer(logdir)        tf.summary.trace_on(graph=True, profiler=False)        for i in range(10):            self.run()    def compare(self, y_true, output):        return tf.square(y_true - output)    def loss_fn(self):        y_true = tf.ones([1,4])        self.output = model([[7.0, 3]])        comp = tf.py_function(self.compare, [y_true , self.output], tf.float32)        loss = tf.reduce_mean(comp) # error        # output2 = model([[7.0, 3]])        # loss = tf.reduce_mean(tf.square(y_true - output2))        tf.print(loss)        tf.print(self.step)        with self.summary_writer.as_default():            tf.summary.scalar('loss', loss, step=self.step)        self.step += 1        return loss    # @tf.function(autograph=True, jit_compile=True)    @tf.function()    def run(self):        train_op = self.optimizer.minimize(self.loss_fn)Test()```As can be seen from the output, the variable `self.step` is not updated in `def loss_fn(self)`. The reason is simple from what is said in https://tensorflow.google.cn/tensorboard/migrate, ```The ""step"" value must be passed into each op via a the step argumentTensorBoard requires a step value to render the data as a time seriesExplicit passing is necessary because the global step from TF 1.x has been removed, so each op must know the desired step variable to read```However, according to https://tensorflow.google.cn/api_docs/python/tf/compat/v1/train/AdamOptimizer?hl=en&version=nightly#expandable-3, `When eager execution is enabled, loss should be a Python function that takes no arguments and computes the value to be minimized`.Definitely these two announcement contradict to each other when trying to use `tf.summary` in `def loss_fn(self)`. Any solutions? I just want to record the loss with `tf.summary`.
"
51569,1,1706,0,0,0,TrentHawkins,0,"title:`ModuleNotFoundError: No module named 'keras'` in most tensorflow calls description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): stock script- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Manjaro Pahvo 21.1.0- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -- TensorFlow installed from (source or binary): binary (official arch repository)- TensorFlow version (use command below): 2.6.0- Python version: 3.9.6- Bazel version (if compiling from source): -- GCC/Compiler version (if compiling from source): -- CUDA/cuDNN version: 11.4.0-1/8.2.2.26-1- GPU model and memory: Nvidia RTX 3060 Ti (8GB)You can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**Most (if not all) of the library calls give a `ModuleNotFoundError: No module named 'keras'`.**Describe the expected behavior**All library calls should work without this import relative reference problem. **[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no- Briefly describe your candidate solution(if contributing): A workaround for now is using `import tensorflow.keras as keras`**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.```import tensorflow as tfprint(tf.version.VERSION)fashion_mnist = tf.keras.datasets.fashion_mnist```(code from: [https://www.tensorflow.org/tutorials/keras/classification](https://www.tensorflow.org/tutorials/keras/classification))**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.```Traceback (most recent call last):File ""<stdin>"", line 1, in <module>File ""/usr/lib/python3.9/site-packages/tensorflow/python/util/lazy_loader.py"", line 62, in __getattr__module = self._load()File ""/usr/lib/python3.9/site-packages/tensorflow/python/util/lazy_loader.py"", line 45, in _loadmodule = importlib.import_module(self.__name__)File ""/usr/lib/python3.9/importlib/__init__.py"", line 127, in import_modulereturn _bootstrap._gcd_import(name[level:], package, level)File ""<frozen importlib._bootstrap>"", line 1030, in _gcd_importFile ""<frozen importlib._bootstrap>"", line 1007, in _find_and_loadFile ""<frozen importlib._bootstrap>"", line 972, in _find_and_load_unlockedFile ""<frozen importlib._bootstrap>"", line 228, in _call_with_frames_removedFile ""<frozen importlib._bootstrap>"", line 1030, in _gcd_importFile ""<frozen importlib._bootstrap>"", line 1007, in _find_and_loadFile ""<frozen importlib._bootstrap>"", line 972, in _find_and_load_unlockedFile ""<frozen importlib._bootstrap>"", line 228, in _call_with_frames_removedFile ""<frozen importlib._bootstrap>"", line 1030, in _gcd_importFile ""<frozen importlib._bootstrap>"", line 1007, in _find_and_loadFile ""<frozen importlib._bootstrap>"", line 972, in _find_and_load_unlockedFile ""<frozen importlib._bootstrap>"", line 228, in _call_with_frames_removedFile ""<frozen importlib._bootstrap>"", line 1030, in _gcd_importFile ""<frozen importlib._bootstrap>"", line 1007, in _find_and_loadFile ""<frozen importlib._bootstrap>"", line 984, in _find_and_load_unlockedModuleNotFoundError: No module named 'keras'```(output of code snippet above)
"
51555,1,102,0,0,0,EthanIrby8,0,"title:InvalidArgumentError:  indices[15,28] = 22105 is not in [0, 22015)  [[node encoder/embedding/embedding_lookup  description:I am using tf 2.4.1, keras 2.4.3, and python 3.8.10 on CPU. I have downloaded pretrained GloVe word embeddings to train an Encoder-Decoder Model for abstractive summarization. I have created two embedding matrices, one to represent the vocabulary of the source input documents and one to represent the summary vocabulary. I am a little unsure if this is the correct practice but the model is able to train on 100 batches until the bug in the title stops training. Currently, the embedding matrix is of size 22015, 200. The first dimension represents the indices of the word embeddings and the second dimension represents the embedding dimension size for each word. The size of the encoder (source documents) vocabulary is taken directly from the embedding matrix = 22015. The summary vocabulary size is 7932 which is also the size of the first dimension of the summary embedding matrix. I should note that the original vocabulary sizes taken directly from dictionaries are different though come before the implementation of glove. The Decoder model is expected to output a probability distribution over the summary vocabulary (7932 ""classes""). It seems that the decoder output head is functioning as intended but the error occurs when the encoder starts to work on its word embeddingsCODE to SIMULATE issue`from keras.layers import Embedding````encoder_vocab_size = 22015emb_dim = 200``````input_dim = encoder_vocab_size output_dim = emb_dim```- Setting the weights of the Embedding layer to pretrained GloVe matrix (22015x200)`weights = [embmatrix]`- The Encoder uses this embedding layer with exact configuration`embedding = Embedding(input_dim, output_dim, input_length=200, weights=weights, is_trainable=False)`- Training the Encoder-Decoder Model``for` epoch in range(25):`    ``for` batch, (encoder_inp_batch, decoder_input_batch, target_input_batch) in enumerate(generator):`        - encoder_inp_batch is shape (batch_size, 200)        - decoder_input_batch is shape (batch_size, 49)        - target_input_batch is shape (batch_size, 49)        with tf.GradientTape() as tape:            - Code breaks here AFTER successfully training over 100 batches of data            encoder_output, encoder_hidden_states = encoder(encoder_inp_batch)The loss used is sparse_categorical_crossentropy as targets are not one-hot encoded. Data is passed to the model in batches of 16. Number of data points used for training ~ 5,000. Still trying to pinpoint where exactly the embedding index out of bounds error might lie. Is it an issue arising from indices in the encoder embedding matrix or does the index out of bounds occur when feeding sequences of encoder training data where indices might be greater than the allowable range up to 22015
"
51541,0,3162,0,0,0,ggous,0,"title:training accuracy different from history object description:Hi , I am dealing with a problem on the accuracy history.As you can see, during training, we have:```Epoch 1/10597/597 [==============================] - 4s 3ms/step - loss: 0.6822 - accuracy: 0.7007 - val_loss: 0.6607 - val_accuracy: 0.9228Epoch 2/10597/597 [==============================] - 1s 2ms/step - loss: 0.6675 - accuracy: 0.7838 - val_loss: 0.6463 - val_accuracy: 0.9228Epoch 3/10597/597 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.8284 - val_loss: 0.6329 - val_accuracy: 0.9228Epoch 4/10597/597 [==============================] - 2s 3ms/step - loss: 0.6408 - accuracy: 0.8598 - val_loss: 0.6204 - val_accuracy: 0.9228Epoch 5/10597/597 [==============================] - 1s 2ms/step - loss: 0.6287 - accuracy: 0.8876 - val_loss: 0.6087 - val_accuracy: 0.9228Epoch 6/10597/597 [==============================] - 1s 2ms/step - loss: 0.6174 - accuracy: 0.9024 - val_loss: 0.5978 - val_accuracy: 0.9228Epoch 7/10597/597 [==============================] - 1s 2ms/step - loss: 0.6068 - accuracy: 0.9103 - val_loss: 0.5875 - val_accuracy: 0.9228Epoch 8/10597/597 [==============================] - 1s 2ms/step - loss: 0.5968 - accuracy: 0.9146 - val_loss: 0.5779 - val_accuracy: 0.9228Epoch 9/10597/597 [==============================] - 1s 2ms/step - loss: 0.5874 - accuracy: 0.9146 - val_loss: 0.5688 - val_accuracy: 0.9228Epoch 10/10597/597 [==============================] - 1s 2ms/step - loss: 0.5785 - accuracy: 0.9146 - val_loss: 0.5603 - val_accuracy: 0.9228```So, accuracy reaches 0.9146.But, If you see the accuracy values in the history:```history.history['accuracy'][0.7472780346870422, 0.7945979833602905, 0.8073701858520508, 0.8146985173225403, 0.82097989320755, 0.8243299722671509, 0.8255862593650818, 0.8262144327163696, 0.8262144327163696, 0.8262144327163696]```are different!Code:```import pandas as pdimport numpy as npimport tensorflow as tfimport matplotlib.pyplot as pltfrom tensorflow.keras.layers import Dense, LSTM, \    Bidirectional, RepeatVectorfrom tensorflow.keras import SequentialX_train = np.load('./X_train.npy')X_val = np.load('./X_val.npy')y_train = np.load('./y_train.npy')y_val = np.load('./y_val.npy')model = Sequential()model.add(Bidirectional(LSTM(2,                        return_sequences=False,                        activation='tanh',                    input_shape=(12,                                 10))))model.add(RepeatVector(8))# Decodermodel.add((LSTM(4,                activation='tanh',                return_sequences=True)))model.add((Dense(1, activation='sigmoid')))                    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.0001),               loss='binary_crossentropy',              metrics=['accuracy'])epochs = 10batch_size = 1history = model.fit(    X_train,    y_train,     validation_data = (X_val, y_val),    steps_per_epoch=int(len(X_train) / batch_size),    validation_steps=int(len(X_val) / batch_size),    epochs=epochs,    batch_size=batch_size,    shuffle=False)fig, axes = plt.subplots(figsize=(20, 12))axes.plot(history.epoch, history.history['accuracy'], label = 'Train acc')axes.plot(history.epoch, history.history['val_accuracy'], label = 'Val acc')axes.legend()```You can download data [here](https://easyupload.io/m/5iv09z)<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**You can see it at the end**Describe the current behavior** Different training accuracy results between history object and history when printing on screen**Describe the expected behavior** We should have the same results!**Standalone code to reproduce the issue**At the beginning                  **System Info**---------------------------------------------------------------------------------------------------------------------------------------------------------------------------== check python ===================================================python version: 3.8.10python branch: python build version: ('default', 'Jun  2 2021 10:49:15')python compiler version: GCC 9.4.0python implementation: CPython== check os platform ================================================= are we in docker =============================================No== compiler =====================================================c++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0Copyright (C) 2019 Free Software Foundation, Inc.This is free software; see the source for copying conditions.  There is NOwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.== check pips ===================================================protobuf                3.6.1               == check for virtualenv =========================================False== env ==========================================================LD_LIBRARY_PATH is unsetDYLD_LIBRARY_PATH is unset== nvidia-smi ===================================================Wed Aug 18 09:48:39 2021       +-----------------------------------------------------------------------------+| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     ||-------------------------------+----------------------+----------------------+| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. ||                               |                      |               MIG M. ||===============================+======================+======================||   0  GeForce GTX 1650    On   | 00000000:01:00.0  On |                  N/A || 24%   37C    P8    10W /  90W |   1096MiB /  3907MiB |      9%      Default ||                               |                      |                  N/A |+-------------------------------+----------------------+----------------------+                                                                               +-----------------------------------------------------------------------------+| Processes:                                                                  ||  GPU   GI   CI        PID   Type   Process name                  GPU Memory ||        ID   ID                                                   Usage      ||=============================================================================||    0   N/A  N/A      1085      G   /usr/lib/xorg/Xorg                275MiB ||    0   N/A  N/A      1727      G   cinnamon                           74MiB ||    0   N/A  N/A      3228      G   ...AAAAAAAAA= --shared-files       30MiB ||    0   N/A  N/A      3715      G   /usr/lib/firefox/firefox          125MiB ||    0   N/A  N/A      3834      G   /usr/lib/firefox/firefox            1MiB ||    0   N/A  N/A      4004      G   /usr/lib/firefox/firefox            1MiB ||    0   N/A  N/A      4518      G   ...onda3/envs/dpl/bin/python        1MiB ||    0   N/A  N/A      4593      C   ...onda3/envs/dpl/bin/python      577MiB |+-----------------------------------------------------------------------------+== cuda libs  ===================================================== tensorflow installed from info ==================== python version  ==============================================(major, minor, micro, releaselevel, serial)(3, 8, 10, 'final', 0)== bazel version  ================================================= check python ===================================================python version: 3.8.8python branch: python build version: ('default', 'Feb 24 2021 21:46:12')python compiler version: GCC 7.3.0python implementation: CPython== check os platform ================================================= are we in docker =============================================No== compiler =====================================================c++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0Copyright (C) 2019 Free Software Foundation, Inc.This is free software; see the source for copying conditions.  There is NOwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.== check pips ===================================================neptune-tensorflow-keras          0.9.1numpy                             1.19.2numpydoc                          1.1.0protobuf                          3.14.0tensorflow                        2.4.1tensorflow-datasets               1.2.0tensorflow-estimator              2.4.0tensorflow-metadata               0.14.0== check for virtualenv =========================================False== tensorflow import ============================================tf.version.VERSION = 2.4.1tf.version.GIT_VERSION = unknowntf.version.COMPILER_VERSION = 5.4.0      8883:	find library=libpthread.so.0 [0]; searching== nvidia-smi ===================================================Wed Aug 18 09:49:20 2021       +-----------------------------------------------------------------------------+| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     ||-------------------------------+----------------------+----------------------+| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. ||                               |                      |               MIG M. ||===============================+======================+======================||   0  GeForce GTX 1650    On   | 00000000:01:00.0  On |                  N/A || 24%   36C    P8    10W /  90W |   1107MiB /  3907MiB |      6%      Default ||                               |                      |                  N/A |+-------------------------------+----------------------+----------------------+                                                                               +-----------------------------------------------------------------------------+| Processes:                                                                  ||  GPU   GI   CI        PID   Type   Process name                  GPU Memory ||        ID   ID                                                   Usage      ||=============================================================================||    0   N/A  N/A      1085      G   /usr/lib/xorg/Xorg                284MiB ||    0   N/A  N/A      1727      G   cinnamon                           77MiB ||    0   N/A  N/A      3228      G   ...AAAAAAAAA= --shared-files       30MiB ||    0   N/A  N/A      3715      G   /usr/lib/firefox/firefox          125MiB ||    0   N/A  N/A      3834      G   /usr/lib/firefox/firefox            1MiB ||    0   N/A  N/A      4004      G   /usr/lib/firefox/firefox            1MiB ||    0   N/A  N/A      4518      G   ...onda3/envs/dpl/bin/python        1MiB ||    0   N/A  N/A      4593      C   ...onda3/envs/dpl/bin/python      577MiB |+-----------------------------------------------------------------------------+== cuda libs  ===================================================== tensorflow installed from info ==================Name: tensorflowVersion: 2.4.1Summary: TensorFlow is an open source machine learning framework for everyone.Home-page: https://www.tensorflow.org/Author-email: packages@tensorflow.orgLicense: Apache 2.0Location: /home/ggousios/miniconda3/envs/dpl/lib/python3.8/site-packagesRequired-by: neptune-tensorflow-keras, neptune-tensorboard== python version  ==============================================(major, minor, micro, releaselevel, serial)(3, 8, 8, 'final', 0)== bazel version  ===============================================
"
51539,1,314,300,0,0,AnranXu,0,"title:Cannot convert models to tensorflowjs from tf hub by url description:I am using tensorflowjs_converter to convert a tflite model to tensorflowjs model.The repository can be download with web browsers but failed when using tensorflowjs_converter.I just redo the ```pip install tensorflowjs``` to make sure that I am in the latest version.```tensorflowjs_converter --input_format=tf_hub 'https://tfhub.dev/sayakpaul/lite-model/arbitrary-image-stylization-inceptionv3/dr/predict/1' ./web_model```It outputs an Http 404 but my connection is fine. When I run the example code it works fine and I downloaded it.```tensorflowjs_converter --input_format=tf_hub 'https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/classification/1' ./web_model  ```If you can tell me the reason or you can successfully convert tflite model from the following URLs, I will appreciate it a lot. Thank you so much. https://tfhub.dev/sayakpaul/lite-model/arbitrary-image-stylization-inceptionv3/dr/predict/1https://tfhub.dev/sayakpaul/lite-model/cartoongan/dr/1
"
51528,0,411,300,0,0,Qrox,0,"title:`tf.GradientTape.batch_jacobian` fails when slicing tensor with `int64` index description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow installed from (source or binary): Binary- TensorFlow version (use command below): 2.5.0- Python version: 3.7.9- Bazel version (if compiling from source): N/A- GCC/Compiler version (if compiling from source): N/A- CUDA/cuDNN version: 11.1.0- GPU model and memory: NVIDIA GeForce RTX 2080 Ti**Describe the current behavior**If the target tensor of `batch_jacobian` is repeated and then sliced using an `int64` index, `batch_jacobian` fails due to concatenating shape vectors of different types. This can happen when `tf.RaggedTensor` is involved in calculation because `tf.RaggedTensor.row_lengths` returns an `int64` tensor.The cause seems to be that `ops\parallel_for\pfor.py:2474` concatenates the batch size and the operation shape without checking the types. The batch size is retrieved at `eager\backprop.py:1293` as an `int32` value, so when the operation involves `int64` indices, it results in an error.**Describe the expected behavior**It should succeed whatever type is used to slice a tensor.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): No- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**```pythonimport tensorflow as tfx = tf.RaggedTensor.from_row_lengths([[1.0], [2.0], [3.0]], [1, 2])length = x.row_lengths();# length = tf.cast(length, tf.int32)print('Type of length is', length.dtype)x = x.to_tensor()with tf.GradientTape() as tape:    tape.watch(x)    y = tf.repeat(x, [2], axis=1)    y = y[:, :tf.math.reduce_max(length), :]g = tape.batch_jacobian(y, x)print('g =', g)```Running the code results in `TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [int32, int64] that don't all match.`If I uncomment line 5 it runs correctly.If I comment out the `tf.repeat` line it also runs correctly.
"
51509,1,2268,0,0,0,fm966mhz,0,"title:Can't create multiple instances of tf.keras.Model subclasses description:<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04.- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No.- TensorFlow installed from (source or binary): pip install.- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0.- Python version: 3.9.6.- Bazel version (if compiling from source): NA.- GCC/Compiler version (if compiling from source): NA.- CUDA/cuDNN version: NA.- GPU model and memory: GTX 2080 Ti.You can collect some of this information using our environment capture[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)You can also obtain the TensorFlow version with:1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**Describe the current behavior**Here is an example of triggering the issue:```pythonimport tensorflow as tfclass MyModel(tf.keras.Model):  def __init__(self, input_shape):    super(MyModel, self).__init__()    self.my_model_input_shape = input_shape    self.dense1 = tf.keras.layers.Dense(5, activation=tf.nn.relu)    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)    input_layer = tf.keras.layers.Input(self.my_model_input_shape)    output_layer = self.call(input_layer)    super(MyModel, self).__init__(      inputs=input_layer,      outputs=output_layer    )  def call(self, inputs, training=None):    x = self.dense1(inputs)    return self.dense2(x) + xmodel_1 = MyModel((10,))model_1.summary()model_2 = MyModel((20,))model_2.summary()```It fails at the creation of `model_2` with```File "".../lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py"", line 530, in _method_wrapper    result = method(self, *args, **kwargs)TypeError: __init__() missing 2 required positional arguments: 'inputs' and 'outputs'```Without the the second 2-param `super().__init__` call at the end of `MyModel.__init__`, it is OK to run, but the summary and `model_1.layers` outputs are different: the one with the second `__init__` call contains more and better information, such as the input and last addition layers and a `Connected to` columns:```Model: ""my_model_1""__________________________________________________________________________________________________Layer (type)                    Output Shape         Param #     Connected to                     ==================================================================================================input_1 (InputLayer)            [(None, 10)]         0                                            __________________________________________________________________________________________________dense (Dense)                   (None, 5)            55          input_1[0][0]                    __________________________________________________________________________________________________dense_1 (Dense)                 (None, 5)            30          dense[0][0]                      __________________________________________________________________________________________________tf.__operators__.add (TFOpLambd (None, 5)            0           dense_1[0][0]                                                                                     dense[0][0]                      ==================================================================================================Total params: 85Trainable params: 85Non-trainable params: 0```I would much prefer the one with better information. I learned this from various examples such as https://github.com/matterport/Mask_RCNN/issues/921#issuecomment-432846634.**Describe the expected behavior**Creating multiple instances of subclasses should have no issue and the variables and models should be independent from each other.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no.- Briefly describe your candidate solution(if contributing): NA.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/Jupyter/any notebook.See above.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.
"
51502,1,1717,77,0,0,OverLordGoldDragon,0,"title:Could not find device for node (`reduce_max` of complex) description:```pythontf.reduce_max(tf.constant([1 + 1j]))```[TF 2.5.0](https://anaconda.org/anaconda/tensorflow), Windows 10, Python 3.9.6.  <b>Error</b>```pythonTraceback (most recent call last):  File ""<stdin>"", line 1, in <module>  File ""D:\Anaconda\envs\tf25_env\lib\site-packages\tensorflow\python\util\dispatch.py"", line 206, in wrapper    return target(*args, **kwargs)  File ""D:\Anaconda\envs\tf25_env\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 2908, in reduce_max    return reduce_max_with_dims(input_tensor, axis, keepdims, name,  File ""D:\Anaconda\envs\tf25_env\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 2920, in reduce_max_with_dims    gen_math_ops._max(input_tensor, dims, keepdims, name=name))  File ""D:\Anaconda\envs\tf25_env\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 5781, in _max    _ops.raise_from_not_ok_status(e, name)  File ""D:\Anaconda\envs\tf25_env\lib\site-packages\tensorflow\python\framework\ops.py"", line 6897, in raise_from_not_ok_status    six.raise_from(core._status_to_exception(e.code, message), None)  File ""<string>"", line 3, in raise_fromtensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'T' of complex128 is not in the list of allowed values: float, double, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64, qint8, quint8, qint32, qint16, quint16        ; NodeDef: {{node Max}}; Op<name=Max; signature=input:T, reduction_indices:Tidx -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]> [Op:Max]```</details>
"
51479,1,0,5,0,0,rajeshdhanda,0,"title:ValueError: Tensor data is null. Run allocate_tensors() first  description:class ImageEncoder(object):    def __init__(self, checkpoint_filename, input_name=""images"",                 output_name=""features""):        self.session = tf.Session()        interpreter = tf.lite.Interpreter(model_path=""model.tflite"")        interpreter.allocate_tensors()        self.input_var = interpreter.get_tensor(0)        self.output_var = interpreter.get_tensor(130)  File ""\tools\generate_detections.py"", line 115, in __init__    self.output_var = interpreter.get_tensor(130)  File ""C:\envs\yolov4-cpu\lib\site-packages\tensorflow\lite\python\interpreter.py"", line 810, in get_tensor    return self._interpreter.GetTensor(tensor_index)_**ValueError: Tensor data is null. Run allocate_tensors() first**_
"
51476,1,4848,1,0,1,rowanz,0,"title:""UnauthenticatedError: ioctl failed"" when creating tensors, after initializing JAX on a TPU description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): TPU v3-8 VM (so Ubuntu 20.04.2 LTS)- TensorFlow installed from (source or binary): Binary (but I'm using what came pre-installed on the VM)- TensorFlow version (use command below): `unknown 2.6.0` -- when I run `pip freeze` the version is `tf-nightly==2.6.0`.- Python version: Python 3.8.5- CUDA/cuDNN version: N/A- GPU model and memory: N/A (Using a TPU)**Describe the current behavior**I'm trying to load a dataset using Tensorflow Datasets, and run code with Jax, on a TPU VM. I'd like to do some preprocessing operations on that dataset in tensorflow. However, after initializing Jax, tensorflow can't seem to create any tensors. Here's a MWE:* Setup: First create a [cloud TPU VM](https://cloud.google.com/tpu/docs/jax-quickstart-tpu-vm), ssh into it, and install JAX: `sudo pip3 install ""jax[tpu]==0.2.18"" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html`*  Now I run:```pythonimport jaximport tensorflow as tfprint('JAX process: {} / {}. Local devices {}'.format(jax.process_index(), jax.process_count(), jax.local_devices()), flush=True)X = tf.constant(1.0, dtype=tf.float32)```and get```python---------------------------------------------------------------------------UnauthenticatedError                      Traceback (most recent call last)<ipython-input-2-9895b44de223> in <module>      2 import tensorflow as tf      3 print('JAX process: {} / {}. Local devices {}'.format(jax.process_index(), jax.process_count(), jax.local_devices()), flush=True)----> 4 X = tf.constant(1.0, dtype=tf.float32)/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)    262     ValueError: if called on a symbolic tensor.    263   """"""--> 264   return _constant_impl(value, dtype, shape, name, verify_shape=False,    265                         allow_broadcast=True)    266/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)    274       with trace.Trace(""tf.constant""):    275         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)--> 276     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)    277    278   g = ops.get_default_graph()/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)    299 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):    300   """"""Implementation of eager constant.""""""--> 301   t = convert_to_eager_tensor(value, ctx, dtype)    302   if shape is None:    303     return t/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)     95     except AttributeError:     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum---> 97   ctx.ensure_initialized()     98   return ops.EagerTensor(value, ctx.device_name, dtype)     99/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py in ensure_initialized(self)    552         pywrap_tfe.TFE_ContextOptionsSetRunEagerOpAsFunction(    553             opts, self._run_eager_op_as_function)--> 554         context_handle = pywrap_tfe.TFE_NewContext(opts)    555       finally:    556         pywrap_tfe.TFE_DeleteContextOptions(opts)UnauthenticatedError: ioctl failed```*My hunch about what is going on*I suspect, but am not sure, that the problem is that Tensorflow is trying to create tensors on the TPU. This isn't intended because I'd like it to create tensors on the CPU instead. I've tried looking through [this guide](https://www.tensorflow.org/api_docs/python/tf/config/set_visible_devices) and also adding this line of code right after importing tensorflow:```tf.config.experimental.set_visible_devices([], 'GPU')```which I saw in a bunch of tutorials, but it doesn't help. (Which is perhaps not surprising because there aren't any GPUs on a TPU VM). When I run `tf.config.list_logical_devices()` afterwards I get```python[LogicalDevice(name='/device:CPU:0', device_type='CPU'), LogicalDevice(name='/device:TPU_SYSTEM:0', device_type='TPU_SYSTEM'), LogicalDevice(name='/device:TPU:0', device_type='TPU'), LogicalDevice(name='/device:TPU:1', device_type='TPU'), LogicalDevice(name='/device:TPU:2', device_type='TPU'), LogicalDevice(name='/device:TPU:3', device_type='TPU'), LogicalDevice(name='/device:TPU:4', device_type='TPU'), LogicalDevice(name='/device:TPU:5', device_type='TPU'), LogicalDevice(name='/device:TPU:6', device_type='TPU'), LogicalDevice(name='/device:TPU:7', device_type='TPU')]```More surprisingly though, for some reason, even when I run ```pythontf.config.experimental.set_visible_devices([], 'GPU')tf.config.experimental.set_visible_devices([], 'TPU_SYSTEM')tf.config.experimental.set_visible_devices([], 'TPU')```I get the same result. so I'm not quite sure how to hide TPUs from tensorflow, or, for that matter, how anyone else is able to do so 濠电姷顣藉Σ鍛村磻閹捐泛绶ゅù鐘差儏閻ゎ喗銇勯弽顐㈠壋缂?**Describe the expected behavior**I'd like the tensor to be initialized (a constant value of 1.0), and on the CPU. The reason I'm creating a tensor here is to do some data preprocessing and augmentation, like [what was used for the flax ImageNet examples](https://github.com/google/flax/blob/main/examples/imagenet/input_pipeline.py), and that augmentation should live on CPU.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): no (I'm not sure what the fix is or what I'm doing wrong).- Briefly describe your candidate solution(if contributing):**Standalone code to reproduce the issue**See above.**Other info / logs**Inspecting the logs in `/tmp/tpu_logs/` I found:```cat /tmp/tpu_logs/tpu_driver.t1v-n-e14a9395-w-0.rowanz.log.ERROR.20210812-234557.16896Log file created at: 2021/08/12 23:45:57Running on machine: t1v-n-e14a9395-w-0Binary: Built on May 19 2021 03:34:24 (1621420438)Binary: Built at cloud-tpus-runtime-release-tool@a3446e09a7af769-324b9e30725.borgtask.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/g3Binary: Built for gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msgE0812 23:45:57.936586   18120 kernel_dma_mapper.cc:88] Error setting number simples with FAILED_PRECONDITION: ioctl failed [type.googleapis.com/util.ErrorSpacePayload='util::PosixErrorSpace::Device or resource busy']E0812 23:45:57.936686   18119 kernel_dma_mapper.cc:88] Error setting number simples with FAILED_PRECONDITION: ioctl failed [type.googleapis.com/util.ErrorSpacePayload='util::PosixErrorSpace::Device or resource busy']E0812 23:45:57.936775   18120 tensor_node.cc:436] [0000:00:05.0 PE0 C1 MC-1 TN0] Failed to set number of simple DMA addresses: FAILED_PRECONDITION: ioctl failed [type.googleapis.com/util.ErrorSpacePayload='util::PosixErrorSpace::Device or resource busy']E0812 23:45:57.936795   18119 tensor_node.cc:436] [0000:00:07.0 PE0 C3 MC-1 TN0] Failed to set number of simple DMA addresses: FAILED_PRECONDITION: ioctl failed [type.googleapis.com/util.ErrorSpacePayload='util::PosixErrorSpace::Device or resource busy']```
"
51472,1,10594,7,0,0,JJKK1313,0,"title:OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature. description:I'm Trying to implement a Triplet lose based NN, and for this cause I have implemented a custom Image Generator.When I start the ```model.fit()``` I get the following error:```    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.```Even Though I'm not even iterating a single tensor in My code.My code is listed here below:the Image Generator:```pythonclass TripletDataGenerator(tf.keras.utils.Sequence):    """"""    NOTE: ON model.fit(SHUFFLE=FALSE) -> MUST BE FALSE!    """"""    def __init__(self,                 df: pd.DataFrame,                 batch_size=256,                 shuffle=True,                 rescale: {float, None} = 1. / 255.,                 target_img_size=(128, 128),                 preprocess_function=None,                 rand_preproc_single: {ImageDataGenerator, dict} = None,                 rand_preproc_batch: list = None):        self.batch_counter = 0        self.last_batch_index = 0        if shuffle:            self.triplets_df = df.sample(frac=1).reset_index(drop=True)  # randomizing it        else:            self.triplets_df = df.reset_index(drop=True)  # randomizing it        self.preprocess_function = preprocess_function        self.rescale = rescale        self.target_img_size = target_img_size        assert batch_size > 0, ""Minimum batch size is 1, must be positive.""        self.batch_size = batch_size        self.shuffle = shuffle        # indexes of rows. every batch we draw 2 samples. 1 similar and 1 dissimilar        self.indexes = np.arange(len(self.triplets_df))        if self.shuffle:            np.random.shuffle(self.indexes)        self.rand_preproc_single = rand_preproc_single        self.rand_preproc_batch = rand_preproc_batch    def __len__(self):        """"""Denotes the number of batches per epoch""""""        return len(self.triplets_df) // self.batch_size    def __getitem__(self, index):        """"""Generate one batch of data""""""        # Generate indexes of the batch        indexes = self.indexes[index * self.batch_size:                               (index + 1) * self.batch_size]        self.last_batch_index = index        anchors = []        positives = []        negatives = []        for idx in indexes:            a, p, n = self.triplets_df.iloc[idx]            anchors.append(self.load_image(a))            positives.append(self.load_image(p))            negatives.append(self.load_image(n))        anchors = np.array(anchors, dtype='float32')        positives = np.array(positives, dtype='float32')        negatives = np.array(negatives, dtype='float32')        labels = np.zeros(self.batch_size)        if self.rand_preproc_batch is not None:            for func in self.rand_preproc_batch:                anchors = func(anchors)                positives = func(positives)                negatives = func(negatives)        return [anchors, positives, negatives], labels    def on_epoch_end(self):        """"""Updates indexes after each epoch""""""        self.batch_counter += self.last_batch_index + 1  # indices starts from 0        if self.batch_counter >= len(self):            if self.shuffle:                np.random.shuffle(self.indexes)                self.triplets_df = self.triplets_df.sample(frac=1).reset_index(drop=True)            self.batch_counter = 0        else:            self.indexes = np.append(self.indexes[self.last_batch_index + 1:],                                     self.indexes[: self.last_batch_index + 1])    def load_image(self, path):        """"""        loads an image using tensorflow tools        :param path: absolute path (refers to the project's folder) to the image        :return: an image array.        """"""        if self.rand_preproc_single is not None:            if isinstance(self.rand_preproc, ImageDataGenerator):                img_arr = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)                img_arr = self.rand_preproc.random_transform(img_arr)                img_arr = cv2.resize(img_arr, self.target_img_size)            else:                img_arr = my_utils.image_augmentations(path, **self.rand_preproc)        else:            img_arr = cv2.imread(path)            img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)            img_arr = cv2.resize(img_arr, self.target_img_size)        if self.preprocess_function is not None:            img_arr = self.preprocess_function(img_arr)        elif self.rescale is not None:            img_arr = img_arr * self.rescale        return img_arr```The model's Architecture is as follows:```pythondef get_siamese_model(input_shape, conv2d_filters):    # Define the tensors for the two input images    anchor_input = Input(input_shape, name=""Anchor_Input"")    positive_input = Input(input_shape, name=""Positive_Input"")    negative_input = Input(input_shape, name=""Negative_Input"")    body = build_body(input_shape, conv2d_filters)    # Generate the encodings (feature vectors) for the two images    encoded_a = body(anchor_input)    encoded_p = body(positive_input)    encoded_n = body(negative_input)    ap_distance = tf.reduce_sum(tf.square(encoded_a - encoded_p), axis=-1, keepdims=True)    an_distance = tf.reduce_sum(tf.square(encoded_a - encoded_n), axis=-1, keepdims=True)    # Connect the inputs with the outputs    siamese_net = Model(inputs=[anchor_input, positive_input, negative_input],                        outputs=[ap_distance, an_distance])    return siamese_net```and loss is:```pythondef get_loss(margin=1.0):    def triplet_loss(y_true, y_pred):        # The output of the network is a tuple containing the distances        # between the anchor and the positive example, and the anchor and        # the negative example.        ap_distance, an_distance = y_pred        # Computing the Triplet Loss by subtracting both distances and        # making sure we don't get a negative value.        loss = ap_distance - an_distance        loss = tf.maximum(loss + margin, 0.0)        return loss    return triplet_loss```Main:```pythonif __name__ == '__main__':    EPOCHS = configurations.EPOCHS    BATCH_SIZE = configurations.BATCH_SIZE    IMG_SIZE = configurations.IMG_SIZE    MONITOR = configurations.MONITOR    PATIENCE = configurations.PATIENCE    EMBEDDING_NODES = configurations.EMBEDDING_NODES    LEARNING_RATE = configurations.LEARNING_RATE    STEPS_PER_EPOCH = configurations.STEPS_PER_EPOCH    VALIDATION_STEPS = configurations.VALIDATION_STEPS    CONV2D_FILTERS = configurations.CONV2D_FILTERS    MARGIN = configurations.MARGIN    DATA_FILE = 'LFW_triplets.csv'    augment_params = None    # augment_params = dict(    #     resize=IMG_SIZE[:-1],    #     random_gray_scale=0.2,    #     random_contrast_range=[0.65, 1.5],    #     hsv_noise_max_amps=[0.02, 0.2, 0],    #     max_brightness_delta=0.15,    #     LR_flip=True,    #     UD_flip=False,    #     rotate_range=30,    #     random_shift=[0.1, 0.1],    #     random_zoom_range=0.3,    #     rescale=1. / 255.)    NOTES = '\n'.join([        f""batch size={BATCH_SIZE}"",        f""learning_rate={LEARNING_RATE}"",        f""embedding_nodes={EMBEDDING_NODES}"",        f""DataFilePath={DATA_FILE}"",        f""BatchNormalization_used={configurations.ADD_BATCHNORM}"",        f""Conv2D_filters_count={CONV2D_FILTERS}"",        f""Loss=triplet_loss"",        ""Augmentations with: HSV, Brightness, Contrast.""    ])    np.random.seed(42)    tf.random.set_seed(42)    t = time.time()    train_gen, test_gen = DataFrameGeneratorClass.create_train_test_generators(csv_path=DATA_FILE, pair_gen=False,                                                                               validation_split=0.3, shuffle=True,                                                                               batch_size=configurations.BATCH_SIZE,                                                                               rescale=1. / 255.,                                                                               img_size=configurations.IMG_SIZE[:-1],                                                                               preprocess_func=None,                                                                               rand_preproc_single=None,                                                                               rand_preproc_batch=None)    dt = time.time() - t    print(f""TOOK {dt} seconds to create Train Generator with {len(train_gen)} Batches""          f"" and Test Generator with {len(test_gen)} Batches"")    siamese_model = get_siamese_model(input_shape=IMG_SIZE, conv2d_filters=CONV2D_FILTERS)    siamese_model.summary()    loss_func = get_loss(margin=MARGIN)    siamese_model.compile(optimizer=Adam(learning_rate=0.0001),                          loss=loss_func)    early_stop = tf.keras.callbacks.EarlyStopping(monitor=MONITOR,                                                  min_delta=1e-5,                                                  patience=PATIENCE,                                                  verbose=1,                                                  mode='auto',                                                  restore_best_weights=True)    date_string = datetime.datetime.today().strftime(""%d-%m-%y_%H%M%S"")    os.mkdir(f'check_points/{date_string}/')    chk_point = tf.keras.callbacks.ModelCheckpoint(f'check_points/{date_string}/',                                                   monitor=configurations.MONITOR,                                                   verbose=1,                                                   save_best_only=True,                                                   save_weights_only=True)    call_backs = [early_stop, chk_point, tf.keras.callbacks.TensorBoard(log_dir=f'logs/TRIPLET_{date_string}', write_images=True)]    history = siamese_model.fit(train_gen,                                shuffle=False,  # ITS MANDATORY WHEN USING MY CUSTOM GENERATOR                                epochs=EPOCHS,                                steps_per_epoch=STEPS_PER_EPOCH,                                validation_steps=VALIDATION_STEPS,                                callbacks=call_backs,                                validation_data=test_gen)    NOTES += f""\n\n{chk_point.monitor}={chk_point.best}""    my_utils.save_results(notes=NOTES,                          history_obj=history,                          directory_dst='results',                          model=siamese_model,                          date_str=""TRIPLET_"" + date_string)```
"
51464,1,0,19,0,0,seermer,0,"title:ResNetV2 implementation problem description:**System information**(not related to installation)- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10- TensorFlow installed from (source or binary): pip- TensorFlow version (use command below): 2.5.0 GPU- Python version: 3.8**Describe the expected behavior**As shown in the image below from **Identity Mappings in Deep Residual Networks** by He et al.\ResNetV2 block has its preactivation **after** branching (bn-relu-conv-bn-relu-conv are all in contained in residual)![image](https://user-images.githubusercontent.com/49250812/129236962-d30af2fc-8bac-4ddf-a951-6d986d25bef3.png)the **only** exception on preactivation is the first block of the first stage\quote from the original paper:> For the first Residual Unit (that follows a stand-alone convolutional layer,> conv1), we adopt the first activation right after conv1 and before splitting into> two paths**Describe the current behavior**ResNetV2 Falsefully put preactivation before branching when channel increases for **all stages**![image](https://user-images.githubusercontent.com/49250812/129238244-89917129-a60f-45a0-a29c-26cd3134a249.png)As shown in above image, the output of `ResNet50V2(weights=None).summary()`\the red arrow I added points to the shortcut path, it is connected to preact_relu, which means the preactivation is done **before** branching to residual and shortcut, which is not true according to the original paper\a simple drawing representing the first block of each stage in current implementation, which is obviously not equivalent to the image from the original paper:![image](https://user-images.githubusercontent.com/49250812/129239135-4cf8449e-87c8-439a-89c1-070ccd09411f.png)
"
51457,1,0,34,0,0,GoldFeniks,0,"title:Dereferencing null pointer in GPU delegate description:The following code in GPU delegate constructor should probably use `options_` instead of `options` that is potentially a null pointerhttps://github.com/tensorflow/tensorflow/blob/8b06339173d65b9f48236f4051d31e32a2e86191/tensorflow/lite/delegates/gpu/delegate.cc#L95-L102
"
51451,0,665,9,0,0,sclarkson,0,"title:Multiple versions of TensorFlow co-installed potentially cause ABI mismatch description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a- TensorFlow installed from (source or binary): both- TensorFlow version (use command below): 2.6.0- Python version: 3.8- Bazel version (if compiling from source): n/a- GCC/Compiler version (if compiling from source): n/a- CUDA/cuDNN version: n/a- GPU model and memory: n/a**Describe the current behavior**When multiple tensorflow instances are installed in different parts of your python path, TensorFlow will attempt to load kernel libraries from all of them, potentially resulting in an ABI mismatch.```$ python3 -c ""import tensorflow""No protocol specifiedTraceback (most recent call last):  File ""<string>"", line 1, in <module>  File ""/home/sclarkson/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 438, in <module>    _ll.load_library(_main_dir)  File ""/home/sclarkson/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library    py_tf.TF_LoadLibrary(lib)tensorflow.python.framework.errors_impl.NotFoundError: /usr/lib/python3/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringB5cxx11ERKNS_15OpKernelContextEb```Observe above, the TensorFlow instance in `~/.local/lib/python3/` attempting to load a shared library from `/usr/lib/python3/`. Because of different compilation options, the library from the system-wide install is expecting symbols that do not exist in the pip install.**Describe the expected behavior**TensorFlow should only load kernels from its own install.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): yes- Briefly describe your candidate solution(if contributing): modify kernel preloading to only use its own install**Standalone code to reproduce the issue**Compile TensorFlow from source and install system-wide.Install TensorFlow from pip with `pip install --user tensorflow`Then run `python -c ""import tensorflow""`**Other info**This is a continuation of #42978
"
51449,1,1372,0,0,0,TillHa,0,"title:dtype of RNN cell's state is changed to tf.float32 during reset_states description:### System information-   **Have I written custom code (as opposed to using a stock example script    provided in TensorFlow)**: yes-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: both win10 and CentOS Linux-   **TensorFlow installed from (source or binary)**: pip-   **TensorFlow version (use command below)**: 2.6.0-   **Python version**: 3.8-   **Bazel version (if compiling from source)**:-   **Exact command to reproduce**: see below### Describe the problemI have implemented a recurrent cell which is to be wrapped within a `tf.keras.layers.RNN.` The cell has a state whose data type is not `tf.float32` but `tf.complex64`. However, each time when `layer.reset_states()` is invoked, the data type of the state is changed to `tf.float32`. I assume, a reason for this issue is line 933, 934 in keras/layers/recurrent.py```      flat_states_variables = tf.nest.map_structure(          backend.variable, flat_init_state_values)```Here, the initialized state values are stored in `flat_init_state_values` and `backend.variable` is called on each of the states. However, no `dtype` argument is passed to `backend.variable`. As a consequence it defaults to `tf.float32` for all states. Then a value error is thrown during the initial symbolic call. See attached stack trace. I would recommend the following patch, which solves the issue for me```      flat_states_variables = tf.nest.map_structure(    lambda var: backend.variable(var, var.dtype), flat_init_state_values)```### Source code / logsCurrently, the example fails at the construction of the RNN layer.```import tensorflow as tfclass RecurrentCell(tf.keras.layers.Layer):    def __init__(self, state_size):        super(RecurrentCell, self).__init__()        self.state_size = state_size    def build(self, input_shape):        super(RecurrentCell, self).build(input_shape)    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):        # explicit initialization with tf.complex64        return tf.zeros((self.state_size, ), dtype=tf.complex64)    @tf.function    def call(self, inputs, states):        # toy example        x = inputs        xfd = tf.signal.rfft(x)[..., :self.state_size]        yfd = tf.multiply(xfd, states)        return tf.signal.irfft(yfd), statesrecCell = RecurrentCell(state_size=5)inp = tf.keras.Input(shape=(None, 8),                     batch_size=32)out = tf.keras.layers.RNN(recCell,                          return_sequences=True,                          stateful=True,                          return_state=False)(inp)model = tf.keras.Model(inputs=[inp], outputs=[out])y = model.predict(tf.random.normal((32, 16, 8)))```Stack trace:[stacktrace.txt](https://github.com/tensorflow/tensorflow/files/6975576/stacktrace.txt)
"
