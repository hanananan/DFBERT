{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'2.0.0+cu118'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device= cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device=', device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([16, 2])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "#加载预训练模型\n",
    "pretrained = BertModel.from_pretrained('bert-base-uncased')\n",
    "#需要移动到cuda上\n",
    "pretrained.to(device)\n",
    "\n",
    "#不训练,不需要计算梯度\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "\n",
    "#定义下游任务模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(1,16)\n",
    "        self.linear2 = torch.nn.Linear(1,16)\n",
    "        self.linear3 = torch.nn.Linear(1,16)\n",
    "        self.linear4 = torch.nn.Linear(1,16)\n",
    "\n",
    "        self.fc = torch.nn.Linear(768+3, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, code_len, keyword_num, permission):\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                       attention_mask=attention_mask,\n",
    "                       token_type_ids=token_type_ids)\n",
    "        # print(code_len)\n",
    "        # code_len = self.linear1(code_len)\n",
    "        # keyword_num = self.linear2(keyword_num)\n",
    "        # keyword_fix = self.linear3(keyword_fix)\n",
    "        # permission = self.linear4(permission)\n",
    "\n",
    "        # code_len = code_len\n",
    "        # keyword_num = keyword_num.unsqueeze(1)\n",
    "        # keyword_fix = keyword_fix.unsqueeze(1)\n",
    "        # permission = permission.unsqueeze(1)\n",
    "        # code_len = torch.transpose(code_len, 0, 1)\n",
    "        # keyword_num = torch.transpose(keyword_num, 0, 1)\n",
    "        # keyword_fix = torch.transpose(keyword_fix, 0, 1)\n",
    "        # permission = torch.transpose(permission, 0, 1)\n",
    "\n",
    "\n",
    "        concat = torch.cat((out.last_hidden_state[:, 0], code_len, keyword_num, permission), dim=1)\n",
    "        out = self.fc(concat)\n",
    "\n",
    "        out = out.softmax(dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "#同样要移动到cuda\n",
    "model.to(device)\n",
    "\n",
    "#虚拟一批数据,需要把所有的数据都移动到cuda上\n",
    "input_ids = torch.ones(16, 100).int().to(device)\n",
    "attention_mask = torch.ones(16, 100).int().to(device)\n",
    "token_type_ids = torch.ones(16, 100).int().to(device)\n",
    "code_len = torch.ones(16,1).float().to(device)\n",
    "keyword_num = torch.ones(16,1).float().to(device)\n",
    "permission = torch.ones(16,1).float().to(device)\n",
    "labels = torch.ones(16,1).float().to(device)\n",
    "\n",
    "#试算\n",
    "model(input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      token_type_ids=token_type_ids,\n",
    "      code_len=code_len,\n",
    "      keyword_num=keyword_num,\n",
    "      permission=permission).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/hananan/.cache/huggingface/datasets/csv/default-08fbfe6a6745fbd4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba227d465ecd46728a872f5d44a2e33f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b2915576af14a739db0d8e289f2fc79"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45cda6bb9f374ca388a4d46d0859ffe4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/hananan/.cache/huggingface/datasets/csv/default-08fbfe6a6745fbd4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:776: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "154fc22fd4f84ca19950e24de16b9679"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/hananan/.cache/huggingface/datasets/csv/default-04f380cb9cfc9313/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56b8cbea9df04d8d97b190e11e704307"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ecf2da49b134e5e9c61c8ec7dbf1524"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9827a73911b94ba8b3020b7de14fc03f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/hananan/.cache/huggingface/datasets/csv/default-04f380cb9cfc9313/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:776: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb7aaa917caf486da50147852f46c81d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(2396,\n ('title:Jacoco plugin causes implicit dependencies between unrelated tasks description:<!--- Please follow the instructions below. We receive dozens of issues every week, so to stay productive, we will close issues that don\\'t provide enough information. Please open Android-related issues on the Android Issue Tracker at https://source.android.com/source/report-bugsPlease open Gradle Native-related issues at https://github.com/gradle/gradle-native/issues--><!--- Provide a brief summary of the issue in the title above -->### Expected BehaviorWe\\'re applying the jacoco plugin to multiple tasks within a single project, all of type JavaExec or JavaExecFork (psxpaul external plugin) like so:```jacoco {    applyTo initTestBDeploy    applyTo checkTestBDeploy    applyTo startTestBDeploy    if(!project.hasProperty(\\'cypressNoCentral\\')) {        applyTo startTestBDeployCentral        applyTo startTestBDeployManaged    }}```Expectation is that these tasks maintain their \"current\" relationship.### Current BehaviorBuild fails with this message:```FAILURE: Build failed with an exception.* What went wrong:Some problems were found with the configuration of task \\':test-data:startTestBDeployCentral\\' (type \\'JavaExecFork\\').  - Gradle detected a problem with the following location: \\'E:\\\\other\\\\workspaces\\\\deployment\\\\deployment\\\\test-data\\'.    Reason: Task \\':test-data:startTestBDeployCentral\\' uses this output of task \\':test-data:checkTestBDeploy\\' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed.    Possible solutions:      1. Declare task \\':test-data:checkTestBDeploy\\' as an input of \\':test-data:startTestBDeployCentral\\'.      2. Declare an explicit dependency on \\':test-data:checkTestBDeploy\\' from \\':test-data:startTestBDeployCentral\\' using Task#dependsOn.      3. Declare an explicit dependency on \\':test-data:checkTestBDeploy\\' from \\':test-data:startTestBDeployCentral\\' using Task#mustRunAfter.    Please refer to https://docs.gradle.org/8.0.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.  - Gradle detected a problem with the following location: \\'E:\\\\other\\\\workspaces\\\\deployment\\\\deployment\\\\test-data\\'.    Reason: Task \\':test-data:startTestBDeployCentral\\' uses this output of task \\':test-data:initTestBDeploy\\' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed.    Possible solutions:      1. Declare task \\':test-data:initTestBDeploy\\' as an input of \\':test-data:startTestBDeployCentral\\'.      2. Declare an explicit dependency on \\':test-data:initTestBDeploy\\' from \\':test-data:startTestBDeployCentral\\' using Task#dependsOn.      3. Declare an explicit dependency on \\':test-data:initTestBDeploy\\' from \\':test-data:startTestBDeployCentral\\' using Task#mustRunAfter.    Please refer to https://docs.gradle.org/8.0.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.  - Gradle detected a problem with the following location: \\'E:\\\\other\\\\workspaces\\\\deployment\\\\deployment\\\\test-data\\'.    Reason: Task \\':test-data:startTestBDeployCentral\\' uses this output of task \\':test-data:startTestBDeploy\\' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed.    Possible solutions:      1. Declare task \\':test-data:startTestBDeploy\\' as an input of \\':test-data:startTestBDeployCentral\\'.      2. Declare an explicit dependency on \\':test-data:startTestBDeploy\\' from \\':test-data:startTestBDeployCentral\\' using Task#dependsOn.      3. Declare an explicit dependency on \\':test-data:startTestBDeploy\\' from \\':test-data:startTestBDeployCentral\\' using Task#mustRunAfter.    Please refer to https://docs.gradle.org/8.0.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.```### Context<!--- How has this issue affected you? What are you trying to accomplish? --><!--- Providing context helps us come up with a solution that is most useful in the real world -->If I remove the `jacoco` block alltogether, all returns to normal. So it must be related to the `applyTo`s in there. The issue *also* appears if I comment out all applyTos except for *one* single one. E.g. if I leave the `applyTo startTestBDeploy` in the block, the message becomes shorter, but the error is still there:```FAILURE: Build failed with an exception.* What went wrong:A problem was found with the configuration of task \\':test-data:startTestBDeployCentral\\' (type \\'JavaExecFork\\').  - Gradle detected a problem with the following location: \\'E:\\\\other\\\\workspaces\\\\deployment\\\\deployment\\\\test-data\\'.    Reason: Task \\':test-data:startTestBDeployCentral\\' uses this output of task \\':test-data:startTestBDeploy\\' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed.    Possible solutions:      1. Declare task \\':test-data:startTestBDeploy\\' as an input of \\':test-data:startTestBDeployCentral\\'.      2. Declare an explicit dependency on \\':test-data:startTestBDeploy\\' from \\':test-data:startTestBDeployCentral\\' using Task#dependsOn.      3. Declare an explicit dependency on \\':test-data:startTestBDeploy\\' from \\':test-data:startTestBDeployCentral\\' using Task#mustRunAfter.    Please refer to https://docs.gradle.org/8.0.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.```### Steps to Reproduce <!---Provide a self-contained example project as an attached archive or a Github project.You can use the following template with a Gradle GitHub action set up to showcase your problem: https://github.com/gradle/gradle-issue-reproducerIn the rare cases where this is infeasible, we will also accept a detailed set of instructions.-->Clone and checkout https://github.com/bdeployteam/bdeploy/tree/gradle-issue. Run `./gradlew runCypressHeadless` or `./gradlew clean build`.You can checkout test-data/build.gradle. It contains the mentioned tasks and the `jacoco` block. Note that some of the mentioned tasks are generated by `createServerTasks`### Your Environment<!--- Include as many relevant details about the environment you experienced the bug in --><!--- A build scan https://scans.gradle.com/get-started is ideal -->Build scan URL: https://gradle.com/s/vvbskvyuxgeq6\\n',\n  4350,\n  0,\n  0,\n  0,\n  1),\n 100,\n ('title:Multiple versions of TensorFlow co-installed potentially cause ABI mismatch description:**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a- TensorFlow installed from (source or binary): both- TensorFlow version (use command below): 2.6.0- Python version: 3.8- Bazel version (if compiling from source): n/a- GCC/Compiler version (if compiling from source): n/a- CUDA/cuDNN version: n/a- GPU model and memory: n/a**Describe the current behavior**When multiple tensorflow instances are installed in different parts of your python path, TensorFlow will attempt to load kernel libraries from all of them, potentially resulting in an ABI mismatch.```$ python3 -c \"import tensorflow\"No protocol specifiedTraceback (most recent call last):  File \"<string>\", line 1, in <module>  File \"/home/sclarkson/.local/lib/python3.8/site-packages/tensorflow/__init__.py\", line 438, in <module>    _ll.load_library(_main_dir)  File \"/home/sclarkson/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py\", line 154, in load_library    py_tf.TF_LoadLibrary(lib)tensorflow.python.framework.errors_impl.NotFoundError: /usr/lib/python3/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringB5cxx11ERKNS_15OpKernelContextEb```Observe above, the TensorFlow instance in `~/.local/lib/python3/` attempting to load a shared library from `/usr/lib/python3/`. Because of different compilation options, the library from the system-wide install is expecting symbols that do not exist in the pip install.**Describe the expected behavior**TensorFlow should only load kernels from its own install.**[Contributing](https://www.tensorflow.org/community/contribute)**- Do you want to contribute a PR? (yes/no): yes- Briefly describe your candidate solution(if contributing): modify kernel preloading to only use its own install**Standalone code to reproduce the issue**Compile TensorFlow from source and install system-wide.Install TensorFlow from pip with `pip install --user tensorflow`Then run `python -c \"import tensorflow\"`**Other info**This is a continuation of #42978\\r\\n',\n  665,\n  0,\n  0,\n  0,\n  0))"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "#定义数据集\n",
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self.dataset = load_dataset(path='csv', data_dir='D:\\\\hzrproject\\\\study\\\\non-cross study\\\\2',data_files='modified_issueDataNoTensorflow.csv')['train']\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        ds = self.dataset[i]['ds']\n",
    "        code_len = self.dataset[i]['code_len']\n",
    "        keyword_num = self.dataset[i]['keyword_num']\n",
    "        keyword_fix = self.dataset[i]['keyword_fix']\n",
    "        permission = self.dataset[i]['permission']\n",
    "        label = self.dataset[i]['label']\n",
    "\n",
    "        return  ds, code_len, keyword_num, keyword_fix, permission, label\n",
    "\n",
    "class Dataset_validation:\n",
    "    def __init__(self):\n",
    "        self.dataset_validation = load_dataset(path='csv',  data_dir='D:\\\\hzrproject\\\\study\\\\non-cross study\\\\2',data_files='modified_issueDataTensorflow_feature3.csv')['train']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_validation)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        ds = self.dataset_validation[i]['ds']\n",
    "        code_len = self.dataset_validation[i]['code_len']\n",
    "        keyword_num = self.dataset_validation[i]['keyword_num']\n",
    "        keyword_fix = self.dataset_validation[i]['keyword_fix']\n",
    "        permission = self.dataset_validation[i]['permission']\n",
    "        label = self.dataset_validation[i]['label']\n",
    "\n",
    "        return  ds, code_len, keyword_num, keyword_fix, permission, label\n",
    "\n",
    "\n",
    "dataset = Dataset()\n",
    "dataset_validation = Dataset_validation()\n",
    "\n",
    "len(dataset), dataset[0], len(dataset_validation), dataset_validation[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    },
    {
     "data": {
      "text/plain": "(torch.Size([16, 500]),\n torch.Size([16, 500]),\n torch.Size([16, 500]),\n torch.Size([16, 1]),\n torch.Size([16, 1]),\n torch.Size([16, 1]),\n tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], device='cuda:0'))"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "#加载字典和分词工具\n",
    "token = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    sents = [i[0] for i in data]\n",
    "    code_len = [i[1] for i in data]\n",
    "    keyword_num = [i[2] for i in data]\n",
    "    permission = [i[4] for i in data]\n",
    "    labels = [i[5] for i in data]\n",
    "\n",
    "    #编码\n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=500,\n",
    "                                   return_tensors='pt',\n",
    "                                   return_length=True)\n",
    "\n",
    "    #input_ids:编码之后的数字\n",
    "    #attention_mask:是补零的位置是0,其他位置是1\n",
    "    input_ids = data['input_ids'].to(device)\n",
    "    attention_mask = data['attention_mask'].to(device)\n",
    "    token_type_ids = data['token_type_ids'].to(device)\n",
    "\n",
    "    code_len = torch.FloatTensor(code_len).to(device)\n",
    "    code_len = code_len.unsqueeze(0).t()\n",
    "\n",
    "    keyword_num = torch.FloatTensor(keyword_num).to(device)\n",
    "    keyword_num = keyword_num.unsqueeze(0).t()\n",
    "\n",
    "    permission = torch.FloatTensor(permission).to(device)\n",
    "    permission = permission.unsqueeze(0).t()\n",
    "\n",
    "    labels = torch.LongTensor(labels).to(device).to(device)\n",
    "\n",
    "    #print(data['length'], data['length'].max())\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, code_len, keyword_num, permission, labels\n",
    "\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=16,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "for i, (input_ids, attention_mask, token_type_ids, code_len, keyword_num, permission,\n",
    "        labels) in enumerate(loader):\n",
    "    break\n",
    "\n",
    "print(len(loader))\n",
    "input_ids.shape, attention_mask.shape, token_type_ids.shape, code_len.shape, keyword_num.shape, permission.shape, labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "0 0.8045823574066162 0.5\n",
      "10 0.7754069566726685 0.5\n",
      "20 0.7055902481079102 0.5625\n",
      "30 0.734526515007019 0.375\n",
      "40 0.7538121342658997 0.5\n",
      "50 0.7638300657272339 0.625\n",
      "60 0.6858932375907898 0.625\n",
      "70 0.5539385676383972 0.6875\n",
      "80 0.7440857887268066 0.6875\n",
      "90 0.7759900093078613 0.4375\n",
      "100 0.7758898138999939 0.5\n",
      "110 0.6311805844306946 0.625\n",
      "120 0.5965038537979126 0.6875\n",
      "130 0.7123362421989441 0.5\n",
      "140 0.8692761659622192 0.4375\n",
      "0.55\n",
      "----------------------\n",
      "epoch: 2\n",
      "0 0.7798970937728882 0.5\n",
      "10 0.8861294388771057 0.25\n",
      "20 0.7917545437812805 0.4375\n",
      "30 0.7390800714492798 0.5625\n",
      "40 0.8200836181640625 0.375\n",
      "50 0.6587197780609131 0.625\n",
      "60 0.7104282379150391 0.5\n",
      "70 0.7075632810592651 0.5\n",
      "80 0.7320884466171265 0.4375\n",
      "90 0.6530802845954895 0.6875\n",
      "100 0.6600080132484436 0.6875\n",
      "110 0.6904046535491943 0.5625\n",
      "120 0.6965038180351257 0.4375\n",
      "130 0.7739806175231934 0.375\n",
      "140 0.6963897347450256 0.5625\n",
      "0.5\n",
      "----------------------\n",
      "epoch: 3\n",
      "0 0.6560837030410767 0.75\n",
      "10 0.687950849533081 0.5625\n",
      "20 0.6318309307098389 0.6875\n",
      "30 0.604418933391571 0.8125\n",
      "40 0.6654337048530579 0.625\n",
      "50 0.6692086458206177 0.6875\n",
      "60 0.7529052495956421 0.5\n",
      "70 0.7006534934043884 0.4375\n",
      "80 0.6479240655899048 0.625\n",
      "90 0.6820236444473267 0.5625\n",
      "100 0.7248483896255493 0.4375\n",
      "110 0.7670097947120667 0.375\n",
      "120 0.7016001343727112 0.5\n",
      "130 0.6407303810119629 0.6875\n",
      "140 0.6632553339004517 0.75\n",
      "0.6\n",
      "----------------------\n",
      "epoch: 4\n",
      "0 0.6523812413215637 0.625\n",
      "10 0.6792579889297485 0.6875\n",
      "20 0.7009521722793579 0.375\n",
      "30 0.7317506670951843 0.4375\n",
      "40 0.6355457901954651 0.75\n",
      "50 0.6454968452453613 0.75\n",
      "60 0.802433967590332 0.25\n",
      "70 0.7446126937866211 0.4375\n",
      "80 0.7058617472648621 0.375\n",
      "90 0.7041261196136475 0.4375\n",
      "100 0.7344650030136108 0.375\n",
      "110 0.7005101442337036 0.5625\n",
      "120 0.6919471621513367 0.5\n",
      "130 0.6077383756637573 0.6875\n",
      "140 0.7205018997192383 0.375\n",
      "0.5083333333333333\n",
      "----------------------\n",
      "epoch: 5\n",
      "0 0.6444171071052551 0.6875\n",
      "10 0.703049898147583 0.5\n",
      "20 0.6912250518798828 0.5\n",
      "30 0.698188304901123 0.5625\n",
      "40 0.6621170043945312 0.5625\n",
      "50 0.6786496639251709 0.5625\n",
      "60 0.6860314011573792 0.5625\n",
      "70 0.7518729567527771 0.25\n",
      "80 0.7506160736083984 0.375\n",
      "90 0.7219666838645935 0.4375\n",
      "100 0.6552985310554504 0.5\n",
      "110 0.6590371131896973 0.5625\n",
      "120 0.6431410908699036 0.6875\n",
      "130 0.6694720983505249 0.5\n",
      "140 0.7073118686676025 0.5\n",
      "0.5166666666666667\n",
      "----------------------\n",
      "epoch: 6\n",
      "0 0.6799978017807007 0.6875\n",
      "10 0.6695665717124939 0.5\n",
      "20 0.6569674015045166 0.6875\n",
      "30 0.6652746796607971 0.625\n",
      "40 0.7136871814727783 0.5\n",
      "50 0.6576574444770813 0.625\n",
      "60 0.6815450191497803 0.5625\n",
      "70 0.7062388062477112 0.375\n",
      "80 0.6921586990356445 0.5625\n",
      "90 0.6814069151878357 0.5625\n",
      "100 0.6597917675971985 0.6875\n",
      "110 0.6911793947219849 0.5625\n",
      "120 0.6359776854515076 0.625\n",
      "130 0.6098706722259521 0.8125\n",
      "140 0.6638623476028442 0.625\n",
      "0.6\n",
      "----------------------\n",
      "epoch: 7\n",
      "0 0.6733230352401733 0.5625\n",
      "10 0.6620447039604187 0.5\n",
      "20 0.6417377591133118 0.5625\n",
      "30 0.7035020589828491 0.5625\n",
      "40 0.6168014407157898 0.75\n",
      "50 0.6517542600631714 0.625\n",
      "60 0.674008309841156 0.5625\n",
      "70 0.7014893293380737 0.4375\n",
      "80 0.6941655874252319 0.5\n",
      "90 0.6563022136688232 0.5625\n",
      "100 0.6822179555892944 0.625\n",
      "110 0.7254895567893982 0.375\n",
      "120 0.6315197944641113 0.75\n",
      "130 0.6687961220741272 0.5625\n",
      "140 0.6875478029251099 0.375\n",
      "0.5541666666666667\n",
      "----------------------\n",
      "epoch: 8\n",
      "0 0.7385880947113037 0.3125\n",
      "10 0.665690004825592 0.625\n",
      "20 0.6351265907287598 0.625\n",
      "30 0.6534041166305542 0.6875\n",
      "40 0.6658180952072144 0.625\n",
      "50 0.6827537417411804 0.5\n",
      "60 0.6918413043022156 0.5625\n",
      "70 0.6742199659347534 0.5625\n",
      "80 0.6460137367248535 0.6875\n",
      "90 0.6600934863090515 0.6875\n",
      "100 0.5948564410209656 0.8125\n",
      "110 0.673298716545105 0.5\n",
      "120 0.6722573041915894 0.625\n",
      "130 0.7142490744590759 0.5\n",
      "140 0.7442750930786133 0.4375\n",
      "0.5833333333333334\n",
      "----------------------\n",
      "epoch: 9\n",
      "0 0.6236774921417236 0.6875\n",
      "10 0.6725518703460693 0.625\n",
      "20 0.6620769500732422 0.625\n",
      "30 0.6756898760795593 0.5\n",
      "40 0.6914600729942322 0.5625\n",
      "50 0.6513002514839172 0.625\n",
      "60 0.669181227684021 0.625\n",
      "70 0.7342914938926697 0.4375\n",
      "80 0.7517176270484924 0.375\n",
      "90 0.6418997645378113 0.625\n",
      "100 0.6876834034919739 0.5\n",
      "110 0.670962393283844 0.625\n",
      "120 0.6942341327667236 0.5625\n",
      "130 0.6547872424125671 0.625\n",
      "140 0.6425970196723938 0.75\n",
      "0.5833333333333334\n",
      "----------------------\n",
      "epoch: 10\n",
      "0 0.6530258059501648 0.625\n",
      "10 0.643660306930542 0.625\n",
      "20 0.6466659903526306 0.6875\n",
      "30 0.6603183746337891 0.625\n",
      "40 0.661389172077179 0.625\n",
      "50 0.6813812851905823 0.4375\n",
      "60 0.7051275372505188 0.4375\n",
      "70 0.6479072570800781 0.5625\n",
      "80 0.6773489713668823 0.5625\n",
      "90 0.6972535848617554 0.5625\n",
      "100 0.6359485983848572 0.625\n",
      "110 0.667229413986206 0.4375\n",
      "120 0.6455026865005493 0.5625\n",
      "130 0.6418469548225403 0.625\n",
      "140 0.6564496755599976 0.6875\n",
      "0.5791666666666667\n",
      "----------------------\n",
      "epoch: 11\n",
      "0 0.7224728465080261 0.375\n",
      "10 0.6242599487304688 0.625\n",
      "20 0.6271129250526428 0.625\n",
      "30 0.6633772850036621 0.625\n",
      "40 0.653823971748352 0.5625\n",
      "50 0.6526439189910889 0.625\n",
      "60 0.650102436542511 0.5625\n",
      "70 0.6069837808609009 0.75\n",
      "80 0.7200441360473633 0.4375\n",
      "90 0.6434242725372314 0.625\n",
      "100 0.6618093252182007 0.625\n",
      "110 0.638872504234314 0.6875\n",
      "120 0.6583030223846436 0.625\n",
      "130 0.676392138004303 0.5625\n",
      "140 0.6705325245857239 0.625\n",
      "0.5958333333333333\n",
      "----------------------\n",
      "epoch: 12\n",
      "0 0.6221234798431396 0.6875\n",
      "10 0.6273216605186462 0.6875\n",
      "20 0.648768961429596 0.6875\n",
      "30 0.6499152183532715 0.6875\n",
      "40 0.680768609046936 0.5\n",
      "50 0.6480615735054016 0.6875\n",
      "60 0.679735541343689 0.5625\n",
      "70 0.6513960361480713 0.5625\n",
      "80 0.7088111639022827 0.4375\n",
      "90 0.6378348469734192 0.625\n",
      "100 0.6753424406051636 0.5625\n",
      "110 0.6483891606330872 0.6875\n",
      "120 0.6736693978309631 0.625\n",
      "130 0.6817764639854431 0.5625\n",
      "140 0.6304126977920532 0.75\n",
      "0.6208333333333333\n",
      "----------------------\n",
      "epoch: 13\n",
      "0 0.6067466139793396 0.875\n",
      "10 0.6786231994628906 0.625\n",
      "20 0.6596341133117676 0.6875\n",
      "30 0.6420776844024658 0.75\n",
      "40 0.7189145684242249 0.375\n",
      "50 0.7103272080421448 0.25\n",
      "60 0.7389079332351685 0.375\n",
      "70 0.6812908053398132 0.5625\n",
      "80 0.5999709963798523 0.6875\n",
      "90 0.6644468903541565 0.625\n",
      "100 0.611088216304779 0.6875\n",
      "110 0.5875215530395508 0.8125\n",
      "120 0.7283151149749756 0.3125\n",
      "130 0.6559750437736511 0.625\n",
      "140 0.6633716821670532 0.625\n",
      "0.5916666666666667\n",
      "----------------------\n",
      "epoch: 14\n",
      "0 0.6769649982452393 0.5625\n",
      "10 0.703205943107605 0.5\n",
      "20 0.6566317081451416 0.5\n",
      "30 0.6644428372383118 0.625\n",
      "40 0.650436282157898 0.4375\n",
      "50 0.7310954332351685 0.4375\n",
      "60 0.6716891527175903 0.5625\n",
      "70 0.6674400568008423 0.6875\n",
      "80 0.6968635320663452 0.4375\n",
      "90 0.7238712906837463 0.5\n",
      "100 0.7283773422241211 0.5625\n",
      "110 0.6773202419281006 0.5\n",
      "120 0.6485576033592224 0.6875\n",
      "130 0.7292512059211731 0.3125\n",
      "140 0.7157991528511047 0.5\n",
      "0.5208333333333334\n",
      "----------------------\n",
      "epoch: 15\n",
      "0 0.6523049473762512 0.4375\n",
      "10 0.6497272253036499 0.5625\n",
      "20 0.6362147331237793 0.6875\n",
      "30 0.5962041616439819 0.8125\n",
      "40 0.6416300535202026 0.8125\n",
      "50 0.7014131546020508 0.5\n",
      "60 0.6755598783493042 0.75\n",
      "70 0.6700364947319031 0.625\n",
      "80 0.6722835302352905 0.6875\n",
      "90 0.5999838709831238 0.75\n",
      "100 0.7352995276451111 0.5625\n",
      "110 0.7559613585472107 0.3125\n",
      "120 0.6164129376411438 0.6875\n",
      "130 0.6666648983955383 0.625\n",
      "140 0.6776748299598694 0.5625\n",
      "0.625\n",
      "----------------------\n",
      "epoch: 16\n",
      "0 0.6394435167312622 0.75\n",
      "10 0.6524365544319153 0.75\n",
      "20 0.6508886814117432 0.6875\n",
      "30 0.6648905873298645 0.6875\n",
      "40 0.6069707870483398 0.75\n",
      "50 0.6651318073272705 0.625\n",
      "60 0.6693012714385986 0.5625\n",
      "70 0.5685632824897766 0.875\n",
      "80 0.6880875825881958 0.625\n",
      "90 0.6640874147415161 0.625\n",
      "100 0.6522656083106995 0.6875\n",
      "110 0.7061906456947327 0.5\n",
      "120 0.6884251832962036 0.5625\n",
      "130 0.694081723690033 0.4375\n",
      "140 0.7340919375419617 0.375\n",
      "0.6333333333333333\n",
      "----------------------\n",
      "epoch: 17\n",
      "0 0.6728272438049316 0.6875\n",
      "10 0.6149414777755737 0.75\n",
      "20 0.6809220910072327 0.625\n",
      "30 0.6716822385787964 0.5625\n",
      "40 0.6788723468780518 0.625\n",
      "50 0.5635849237442017 0.9375\n",
      "60 0.6446552276611328 0.625\n",
      "70 0.6245861053466797 0.8125\n",
      "80 0.6289273500442505 0.6875\n",
      "90 0.6817191243171692 0.5\n",
      "100 0.7077181935310364 0.5625\n",
      "110 0.6263730525970459 0.6875\n",
      "120 0.5987226366996765 0.6875\n",
      "130 0.6603846549987793 0.5625\n",
      "140 0.6354910731315613 0.625\n",
      "0.6625\n",
      "----------------------\n",
      "epoch: 18\n",
      "0 0.6126004457473755 0.75\n",
      "10 0.6367055773735046 0.6875\n",
      "20 0.6347171068191528 0.75\n",
      "30 0.6473264098167419 0.5625\n",
      "40 0.6625422835350037 0.75\n",
      "50 0.630407989025116 0.75\n",
      "60 0.6728489398956299 0.5625\n",
      "70 0.6866986751556396 0.5625\n",
      "80 0.704708993434906 0.375\n",
      "90 0.6244152784347534 0.5625\n",
      "100 0.6779216527938843 0.4375\n",
      "110 0.645684003829956 0.6875\n",
      "120 0.6629081964492798 0.8125\n",
      "130 0.6147438287734985 0.6875\n",
      "140 0.634627103805542 0.5625\n",
      "0.6333333333333333\n",
      "----------------------\n",
      "epoch: 19\n",
      "0 0.6873432397842407 0.625\n",
      "10 0.6669694781303406 0.5625\n",
      "20 0.6777732968330383 0.5625\n",
      "30 0.6927515268325806 0.375\n",
      "40 0.6511500477790833 0.625\n",
      "50 0.6667686700820923 0.6875\n",
      "60 0.6716582179069519 0.625\n",
      "70 0.6636344194412231 0.75\n",
      "80 0.6267994046211243 0.6875\n",
      "90 0.7194879651069641 0.4375\n",
      "100 0.6479150652885437 0.6875\n",
      "110 0.6408025622367859 0.6875\n",
      "120 0.708281397819519 0.5\n",
      "130 0.6178443431854248 0.6875\n",
      "140 0.64982670545578 0.5625\n",
      "0.6041666666666666\n",
      "----------------------\n",
      "epoch: 20\n",
      "0 0.6854376196861267 0.75\n",
      "10 0.730705976486206 0.5\n",
      "20 0.6982861757278442 0.375\n",
      "30 0.6082534193992615 0.75\n",
      "40 0.634019136428833 0.625\n",
      "50 0.627353310585022 0.6875\n",
      "60 0.6909893155097961 0.4375\n",
      "70 0.6341719627380371 0.625\n",
      "80 0.6542325019836426 0.5625\n",
      "90 0.5754960775375366 0.875\n",
      "100 0.6418826580047607 0.5625\n",
      "110 0.6583823561668396 0.5625\n",
      "120 0.7030687928199768 0.5625\n",
      "130 0.6700112223625183 0.5625\n",
      "140 0.6679366827011108 0.625\n",
      "0.6041666666666666\n",
      "----------------------\n",
      "epoch: 21\n",
      "0 0.6286996006965637 0.625\n",
      "10 0.666836142539978 0.625\n",
      "20 0.6904133558273315 0.5\n",
      "30 0.6430650949478149 0.75\n",
      "40 0.6883634924888611 0.4375\n",
      "50 0.6319474577903748 0.75\n",
      "60 0.6565605401992798 0.5625\n",
      "70 0.6134943962097168 0.625\n",
      "80 0.6520004272460938 0.5625\n",
      "90 0.6437283754348755 0.625\n",
      "100 0.713365912437439 0.5625\n",
      "110 0.6604384183883667 0.5625\n",
      "120 0.6113016605377197 0.5625\n",
      "130 0.6347703337669373 0.5625\n",
      "140 0.6936299800872803 0.4375\n",
      "0.5833333333333334\n",
      "----------------------\n",
      "epoch: 22\n",
      "0 0.6066491603851318 0.75\n",
      "10 0.5904314517974854 0.8125\n",
      "20 0.6827990412712097 0.5625\n",
      "30 0.6839372515678406 0.5\n",
      "40 0.6788914799690247 0.75\n",
      "50 0.6990482211112976 0.5625\n",
      "60 0.613045334815979 0.8125\n",
      "70 0.6354420185089111 0.625\n",
      "80 0.6615197062492371 0.6875\n",
      "90 0.7046638131141663 0.375\n",
      "100 0.6534401774406433 0.6875\n",
      "110 0.6845786571502686 0.5625\n",
      "120 0.6517484188079834 0.6875\n",
      "130 0.6503862738609314 0.6875\n",
      "140 0.6486234068870544 0.75\n",
      "0.6541666666666667\n",
      "----------------------\n",
      "epoch: 23\n",
      "0 0.6459100842475891 0.6875\n",
      "10 0.6038811206817627 0.75\n",
      "20 0.6548725962638855 0.5\n",
      "30 0.6451855301856995 0.625\n",
      "40 0.6622707843780518 0.4375\n",
      "50 0.6094279289245605 0.8125\n",
      "60 0.6528302431106567 0.6875\n",
      "70 0.6594839096069336 0.5625\n",
      "80 0.6831156015396118 0.5625\n",
      "90 0.6225103139877319 0.75\n",
      "100 0.7443372011184692 0.3125\n",
      "110 0.6813753247261047 0.5\n",
      "120 0.6946914196014404 0.5\n",
      "130 0.6088164448738098 0.625\n",
      "140 0.7118214964866638 0.5\n",
      "0.5875\n",
      "----------------------\n",
      "epoch: 24\n",
      "0 0.6276247501373291 0.6875\n",
      "10 0.6039169430732727 0.75\n",
      "20 0.6208943128585815 0.6875\n",
      "30 0.7023070454597473 0.5\n",
      "40 0.684304416179657 0.5\n",
      "50 0.6651975512504578 0.5625\n",
      "60 0.658437967300415 0.5625\n",
      "70 0.6060071587562561 0.75\n",
      "80 0.700423002243042 0.3125\n",
      "90 0.6986303925514221 0.4375\n",
      "100 0.6698201298713684 0.6875\n",
      "110 0.7061233520507812 0.375\n",
      "120 0.7237281799316406 0.5\n",
      "130 0.618470311164856 0.6875\n",
      "140 0.6359848380088806 0.5625\n",
      "0.5708333333333333\n",
      "----------------------\n",
      "epoch: 25\n",
      "0 0.751022458076477 0.5625\n",
      "10 0.6089838147163391 0.8125\n",
      "20 0.652605414390564 0.5625\n",
      "30 0.64817214012146 0.8125\n",
      "40 0.6345869302749634 0.75\n",
      "50 0.6496516466140747 0.5\n",
      "60 0.674968421459198 0.6875\n",
      "70 0.525130033493042 1.0\n",
      "80 0.6465969681739807 0.6875\n",
      "90 0.5657581686973572 0.875\n",
      "100 0.6489777565002441 0.5625\n",
      "110 0.7246318459510803 0.5\n",
      "120 0.6425466537475586 0.625\n",
      "130 0.6886032223701477 0.5\n",
      "140 0.7263613343238831 0.375\n",
      "0.6541666666666667\n",
      "----------------------\n",
      "epoch: 26\n",
      "0 0.6946176290512085 0.4375\n",
      "10 0.6501110792160034 0.625\n",
      "20 0.6800856590270996 0.5\n",
      "30 0.616184651851654 0.6875\n",
      "40 0.6040729880332947 0.6875\n",
      "50 0.6735637187957764 0.6875\n",
      "60 0.7004139423370361 0.5\n",
      "70 0.6597728133201599 0.625\n",
      "80 0.6574071645736694 0.6875\n",
      "90 0.6678491234779358 0.4375\n",
      "100 0.6143708825111389 0.625\n",
      "110 0.6355838775634766 0.8125\n",
      "120 0.605330765247345 0.625\n",
      "130 0.671193540096283 0.625\n",
      "140 0.6208282113075256 0.625\n",
      "0.6125\n",
      "----------------------\n",
      "epoch: 27\n",
      "0 0.5900127291679382 0.6875\n",
      "10 0.6704957485198975 0.625\n",
      "20 0.6403638124465942 0.6875\n",
      "30 0.6707992553710938 0.5625\n",
      "40 0.587838351726532 0.75\n",
      "50 0.7232474088668823 0.5\n",
      "60 0.6345467567443848 0.625\n",
      "70 0.5819399952888489 0.75\n",
      "80 0.6506960391998291 0.5625\n",
      "90 0.7071221470832825 0.5625\n",
      "100 0.6710730195045471 0.6875\n",
      "110 0.6420342922210693 0.625\n",
      "120 0.6112499237060547 0.8125\n",
      "130 0.674042820930481 0.5\n",
      "140 0.6087791323661804 0.8125\n",
      "0.65\n",
      "----------------------\n",
      "epoch: 28\n",
      "0 0.6562075018882751 0.5625\n",
      "10 0.6300870776176453 0.6875\n",
      "20 0.6389068365097046 0.6875\n",
      "30 0.6453021764755249 0.6875\n",
      "40 0.7000861167907715 0.625\n",
      "50 0.5636920928955078 0.75\n",
      "60 0.6060540676116943 0.8125\n",
      "70 0.6442953944206238 0.5625\n",
      "80 0.597945511341095 0.75\n",
      "90 0.693061351776123 0.5\n",
      "100 0.662574052810669 0.5\n",
      "110 0.6807786226272583 0.5625\n",
      "120 0.664394736289978 0.5\n",
      "130 0.6545920968055725 0.8125\n",
      "140 0.6805537939071655 0.625\n",
      "0.6416666666666667\n",
      "----------------------\n",
      "epoch: 29\n",
      "0 0.6032558679580688 0.625\n",
      "10 0.6354485750198364 0.625\n",
      "20 0.6578739881515503 0.5625\n",
      "30 0.6836629509925842 0.5625\n",
      "40 0.6421810388565063 0.625\n",
      "50 0.6727343201637268 0.6875\n",
      "60 0.6902915239334106 0.5625\n",
      "70 0.6917020678520203 0.5\n",
      "80 0.6125543117523193 0.6875\n",
      "90 0.6877774000167847 0.625\n",
      "100 0.6530358791351318 0.5625\n",
      "110 0.7093677520751953 0.3125\n",
      "120 0.6071374416351318 0.6875\n",
      "130 0.6249877214431763 0.6875\n",
      "140 0.6504631638526917 0.6875\n",
      "0.6\n",
      "----------------------\n",
      "epoch: 30\n",
      "0 0.6272600889205933 0.625\n",
      "10 0.6998326778411865 0.5625\n",
      "20 0.6098086833953857 0.75\n",
      "30 0.6846608519554138 0.625\n",
      "40 0.6223660111427307 0.6875\n",
      "50 0.6947112083435059 0.4375\n",
      "60 0.6343199610710144 0.75\n",
      "70 0.6092386841773987 0.75\n",
      "80 0.6377160549163818 0.75\n",
      "90 0.6968005895614624 0.5625\n",
      "100 0.6404634118080139 0.5625\n",
      "110 0.7459888458251953 0.5\n",
      "120 0.6250734329223633 0.625\n",
      "130 0.6579431295394897 0.5625\n",
      "140 0.5945043563842773 0.75\n",
      "0.6333333333333333\n",
      "----------------------\n",
      "epoch: 31\n",
      "0 0.6855804920196533 0.6875\n",
      "10 0.6747550368309021 0.8125\n",
      "20 0.6919559836387634 0.5\n",
      "30 0.6814875602722168 0.5625\n",
      "40 0.5929500460624695 0.8125\n",
      "50 0.6531014442443848 0.625\n",
      "60 0.5759945511817932 0.6875\n",
      "70 0.6444506049156189 0.75\n",
      "80 0.6945610642433167 0.5\n",
      "90 0.5983253121376038 0.6875\n",
      "100 0.6410918235778809 0.8125\n",
      "110 0.6257574558258057 0.75\n",
      "120 0.629318118095398 0.625\n",
      "130 0.5884805917739868 0.8125\n",
      "140 0.6270318627357483 0.75\n",
      "0.6916666666666667\n",
      "----------------------\n",
      "epoch: 32\n",
      "0 0.6678350567817688 0.625\n",
      "10 0.65867018699646 0.5625\n",
      "20 0.6620697379112244 0.75\n",
      "30 0.6318955421447754 0.6875\n",
      "40 0.6534558534622192 0.5625\n",
      "50 0.6624284982681274 0.6875\n",
      "60 0.6675585508346558 0.5625\n",
      "70 0.642317533493042 0.5\n",
      "80 0.7235135436058044 0.375\n",
      "90 0.6383345127105713 0.625\n",
      "100 0.7168138027191162 0.4375\n",
      "110 0.6841881275177002 0.5625\n",
      "120 0.606975257396698 0.625\n",
      "130 0.6710523366928101 0.625\n",
      "140 0.6234342455863953 0.8125\n",
      "0.6\n",
      "----------------------\n",
      "epoch: 33\n",
      "0 0.6492235660552979 0.75\n",
      "10 0.48283565044403076 0.875\n",
      "20 0.6420837044715881 0.625\n",
      "30 0.5260489583015442 0.8125\n",
      "40 0.7028614282608032 0.375\n",
      "50 0.6459869742393494 0.625\n",
      "60 0.6807748079299927 0.4375\n",
      "70 0.6632390022277832 0.6875\n",
      "80 0.5863410830497742 0.6875\n",
      "90 0.644690990447998 0.75\n",
      "100 0.6046047806739807 0.875\n",
      "110 0.6119556427001953 0.75\n",
      "120 0.6044208407402039 0.75\n",
      "130 0.6389446258544922 0.6875\n",
      "140 0.5752412676811218 0.8125\n",
      "0.7\n",
      "----------------------\n",
      "epoch: 34\n",
      "0 0.6740243434906006 0.5625\n",
      "10 0.6687941551208496 0.75\n",
      "20 0.625758171081543 0.6875\n",
      "30 0.5678443312644958 0.875\n",
      "40 0.6353119611740112 0.625\n",
      "50 0.6203577518463135 0.75\n",
      "60 0.6820321083068848 0.625\n",
      "70 0.6129055023193359 0.625\n",
      "80 0.59942227602005 0.75\n",
      "90 0.6217998266220093 0.6875\n",
      "100 0.6148995757102966 0.75\n",
      "110 0.6649264693260193 0.5625\n",
      "120 0.6199493408203125 0.75\n",
      "130 0.6639959216117859 0.5625\n",
      "140 0.5893775820732117 0.6875\n",
      "0.6833333333333333\n",
      "----------------------\n",
      "epoch: 35\n",
      "0 0.6495640277862549 0.625\n",
      "10 0.6423675417900085 0.5625\n",
      "20 0.650178849697113 0.625\n",
      "30 0.5870355367660522 0.8125\n",
      "40 0.5749914050102234 0.8125\n",
      "50 0.5555688142776489 0.9375\n",
      "60 0.6817619800567627 0.625\n",
      "70 0.6531931161880493 0.625\n",
      "80 0.5848272442817688 0.6875\n",
      "90 0.5700268149375916 0.875\n",
      "100 0.5952149033546448 0.75\n",
      "110 0.627395749092102 0.75\n",
      "120 0.6280180811882019 0.6875\n",
      "130 0.5921968817710876 0.75\n",
      "140 0.5901570320129395 0.875\n",
      "0.7333333333333333\n",
      "----------------------\n",
      "epoch: 36\n",
      "0 0.6630933284759521 0.75\n",
      "10 0.6485825181007385 0.6875\n",
      "20 0.6691997051239014 0.5\n",
      "30 0.6959658861160278 0.5\n",
      "40 0.6466368436813354 0.75\n",
      "50 0.6389866471290588 0.625\n",
      "60 0.6502406597137451 0.4375\n",
      "70 0.6192210912704468 0.6875\n",
      "80 0.6701194643974304 0.625\n",
      "90 0.6514662504196167 0.625\n",
      "100 0.6926171779632568 0.625\n",
      "110 0.6803860664367676 0.75\n",
      "120 0.6006938219070435 0.75\n",
      "130 0.5943874716758728 0.8125\n",
      "140 0.5902697443962097 0.75\n",
      "0.6583333333333333\n",
      "----------------------\n",
      "epoch: 37\n",
      "0 0.6132142543792725 0.75\n",
      "10 0.6277428865432739 0.75\n",
      "20 0.5959560871124268 0.75\n",
      "30 0.6599580645561218 0.5625\n",
      "40 0.5613860487937927 0.8125\n",
      "50 0.6396682858467102 0.625\n",
      "60 0.6535996198654175 0.625\n",
      "70 0.612362802028656 0.8125\n",
      "80 0.6343896389007568 0.625\n",
      "90 0.5860381722450256 0.875\n",
      "100 0.591991662979126 0.625\n",
      "110 0.589179515838623 0.75\n",
      "120 0.6342712640762329 0.625\n",
      "130 0.6485078930854797 0.5625\n",
      "140 0.5901700854301453 0.8125\n",
      "0.7041666666666667\n",
      "----------------------\n",
      "epoch: 38\n",
      "0 0.5962506532669067 0.6875\n",
      "10 0.6375657916069031 0.625\n",
      "20 0.693819522857666 0.5625\n",
      "30 0.6223262548446655 0.5625\n",
      "40 0.5318902730941772 0.875\n",
      "50 0.6994321942329407 0.5\n",
      "60 0.6673007607460022 0.5625\n",
      "70 0.7246756553649902 0.375\n",
      "80 0.6398695111274719 0.5625\n",
      "90 0.5769243240356445 0.75\n",
      "100 0.6133058667182922 0.625\n",
      "110 0.6944059729576111 0.5\n",
      "120 0.6015397310256958 0.8125\n",
      "130 0.6661199331283569 0.5625\n",
      "140 0.6483129262924194 0.5625\n",
      "0.6083333333333333\n",
      "----------------------\n",
      "epoch: 39\n",
      "0 0.6804672479629517 0.625\n",
      "10 0.6531561613082886 0.6875\n",
      "20 0.5673447251319885 0.8125\n",
      "30 0.6269317269325256 0.625\n",
      "40 0.6314389705657959 0.75\n",
      "50 0.6822404861450195 0.5625\n",
      "60 0.6183931231498718 0.8125\n",
      "70 0.5882652997970581 0.75\n",
      "80 0.651251494884491 0.6875\n",
      "90 0.6183972358703613 0.8125\n",
      "100 0.6428607106208801 0.6875\n",
      "110 0.6708887815475464 0.625\n",
      "120 0.580722451210022 0.75\n",
      "130 0.6576517820358276 0.5625\n",
      "140 0.6277530789375305 0.8125\n",
      "0.7041666666666667\n",
      "----------------------\n",
      "epoch: 40\n",
      "0 0.6414154171943665 0.75\n",
      "10 0.6571139097213745 0.625\n",
      "20 0.6742235422134399 0.6875\n",
      "30 0.6226889491081238 0.6875\n",
      "40 0.681885838508606 0.5625\n",
      "50 0.6145281195640564 0.6875\n",
      "60 0.5944684147834778 0.8125\n",
      "70 0.6812824606895447 0.5\n",
      "80 0.5821828246116638 0.75\n",
      "90 0.6720594763755798 0.5625\n",
      "100 0.5819492340087891 0.75\n",
      "110 0.6719135046005249 0.75\n",
      "120 0.6335167288780212 0.75\n",
      "130 0.6523086428642273 0.6875\n",
      "140 0.6019942164421082 0.8125\n",
      "0.6916666666666667\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "#训练\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(40):\n",
    "    all_right = 0\n",
    "    all_label = 0\n",
    "    print(f\"epoch: {epoch+1}\")\n",
    "    for i, (input_ids, attention_mask, token_type_ids, code_len, keyword_num, permission,\n",
    "            labels) in enumerate(loader):\n",
    "        out = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    code_len=code_len,\n",
    "                    keyword_num=keyword_num,\n",
    "                    permission=permission)\n",
    "\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "\n",
    "            out = out.argmax(dim=1)\n",
    "            accuracy = (out == labels).sum().item() / len(labels)\n",
    "\n",
    "            all_right = all_right + (out == labels).sum().item()\n",
    "            all_label = all_label + len(labels)\n",
    "            print(i, loss.item(), accuracy)\n",
    "        if i == 1000:\n",
    "            break\n",
    "\n",
    "    print(all_right/all_label)\n",
    "    print(\"----------------------\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.52\n",
      "Precision: 0.5121951219512195\n",
      "Recall: 0.84\n",
      "F1-score: 0.6363636363636365\n",
      "AUC score: 0.52\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=dataset_validation,\n",
    "                                              batch_size=10,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids, code_len, keyword_num, permission,\n",
    "            labels) in enumerate(loader_test):\n",
    "\n",
    "        if i == 20:\n",
    "            break\n",
    "\n",
    "        print(i)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    code_len=code_len,\n",
    "                    keyword_num=keyword_num,\n",
    "                    permission=permission)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(out.cpu().numpy())\n",
    "\n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "    # print(y_true)\n",
    "    # print(y_pred)\n",
    "    accuracy = correct / total\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, pos_label=1, average='binary')\n",
    "    auc_score = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-score: {f1_score}\")\n",
    "    print(f\"AUC score: {auc_score}\")\n",
    "\n",
    "\n",
    "test()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
